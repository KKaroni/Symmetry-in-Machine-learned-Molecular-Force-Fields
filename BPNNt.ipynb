{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "energies_water = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_water),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_water = (xyz_file[:,1:-1])\n",
    "coordinates_water = coordinates_water.astype(np.float)\n",
    "atomic_numbers_water = (xyz_file[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers_water = atomic_numbers_water.astype(int)\n",
    "atomic_numbers_water = np.reshape(atomic_numbers_water,(len(coordinates_water),1))\n",
    "#atomic_numbers_water = torch.from_numpy(atomic_numbers_water)\n",
    "print(type(atomic_numbers_water))\n",
    "print(atomic_numbers_water)\n",
    "print(np.shape(atomic_numbers_water))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coordinates_water)\n",
    "print(np.shape(coordinates_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0                                     # i-th molecule\n",
    "N = 3                                     # N atoms per molecule\n",
    "coord = coordinates_water[N*i:N*(i+1),:]  # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                          # pairwise distances between all of its 3 atom\n",
    "    \n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "        for j in range(i+1,N):\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = 3\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lambdaa = 1     \n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 \n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_ik = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_ik)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Rotation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix():\n",
    "    theta = np.arccos(2*np.random.uniform(low = 0,high = 1)-1)\n",
    "    phi = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    u = np.array([np.sin(theta)*np.cos(phi),np.sin(theta)*np.sin(phi),np.cos(theta)])\n",
    "    theta = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    A = np.zeros((3,3))\n",
    "    A[0][0] = np.cos(theta) + (u[0]**2)*(1-np.cos(theta))\n",
    "    A[0][1] = u[0]*u[1]*(1-np.cos(theta)) - u[2]*np.sin(theta)\n",
    "    A[0][2] = u[0]*u[2]*(1-np.cos(theta)) + u[1]*np.sin(theta)\n",
    "    A[1][0] = u[1]*u[0]*(1-np.cos(theta)) + u[2]*np.sin(theta)\n",
    "    A[1][1] = np.cos(theta) + (u[1]**2)*(1-np.cos(theta))\n",
    "    A[1][2] = u[1]*u[2]*(1-np.cos(theta)) - u[0]*np.sin(theta)\n",
    "    A[2][0] = u[2]*u[0]*(1-np.cos(theta)) - u[1]*np.sin(theta)\n",
    "    A[2][1] = u[2]*u[1]*(1-np.cos(theta)) + u[0]*np.sin(theta)\n",
    "    A[2][2] = np.cos(theta) + (u[2]**2)*(1-np.cos(theta))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly rotate molecules in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(A,data):\n",
    "    data = np.array(data)\n",
    "    m = np.shape(data)[1]\n",
    "    for i in range(m):\n",
    "        data[:,i] = np.matmul(A,data[:,i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_H2O                    = 3       # number of atoms per molecule\n",
    "number_of_features_H2O   = 15       # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "data_size_H2O            = np.shape(energies_water)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(N,number_of_features,coordinates,energies,atomic_numbers, use_atom_num_as_feat,batches):\n",
    "    \n",
    "\n",
    "    # Randomly picking the parameters from within a range\n",
    "    heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "    random.shuffle(heta)\n",
    "\n",
    "    Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "    random.shuffle(Rs)\n",
    "\n",
    "    lambdaa = np.ones(number_of_features)\n",
    "    random.shuffle(lambdaa)\n",
    "\n",
    "    zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "    random.shuffle(zeta)\n",
    "\n",
    "\n",
    "    \n",
    "# #     if number_of_features == 6:\n",
    "\n",
    "# or set hand-picked ones\n",
    "    heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808, 1, 2, 0.1 , 0.05,1.2, 0.1, 0.3, 0.5,0.01] \n",
    "    zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. , 9.,  1.9, 4.2, 5.8, 6.4, 0.001, 0.3, 2, 0.6 ]\n",
    "    Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6,   0.8, 0.4, 0.2, 1.,  0.,  0.6,  1, 0.1, 1.2]\n",
    "    lambdaa = [1., 1., 1., 1., 1., 1.,1., 1., 1., 1., 1., 1., 1,1,1]\n",
    "        \n",
    "\n",
    "\n",
    "    data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "    test_set_size        = 100\n",
    "    training_set_size    = data_size - test_set_size\n",
    "\n",
    "    \n",
    "    #Randomly rotate each molecule in the data\n",
    "    rotated_molec_coord = np.zeros((np.shape(coordinates)))\n",
    "    for i in range(data_size):\n",
    "        coord = coordinates_water[N*i:N*(i+1),:]\n",
    "        coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "        coord = np.transpose(coord)\n",
    "        A = random_rotation_matrix()\n",
    "        rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "    \n",
    "    coordinates = np.vstack((coordinates,rotated_molec_coord))  # randomly rotate trainig and test set and compute features for them as well for later use\n",
    "    \n",
    "    \n",
    "    G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "    for i in range(2*data_size):                # factor 2 because we copied the data and randomly translated it\n",
    "        coord = coordinates[N*i:N*(i+1),:]\n",
    "        Dp    = pairwise_distances(coord)\n",
    "        for j in range(0,number_of_features,2):\n",
    "            if j < number_of_features - 1:      # for j = number_of_features compute either 1 or 2 symmetry functions depending on whether number_of_features is odd or even\n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "            else:                  \n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                if number_of_features % 2 == 0:    #i.e. if number of features is even\n",
    "                    G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "\n",
    "    \n",
    "    # Computing variance and mean on the training data only!\n",
    "    G_train = G[:training_set_size,:]\n",
    "    var  = np.std(G_train,axis=0)\n",
    "    mean = np.mean(G_train,axis=0)\n",
    "    print(mean)\n",
    "    G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "    print(np.shape(G))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(G)[0]):\n",
    "        for j in range(np.shape(G)[1]):\n",
    "            G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "        \n",
    "    print(type(G_norm))  \n",
    "    \n",
    "    if use_atom_num_as_feat == 1:\n",
    "        G_norm = np.append(G_norm, atomic_numbers, axis=1)    # Adding atomic number as a feature\n",
    "       \n",
    "        \n",
    "    data_set = np.vsplit(G_norm,data_size*2)       # Group every three atoms that constitute a molecule together\n",
    "    #data_set = np.random.permutation(training_set)\n",
    "    data_set = torch.FloatTensor(data_set)                    # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "   \n",
    "    # print(data_set[0])\n",
    "    # print(data_set[0][1][1])\n",
    "\n",
    "    labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "    \n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "#     shuffler = np.random.permutation(len(labels))\n",
    "\n",
    "#     data_set = data_set[shuffler]\n",
    "\n",
    "#     labels = labels[shuffler]\n",
    "\n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Computing variance and mean on the training data only!\n",
    "    lab_train = labels[:training_set_size]\n",
    "    var_lab  = np.std(lab_train,axis=0)\n",
    "    mean_lab = np.mean(lab_train,axis=0)\n",
    "\n",
    "    labels_norm = np.zeros((np.shape(labels)))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(labels)[0]):\n",
    "        labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "    labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "    # Splitting the dataset into training and test set\n",
    "    training_set         = data_set[:training_set_size]\n",
    "    test_set             = data_set[training_set_size:data_size]\n",
    "    training_set_rot     = data_set[data_size:data_size+training_set_size]\n",
    "    test_set_rot         = data_set[data_size+training_set_size:]\n",
    "\n",
    "    train_labels         = labels_norm[:training_set_size]\n",
    "    train_labels         = torch.FloatTensor(train_labels)\n",
    "    test_labels          = labels_norm[training_set_size:]\n",
    "    test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = TensorDataset(training_set, train_labels)\n",
    "    #print(dataset[0])\n",
    "\n",
    "    # Creating the batches\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batches, #25, #300,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "    print(np.shape(training_set))\n",
    "    \n",
    "    return [training_set, test_set, train_labels, test_labels, dataloader,var_lab,mean_lab, test_set_rot,labels_norm,training_set_rot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "training_set = data_water[0]\n",
    "print('Training set:')\n",
    "print(type(training_set))\n",
    "print(np.shape(training_set))\n",
    "\n",
    "test_set     = data_water[1]\n",
    "print('\\n')\n",
    "print('Test set:')\n",
    "print(type(test_set))\n",
    "print(np.shape(test_set))\n",
    "\n",
    "train_labels = data_water[2]\n",
    "print('\\n')\n",
    "print('Training labels:')\n",
    "print(type(train_labels))\n",
    "print(np.shape(train_labels))\n",
    "\n",
    "test_labels  = data_water[3]\n",
    "print('\\n')\n",
    "print('Test labels:')\n",
    "print(type(test_labels))\n",
    "print(np.shape(test_labels))\n",
    "\n",
    "dataloader   = data_water[4]\n",
    "print('\\n')\n",
    "print('data_waterloader:')\n",
    "print(type(dataloader))\n",
    "print(np.shape(dataloader))\n",
    "\n",
    "var_lab = data_water[5]\n",
    "print('\\n')\n",
    "print('Variance of labels:')\n",
    "print(type(var_lab))\n",
    "print(var_lab)\n",
    "\n",
    "mean_lab = data_water[6]\n",
    "print('\\n')\n",
    "print('Mean value of labels:')\n",
    "print(type(mean_lab))\n",
    "print(mean_lab)\n",
    "\n",
    "test_set_rot = data_water[7]\n",
    "print('\\n')\n",
    "print('Rotated test set:')\n",
    "print(type(test_set_rot))\n",
    "print(np.shape(test_set_rot))\n",
    "\n",
    "labels_norm = data_water[8]\n",
    "print('\\n')\n",
    "print('Normalised labels:')\n",
    "print(type(labels_norm))\n",
    "print(np.shape(labels_norm))\n",
    "\n",
    "training_set_rot = data_water[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int(math.sqrt(4))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets_H2O, self).__init__()\n",
    "        num_hid_feat = 10#math.ceil(number_of_features/2)#int(math.sqrt(number_of_features))##\n",
    "        self.fc1 = nn.Linear(number_of_features, num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat, num_hid_feat)\n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.relu(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN_H2O, self).__init__()\n",
    "        self.network1 = Subnets_H2O(number_of_features)\n",
    "        self.network2 = Subnets_H2O(number_of_features)\n",
    "        self.network3 = Subnets_H2O(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "# model = BPNN_H2O(3) #+1)                # +1 because he have added the atomic number as a feature\n",
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O)# +1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network1.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc3.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network2.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc3.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network3.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O+1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(learning_rate, nepochs,net,dataloader,test_set,test_labels):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), learning_rate) \n",
    "    #torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    train_loss = np.zeros(nepochs)\n",
    "    test_loss = np.zeros(nepochs)\n",
    "\n",
    "    train_acc = np.zeros(nepochs)\n",
    "    test_acc = np.zeros(nepochs)\n",
    "\n",
    "    #===========================================================================\n",
    "    for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "    #===========================================================================\n",
    "    \n",
    "        running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "        epoch_train_loss = 0.0             \n",
    "        epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "        counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                                  # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                                  # be 36 elements in each batch.\n",
    "        #---------------------------------------\n",
    "        for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "        #---------------------------------------\n",
    "            inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                                  # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                                  # after each .step() call. \n",
    "\n",
    "            # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "            outputs = torch.zeros(np.shape(inputs)[0])\n",
    "            for j in range(np.shape(inputs)[0]):\n",
    "                outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "                # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "                # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "                # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "            loss.backward()                   # performs one back-propagation step \n",
    "            optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean loss per training example\n",
    "                                               # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "            epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                               # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                               # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "            net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "            with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                              # the requires_grad flags to false, i.e. makes all the \n",
    "                                                              # operations in the block have no gradients\n",
    "                for k in range(np.shape(test_set)[0]):\n",
    "                       net_test_set[k] = net(test_set[k][0],test_set[k][1],test_set[k][2])\n",
    "                epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch           \n",
    "            if i % 10 == 2:    # print average loss every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "            counter += 1\n",
    "            #------------------------------------       \n",
    "        # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "        train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                       # to obtain average training loss for each epoch\n",
    "        test_loss[epoch] = epoch_test_loss/counter\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_test_loss = 0.0\n",
    "\n",
    "    #=================================================================================\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_for_different_num_feat(num_feat_H2O,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat):\n",
    "    N_H2O           = 3            # number of atoms per molecule\n",
    "    learning_rate   = 0.005#0.0001 #0.005\n",
    "    batch_size      = 25#300    # 25\n",
    "\n",
    "    data_size_H2O   = np.shape(energies_water)[0]\n",
    "    test_set_size   = 100\n",
    "    \n",
    "    data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "    \n",
    "    training_set = data_water[0];     test_set     = data_water[1];\n",
    "    train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "    dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "    mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "    labels_norm = data_water[8];      training_set_rot = data_water[9]\n",
    "        \n",
    "    # Create and initialise the model\n",
    "    net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature\n",
    "\n",
    "    # Train for nepochs using learning_rate = 0.0001\n",
    "\n",
    "    print(type(dataloader))\n",
    "    losses = training(learning_rate , nepochs, net, dataloader,test_set,test_labels)\n",
    "    train_loss_G_feat = losses[0]\n",
    "    test_loss_G_feat  = losses[1]\n",
    "    \n",
    "    return train_loss_G_feat, test_loss_G_feat, test_set, net, train_labels,mean_lab, test_labels, var_lab, test_set_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 4 Behler and Parinello features, batch size = 300 \n",
    "number_of_features_H2O   = 4       # number of features (radial and angular symmetry functions) to be used\n",
    "use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "nepochs                  = 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_G4 =  run_for_different_num_feat(4,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_4_feat = losses_G4[0]\n",
    "# test_loss_G_4_feat  = losses_G4[1]\n",
    "# test_set            = losses_G4[2]\n",
    "# net                 = losses_G4[3]\n",
    "# train_labels        = losses_G4[4]\n",
    "# mean_lab            = losses_G4[5]\n",
    "# test_labels         = losses_G4[6]\n",
    "# var_lab             = losses_G4[7]\n",
    "# test_set_rot        = losses_G4[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss functions\n",
    "def plot_train_test_loss(train_loss, test_loss, nepochs, num_feat_G):\n",
    "    x = np.arange(1,nepochs+1)\n",
    "\n",
    "    plt.plot(x,train_loss,'blue',label = 'Training loss')\n",
    "    plt.plot(x,test_loss,'red',label = 'Test loss')\n",
    "#    plt.yscale('log')\n",
    "\n",
    "    #plt.ylim([0,0.075])\n",
    "\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.xlabel('Epoch',fontsize=14)\n",
    "    plt.ylabel('Loss function',fontsize=14)\n",
    "    plt.title('Average loss function per epoch for $H_2O$ training and test set',fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('params_loss_graph_H2O_G_{0}'.format(num_feat_G),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_train_test_loss(train_loss_G_4_feat,test_loss_G_4_feat,nepochs,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(train_labels))\n",
    "# train_labels = np.array(train_labels)\n",
    "# print(np.mean(train_labels,axis=0))\n",
    "\n",
    "# print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_prediction(prediction,test_labels,var_lab,mean_lab, number_of_features_H2O):\n",
    "    prediction = torch.tensor(prediction)\n",
    "\n",
    "    x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "    print(min(torch.cat((test_labels,prediction),0)))\n",
    "    y = x\n",
    "    plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "    plt.plot(x,y, color='red',label = 'y=x')\n",
    "    plt.grid()\n",
    "    #plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "    #plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "    #plt.ylim([-13822,-13800])\n",
    "    #plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    #plt.tick_params(axis='both',labelsize=14)\n",
    "    plt.xlabel('Actual energy (kcal/mol)',fontsize=14)\n",
    "    plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "    plt.title('Actual vs Predicted $H_2O$ energy',fontsize=15)\n",
    "#    plt.title('Actual vs predicted $H_2O$ energy - Augmented data (10 rotations)',fontsize=15)    \n",
    "    plt.legend()\n",
    "    plt.savefig('predicted_energies_H2O_G_{0}'.format(number_of_features_H2O),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(net.network1.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 6 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 6       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G6 =  run_for_different_num_feat(6,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_6_feat = vars_G6[0]\n",
    "# test_loss_G_6_feat  = vars_G6[1]\n",
    "# test_set            = vars_G6[2]\n",
    "# net                 = vars_G6[3]\n",
    "# train_labels        = vars_G6[4]\n",
    "# mean_lab            = vars_G6[5]\n",
    "# test_labels         = vars_G6[6]\n",
    "# var_lab             = vars_G6[7]\n",
    "# test_set_rot        = vars_G6[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_train_test_loss(train_loss_G_6_feat,test_loss_G_6_feat,nepochs,number_of_features_H2O)\n",
    "\n",
    "# test_set_size = 100\n",
    "\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for 9 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 9       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G9 =  run_for_different_num_feat(9,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_9_feat = vars_G9[0]\n",
    "# test_loss_G_9_feat  = vars_G9[1]\n",
    "# test_set            = vars_G9[2]\n",
    "# net                 = vars_G9[3]\n",
    "# train_labels        = vars_G9[4]\n",
    "# mean_lab            = vars_G9[5]\n",
    "# test_labels         = vars_G9[6]\n",
    "# var_lab             = vars_G9[7]\n",
    "# test_set_rot        = vars_G9[8]\n",
    "# plot_train_test_loss(train_loss_G_9_feat,test_loss_G_9_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 12 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 12       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G12 =  run_for_different_num_feat(12,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_12_feat = vars_G12[0]\n",
    "# test_loss_G_12_feat  = vars_G12[1]\n",
    "# test_set            = vars_G12[2]\n",
    "# net                 = vars_G12[3]\n",
    "# train_labels        = vars_G12[4]\n",
    "# mean_lab            = vars_G12[5]\n",
    "# test_labels         = vars_G12[6]\n",
    "# var_lab             = vars_G12[7]\n",
    "# test_set_rot        = vars_G12[8]\n",
    "# plot_train_test_loss(train_loss_G_12_feat,test_loss_G_12_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for 16 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 16       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G16 =  run_for_different_num_feat(16,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_16_feat = vars_G16[0]\n",
    "# test_loss_G_16_feat  = vars_G16[1]\n",
    "# test_set            = vars_G16[2]\n",
    "# net                 = vars_G16[3]\n",
    "# train_labels        = vars_G16[4]\n",
    "# mean_lab            = vars_G16[5]\n",
    "# test_labels         = vars_G16[6]\n",
    "# var_lab             = vars_G16[7]\n",
    "#test_set_rot         = vars_G16[8]\n",
    "\n",
    "# plot_train_test_loss(train_loss_G_16_feat,test_loss_G_16_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Comparing convergence plots for different numbers of features\n",
    "# x = np.arange(1,nepochs+1)\n",
    "\n",
    "# plt.plot(x[:2000],train_loss_G_4_feat[:2000],'blue',label = '4 features')\n",
    "# #plt.plot(x[:2000],train_loss_G_6_feat[:2000],'red',label = '6 features')\n",
    "# plt.plot(x[:2000],train_loss_G_9_feat[:2000],'green',label = '9 features')\n",
    "# #plt.plot(x[:2000],train_loss_G_12_feat[:2000],'orange',label = '12 features')\n",
    "# plt.plot(x[:2000],train_loss_G_16_feat[:2000],'cyan',label = '16 features')\n",
    "\n",
    "\n",
    "# #plt.ylim([0,0.075])\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# plt.xlabel('Epoch',fontsize=14)\n",
    "# plt.ylabel('Training loss function',fontsize=14)\n",
    "# plt.title('Convergence plot for different numbers of features',fontsize=14)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.savefig('loss_H2O_compar_diff_num_feat_BP_symm_random_feat',bbox_inches='tight')\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 6 Behler and Parinello features, batch size = 600 \n",
    "number_of_features_H2O   = 6       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "vars_G6 =  run_for_different_num_feat(6,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "train_loss_G_6_feat = vars_G6[0]\n",
    "test_loss_G_6_feat  = vars_G6[1]\n",
    "test_set            = vars_G6[2]\n",
    "net                 = vars_G6[3]\n",
    "train_labels        = vars_G6[4]\n",
    "mean_lab            = vars_G6[5]\n",
    "test_labels         = vars_G6[6]\n",
    "var_lab             = vars_G6[7]\n",
    "test_set_rot        = vars_G6[8]\n",
    "\n",
    "plot_train_test_loss(train_loss_G_6_feat,test_loss_G_6_feat,nepochs,number_of_features_H2O)\n",
    "test_set_size = 100\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotating test set molecules and checking performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(test_set_rot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rotated = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_rot[i]\n",
    "    prediction_rotated[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "\n",
    "prediction_rotated = torch.tensor(prediction_rotated)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_rotated),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set',markersize=8)\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_rotated*var_lab+mean_lab, '*', color='yellow', label = 'Rotated test set')\n",
    "\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for rotated $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('rotated_predicted_energies_H2O_G_feat',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N                        = 3         # number of atoms per molecule\n",
    "number_of_features_xyz   = 3         # number of features for each atom\n",
    "data_size_H2O            = np.shape(energies_water)[0]                                  \n",
    "training_set_size    = data_size_H2O - test_set_size\n",
    "\n",
    "\n",
    "for i in range(np.shape(energies_water)[0]):  # Moving all oxygens to the origin!!!!!\n",
    "    coord = coordinates_water[N*i:N*(i+1),:]\n",
    "    coord = coord - coord[2,:]                \n",
    "    coordinates_water[N*i:N*(i+1),:] = coord\n",
    "\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates_water[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(coordinates_water), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates_water)[0]):\n",
    "    for j in range(1,np.shape(coordinates_water)[1]):  # omit first column since for our dataset x=0 always\n",
    "        coordinates_water_norm[i,j] = (coordinates_water[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "data_set_xyz = np.vsplit(coordinates_water_norm,data_size_H2O)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "#################################################################################\n",
    "#Randomly rotating each molecule in the dataset around its oxygen \n",
    "rotated_training_set_xyz= np.zeros((np.shape(training_set_xyz)))\n",
    "for i in range(training_set_size):\n",
    "    coord = training_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_training_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_training_set_xyz))\n",
    "rotated_training_set_xyz = torch.FloatTensor(rotated_training_set_xyz)\n",
    "training_set_xyz = rotated_training_set_xyz\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=300, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training new neural net on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xyz = BPNN_H2O(3)    # 3 features (x,y and z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_xyz.parameters(), lr=0.005) #0.0001)#0.005)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20000\n",
    "\n",
    "train_loss_xyz = np.zeros(nepochs)\n",
    "test_loss_xyz = np.zeros(nepochs)\n",
    "\n",
    "train_acc_xyz = np.zeros(nepochs)\n",
    "test_acc_xyz = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss_xyz = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss_xyz = 0.0             \n",
    "    epoch_test_loss_xyz = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net_xyz(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "                       \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_xyz += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss_xyz += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set_xyz = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                    net_test_set_xyz[k] = net_xyz(test_set_xyz[k][0],test_set_xyz[k][1],test_set_xyz[k][2])\n",
    "            epoch_test_loss_xyz += criterion(net_test_set_xyz, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.10f' %\n",
    "                  (epoch + 1, i + 1, running_loss_xyz/10))\n",
    "            running_loss_xyz = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss_xyz[epoch] = epoch_train_loss_xyz/counter   # divide by number or training examples in one batch \n",
    "                                                           # to obtain average training loss for each epoch\n",
    "#     if (abs((test_loss_xyz[epoch] - test_loss_xyz[epoch - 1])/test_loss_xyz[epoch - 1])< 0.1):\n",
    "#         break\n",
    "    test_loss_xyz[epoch] = epoch_test_loss_xyz/counter\n",
    "    epoch_train_loss_xyz = 0.0\n",
    "    epoch_test_loss_xyz = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss_xyz[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss_xyz[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "#plt.ylim([-13822,-13800])\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch (training with (x,y,z))',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_training_x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(1,nepochs+1)\n",
    "# # plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# # plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "# plt.plot(x[:1500],train_loss_xyz[:1500],'green',label = 'xyz features')\n",
    "# plt.plot(x[:1500],train_loss_G_3_feat[:1500],'y',label = '3 Symmetry functions')\n",
    "\n",
    "# #plt.ylim([-13822,-13800])\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# plt.xlabel('Epoch',fontsize=14)\n",
    "# plt.ylabel('Loss function',fontsize=14)\n",
    "# plt.title('Convergence: Training with (x,y,z) vs with 3 symmetry functions)',fontsize=14)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.savefig('x_y_z_loss_graph_H2O_compared_with_3G',bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xyz = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_xyz[i]\n",
    "    prediction_xyz[i] = net_xyz(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, using (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_train_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prediction_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_test_set_xyz= np.zeros((np.shape(test_set_xyz)))\n",
    "for i in range(test_set_size):\n",
    "    coord = test_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_test_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_test_set_xyz))\n",
    "rotated_test_set_xyz = torch.FloatTensor(rotated_test_set_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xyz_rot = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = rotated_test_set_xyz[i]\n",
    "    prediction_xyz_rot[i] = net_xyz(x1, x2, x3)#[0]\n",
    "print(prediction_xyz*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_xyz_rot = torch.tensor(prediction_xyz_rot)\n",
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz_rot),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz_rot*var_lab+mean_lab, '*', color='orange', label = 'Rotated test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy(kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, with (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1> Augmenting the dataset and training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N                        = 3         # number of atoms per molecule\n",
    "number_of_features_xyz   = 3         # number of features for each atom\n",
    "data_size_H2O            = np.shape(energies_water)[0]  \n",
    "print('data_size_H2O = ', data_size_H2O)\n",
    "number_of_rotations = 101\n",
    "\n",
    "coordinates_water = (xyz_file[:,1:-1])\n",
    "shape_coord_h2o = np.shape(coordinates_water)\n",
    "coordinates_water = coordinates_water.astype(np.float)\n",
    "new_labels        = labels_norm #torch.zeros((data_size_H2O*(number_of_rotations+1)))###### labels_norm\n",
    "\n",
    "    \n",
    "##########################################################################################################################################\n",
    "# for n in range(number_of_rotations):    \n",
    "#     #Randomly rotate each molecule in the data\n",
    "# #    print(np.shape(coordinates_water))\n",
    "#     rotated_molec_coord = np.zeros(shape_coord_h2o)\n",
    "#     for i in range(data_size_H2O):\n",
    "#         coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#         coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "#         coord = np.transpose(coord)\n",
    "#         A = random_rotation_matrix()\n",
    "#         rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "#     coordinates_water = np.vstack((coordinates_water,rotated_molec_coord))  # randomly rotate trainig and test set and compute features for them as well for later use\n",
    "#     new_labels         = torch.cat((new_labels,labels_norm))   \n",
    "##########################################################################################################################################\n",
    "\n",
    "final_coords = np.zeros(((number_of_rotations)*len(coordinates_water),3))\n",
    "\n",
    "for j in range(number_of_rotations):\n",
    "    #Randomly rotate each molecule in the data\n",
    "    rotated_molec_coord = np.zeros((np.shape(coordinates_water)))\n",
    "    for i in range(data_size_H2O):\n",
    "        coord = coordinates_water[N*i:N*(i+1),:]\n",
    "        coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "        coord = np.transpose(coord)\n",
    "        A = random_rotation_matrix()\n",
    "        rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "    final_coords[len(coordinates_water)*j:len(coordinates_water)*(j+1),:] = rotated_molec_coord\n",
    "#    print(rotated_molec_coord)\n",
    "\n",
    "for j in range(number_of_rotations-1):\n",
    "    new_labels     = torch.cat((new_labels,labels_norm))\n",
    "\n",
    "print(np.shape(new_labels))\n",
    "print(np.shape(final_coords))\n",
    "##########################################################################################################################################\n",
    "# for i in range(np.shape(energies_water)[0]):  # Moving all oxygens to the origin!!!!!\n",
    "#     coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#     coord = coord - coord[2,:]                \n",
    "#     coordinates_water[N*i:N*(i+1),:] = coord\n",
    "# final_coords = np.zeros(((number_of_rotations+1)*len(coordinates_water),3))\n",
    "#     #Randomly rotate each molecule in the data\n",
    "# #    print(np.shape(coordinates_water))\n",
    "# for i in range(data_size_H2O):\n",
    "#     coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#     coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "#     final_coords[i*(number_of_rotations+1)*N:i*(number_of_rotations+1)*N+N,:] = coord\n",
    "\n",
    "#     coord = np.transpose(coord)\n",
    "#     for n in range(number_of_rotations):    \n",
    "#         A = random_rotation_matrix()\n",
    "#         final_coords[i*(number_of_rotations+1)*N+(n+1)*N:i*(number_of_rotations+1)*N+(n+1)*N+N,:] = np.transpose(rotate_data(A,coord))\n",
    "#         print(i*(number_of_rotations+1)*N+(n+1)*N+N)\n",
    "#     new_labels[i*(number_of_rotations+1):(i+1)*(number_of_rotations+1)] = labels_norm[i]\n",
    "\n",
    "# # print(np.shape(final_coords))        \n",
    "# # print(np.shape(new_labels))        \n",
    "# #for n in range(number_of_rotations):    \n",
    "# #    new_labels         = torch.cat((new_labels,labels_norm))  \n",
    "##########################################################################################################################################\n",
    "\n",
    "#new_labels = torch.reshape(new_labels, (5000, 1))    \n",
    "\n",
    "\n",
    "new_data_size            = len(new_labels)\n",
    "test_set_size            = 2000\n",
    "training_set_size        = new_data_size - test_set_size\n",
    "\n",
    "# Computing standard deviation and mean on the training data only!\n",
    "coord_train = final_coords[:training_set_size,:]          ###########\n",
    "std_train_xyz  = np.std(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(final_coords), number_of_features_xyz)) ###\n",
    "# normalize all data (training and test), using training set mean and standard deviation\n",
    "for i in range(np.shape(final_coords)[0]):###\n",
    "    for j in range(1,np.shape(final_coords)[1]):  # omit first column since for our dataset x=0 always######\n",
    "        coordinates_water_norm[i,j] = (final_coords[i,j]-mean_train_xyz[j])/std_train_xyz[j]####\n",
    "\n",
    "\n",
    "print(new_data_size)\n",
    "data_set_xyz = np.vsplit(final_coords,new_data_size)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "# print(np.shape(data_set_xyz))\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# #print(np.shape(labels))\n",
    "# print(np.shape(data_set_xyz))\n",
    "shuffler = np.random.permutation(len(new_labels))\n",
    "\n",
    "data_set_xyz = data_set_xyz[shuffler]\n",
    "\n",
    "new_labels = new_labels[shuffler]\n",
    "\n",
    "# #print(np.shape(labels))\n",
    "# #print(np.shape(data_set))\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = new_labels[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = new_labels[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=128*10,#1000, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(final_coords))\n",
    "print(np.shape(new_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_labels[0:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xyz = BPNN_H2O(3)   #+1) # +1 if you are adding atomic number as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_xyz.parameters(), lr=0.01)#0.01) #0.0001)#0.005)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 1000\n",
    "\n",
    "train_loss_xyz = np.zeros(nepochs)\n",
    "test_loss_xyz = np.zeros(nepochs)\n",
    "\n",
    "train_acc_xyz = np.zeros(nepochs)\n",
    "test_acc_xyz = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss_xyz = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss_xyz = 0.0             \n",
    "    epoch_test_loss_xyz = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net_xyz(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "                       \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_xyz += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss_xyz += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set_xyz = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                    net_test_set_xyz[k] = net_xyz(test_set_xyz[k][0],test_set_xyz[k][1],test_set_xyz[k][2])\n",
    "            epoch_test_loss_xyz += criterion(net_test_set_xyz, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.10f' %\n",
    "                  (epoch + 1, i + 1, running_loss_xyz/10))\n",
    "            running_loss_xyz = 0.0\n",
    "        counter += 1\n",
    "        \n",
    "        \n",
    "    prediction = np.zeros(test_set_size)\n",
    "    for i in range(test_set_size):\n",
    "        x1,x2,x3 = test_set_xyz[i]\n",
    "        prediction[i] = net_xyz(x1, x2, x3)#[0]    \n",
    "    plot_prediction(prediction,test_labels,var_lab,mean_lab,3)\n",
    "        \n",
    "        \n",
    "#----------------------------------------------------------------------------------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss_xyz[epoch] = epoch_train_loss_xyz/counter   # divide by number or training examples in one batch \n",
    "                                                           # to obtain average training loss for each epoch\n",
    "#     if (abs((test_loss_xyz[epoch] - test_loss_xyz[epoch - 1])/test_loss_xyz[epoch - 1])< 0.1):\n",
    "#         break\n",
    "    test_loss_xyz[epoch] = epoch_test_loss_xyz/counter\n",
    "    epoch_train_loss_xyz = 0.0\n",
    "    epoch_test_loss_xyz = 0.0\n",
    "\n",
    "#===============================================================================================================\n",
    "    \n",
    "print('Finished Training')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_11 = np.hstack((tr1,train_loss_xyz))#[10:]))\n",
    "# test_11 = np.hstack((te1,test_loss_xyz))#[10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_101 = train_loss_xyz\n",
    "# test_101 = test_loss_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(1,len(train_loss_xyz)+1)\n",
    "# plt.plot(x,train_loss_xyz,'blue',label = 'Training loss')\n",
    "# plt.plot(x,test_loss_xyz,'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:],train_101[:],'green',label = 'train')\n",
    "plt.plot(x[:],test_101[:],'red',label = 'test')\n",
    "plt.yscale('log')\n",
    "\n",
    "#plt.ylim([-13822,-13800])\n",
    "\n",
    "#plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Augmented dataset convergence (100 extra rotations)',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('augmented_convergence_101',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr1 = train_loss_xyz\n",
    "# te1 = test_loss_xyz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_train_test_loss(train_loss_xyz_aug,test_loss_xyz_aug,nepochs,3)\n",
    "\n",
    "test_set_size = 2000\n",
    "\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_xyz[i]\n",
    "    prediction[i] = net_xyz(x1, x2, x3)#[0]    \n",
    "#print(prediction)\n",
    "\n",
    "rotated_test_set_aug= np.zeros((np.shape(test_set_xyz)))\n",
    "for i in range(test_set_size):\n",
    "    coord = test_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_test_set_aug[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "rotated_test_set_aug = torch.FloatTensor(rotated_test_set_aug)\n",
    "\n",
    "prediction_rot_aug = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = rotated_test_set_aug[i]\n",
    "    prediction_rot_aug[i] = net_xyz(x1, x2, x3)#[0]\n",
    "\n",
    "prediction_rot_aug = torch.tensor(prediction_rot_aug)\n",
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_rot_aug),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "#plt.plot(test_labels*var_lab+mean_lab,prediction_rot_aug*var_lab+mean_lab, '*', color='orange', label = 'Rotated test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy(kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction $H_2O$, augmented data (100 rotations)',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('predicted_energies_H2O_aug_101',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #plot_train_test_loss(train_loss_xyz_aug,test_loss_xyz_aug,nepochs,3)\n",
    "\n",
    "# test_set_size = 200\n",
    "# test_set_xyz_reduced = test_set_xyz[0:200]\n",
    "# test_labels_reduced = test_labels[0:200]\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set_xyz_reduced[i]\n",
    "#     prediction[i] = net_xyz(x1, x2, x3)#[0]    \n",
    "# #print(prediction)\n",
    "\n",
    "# rotated_test_set_aug = np.zeros((np.shape(test_set_xyz_reduced)))\n",
    "# for i in range(test_set_size):\n",
    "#     coord = test_set_xyz_reduced[N*i:N*(i+1),:]\n",
    "#     coord = np.transpose(coord)\n",
    "#     A = random_rotation_matrix()\n",
    "#     rotated_test_set_aug[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "# rotated_test_set_aug = torch.FloatTensor(rotated_test_set_aug)\n",
    "\n",
    "# prediction_rot_aug = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = rotated_test_set_aug[i]\n",
    "#     prediction_rot_aug[i] = net_xyz(x1, x2, x3)#[0]\n",
    "\n",
    "# prediction_rot_aug = torch.tensor(prediction_rot_aug)\n",
    "# prediction = torch.tensor(prediction)\n",
    "\n",
    "# x = np.linspace(min(test_labels_reduced*var_lab+mean_lab), max(test_labels_reduced*var_lab+mean_lab))\n",
    "# print(min(torch.cat((test_labels_reduced,prediction_rot_aug),0)))\n",
    "# y = x\n",
    "# plt.plot(test_labels_reduced*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "# plt.plot(test_labels_reduced*var_lab+mean_lab,prediction_rot_aug*var_lab+mean_lab, '*', color='orange', label = 'Rotated test set')\n",
    "# plt.plot(x,y, color='red',label = 'y=x')\n",
    "# plt.grid()\n",
    "# #plt.xlim([min(torch.cat((test_labels_reduced,prediction),0)), max(torch.cat((test_labels_reduced,prediction),0))])\n",
    "# #plt.ylim([min(torch.cat((test_labels_reduced,prediction),0)), max(torch.cat((test_labels_reduced,prediction)))])\n",
    "# #plt.ylim([-13822,-13800])\n",
    "# #plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# #plt.tick_params(axis='both',labelsize=14)\n",
    "# plt.xlabel('Actual $H_2O$ energy(kcal/mol)',fontsize=14)\n",
    "# plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "# plt.title('Energy prediction $H_2O$, augmented data (50 rotations)',fontsize=15)\n",
    "# plt.legend()\n",
    "# #plt.savefig('rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "# plt.savefig('predicted_energies_H2O_aug_51_rot',bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Redundant Internal Coordinates - Pairwise Distances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size            = np.shape(energies_water)[0]        # We have 1000 water molecule conformations\n",
    "test_set_size        = 100\n",
    "training_set_size    = data_size - test_set_size\n",
    "    \n",
    "number_of_features = 3\n",
    "G = np.zeros((len(coordinates_water), number_of_features)) \n",
    "\n",
    "for i in range(data_size):                \n",
    "    coord = coordinates_water[N*i:N*(i+1),:]\n",
    "    G[N*i:N*(i+1),:]   = pairwise_distances(coord)\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.std(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "print(mean)\n",
    "G_norm = np.zeros((len(coordinates_water), number_of_features))\n",
    "print(np.shape(G))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "        \n",
    "print(type(G_norm))  \n",
    "    \n",
    "if use_atom_num_as_feat == 1:\n",
    "    G_norm = np.append(G_norm, atomic_numbers, axis=1)    # Adding atomic number as a feature\n",
    "       \n",
    "        \n",
    "data_set = np.vsplit(G_norm,data_size)       # Group every three atoms that constitute a molecule together  !!!!!!! G_norm\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)                    # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "   \n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies_water          # turning energies_water into a (1000) tensor\n",
    "\n",
    "    \n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "#     shuffler = np.random.permutation(len(labels))\n",
    "\n",
    "#     data_set = data_set[shuffler]\n",
    "\n",
    "#     labels = labels[shuffler]\n",
    "\n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "    \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.std(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)   # !!!!!!!!!!labels_norm    \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:data_size]\n",
    "raining_set_rot     = data_set[data_size:data_size+training_set_size]\n",
    "test_set_rot         = data_set[data_size+training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25, #300,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = BPNN_H2O(3)   #+1) # +1 if you are adding atomic number as a feature\n",
    "nepochs= 120\n",
    "learning_rate = 0.005\n",
    "losses = training(learning_rate , nepochs, net, dataloader,test_set,test_labels)\n",
    "train_loss_G_ric1_feat = losses[0]\n",
    "test_loss_G_ric1_feat  = losses[1]\n",
    "\n",
    "plot_train_test_loss(train_loss_G_ric1_feat,test_loss_G_ric1_feat,nepochs,number_of_features_H2O)\n",
    "test_set_size = 100\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "\n",
    "plt.plot(x[:],train_loss_G_6_feat[:],'green',label = 'BP Symmetry functions')\n",
    "plt.plot(x[:],train_loss_G_ric1_feat[:],'red',label = 'Pairwise distances')\n",
    "plt.yscale('log')\n",
    "\n",
    "#plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Training Loss function',fontsize=14)\n",
    "plt.title('Convergence using BP symmetry functions vs Pairwise distances',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('bp_vs_pair_dist',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2> Redundant Internal Coordinates - Pairwise Distances and Angles**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size            = np.shape(energies_water)[0]        # We have 1000 water molecule conformations\n",
    "test_set_size        = 100\n",
    "training_set_size    = data_size - test_set_size\n",
    "    \n",
    "number_of_features = 3 # 3 distances and 1 angle\n",
    "\n",
    "G = np.zeros((len(coordinates_water), number_of_features)) \n",
    "costheta = np.zeros((len(coordinates_water),1))\n",
    "for i in range(data_size):                \n",
    "    coord = coordinates_water[N*i:N*(i+1),:]\n",
    "    G[N*i:N*(i+1),:]      = pairwise_distances(coord)\n",
    "    costhetaijk = np.zeros((3,1))\n",
    "    Dp = pairwise_distances(coord)\n",
    "\n",
    "    R_vec_1 = coord[0,:] - coord[1,:]\n",
    "    R_vec_2 = coord[0,:] - coord[2,:]\n",
    "    costhetaijk[0]  = np.dot(R_vec_1, R_vec_2)/(Dp[0][1]*Dp[0][2])\n",
    "    \n",
    "    R_vec_1 = coord[1,:] - coord[0,:]\n",
    "    R_vec_2 = coord[1,:] - coord[2,:]\n",
    "    costhetaijk[1]  = np.dot(R_vec_1, R_vec_2)/(Dp[1][0]*Dp[1][2])\n",
    "    \n",
    "    R_vec_1 = coord[2,:] - coord[0,:]\n",
    "    R_vec_2 = coord[2,:] - coord[1,:]\n",
    "    costhetaijk[2]  = np.dot(R_vec_1, R_vec_2)/(Dp[2][0]*Dp[2][1])\n",
    "    \n",
    "#    print(costhetaijk)\n",
    "\n",
    "    costheta[N*i:N*(i+1),:]  = costhetaijk\n",
    "\n",
    "G = np.append(G, costheta, axis=1)\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.std(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "print(mean)\n",
    "G_norm = np.zeros((len(coordinates_water), number_of_features+1))\n",
    "print(np.shape(G))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "        \n",
    "print(type(G_norm))  \n",
    "    \n",
    "if use_atom_num_as_feat == 1:\n",
    "    G_norm = np.append(G_norm, atomic_numbers, axis=1)    # Adding atomic number as a feature\n",
    "       \n",
    "        \n",
    "data_set = np.vsplit(G_norm,data_size)       # Group every three atoms that constitute a molecule together  !!!!!!! G_norm\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)                    # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "   \n",
    "\n",
    "labels = energies_water          # turning energies_water into a (1000) tensor    \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.std(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)   # !!!!!!!!!!labels_norm    \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:data_size]\n",
    "raining_set_rot     = data_set[data_size:data_size+training_set_size]\n",
    "test_set_rot         = data_set[data_size+training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25, #300,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = BPNN_H2O(4)   #+1) # +1 if you are adding atomic number as a feature\n",
    "nepochs= 120\n",
    "learning_rate = 0.005\n",
    "losses = training(learning_rate , nepochs, net, dataloader,test_set,test_labels)\n",
    "train_loss_G_ric2_feat = losses[0]\n",
    "test_loss_G_ric2_feat  = losses[1]\n",
    "\n",
    "plot_train_test_loss(train_loss_G_ric2_feat,test_loss_G_ric2_feat,nepochs,number_of_features_H2O)\n",
    "test_set_size = 100\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "\n",
    "plt.plot(x[:],train_loss_G_6_feat[:],'green',label = 'BP Symmetry functions')\n",
    "plt.plot(x[:],train_loss_G_ric1_feat[:],'red',label = 'Pairwise distances')\n",
    "plt.plot(x[:],train_loss_G_ric2_feat[:],'blue',label = 'Pairwise distances & Angles')\n",
    "\n",
    "plt.yscale('log')\n",
    "\n",
    "#plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Training Loss function',fontsize=14)\n",
    "plt.title('Training loss plots ',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('bp_vs_pair_dist',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Atomic local frames** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each atom will be represented by three features - its x,y and z coordinates with respect to the origin. For each water molecule, the origin is placed on the oxygen atom. The x-axis is set to coincide with the first OH bond, while the xy plane is the plane defined by the three atoms H-O-H. The z axis is set to be perpendicular to the H-O-H (xy) plane, so that the x,y and z axes form an orthogonal coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(coordinates_water))\n",
    "print(coordinates_water)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size            = np.shape(energies_water)[0]        # We have 1000 water molecule conformations\n",
    "test_set_size        = 100\n",
    "training_set_size    = data_size - test_set_size\n",
    "    \n",
    "number_of_features = 3    # 3 cartesian coordinates\n",
    "\n",
    "coord = coordinates_water[0]\n",
    "print(coord)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Predicting Hydrogen ($H_2$) Energies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_H2 = np.genfromtxt('./h2/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_H2),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('h2', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_H2 = (xyz_file[:,1:-1])\n",
    "coordinates_H2 = coordinates_H2.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers_H2 = (xyz_file[:,-1])\n",
    "\n",
    "atomic_numbers_H2 = atomic_numbers_H2.astype(int)\n",
    "atomic_numbers_H2 = np.reshape(atomic_numbers_H2,(len(coordinates_H2),1))\n",
    "#atomic_numbers = torch.from_numpy(atomic_numbers)\n",
    "print(type(atomic_numbers_H2))\n",
    "#print(atomic_numbers_H2)\n",
    "print(np.shape(atomic_numbers_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_H2                    = 2           # number of atoms per molecule\n",
    "# number_of_features_H2   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "#                                    # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_H2 = create_dataset(N_H2,number_of_features_H2,coordinates_H2,energies_H2, \\\n",
    "#                       atomic_numbers_H2,1, 200)\n",
    "\n",
    "# training_set_H2 = data_H2[0]\n",
    "# print('Training set:')\n",
    "# print(type(training_set_H2))\n",
    "# print(np.shape(training_set_H2))\n",
    "# test_set_H2     = data_H2[1]\n",
    "# print('\\n')\n",
    "# print('Test set:')\n",
    "# print(type(test_set_H2))\n",
    "# print(np.shape(test_set_H2))\n",
    "# train_labels_H2 = data_H2[2]\n",
    "# print('\\n')\n",
    "# print('Training labels:')\n",
    "# print(type(train_labels_H2))\n",
    "# print(np.shape(train_labels_H2))\n",
    "# test_labels_H2  = data_H2[3]\n",
    "# print('\\n')\n",
    "# print('Test labels:')\n",
    "# print(type(test_labels_H2))\n",
    "# print(np.shape(test_labels_H2))\n",
    "# dataloader_H2   = data_H2[4]\n",
    "# print('\\n')\n",
    "# print('data_H2loader:')\n",
    "# print(type(dataloader_H2))\n",
    "# print(np.shape(dataloader_H2))\n",
    "\n",
    "# var_lab_H2 = data_H2[5]\n",
    "# print('\\n')\n",
    "# print('Variance of labels:')\n",
    "# print(type(var_lab_H2))\n",
    "# print(var_lab_H2)\n",
    "\n",
    "# mean_lab_H2 = data_H2[6]\n",
    "# print('\\n')\n",
    "# print('Mean value of labels:')\n",
    "# print(type(mean_lab_H2))\n",
    "# print(mean_lab_H2)\n",
    "\n",
    "# test_set_rot_H2 = data_H2[7]\n",
    "# print('\\n')\n",
    "# print('Rotated test set:')\n",
    "# print(type(test_set_rot_H2))\n",
    "# print(np.shape(test_set_rot_H2))\n",
    "\n",
    "# labels_norm_H2 = data_H2[8]\n",
    "# print('\\n')\n",
    "# print('Normalised labels:')\n",
    "# print(type(labels_norm_H2))\n",
    "# print(np.shape(labels_norm_H2))\n",
    "\n",
    "# training_set_rot = data_H2[9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating features using only radial symmetry functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_H2                    = 2           # number of atoms per molecule\n",
    "number_of_features_H2   = 6           # number of features (symmetry functions) for each atom\n",
    "                                       \n",
    "    \n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features_H2)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features_H2)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features_H2)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features_H2)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "\n",
    "data_size_H2            = np.shape(energies_H2)[0]        # We have 500 H2 molecule conformations\n",
    "training_set_size_H2    = data_size_H2 - 50\n",
    "\n",
    "    \n",
    "G_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size_H2):\n",
    "    coord = coordinates_H2[N_H2*i:N_H2*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features_H2):\n",
    "        G_H2[N_H2*i:N_H2*(i+1),j]   = radial_BP_symm_func(Dp,N_H2,heta[j],Rs[j])     \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train_H2 = G_H2[:training_set_size_H2,:]\n",
    "var_train_H2  = np.var(G_train_H2,axis=0)\n",
    "mean_train_H2 = np.mean(G_train_H2,axis=0)\n",
    "\n",
    "G_norm_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G_H2)[0]):\n",
    "    for j in range(np.shape(G_H2)[1]):\n",
    "        G_norm_H2[i,j] = (G_H2[i,j]-mean_train_H2[j])/var_train_H2[j]   \n",
    "\n",
    "G_norm_H2 = np.append(G_norm_H2, atomic_numbers_H2, axis=1)\n",
    "        \n",
    "        \n",
    "data_set_H2 = np.vsplit(G_norm_H2,data_size_H2)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set_H2 = torch.FloatTensor(data_set_H2)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels_H2 = energies_H2          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train_H2 = labels_H2[:training_set_size_H2]\n",
    "var_lab_H2  = np.var(lab_train_H2,axis=0)\n",
    "mean_lab_H2 = np.mean(lab_train_H2,axis=0)\n",
    "print(mean_lab_H2)\n",
    "\n",
    "labels_norm_H2 = np.zeros((np.shape(labels_H2)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels_H2)[0]):\n",
    "    labels_norm_H2[i] = (labels_H2[i]-mean_lab_H2)/var_lab_H2  \n",
    "    \n",
    "    \n",
    "labels_norm_H2 = torch.FloatTensor(labels_norm_H2)      \n",
    "    \n",
    "    \n",
    "# # Splitting the dataset into training and test set\n",
    "# training_set_H2         = data_set_H2[:training_set_size_H2]\n",
    "# test_set_H2             = data_set_H2[training_set_size_H2:]\n",
    "\n",
    "# train_labels_H2         = labels_norm_H2[:training_set_size_H2]\n",
    "# test_labels_H2          = labels_norm_H2[training_set_size_H2:]\n",
    "\n",
    "# # Dataset\n",
    "# dataset_H2 = TensorDataset(training_set_H2, train_labels_H2)\n",
    "# #print(dataset[0])\n",
    "\n",
    "# # Creating the batches\n",
    "# dataloader_H2 = torch.utils.data.DataLoader(dataset_H2, batch_size=400,\n",
    "#                                            shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "# print(np.shape(training_set_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(G_norm[:100,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets_h2, self).__init__()\n",
    "        self.fc1 = nn.Linear(7,3)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(3,3)        \n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN_h2, self).__init__()\n",
    "        self.network1 = Subnets_h2()\n",
    "        self.network2 = Subnets_h2()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model_h2 = BPNN_h2()\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Copying weights!')\n",
    "#'Network1','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network1.fc1.weight = net.network1.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc1.bias = net.network1.fc1.bias\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network1.fc2.weight = net.network1.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc2.bias = net.network1.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network1.fc3.weight = net.network1.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc3.bias = net.network1.fc3.bias\n",
    "\n",
    "\n",
    "\n",
    "#'Network2','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network2.fc1.weight = net.network2.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc1.bias = net.network2.fc1.bias\n",
    "\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network2.fc2.weight = net.network2.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc2.bias = net.network2.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network2.fc3.weight = net.network2.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc3.bias = net.network2.fc3.bias\n",
    "\n",
    "\n",
    "print('Finished copying weights!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, x2 = data_set_H2[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "output = model_h2(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "print(output*var_lab_H2+mean_lab_H2)\n",
    "print(np.shape(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_H2 = torch.zeros(data_size_H2)\n",
    "for i in range(data_size_H2):\n",
    "    x1, x2 = data_set_H2[i]\n",
    "#     print('x1',x1)\n",
    "#     print('x2',x2)\n",
    "    with torch.no_grad():\n",
    "        prediction_H2[i] = model_h2(x1, x2)\n",
    "    print(prediction_H2[i]*var_lab_H2+mean_lab_H2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(prediction_H2*var_lab_H2+mean_lab_H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(labels_norm_H2))\n",
    "labels_norm_H2 = np.array(labels_norm_H2)\n",
    "print(np.mean(labels_norm_H2*var_lab_H2+mean_lab_H2,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_H2 = torch.tensor(prediction_H2)\n",
    "\n",
    "x = np.linspace(min(labels_H2), max(labels_H2))\n",
    "#print(min(torch.cat((test_labels_H2,prediction_H2),0)))\n",
    "y = x\n",
    "plt.plot(labels_H2,prediction_H2*var_lab_H2+mean_lab_H2, 'o', color='blue', label = 'test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2)))])\n",
    "#plt.ylim([-500,-700])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2_using_H2O_trained_net',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Drawing a neural network using matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            x = n*h_spacing + left\n",
    "            y = layer_top - m*v_spacing\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                            color='w', ec='k', zorder=2.7)\n",
    "            ax.add_artist(circle)\n",
    "            if n == 0:\n",
    "                lab   = \"$G_{0}$\".format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 1:\n",
    "                lab   = '$y^1_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 2:\n",
    "                lab   = '$y^2_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 3:\n",
    "                lab   = '$E_{tot}$'\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            \n",
    "            \n",
    "        # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                    [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .9, .1, .9, [1, 5, 5, 1])\n",
    "fig.savefig('fc_nn_gen.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
