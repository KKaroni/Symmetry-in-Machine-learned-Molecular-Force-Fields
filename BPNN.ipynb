{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (1000,) entries\n"
     ]
    }
   ],
   "source": [
    "energies = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c38e52de7b08>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #6 (got 1 columns instead of 5)\n",
      "    Line #11 (got 1 columns instead of 5)\n",
      "    Line #16 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #26 (got 1 columns instead of 5)\n",
      "    Line #31 (got 1 columns instead of 5)\n",
      "    Line #36 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #46 (got 1 columns instead of 5)\n",
      "    Line #51 (got 1 columns instead of 5)\n",
      "    Line #56 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #66 (got 1 columns instead of 5)\n",
      "    Line #71 (got 1 columns instead of 5)\n",
      "    Line #76 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #86 (got 1 columns instead of 5)\n",
      "    Line #91 (got 1 columns instead of 5)\n",
      "    Line #96 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #106 (got 1 columns instead of 5)\n",
      "    Line #111 (got 1 columns instead of 5)\n",
      "    Line #116 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #126 (got 1 columns instead of 5)\n",
      "    Line #131 (got 1 columns instead of 5)\n",
      "    Line #136 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #146 (got 1 columns instead of 5)\n",
      "    Line #151 (got 1 columns instead of 5)\n",
      "    Line #156 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #166 (got 1 columns instead of 5)\n",
      "    Line #171 (got 1 columns instead of 5)\n",
      "    Line #176 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #186 (got 1 columns instead of 5)\n",
      "    Line #191 (got 1 columns instead of 5)\n",
      "    Line #196 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #206 (got 1 columns instead of 5)\n",
      "    Line #211 (got 1 columns instead of 5)\n",
      "    Line #216 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #226 (got 1 columns instead of 5)\n",
      "    Line #231 (got 1 columns instead of 5)\n",
      "    Line #236 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #246 (got 1 columns instead of 5)\n",
      "    Line #251 (got 1 columns instead of 5)\n",
      "    Line #256 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #266 (got 1 columns instead of 5)\n",
      "    Line #271 (got 1 columns instead of 5)\n",
      "    Line #276 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #286 (got 1 columns instead of 5)\n",
      "    Line #291 (got 1 columns instead of 5)\n",
      "    Line #296 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #306 (got 1 columns instead of 5)\n",
      "    Line #311 (got 1 columns instead of 5)\n",
      "    Line #316 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #326 (got 1 columns instead of 5)\n",
      "    Line #331 (got 1 columns instead of 5)\n",
      "    Line #336 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #346 (got 1 columns instead of 5)\n",
      "    Line #351 (got 1 columns instead of 5)\n",
      "    Line #356 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #366 (got 1 columns instead of 5)\n",
      "    Line #371 (got 1 columns instead of 5)\n",
      "    Line #376 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #386 (got 1 columns instead of 5)\n",
      "    Line #391 (got 1 columns instead of 5)\n",
      "    Line #396 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #406 (got 1 columns instead of 5)\n",
      "    Line #411 (got 1 columns instead of 5)\n",
      "    Line #416 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #426 (got 1 columns instead of 5)\n",
      "    Line #431 (got 1 columns instead of 5)\n",
      "    Line #436 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #446 (got 1 columns instead of 5)\n",
      "    Line #451 (got 1 columns instead of 5)\n",
      "    Line #456 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #466 (got 1 columns instead of 5)\n",
      "    Line #471 (got 1 columns instead of 5)\n",
      "    Line #476 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #486 (got 1 columns instead of 5)\n",
      "    Line #491 (got 1 columns instead of 5)\n",
      "    Line #496 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #506 (got 1 columns instead of 5)\n",
      "    Line #511 (got 1 columns instead of 5)\n",
      "    Line #516 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #526 (got 1 columns instead of 5)\n",
      "    Line #531 (got 1 columns instead of 5)\n",
      "    Line #536 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #546 (got 1 columns instead of 5)\n",
      "    Line #551 (got 1 columns instead of 5)\n",
      "    Line #556 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #566 (got 1 columns instead of 5)\n",
      "    Line #571 (got 1 columns instead of 5)\n",
      "    Line #576 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #586 (got 1 columns instead of 5)\n",
      "    Line #591 (got 1 columns instead of 5)\n",
      "    Line #596 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #606 (got 1 columns instead of 5)\n",
      "    Line #611 (got 1 columns instead of 5)\n",
      "    Line #616 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #626 (got 1 columns instead of 5)\n",
      "    Line #631 (got 1 columns instead of 5)\n",
      "    Line #636 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #646 (got 1 columns instead of 5)\n",
      "    Line #651 (got 1 columns instead of 5)\n",
      "    Line #656 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #666 (got 1 columns instead of 5)\n",
      "    Line #671 (got 1 columns instead of 5)\n",
      "    Line #676 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #686 (got 1 columns instead of 5)\n",
      "    Line #691 (got 1 columns instead of 5)\n",
      "    Line #696 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #706 (got 1 columns instead of 5)\n",
      "    Line #711 (got 1 columns instead of 5)\n",
      "    Line #716 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #726 (got 1 columns instead of 5)\n",
      "    Line #731 (got 1 columns instead of 5)\n",
      "    Line #736 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #746 (got 1 columns instead of 5)\n",
      "    Line #751 (got 1 columns instead of 5)\n",
      "    Line #756 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #766 (got 1 columns instead of 5)\n",
      "    Line #771 (got 1 columns instead of 5)\n",
      "    Line #776 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #786 (got 1 columns instead of 5)\n",
      "    Line #791 (got 1 columns instead of 5)\n",
      "    Line #796 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #806 (got 1 columns instead of 5)\n",
      "    Line #811 (got 1 columns instead of 5)\n",
      "    Line #816 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #826 (got 1 columns instead of 5)\n",
      "    Line #831 (got 1 columns instead of 5)\n",
      "    Line #836 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #846 (got 1 columns instead of 5)\n",
      "    Line #851 (got 1 columns instead of 5)\n",
      "    Line #856 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #866 (got 1 columns instead of 5)\n",
      "    Line #871 (got 1 columns instead of 5)\n",
      "    Line #876 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #886 (got 1 columns instead of 5)\n",
      "    Line #891 (got 1 columns instead of 5)\n",
      "    Line #896 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #906 (got 1 columns instead of 5)\n",
      "    Line #911 (got 1 columns instead of 5)\n",
      "    Line #916 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #926 (got 1 columns instead of 5)\n",
      "    Line #931 (got 1 columns instead of 5)\n",
      "    Line #936 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #946 (got 1 columns instead of 5)\n",
      "    Line #951 (got 1 columns instead of 5)\n",
      "    Line #956 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #966 (got 1 columns instead of 5)\n",
      "    Line #971 (got 1 columns instead of 5)\n",
      "    Line #976 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #986 (got 1 columns instead of 5)\n",
      "    Line #991 (got 1 columns instead of 5)\n",
      "    Line #996 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1006 (got 1 columns instead of 5)\n",
      "    Line #1011 (got 1 columns instead of 5)\n",
      "    Line #1016 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1026 (got 1 columns instead of 5)\n",
      "    Line #1031 (got 1 columns instead of 5)\n",
      "    Line #1036 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1046 (got 1 columns instead of 5)\n",
      "    Line #1051 (got 1 columns instead of 5)\n",
      "    Line #1056 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1066 (got 1 columns instead of 5)\n",
      "    Line #1071 (got 1 columns instead of 5)\n",
      "    Line #1076 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1086 (got 1 columns instead of 5)\n",
      "    Line #1091 (got 1 columns instead of 5)\n",
      "    Line #1096 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1106 (got 1 columns instead of 5)\n",
      "    Line #1111 (got 1 columns instead of 5)\n",
      "    Line #1116 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1126 (got 1 columns instead of 5)\n",
      "    Line #1131 (got 1 columns instead of 5)\n",
      "    Line #1136 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1146 (got 1 columns instead of 5)\n",
      "    Line #1151 (got 1 columns instead of 5)\n",
      "    Line #1156 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1166 (got 1 columns instead of 5)\n",
      "    Line #1171 (got 1 columns instead of 5)\n",
      "    Line #1176 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1186 (got 1 columns instead of 5)\n",
      "    Line #1191 (got 1 columns instead of 5)\n",
      "    Line #1196 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1206 (got 1 columns instead of 5)\n",
      "    Line #1211 (got 1 columns instead of 5)\n",
      "    Line #1216 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1226 (got 1 columns instead of 5)\n",
      "    Line #1231 (got 1 columns instead of 5)\n",
      "    Line #1236 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1246 (got 1 columns instead of 5)\n",
      "    Line #1251 (got 1 columns instead of 5)\n",
      "    Line #1256 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1266 (got 1 columns instead of 5)\n",
      "    Line #1271 (got 1 columns instead of 5)\n",
      "    Line #1276 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1286 (got 1 columns instead of 5)\n",
      "    Line #1291 (got 1 columns instead of 5)\n",
      "    Line #1296 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1306 (got 1 columns instead of 5)\n",
      "    Line #1311 (got 1 columns instead of 5)\n",
      "    Line #1316 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1326 (got 1 columns instead of 5)\n",
      "    Line #1331 (got 1 columns instead of 5)\n",
      "    Line #1336 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1346 (got 1 columns instead of 5)\n",
      "    Line #1351 (got 1 columns instead of 5)\n",
      "    Line #1356 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1366 (got 1 columns instead of 5)\n",
      "    Line #1371 (got 1 columns instead of 5)\n",
      "    Line #1376 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1386 (got 1 columns instead of 5)\n",
      "    Line #1391 (got 1 columns instead of 5)\n",
      "    Line #1396 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1406 (got 1 columns instead of 5)\n",
      "    Line #1411 (got 1 columns instead of 5)\n",
      "    Line #1416 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1426 (got 1 columns instead of 5)\n",
      "    Line #1431 (got 1 columns instead of 5)\n",
      "    Line #1436 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1446 (got 1 columns instead of 5)\n",
      "    Line #1451 (got 1 columns instead of 5)\n",
      "    Line #1456 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1466 (got 1 columns instead of 5)\n",
      "    Line #1471 (got 1 columns instead of 5)\n",
      "    Line #1476 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1486 (got 1 columns instead of 5)\n",
      "    Line #1491 (got 1 columns instead of 5)\n",
      "    Line #1496 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1506 (got 1 columns instead of 5)\n",
      "    Line #1511 (got 1 columns instead of 5)\n",
      "    Line #1516 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1526 (got 1 columns instead of 5)\n",
      "    Line #1531 (got 1 columns instead of 5)\n",
      "    Line #1536 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1546 (got 1 columns instead of 5)\n",
      "    Line #1551 (got 1 columns instead of 5)\n",
      "    Line #1556 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1566 (got 1 columns instead of 5)\n",
      "    Line #1571 (got 1 columns instead of 5)\n",
      "    Line #1576 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1586 (got 1 columns instead of 5)\n",
      "    Line #1591 (got 1 columns instead of 5)\n",
      "    Line #1596 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1606 (got 1 columns instead of 5)\n",
      "    Line #1611 (got 1 columns instead of 5)\n",
      "    Line #1616 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1626 (got 1 columns instead of 5)\n",
      "    Line #1631 (got 1 columns instead of 5)\n",
      "    Line #1636 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1646 (got 1 columns instead of 5)\n",
      "    Line #1651 (got 1 columns instead of 5)\n",
      "    Line #1656 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1666 (got 1 columns instead of 5)\n",
      "    Line #1671 (got 1 columns instead of 5)\n",
      "    Line #1676 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1686 (got 1 columns instead of 5)\n",
      "    Line #1691 (got 1 columns instead of 5)\n",
      "    Line #1696 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1706 (got 1 columns instead of 5)\n",
      "    Line #1711 (got 1 columns instead of 5)\n",
      "    Line #1716 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1726 (got 1 columns instead of 5)\n",
      "    Line #1731 (got 1 columns instead of 5)\n",
      "    Line #1736 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1746 (got 1 columns instead of 5)\n",
      "    Line #1751 (got 1 columns instead of 5)\n",
      "    Line #1756 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1766 (got 1 columns instead of 5)\n",
      "    Line #1771 (got 1 columns instead of 5)\n",
      "    Line #1776 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1786 (got 1 columns instead of 5)\n",
      "    Line #1791 (got 1 columns instead of 5)\n",
      "    Line #1796 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1806 (got 1 columns instead of 5)\n",
      "    Line #1811 (got 1 columns instead of 5)\n",
      "    Line #1816 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1826 (got 1 columns instead of 5)\n",
      "    Line #1831 (got 1 columns instead of 5)\n",
      "    Line #1836 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1846 (got 1 columns instead of 5)\n",
      "    Line #1851 (got 1 columns instead of 5)\n",
      "    Line #1856 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1866 (got 1 columns instead of 5)\n",
      "    Line #1871 (got 1 columns instead of 5)\n",
      "    Line #1876 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1886 (got 1 columns instead of 5)\n",
      "    Line #1891 (got 1 columns instead of 5)\n",
      "    Line #1896 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1906 (got 1 columns instead of 5)\n",
      "    Line #1911 (got 1 columns instead of 5)\n",
      "    Line #1916 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1926 (got 1 columns instead of 5)\n",
      "    Line #1931 (got 1 columns instead of 5)\n",
      "    Line #1936 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1946 (got 1 columns instead of 5)\n",
      "    Line #1951 (got 1 columns instead of 5)\n",
      "    Line #1956 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1966 (got 1 columns instead of 5)\n",
      "    Line #1971 (got 1 columns instead of 5)\n",
      "    Line #1976 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1986 (got 1 columns instead of 5)\n",
      "    Line #1991 (got 1 columns instead of 5)\n",
      "    Line #1996 (got 1 columns instead of 5)\n",
      "    Line #2001 (got 1 columns instead of 5)\n",
      "    Line #2006 (got 1 columns instead of 5)\n",
      "    Line #2011 (got 1 columns instead of 5)\n",
      "    Line #2016 (got 1 columns instead of 5)\n",
      "    Line #2021 (got 1 columns instead of 5)\n",
      "    Line #2026 (got 1 columns instead of 5)\n",
      "    Line #2031 (got 1 columns instead of 5)\n",
      "    Line #2036 (got 1 columns instead of 5)\n",
      "    Line #2041 (got 1 columns instead of 5)\n",
      "    Line #2046 (got 1 columns instead of 5)\n",
      "    Line #2051 (got 1 columns instead of 5)\n",
      "    Line #2056 (got 1 columns instead of 5)\n",
      "    Line #2061 (got 1 columns instead of 5)\n",
      "    Line #2066 (got 1 columns instead of 5)\n",
      "    Line #2071 (got 1 columns instead of 5)\n",
      "    Line #2076 (got 1 columns instead of 5)\n",
      "    Line #2081 (got 1 columns instead of 5)\n",
      "    Line #2086 (got 1 columns instead of 5)\n",
      "    Line #2091 (got 1 columns instead of 5)\n",
      "    Line #2096 (got 1 columns instead of 5)\n",
      "    Line #2101 (got 1 columns instead of 5)\n",
      "    Line #2106 (got 1 columns instead of 5)\n",
      "    Line #2111 (got 1 columns instead of 5)\n",
      "    Line #2116 (got 1 columns instead of 5)\n",
      "    Line #2121 (got 1 columns instead of 5)\n",
      "    Line #2126 (got 1 columns instead of 5)\n",
      "    Line #2131 (got 1 columns instead of 5)\n",
      "    Line #2136 (got 1 columns instead of 5)\n",
      "    Line #2141 (got 1 columns instead of 5)\n",
      "    Line #2146 (got 1 columns instead of 5)\n",
      "    Line #2151 (got 1 columns instead of 5)\n",
      "    Line #2156 (got 1 columns instead of 5)\n",
      "    Line #2161 (got 1 columns instead of 5)\n",
      "    Line #2166 (got 1 columns instead of 5)\n",
      "    Line #2171 (got 1 columns instead of 5)\n",
      "    Line #2176 (got 1 columns instead of 5)\n",
      "    Line #2181 (got 1 columns instead of 5)\n",
      "    Line #2186 (got 1 columns instead of 5)\n",
      "    Line #2191 (got 1 columns instead of 5)\n",
      "    Line #2196 (got 1 columns instead of 5)\n",
      "    Line #2201 (got 1 columns instead of 5)\n",
      "    Line #2206 (got 1 columns instead of 5)\n",
      "    Line #2211 (got 1 columns instead of 5)\n",
      "    Line #2216 (got 1 columns instead of 5)\n",
      "    Line #2221 (got 1 columns instead of 5)\n",
      "    Line #2226 (got 1 columns instead of 5)\n",
      "    Line #2231 (got 1 columns instead of 5)\n",
      "    Line #2236 (got 1 columns instead of 5)\n",
      "    Line #2241 (got 1 columns instead of 5)\n",
      "    Line #2246 (got 1 columns instead of 5)\n",
      "    Line #2251 (got 1 columns instead of 5)\n",
      "    Line #2256 (got 1 columns instead of 5)\n",
      "    Line #2261 (got 1 columns instead of 5)\n",
      "    Line #2266 (got 1 columns instead of 5)\n",
      "    Line #2271 (got 1 columns instead of 5)\n",
      "    Line #2276 (got 1 columns instead of 5)\n",
      "    Line #2281 (got 1 columns instead of 5)\n",
      "    Line #2286 (got 1 columns instead of 5)\n",
      "    Line #2291 (got 1 columns instead of 5)\n",
      "    Line #2296 (got 1 columns instead of 5)\n",
      "    Line #2301 (got 1 columns instead of 5)\n",
      "    Line #2306 (got 1 columns instead of 5)\n",
      "    Line #2311 (got 1 columns instead of 5)\n",
      "    Line #2316 (got 1 columns instead of 5)\n",
      "    Line #2321 (got 1 columns instead of 5)\n",
      "    Line #2326 (got 1 columns instead of 5)\n",
      "    Line #2331 (got 1 columns instead of 5)\n",
      "    Line #2336 (got 1 columns instead of 5)\n",
      "    Line #2341 (got 1 columns instead of 5)\n",
      "    Line #2346 (got 1 columns instead of 5)\n",
      "    Line #2351 (got 1 columns instead of 5)\n",
      "    Line #2356 (got 1 columns instead of 5)\n",
      "    Line #2361 (got 1 columns instead of 5)\n",
      "    Line #2366 (got 1 columns instead of 5)\n",
      "    Line #2371 (got 1 columns instead of 5)\n",
      "    Line #2376 (got 1 columns instead of 5)\n",
      "    Line #2381 (got 1 columns instead of 5)\n",
      "    Line #2386 (got 1 columns instead of 5)\n",
      "    Line #2391 (got 1 columns instead of 5)\n",
      "    Line #2396 (got 1 columns instead of 5)\n",
      "    Line #2401 (got 1 columns instead of 5)\n",
      "    Line #2406 (got 1 columns instead of 5)\n",
      "    Line #2411 (got 1 columns instead of 5)\n",
      "    Line #2416 (got 1 columns instead of 5)\n",
      "    Line #2421 (got 1 columns instead of 5)\n",
      "    Line #2426 (got 1 columns instead of 5)\n",
      "    Line #2431 (got 1 columns instead of 5)\n",
      "    Line #2436 (got 1 columns instead of 5)\n",
      "    Line #2441 (got 1 columns instead of 5)\n",
      "    Line #2446 (got 1 columns instead of 5)\n",
      "    Line #2451 (got 1 columns instead of 5)\n",
      "    Line #2456 (got 1 columns instead of 5)\n",
      "    Line #2461 (got 1 columns instead of 5)\n",
      "    Line #2466 (got 1 columns instead of 5)\n",
      "    Line #2471 (got 1 columns instead of 5)\n",
      "    Line #2476 (got 1 columns instead of 5)\n",
      "    Line #2481 (got 1 columns instead of 5)\n",
      "    Line #2486 (got 1 columns instead of 5)\n",
      "    Line #2491 (got 1 columns instead of 5)\n",
      "    Line #2496 (got 1 columns instead of 5)\n",
      "    Line #2501 (got 1 columns instead of 5)\n",
      "    Line #2506 (got 1 columns instead of 5)\n",
      "    Line #2511 (got 1 columns instead of 5)\n",
      "    Line #2516 (got 1 columns instead of 5)\n",
      "    Line #2521 (got 1 columns instead of 5)\n",
      "    Line #2526 (got 1 columns instead of 5)\n",
      "    Line #2531 (got 1 columns instead of 5)\n",
      "    Line #2536 (got 1 columns instead of 5)\n",
      "    Line #2541 (got 1 columns instead of 5)\n",
      "    Line #2546 (got 1 columns instead of 5)\n",
      "    Line #2551 (got 1 columns instead of 5)\n",
      "    Line #2556 (got 1 columns instead of 5)\n",
      "    Line #2561 (got 1 columns instead of 5)\n",
      "    Line #2566 (got 1 columns instead of 5)\n",
      "    Line #2571 (got 1 columns instead of 5)\n",
      "    Line #2576 (got 1 columns instead of 5)\n",
      "    Line #2581 (got 1 columns instead of 5)\n",
      "    Line #2586 (got 1 columns instead of 5)\n",
      "    Line #2591 (got 1 columns instead of 5)\n",
      "    Line #2596 (got 1 columns instead of 5)\n",
      "    Line #2601 (got 1 columns instead of 5)\n",
      "    Line #2606 (got 1 columns instead of 5)\n",
      "    Line #2611 (got 1 columns instead of 5)\n",
      "    Line #2616 (got 1 columns instead of 5)\n",
      "    Line #2621 (got 1 columns instead of 5)\n",
      "    Line #2626 (got 1 columns instead of 5)\n",
      "    Line #2631 (got 1 columns instead of 5)\n",
      "    Line #2636 (got 1 columns instead of 5)\n",
      "    Line #2641 (got 1 columns instead of 5)\n",
      "    Line #2646 (got 1 columns instead of 5)\n",
      "    Line #2651 (got 1 columns instead of 5)\n",
      "    Line #2656 (got 1 columns instead of 5)\n",
      "    Line #2661 (got 1 columns instead of 5)\n",
      "    Line #2666 (got 1 columns instead of 5)\n",
      "    Line #2671 (got 1 columns instead of 5)\n",
      "    Line #2676 (got 1 columns instead of 5)\n",
      "    Line #2681 (got 1 columns instead of 5)\n",
      "    Line #2686 (got 1 columns instead of 5)\n",
      "    Line #2691 (got 1 columns instead of 5)\n",
      "    Line #2696 (got 1 columns instead of 5)\n",
      "    Line #2701 (got 1 columns instead of 5)\n",
      "    Line #2706 (got 1 columns instead of 5)\n",
      "    Line #2711 (got 1 columns instead of 5)\n",
      "    Line #2716 (got 1 columns instead of 5)\n",
      "    Line #2721 (got 1 columns instead of 5)\n",
      "    Line #2726 (got 1 columns instead of 5)\n",
      "    Line #2731 (got 1 columns instead of 5)\n",
      "    Line #2736 (got 1 columns instead of 5)\n",
      "    Line #2741 (got 1 columns instead of 5)\n",
      "    Line #2746 (got 1 columns instead of 5)\n",
      "    Line #2751 (got 1 columns instead of 5)\n",
      "    Line #2756 (got 1 columns instead of 5)\n",
      "    Line #2761 (got 1 columns instead of 5)\n",
      "    Line #2766 (got 1 columns instead of 5)\n",
      "    Line #2771 (got 1 columns instead of 5)\n",
      "    Line #2776 (got 1 columns instead of 5)\n",
      "    Line #2781 (got 1 columns instead of 5)\n",
      "    Line #2786 (got 1 columns instead of 5)\n",
      "    Line #2791 (got 1 columns instead of 5)\n",
      "    Line #2796 (got 1 columns instead of 5)\n",
      "    Line #2801 (got 1 columns instead of 5)\n",
      "    Line #2806 (got 1 columns instead of 5)\n",
      "    Line #2811 (got 1 columns instead of 5)\n",
      "    Line #2816 (got 1 columns instead of 5)\n",
      "    Line #2821 (got 1 columns instead of 5)\n",
      "    Line #2826 (got 1 columns instead of 5)\n",
      "    Line #2831 (got 1 columns instead of 5)\n",
      "    Line #2836 (got 1 columns instead of 5)\n",
      "    Line #2841 (got 1 columns instead of 5)\n",
      "    Line #2846 (got 1 columns instead of 5)\n",
      "    Line #2851 (got 1 columns instead of 5)\n",
      "    Line #2856 (got 1 columns instead of 5)\n",
      "    Line #2861 (got 1 columns instead of 5)\n",
      "    Line #2866 (got 1 columns instead of 5)\n",
      "    Line #2871 (got 1 columns instead of 5)\n",
      "    Line #2876 (got 1 columns instead of 5)\n",
      "    Line #2881 (got 1 columns instead of 5)\n",
      "    Line #2886 (got 1 columns instead of 5)\n",
      "    Line #2891 (got 1 columns instead of 5)\n",
      "    Line #2896 (got 1 columns instead of 5)\n",
      "    Line #2901 (got 1 columns instead of 5)\n",
      "    Line #2906 (got 1 columns instead of 5)\n",
      "    Line #2911 (got 1 columns instead of 5)\n",
      "    Line #2916 (got 1 columns instead of 5)\n",
      "    Line #2921 (got 1 columns instead of 5)\n",
      "    Line #2926 (got 1 columns instead of 5)\n",
      "    Line #2931 (got 1 columns instead of 5)\n",
      "    Line #2936 (got 1 columns instead of 5)\n",
      "    Line #2941 (got 1 columns instead of 5)\n",
      "    Line #2946 (got 1 columns instead of 5)\n",
      "    Line #2951 (got 1 columns instead of 5)\n",
      "    Line #2956 (got 1 columns instead of 5)\n",
      "    Line #2961 (got 1 columns instead of 5)\n",
      "    Line #2966 (got 1 columns instead of 5)\n",
      "    Line #2971 (got 1 columns instead of 5)\n",
      "    Line #2976 (got 1 columns instead of 5)\n",
      "    Line #2981 (got 1 columns instead of 5)\n",
      "    Line #2986 (got 1 columns instead of 5)\n",
      "    Line #2991 (got 1 columns instead of 5)\n",
      "    Line #2996 (got 1 columns instead of 5)\n",
      "    Line #3001 (got 1 columns instead of 5)\n",
      "    Line #3006 (got 1 columns instead of 5)\n",
      "    Line #3011 (got 1 columns instead of 5)\n",
      "    Line #3016 (got 1 columns instead of 5)\n",
      "    Line #3021 (got 1 columns instead of 5)\n",
      "    Line #3026 (got 1 columns instead of 5)\n",
      "    Line #3031 (got 1 columns instead of 5)\n",
      "    Line #3036 (got 1 columns instead of 5)\n",
      "    Line #3041 (got 1 columns instead of 5)\n",
      "    Line #3046 (got 1 columns instead of 5)\n",
      "    Line #3051 (got 1 columns instead of 5)\n",
      "    Line #3056 (got 1 columns instead of 5)\n",
      "    Line #3061 (got 1 columns instead of 5)\n",
      "    Line #3066 (got 1 columns instead of 5)\n",
      "    Line #3071 (got 1 columns instead of 5)\n",
      "    Line #3076 (got 1 columns instead of 5)\n",
      "    Line #3081 (got 1 columns instead of 5)\n",
      "    Line #3086 (got 1 columns instead of 5)\n",
      "    Line #3091 (got 1 columns instead of 5)\n",
      "    Line #3096 (got 1 columns instead of 5)\n",
      "    Line #3101 (got 1 columns instead of 5)\n",
      "    Line #3106 (got 1 columns instead of 5)\n",
      "    Line #3111 (got 1 columns instead of 5)\n",
      "    Line #3116 (got 1 columns instead of 5)\n",
      "    Line #3121 (got 1 columns instead of 5)\n",
      "    Line #3126 (got 1 columns instead of 5)\n",
      "    Line #3131 (got 1 columns instead of 5)\n",
      "    Line #3136 (got 1 columns instead of 5)\n",
      "    Line #3141 (got 1 columns instead of 5)\n",
      "    Line #3146 (got 1 columns instead of 5)\n",
      "    Line #3151 (got 1 columns instead of 5)\n",
      "    Line #3156 (got 1 columns instead of 5)\n",
      "    Line #3161 (got 1 columns instead of 5)\n",
      "    Line #3166 (got 1 columns instead of 5)\n",
      "    Line #3171 (got 1 columns instead of 5)\n",
      "    Line #3176 (got 1 columns instead of 5)\n",
      "    Line #3181 (got 1 columns instead of 5)\n",
      "    Line #3186 (got 1 columns instead of 5)\n",
      "    Line #3191 (got 1 columns instead of 5)\n",
      "    Line #3196 (got 1 columns instead of 5)\n",
      "    Line #3201 (got 1 columns instead of 5)\n",
      "    Line #3206 (got 1 columns instead of 5)\n",
      "    Line #3211 (got 1 columns instead of 5)\n",
      "    Line #3216 (got 1 columns instead of 5)\n",
      "    Line #3221 (got 1 columns instead of 5)\n",
      "    Line #3226 (got 1 columns instead of 5)\n",
      "    Line #3231 (got 1 columns instead of 5)\n",
      "    Line #3236 (got 1 columns instead of 5)\n",
      "    Line #3241 (got 1 columns instead of 5)\n",
      "    Line #3246 (got 1 columns instead of 5)\n",
      "    Line #3251 (got 1 columns instead of 5)\n",
      "    Line #3256 (got 1 columns instead of 5)\n",
      "    Line #3261 (got 1 columns instead of 5)\n",
      "    Line #3266 (got 1 columns instead of 5)\n",
      "    Line #3271 (got 1 columns instead of 5)\n",
      "    Line #3276 (got 1 columns instead of 5)\n",
      "    Line #3281 (got 1 columns instead of 5)\n",
      "    Line #3286 (got 1 columns instead of 5)\n",
      "    Line #3291 (got 1 columns instead of 5)\n",
      "    Line #3296 (got 1 columns instead of 5)\n",
      "    Line #3301 (got 1 columns instead of 5)\n",
      "    Line #3306 (got 1 columns instead of 5)\n",
      "    Line #3311 (got 1 columns instead of 5)\n",
      "    Line #3316 (got 1 columns instead of 5)\n",
      "    Line #3321 (got 1 columns instead of 5)\n",
      "    Line #3326 (got 1 columns instead of 5)\n",
      "    Line #3331 (got 1 columns instead of 5)\n",
      "    Line #3336 (got 1 columns instead of 5)\n",
      "    Line #3341 (got 1 columns instead of 5)\n",
      "    Line #3346 (got 1 columns instead of 5)\n",
      "    Line #3351 (got 1 columns instead of 5)\n",
      "    Line #3356 (got 1 columns instead of 5)\n",
      "    Line #3361 (got 1 columns instead of 5)\n",
      "    Line #3366 (got 1 columns instead of 5)\n",
      "    Line #3371 (got 1 columns instead of 5)\n",
      "    Line #3376 (got 1 columns instead of 5)\n",
      "    Line #3381 (got 1 columns instead of 5)\n",
      "    Line #3386 (got 1 columns instead of 5)\n",
      "    Line #3391 (got 1 columns instead of 5)\n",
      "    Line #3396 (got 1 columns instead of 5)\n",
      "    Line #3401 (got 1 columns instead of 5)\n",
      "    Line #3406 (got 1 columns instead of 5)\n",
      "    Line #3411 (got 1 columns instead of 5)\n",
      "    Line #3416 (got 1 columns instead of 5)\n",
      "    Line #3421 (got 1 columns instead of 5)\n",
      "    Line #3426 (got 1 columns instead of 5)\n",
      "    Line #3431 (got 1 columns instead of 5)\n",
      "    Line #3436 (got 1 columns instead of 5)\n",
      "    Line #3441 (got 1 columns instead of 5)\n",
      "    Line #3446 (got 1 columns instead of 5)\n",
      "    Line #3451 (got 1 columns instead of 5)\n",
      "    Line #3456 (got 1 columns instead of 5)\n",
      "    Line #3461 (got 1 columns instead of 5)\n",
      "    Line #3466 (got 1 columns instead of 5)\n",
      "    Line #3471 (got 1 columns instead of 5)\n",
      "    Line #3476 (got 1 columns instead of 5)\n",
      "    Line #3481 (got 1 columns instead of 5)\n",
      "    Line #3486 (got 1 columns instead of 5)\n",
      "    Line #3491 (got 1 columns instead of 5)\n",
      "    Line #3496 (got 1 columns instead of 5)\n",
      "    Line #3501 (got 1 columns instead of 5)\n",
      "    Line #3506 (got 1 columns instead of 5)\n",
      "    Line #3511 (got 1 columns instead of 5)\n",
      "    Line #3516 (got 1 columns instead of 5)\n",
      "    Line #3521 (got 1 columns instead of 5)\n",
      "    Line #3526 (got 1 columns instead of 5)\n",
      "    Line #3531 (got 1 columns instead of 5)\n",
      "    Line #3536 (got 1 columns instead of 5)\n",
      "    Line #3541 (got 1 columns instead of 5)\n",
      "    Line #3546 (got 1 columns instead of 5)\n",
      "    Line #3551 (got 1 columns instead of 5)\n",
      "    Line #3556 (got 1 columns instead of 5)\n",
      "    Line #3561 (got 1 columns instead of 5)\n",
      "    Line #3566 (got 1 columns instead of 5)\n",
      "    Line #3571 (got 1 columns instead of 5)\n",
      "    Line #3576 (got 1 columns instead of 5)\n",
      "    Line #3581 (got 1 columns instead of 5)\n",
      "    Line #3586 (got 1 columns instead of 5)\n",
      "    Line #3591 (got 1 columns instead of 5)\n",
      "    Line #3596 (got 1 columns instead of 5)\n",
      "    Line #3601 (got 1 columns instead of 5)\n",
      "    Line #3606 (got 1 columns instead of 5)\n",
      "    Line #3611 (got 1 columns instead of 5)\n",
      "    Line #3616 (got 1 columns instead of 5)\n",
      "    Line #3621 (got 1 columns instead of 5)\n",
      "    Line #3626 (got 1 columns instead of 5)\n",
      "    Line #3631 (got 1 columns instead of 5)\n",
      "    Line #3636 (got 1 columns instead of 5)\n",
      "    Line #3641 (got 1 columns instead of 5)\n",
      "    Line #3646 (got 1 columns instead of 5)\n",
      "    Line #3651 (got 1 columns instead of 5)\n",
      "    Line #3656 (got 1 columns instead of 5)\n",
      "    Line #3661 (got 1 columns instead of 5)\n",
      "    Line #3666 (got 1 columns instead of 5)\n",
      "    Line #3671 (got 1 columns instead of 5)\n",
      "    Line #3676 (got 1 columns instead of 5)\n",
      "    Line #3681 (got 1 columns instead of 5)\n",
      "    Line #3686 (got 1 columns instead of 5)\n",
      "    Line #3691 (got 1 columns instead of 5)\n",
      "    Line #3696 (got 1 columns instead of 5)\n",
      "    Line #3701 (got 1 columns instead of 5)\n",
      "    Line #3706 (got 1 columns instead of 5)\n",
      "    Line #3711 (got 1 columns instead of 5)\n",
      "    Line #3716 (got 1 columns instead of 5)\n",
      "    Line #3721 (got 1 columns instead of 5)\n",
      "    Line #3726 (got 1 columns instead of 5)\n",
      "    Line #3731 (got 1 columns instead of 5)\n",
      "    Line #3736 (got 1 columns instead of 5)\n",
      "    Line #3741 (got 1 columns instead of 5)\n",
      "    Line #3746 (got 1 columns instead of 5)\n",
      "    Line #3751 (got 1 columns instead of 5)\n",
      "    Line #3756 (got 1 columns instead of 5)\n",
      "    Line #3761 (got 1 columns instead of 5)\n",
      "    Line #3766 (got 1 columns instead of 5)\n",
      "    Line #3771 (got 1 columns instead of 5)\n",
      "    Line #3776 (got 1 columns instead of 5)\n",
      "    Line #3781 (got 1 columns instead of 5)\n",
      "    Line #3786 (got 1 columns instead of 5)\n",
      "    Line #3791 (got 1 columns instead of 5)\n",
      "    Line #3796 (got 1 columns instead of 5)\n",
      "    Line #3801 (got 1 columns instead of 5)\n",
      "    Line #3806 (got 1 columns instead of 5)\n",
      "    Line #3811 (got 1 columns instead of 5)\n",
      "    Line #3816 (got 1 columns instead of 5)\n",
      "    Line #3821 (got 1 columns instead of 5)\n",
      "    Line #3826 (got 1 columns instead of 5)\n",
      "    Line #3831 (got 1 columns instead of 5)\n",
      "    Line #3836 (got 1 columns instead of 5)\n",
      "    Line #3841 (got 1 columns instead of 5)\n",
      "    Line #3846 (got 1 columns instead of 5)\n",
      "    Line #3851 (got 1 columns instead of 5)\n",
      "    Line #3856 (got 1 columns instead of 5)\n",
      "    Line #3861 (got 1 columns instead of 5)\n",
      "    Line #3866 (got 1 columns instead of 5)\n",
      "    Line #3871 (got 1 columns instead of 5)\n",
      "    Line #3876 (got 1 columns instead of 5)\n",
      "    Line #3881 (got 1 columns instead of 5)\n",
      "    Line #3886 (got 1 columns instead of 5)\n",
      "    Line #3891 (got 1 columns instead of 5)\n",
      "    Line #3896 (got 1 columns instead of 5)\n",
      "    Line #3901 (got 1 columns instead of 5)\n",
      "    Line #3906 (got 1 columns instead of 5)\n",
      "    Line #3911 (got 1 columns instead of 5)\n",
      "    Line #3916 (got 1 columns instead of 5)\n",
      "    Line #3921 (got 1 columns instead of 5)\n",
      "    Line #3926 (got 1 columns instead of 5)\n",
      "    Line #3931 (got 1 columns instead of 5)\n",
      "    Line #3936 (got 1 columns instead of 5)\n",
      "    Line #3941 (got 1 columns instead of 5)\n",
      "    Line #3946 (got 1 columns instead of 5)\n",
      "    Line #3951 (got 1 columns instead of 5)\n",
      "    Line #3956 (got 1 columns instead of 5)\n",
      "    Line #3961 (got 1 columns instead of 5)\n",
      "    Line #3966 (got 1 columns instead of 5)\n",
      "    Line #3971 (got 1 columns instead of 5)\n",
      "    Line #3976 (got 1 columns instead of 5)\n",
      "    Line #3981 (got 1 columns instead of 5)\n",
      "    Line #3986 (got 1 columns instead of 5)\n",
      "    Line #3991 (got 1 columns instead of 5)\n",
      "    Line #3996 (got 1 columns instead of 5)\n",
      "    Line #4001 (got 1 columns instead of 5)\n",
      "    Line #4006 (got 1 columns instead of 5)\n",
      "    Line #4011 (got 1 columns instead of 5)\n",
      "    Line #4016 (got 1 columns instead of 5)\n",
      "    Line #4021 (got 1 columns instead of 5)\n",
      "    Line #4026 (got 1 columns instead of 5)\n",
      "    Line #4031 (got 1 columns instead of 5)\n",
      "    Line #4036 (got 1 columns instead of 5)\n",
      "    Line #4041 (got 1 columns instead of 5)\n",
      "    Line #4046 (got 1 columns instead of 5)\n",
      "    Line #4051 (got 1 columns instead of 5)\n",
      "    Line #4056 (got 1 columns instead of 5)\n",
      "    Line #4061 (got 1 columns instead of 5)\n",
      "    Line #4066 (got 1 columns instead of 5)\n",
      "    Line #4071 (got 1 columns instead of 5)\n",
      "    Line #4076 (got 1 columns instead of 5)\n",
      "    Line #4081 (got 1 columns instead of 5)\n",
      "    Line #4086 (got 1 columns instead of 5)\n",
      "    Line #4091 (got 1 columns instead of 5)\n",
      "    Line #4096 (got 1 columns instead of 5)\n",
      "    Line #4101 (got 1 columns instead of 5)\n",
      "    Line #4106 (got 1 columns instead of 5)\n",
      "    Line #4111 (got 1 columns instead of 5)\n",
      "    Line #4116 (got 1 columns instead of 5)\n",
      "    Line #4121 (got 1 columns instead of 5)\n",
      "    Line #4126 (got 1 columns instead of 5)\n",
      "    Line #4131 (got 1 columns instead of 5)\n",
      "    Line #4136 (got 1 columns instead of 5)\n",
      "    Line #4141 (got 1 columns instead of 5)\n",
      "    Line #4146 (got 1 columns instead of 5)\n",
      "    Line #4151 (got 1 columns instead of 5)\n",
      "    Line #4156 (got 1 columns instead of 5)\n",
      "    Line #4161 (got 1 columns instead of 5)\n",
      "    Line #4166 (got 1 columns instead of 5)\n",
      "    Line #4171 (got 1 columns instead of 5)\n",
      "    Line #4176 (got 1 columns instead of 5)\n",
      "    Line #4181 (got 1 columns instead of 5)\n",
      "    Line #4186 (got 1 columns instead of 5)\n",
      "    Line #4191 (got 1 columns instead of 5)\n",
      "    Line #4196 (got 1 columns instead of 5)\n",
      "    Line #4201 (got 1 columns instead of 5)\n",
      "    Line #4206 (got 1 columns instead of 5)\n",
      "    Line #4211 (got 1 columns instead of 5)\n",
      "    Line #4216 (got 1 columns instead of 5)\n",
      "    Line #4221 (got 1 columns instead of 5)\n",
      "    Line #4226 (got 1 columns instead of 5)\n",
      "    Line #4231 (got 1 columns instead of 5)\n",
      "    Line #4236 (got 1 columns instead of 5)\n",
      "    Line #4241 (got 1 columns instead of 5)\n",
      "    Line #4246 (got 1 columns instead of 5)\n",
      "    Line #4251 (got 1 columns instead of 5)\n",
      "    Line #4256 (got 1 columns instead of 5)\n",
      "    Line #4261 (got 1 columns instead of 5)\n",
      "    Line #4266 (got 1 columns instead of 5)\n",
      "    Line #4271 (got 1 columns instead of 5)\n",
      "    Line #4276 (got 1 columns instead of 5)\n",
      "    Line #4281 (got 1 columns instead of 5)\n",
      "    Line #4286 (got 1 columns instead of 5)\n",
      "    Line #4291 (got 1 columns instead of 5)\n",
      "    Line #4296 (got 1 columns instead of 5)\n",
      "    Line #4301 (got 1 columns instead of 5)\n",
      "    Line #4306 (got 1 columns instead of 5)\n",
      "    Line #4311 (got 1 columns instead of 5)\n",
      "    Line #4316 (got 1 columns instead of 5)\n",
      "    Line #4321 (got 1 columns instead of 5)\n",
      "    Line #4326 (got 1 columns instead of 5)\n",
      "    Line #4331 (got 1 columns instead of 5)\n",
      "    Line #4336 (got 1 columns instead of 5)\n",
      "    Line #4341 (got 1 columns instead of 5)\n",
      "    Line #4346 (got 1 columns instead of 5)\n",
      "    Line #4351 (got 1 columns instead of 5)\n",
      "    Line #4356 (got 1 columns instead of 5)\n",
      "    Line #4361 (got 1 columns instead of 5)\n",
      "    Line #4366 (got 1 columns instead of 5)\n",
      "    Line #4371 (got 1 columns instead of 5)\n",
      "    Line #4376 (got 1 columns instead of 5)\n",
      "    Line #4381 (got 1 columns instead of 5)\n",
      "    Line #4386 (got 1 columns instead of 5)\n",
      "    Line #4391 (got 1 columns instead of 5)\n",
      "    Line #4396 (got 1 columns instead of 5)\n",
      "    Line #4401 (got 1 columns instead of 5)\n",
      "    Line #4406 (got 1 columns instead of 5)\n",
      "    Line #4411 (got 1 columns instead of 5)\n",
      "    Line #4416 (got 1 columns instead of 5)\n",
      "    Line #4421 (got 1 columns instead of 5)\n",
      "    Line #4426 (got 1 columns instead of 5)\n",
      "    Line #4431 (got 1 columns instead of 5)\n",
      "    Line #4436 (got 1 columns instead of 5)\n",
      "    Line #4441 (got 1 columns instead of 5)\n",
      "    Line #4446 (got 1 columns instead of 5)\n",
      "    Line #4451 (got 1 columns instead of 5)\n",
      "    Line #4456 (got 1 columns instead of 5)\n",
      "    Line #4461 (got 1 columns instead of 5)\n",
      "    Line #4466 (got 1 columns instead of 5)\n",
      "    Line #4471 (got 1 columns instead of 5)\n",
      "    Line #4476 (got 1 columns instead of 5)\n",
      "    Line #4481 (got 1 columns instead of 5)\n",
      "    Line #4486 (got 1 columns instead of 5)\n",
      "    Line #4491 (got 1 columns instead of 5)\n",
      "    Line #4496 (got 1 columns instead of 5)\n",
      "    Line #4501 (got 1 columns instead of 5)\n",
      "    Line #4506 (got 1 columns instead of 5)\n",
      "    Line #4511 (got 1 columns instead of 5)\n",
      "    Line #4516 (got 1 columns instead of 5)\n",
      "    Line #4521 (got 1 columns instead of 5)\n",
      "    Line #4526 (got 1 columns instead of 5)\n",
      "    Line #4531 (got 1 columns instead of 5)\n",
      "    Line #4536 (got 1 columns instead of 5)\n",
      "    Line #4541 (got 1 columns instead of 5)\n",
      "    Line #4546 (got 1 columns instead of 5)\n",
      "    Line #4551 (got 1 columns instead of 5)\n",
      "    Line #4556 (got 1 columns instead of 5)\n",
      "    Line #4561 (got 1 columns instead of 5)\n",
      "    Line #4566 (got 1 columns instead of 5)\n",
      "    Line #4571 (got 1 columns instead of 5)\n",
      "    Line #4576 (got 1 columns instead of 5)\n",
      "    Line #4581 (got 1 columns instead of 5)\n",
      "    Line #4586 (got 1 columns instead of 5)\n",
      "    Line #4591 (got 1 columns instead of 5)\n",
      "    Line #4596 (got 1 columns instead of 5)\n",
      "    Line #4601 (got 1 columns instead of 5)\n",
      "    Line #4606 (got 1 columns instead of 5)\n",
      "    Line #4611 (got 1 columns instead of 5)\n",
      "    Line #4616 (got 1 columns instead of 5)\n",
      "    Line #4621 (got 1 columns instead of 5)\n",
      "    Line #4626 (got 1 columns instead of 5)\n",
      "    Line #4631 (got 1 columns instead of 5)\n",
      "    Line #4636 (got 1 columns instead of 5)\n",
      "    Line #4641 (got 1 columns instead of 5)\n",
      "    Line #4646 (got 1 columns instead of 5)\n",
      "    Line #4651 (got 1 columns instead of 5)\n",
      "    Line #4656 (got 1 columns instead of 5)\n",
      "    Line #4661 (got 1 columns instead of 5)\n",
      "    Line #4666 (got 1 columns instead of 5)\n",
      "    Line #4671 (got 1 columns instead of 5)\n",
      "    Line #4676 (got 1 columns instead of 5)\n",
      "    Line #4681 (got 1 columns instead of 5)\n",
      "    Line #4686 (got 1 columns instead of 5)\n",
      "    Line #4691 (got 1 columns instead of 5)\n",
      "    Line #4696 (got 1 columns instead of 5)\n",
      "    Line #4701 (got 1 columns instead of 5)\n",
      "    Line #4706 (got 1 columns instead of 5)\n",
      "    Line #4711 (got 1 columns instead of 5)\n",
      "    Line #4716 (got 1 columns instead of 5)\n",
      "    Line #4721 (got 1 columns instead of 5)\n",
      "    Line #4726 (got 1 columns instead of 5)\n",
      "    Line #4731 (got 1 columns instead of 5)\n",
      "    Line #4736 (got 1 columns instead of 5)\n",
      "    Line #4741 (got 1 columns instead of 5)\n",
      "    Line #4746 (got 1 columns instead of 5)\n",
      "    Line #4751 (got 1 columns instead of 5)\n",
      "    Line #4756 (got 1 columns instead of 5)\n",
      "    Line #4761 (got 1 columns instead of 5)\n",
      "    Line #4766 (got 1 columns instead of 5)\n",
      "    Line #4771 (got 1 columns instead of 5)\n",
      "    Line #4776 (got 1 columns instead of 5)\n",
      "    Line #4781 (got 1 columns instead of 5)\n",
      "    Line #4786 (got 1 columns instead of 5)\n",
      "    Line #4791 (got 1 columns instead of 5)\n",
      "    Line #4796 (got 1 columns instead of 5)\n",
      "    Line #4801 (got 1 columns instead of 5)\n",
      "    Line #4806 (got 1 columns instead of 5)\n",
      "    Line #4811 (got 1 columns instead of 5)\n",
      "    Line #4816 (got 1 columns instead of 5)\n",
      "    Line #4821 (got 1 columns instead of 5)\n",
      "    Line #4826 (got 1 columns instead of 5)\n",
      "    Line #4831 (got 1 columns instead of 5)\n",
      "    Line #4836 (got 1 columns instead of 5)\n",
      "    Line #4841 (got 1 columns instead of 5)\n",
      "    Line #4846 (got 1 columns instead of 5)\n",
      "    Line #4851 (got 1 columns instead of 5)\n",
      "    Line #4856 (got 1 columns instead of 5)\n",
      "    Line #4861 (got 1 columns instead of 5)\n",
      "    Line #4866 (got 1 columns instead of 5)\n",
      "    Line #4871 (got 1 columns instead of 5)\n",
      "    Line #4876 (got 1 columns instead of 5)\n",
      "    Line #4881 (got 1 columns instead of 5)\n",
      "    Line #4886 (got 1 columns instead of 5)\n",
      "    Line #4891 (got 1 columns instead of 5)\n",
      "    Line #4896 (got 1 columns instead of 5)\n",
      "    Line #4901 (got 1 columns instead of 5)\n",
      "    Line #4906 (got 1 columns instead of 5)\n",
      "    Line #4911 (got 1 columns instead of 5)\n",
      "    Line #4916 (got 1 columns instead of 5)\n",
      "    Line #4921 (got 1 columns instead of 5)\n",
      "    Line #4926 (got 1 columns instead of 5)\n",
      "    Line #4931 (got 1 columns instead of 5)\n",
      "    Line #4936 (got 1 columns instead of 5)\n",
      "    Line #4941 (got 1 columns instead of 5)\n",
      "    Line #4946 (got 1 columns instead of 5)\n",
      "    Line #4951 (got 1 columns instead of 5)\n",
      "    Line #4956 (got 1 columns instead of 5)\n",
      "    Line #4961 (got 1 columns instead of 5)\n",
      "    Line #4966 (got 1 columns instead of 5)\n",
      "    Line #4971 (got 1 columns instead of 5)\n",
      "    Line #4976 (got 1 columns instead of 5)\n",
      "    Line #4981 (got 1 columns instead of 5)\n",
      "    Line #4986 (got 1 columns instead of 5)\n",
      "    Line #4991 (got 1 columns instead of 5)\n",
      "    Line #4996 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates = (xyz_file[:,1:-1])\n",
    "coordinates = coordinates.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[0.         0.769767   0.55746937]\n",
      "[ 0.         -0.71017975  0.50340914]\n",
      "[ 0.         -0.0037242  -0.06630491]\n",
      "gfjkhgfjhgfkgh\n",
      "[0.         0.77715107 0.59586089]\n",
      "[ 0.         -0.77642641  0.59515778]\n",
      "[ 0.000000e+00 -4.529000e-05 -7.443867e-02]\n",
      "-13815.2523726009\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(coordinates))\n",
    "print(type(coordinates))\n",
    "print(coordinates[0])\n",
    "print(coordinates[1])\n",
    "print(coordinates[2])\n",
    "\n",
    "print('gfjkhgfjhgfkgh')\n",
    "print(coordinates[3])\n",
    "print(coordinates[4])\n",
    "print(coordinates[5])\n",
    "\n",
    "print(energies[0])\n",
    "# There is 1000 water molecules and each of them consists of 3 atoms, so we have 3000 atoms in total and each\n",
    "# of them has 3 coordinates.\n",
    "# Thus the coordinates array has 3000 lines, each of them corresponding to one atom (the first three lines\n",
    "# correspopnd to the first water molecule) and 3 columns corresponding to the x, y and z coordinates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.769767    0.55746937]\n",
      " [ 0.         -0.71017975  0.50340914]\n",
      " [ 0.         -0.0037242  -0.06630491]\n",
      " ...\n",
      " [ 0.          0.81441381  0.59863567]\n",
      " [ 0.         -0.76145415  0.54978922]\n",
      " [ 0.         -0.00330998 -0.07177656]]\n"
     ]
    }
   ],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.48093379 0.99367147]\n",
      " [1.48093379 0.         0.9075536 ]\n",
      " [0.99367147 0.9075536  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                # i-th water molecule\n",
    "coord = coordinates[3*i:3*(i+1),:]   # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                     # pairwise distances between all of its 3 atom\n",
    "\n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "#        print('i=',i)\n",
    "        for j in range(i+1,N):\n",
    "#            print(j)\n",
    "#            pairwise_dist_matrix[i][j] = \\\n",
    "#            np.sqrt(  (coord[i][0] - coord[j][0] )**2 + (coord[i][1] - coord[j][1] )**2 +(coord[i][2] - coord[j][2] )**2   )\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65865495 1.67618952 1.79537284]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = len(coord)\n",
    "\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.19473151 5.24520024 5.28298916]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     #1\n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n",
      "torch.Size([900, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "N                    = 3           # number of atoms per molecule\n",
    "number_of_features   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "    \n",
    "    \n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1., 1., 1.]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "training_set_size    = data_size - 100\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features,2):\n",
    "        G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])     \n",
    "        G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.var(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set = np.vsplit(G,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.var(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "print(mean_lab)\n",
    "\n",
    "labelss = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labelss[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labelss = torch.FloatTensor(labelss)      \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:]\n",
    "\n",
    "train_labels         = labelss[:training_set_size]\n",
    "test_labels          = labelss[training_set_size:]\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n",
      "[[0.33333333 2.        ]\n",
      " [3.         4.        ]\n",
      " [5.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "# for i in range(3):\n",
    "#     a[i,0] = a[i,0]/(sum(a[:,0])/3)\n",
    "# print(a)\n",
    "\n",
    "b = (sum(a[:,0])/3)\n",
    "print(b)\n",
    "print(type(b))\n",
    "a[0,0] = a[0,0]/b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([ -29.2808,    1.3305,   -9.2254, -144.8022,  -13.5390,  -43.2955])\n",
      "x2 tensor([-16.4986,   0.6675,  -0.0754,  74.7737,  -0.4341,  16.6143])\n",
      "x3 tensor([ 85.2207,  -2.0988,  16.9244, 191.8523,  25.0267,  65.6689])\n",
      "output\n",
      "tensor([-0.0143], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 3)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "#         self.fc6 = nn.Linear(6, 4)\n",
    "#         self.fc7 = nn.Linear(4, 2)\n",
    "#         self.fc8 = nn.Linear(2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = torch.tanh(self.fc4(x))\n",
    "#         x = torch.tanh(self.fc5(x))\n",
    "#         x = torch.tanh(self.fc6(x))\n",
    "#         x = torch.tanh(self.fc7(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN, self).__init__()\n",
    "        self.network1 = Subnets()\n",
    "        self.network2 = Subnets()\n",
    "        self.network3 = Subnets()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "#         print(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN()\n",
    "N = 1\n",
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class simplenn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(simplenn, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 3)        # where fc stands for fully connected \n",
    "#         self.fc2 = nn.Linear(3, 1)        \n",
    "   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "#         x = self.fc2(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "#         return x\n",
    "\n",
    "# mod = simplenn()\n",
    "\n",
    "# print(mod.fc1.weight)\n",
    "# print(mod.fc1.bias)\n",
    "\n",
    "# print(mod.fc2.weight)\n",
    "# print(mod.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# x1 = x1[:2]\n",
    "# x2 = x2[:2]\n",
    "\n",
    "# x1[0] = -18650\n",
    "# x1[1] = 109075\n",
    "# print('x1',x1)\n",
    "\n",
    "# x2[0] = -6\n",
    "# x2[1] = 7\n",
    "# print('x2',x2)\n",
    "\n",
    "# output1 = mod(x1)\n",
    "# print('output1')\n",
    "# print(output1)\n",
    "\n",
    "# output2 = mod(x2)\n",
    "# print('output2')\n",
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([ -29.2808,    1.3305,   -9.2254, -144.8022,  -13.5390,  -43.2955])\n",
      "x2 tensor([-16.4986,   0.6675,  -0.0754,  74.7737,  -0.4341,  16.6143])\n",
      "x3 tensor([ 85.2207,  -2.0988,  16.9244, 191.8523,  25.0267,  65.6689])\n",
      "output\n",
      "tensor([-0.0143], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.1589, -0.2502,  0.2748,  0.3607, -0.2570, -0.0059],\n",
      "        [ 0.2620,  0.3297, -0.2469,  0.3697,  0.2121,  0.3243],\n",
      "        [ 0.1323,  0.2603,  0.0840, -0.2077, -0.3135,  0.1061]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.3242, -0.1442, -0.3936], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.1456,  0.1234, -0.4108],\n",
      "        [ 0.3078,  0.2989,  0.4993],\n",
      "        [ 0.1977,  0.3343, -0.3881]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([ 0.3793, -0.2828, -0.4646], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(model.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(model.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 1.53215\n",
      "[2,     1] loss: 0.04168\n",
      "[3,     1] loss: 0.04429\n",
      "[4,     1] loss: 0.04196\n",
      "[5,     1] loss: 0.04111\n",
      "[6,     1] loss: 0.05609\n",
      "[7,     1] loss: 0.03674\n",
      "[8,     1] loss: 0.03561\n",
      "[9,     1] loss: 0.02875\n",
      "[10,     1] loss: 0.02634\n",
      "[11,     1] loss: 0.02444\n",
      "[12,     1] loss: 0.02157\n",
      "[13,     1] loss: 0.02146\n",
      "[14,     1] loss: 0.01612\n",
      "[15,     1] loss: 0.03354\n",
      "[16,     1] loss: 0.02588\n",
      "[17,     1] loss: 0.02020\n",
      "[18,     1] loss: 0.03385\n",
      "[19,     1] loss: 0.01687\n",
      "[20,     1] loss: 0.03260\n",
      "[21,     1] loss: 0.02434\n",
      "[22,     1] loss: 0.02056\n",
      "[23,     1] loss: 0.02259\n",
      "[24,     1] loss: 0.02248\n",
      "[25,     1] loss: 0.02861\n",
      "[26,     1] loss: 0.03283\n",
      "[27,     1] loss: 0.03709\n",
      "[28,     1] loss: 0.01693\n",
      "[29,     1] loss: 0.02925\n",
      "[30,     1] loss: 0.02908\n",
      "[31,     1] loss: 0.02699\n",
      "[32,     1] loss: 0.01130\n",
      "[33,     1] loss: 0.01967\n",
      "[34,     1] loss: 0.01785\n",
      "[35,     1] loss: 0.01559\n",
      "[36,     1] loss: 0.02659\n",
      "[37,     1] loss: 0.01334\n",
      "[38,     1] loss: 0.03609\n",
      "[39,     1] loss: 0.02880\n",
      "[40,     1] loss: 0.01707\n",
      "[41,     1] loss: 0.02078\n",
      "[42,     1] loss: 0.02354\n",
      "[43,     1] loss: 0.02018\n",
      "[44,     1] loss: 0.01330\n",
      "[45,     1] loss: 0.02884\n",
      "[46,     1] loss: 0.01899\n",
      "[47,     1] loss: 0.03135\n",
      "[48,     1] loss: 0.03652\n",
      "[49,     1] loss: 0.01741\n",
      "[50,     1] loss: 0.03308\n",
      "[51,     1] loss: 0.00981\n",
      "[52,     1] loss: 0.01305\n",
      "[53,     1] loss: 0.02993\n",
      "[54,     1] loss: 0.02230\n",
      "[55,     1] loss: 0.02562\n",
      "[56,     1] loss: 0.01747\n",
      "[57,     1] loss: 0.01563\n",
      "[58,     1] loss: 0.01637\n",
      "[59,     1] loss: 0.02208\n",
      "[60,     1] loss: 0.03525\n",
      "[61,     1] loss: 0.01628\n",
      "[62,     1] loss: 0.01920\n",
      "[63,     1] loss: 0.01453\n",
      "[64,     1] loss: 0.01572\n",
      "[65,     1] loss: 0.01294\n",
      "[66,     1] loss: 0.01163\n",
      "[67,     1] loss: 0.01315\n",
      "[68,     1] loss: 0.00938\n",
      "[69,     1] loss: 0.01133\n",
      "[70,     1] loss: 0.01554\n",
      "[71,     1] loss: 0.00727\n",
      "[72,     1] loss: 0.01734\n",
      "[73,     1] loss: 0.01219\n",
      "[74,     1] loss: 0.01347\n",
      "[75,     1] loss: 0.01446\n",
      "[76,     1] loss: 0.01117\n",
      "[77,     1] loss: 0.01087\n",
      "[78,     1] loss: 0.01023\n",
      "[79,     1] loss: 0.00972\n",
      "[80,     1] loss: 0.00795\n",
      "[81,     1] loss: 0.01214\n",
      "[82,     1] loss: 0.01519\n",
      "[83,     1] loss: 0.00955\n",
      "[84,     1] loss: 0.01240\n",
      "[85,     1] loss: 0.01260\n",
      "[86,     1] loss: 0.01109\n",
      "[87,     1] loss: 0.01238\n",
      "[88,     1] loss: 0.01453\n",
      "[89,     1] loss: 0.01232\n",
      "[90,     1] loss: 0.01172\n",
      "[91,     1] loss: 0.01210\n",
      "[92,     1] loss: 0.01292\n",
      "[93,     1] loss: 0.01574\n",
      "[94,     1] loss: 0.01474\n",
      "[95,     1] loss: 0.01640\n",
      "[96,     1] loss: 0.02244\n",
      "[97,     1] loss: 0.00937\n",
      "[98,     1] loss: 0.01415\n",
      "[99,     1] loss: 0.00754\n",
      "[100,     1] loss: 0.00970\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = BPNN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 100\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        # forward + backward + optimize\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])        #??????????????????????????????????????\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0])\n",
    "        with torch.no_grad():\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[j][0],test_set[j][1],test_set[j][2])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item()\n",
    "        if i % 100 == 0:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, loss))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "    \n",
    "    train_loss[epoch] = epoch_train_loss/counter\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.00101\n",
      "[2,     1] loss: 0.00073\n",
      "[3,     1] loss: 0.00174\n",
      "[4,     1] loss: 0.00094\n",
      "[5,     1] loss: 0.00150\n",
      "[6,     1] loss: 0.00141\n",
      "[7,     1] loss: 0.00113\n",
      "[8,     1] loss: 0.00096\n",
      "[9,     1] loss: 0.00090\n",
      "[10,     1] loss: 0.00087\n",
      "[11,     1] loss: 0.00087\n",
      "[12,     1] loss: 0.00086\n",
      "[13,     1] loss: 0.00086\n",
      "[14,     1] loss: 0.00086\n",
      "[15,     1] loss: 0.00086\n",
      "[16,     1] loss: 0.00086\n",
      "[17,     1] loss: 0.00086\n",
      "[18,     1] loss: 0.00085\n",
      "[19,     1] loss: 0.00085\n",
      "[20,     1] loss: 0.00085\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "for epoch in range(nepochs):  # loop over the dataset multiple times\n",
    "    running_loss = 0.0\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "    counter = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        # forward + backward + optimize\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])        #??????????????????????????????????????\n",
    "        \n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        epoch_train_loss += loss.item()\n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0])\n",
    "        with torch.no_grad():\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[j][0],test_set[j][1],test_set[j][2])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item()\n",
    "            \n",
    "        if i % 100 == 0:    # print every 200 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, loss))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "    \n",
    "    train_loss[epoch] = epoch_train_loss/counter\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -47.1594,    1.0143,   -9.9748, -152.5495,  -14.7487,  -46.0137])\n",
      "output\n",
      "tensor([-0.4905], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = test_set[10]\n",
    "x1 = x1\n",
    "print(x1)\n",
    "\n",
    "output = net(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.38379806 -0.43183175 -0.10768666 -0.28911862 -0.2933954  -0.09560608\n",
      "  0.17479777  0.11391863  0.0398407   0.11770698 -0.49049217 -0.37092209\n",
      " -0.18214896 -0.35842845 -0.44311005  0.07066074 -0.10281181  0.08601776\n",
      " -0.11643607 -0.52850407  0.45209542  0.0333339  -0.14941321  0.04122972\n",
      " -0.44853851 -0.02435797 -0.41270384  0.12020537 -0.2843695   0.17217353\n",
      "  0.42027435  0.35376528  0.48059544 -0.32955629 -0.04051489 -0.36901096\n",
      " -0.38726965 -0.17070675  0.17966032 -0.40893677 -0.0494225  -0.46546578\n",
      "  0.70486772 -0.14960983 -0.25169581  0.37484911 -0.24284348  0.02676223\n",
      " -0.06676103  0.10369098  0.0813157  -0.17234126 -0.17612308 -0.20321867\n",
      " -0.05150425 -0.01047748 -0.44716352  0.27940097 -0.18933389 -0.47698048\n",
      "  0.05532885 -0.22397476  0.31280631 -0.50147969 -0.04566525  0.09360004\n",
      " -0.14956683  0.44207636  0.77607536 -0.30541712 -0.1053791   0.05604938\n",
      "  0.2744872   0.6012578   0.57477689  0.10893726 -0.0654364   0.02171183\n",
      " -0.39857978 -0.39718673 -0.02273622 -0.29920617  0.11235061  0.08597037\n",
      " -0.18082917 -0.30608249 -0.0391648  -0.32862148  0.69185066 -0.45300376\n",
      "  0.32361874 -0.32215264 -0.48281333 -0.37319317 -0.40621144  0.4914538\n",
      " -0.07246302  0.0785802   0.02798313 -0.12211607]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900,)\n",
      "-2.6490954e-09\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n"
     ]
    }
   ],
   "source": [
    "print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5770, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue')\n",
    "plt.plot(x,y, color='red')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual Energy value')\n",
    "plt.ylabel('Predicted Energy value')\n",
    "plt.title('Actual vs Predicted Energy value for Water molecules')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.2217, -0.7113,  1.3080, -0.2968,  0.9527,  0.3063],\n",
      "        [-0.2508,  0.4203, -0.2437, -0.1268, -0.1178, -0.0207],\n",
      "        [ 0.2447,  0.8685, -1.4775,  0.0635, -1.6685,  0.3466]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.6076,  0.3954,  1.7372], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.3801,  0.1915, -0.3088],\n",
      "        [ 0.0222, -0.3973, -0.1179],\n",
      "        [-0.2653,  0.1705,  0.4331]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.4189,  0.7103, -0.0386], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(net.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
