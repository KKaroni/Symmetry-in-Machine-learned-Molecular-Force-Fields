{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (1000,) entries\n"
     ]
    }
   ],
   "source": [
    "energies = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c38e52de7b08>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #6 (got 1 columns instead of 5)\n",
      "    Line #11 (got 1 columns instead of 5)\n",
      "    Line #16 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #26 (got 1 columns instead of 5)\n",
      "    Line #31 (got 1 columns instead of 5)\n",
      "    Line #36 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #46 (got 1 columns instead of 5)\n",
      "    Line #51 (got 1 columns instead of 5)\n",
      "    Line #56 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #66 (got 1 columns instead of 5)\n",
      "    Line #71 (got 1 columns instead of 5)\n",
      "    Line #76 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #86 (got 1 columns instead of 5)\n",
      "    Line #91 (got 1 columns instead of 5)\n",
      "    Line #96 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #106 (got 1 columns instead of 5)\n",
      "    Line #111 (got 1 columns instead of 5)\n",
      "    Line #116 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #126 (got 1 columns instead of 5)\n",
      "    Line #131 (got 1 columns instead of 5)\n",
      "    Line #136 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #146 (got 1 columns instead of 5)\n",
      "    Line #151 (got 1 columns instead of 5)\n",
      "    Line #156 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #166 (got 1 columns instead of 5)\n",
      "    Line #171 (got 1 columns instead of 5)\n",
      "    Line #176 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #186 (got 1 columns instead of 5)\n",
      "    Line #191 (got 1 columns instead of 5)\n",
      "    Line #196 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #206 (got 1 columns instead of 5)\n",
      "    Line #211 (got 1 columns instead of 5)\n",
      "    Line #216 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #226 (got 1 columns instead of 5)\n",
      "    Line #231 (got 1 columns instead of 5)\n",
      "    Line #236 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #246 (got 1 columns instead of 5)\n",
      "    Line #251 (got 1 columns instead of 5)\n",
      "    Line #256 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #266 (got 1 columns instead of 5)\n",
      "    Line #271 (got 1 columns instead of 5)\n",
      "    Line #276 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #286 (got 1 columns instead of 5)\n",
      "    Line #291 (got 1 columns instead of 5)\n",
      "    Line #296 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #306 (got 1 columns instead of 5)\n",
      "    Line #311 (got 1 columns instead of 5)\n",
      "    Line #316 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #326 (got 1 columns instead of 5)\n",
      "    Line #331 (got 1 columns instead of 5)\n",
      "    Line #336 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #346 (got 1 columns instead of 5)\n",
      "    Line #351 (got 1 columns instead of 5)\n",
      "    Line #356 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #366 (got 1 columns instead of 5)\n",
      "    Line #371 (got 1 columns instead of 5)\n",
      "    Line #376 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #386 (got 1 columns instead of 5)\n",
      "    Line #391 (got 1 columns instead of 5)\n",
      "    Line #396 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #406 (got 1 columns instead of 5)\n",
      "    Line #411 (got 1 columns instead of 5)\n",
      "    Line #416 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #426 (got 1 columns instead of 5)\n",
      "    Line #431 (got 1 columns instead of 5)\n",
      "    Line #436 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #446 (got 1 columns instead of 5)\n",
      "    Line #451 (got 1 columns instead of 5)\n",
      "    Line #456 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #466 (got 1 columns instead of 5)\n",
      "    Line #471 (got 1 columns instead of 5)\n",
      "    Line #476 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #486 (got 1 columns instead of 5)\n",
      "    Line #491 (got 1 columns instead of 5)\n",
      "    Line #496 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #506 (got 1 columns instead of 5)\n",
      "    Line #511 (got 1 columns instead of 5)\n",
      "    Line #516 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #526 (got 1 columns instead of 5)\n",
      "    Line #531 (got 1 columns instead of 5)\n",
      "    Line #536 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #546 (got 1 columns instead of 5)\n",
      "    Line #551 (got 1 columns instead of 5)\n",
      "    Line #556 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #566 (got 1 columns instead of 5)\n",
      "    Line #571 (got 1 columns instead of 5)\n",
      "    Line #576 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #586 (got 1 columns instead of 5)\n",
      "    Line #591 (got 1 columns instead of 5)\n",
      "    Line #596 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #606 (got 1 columns instead of 5)\n",
      "    Line #611 (got 1 columns instead of 5)\n",
      "    Line #616 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #626 (got 1 columns instead of 5)\n",
      "    Line #631 (got 1 columns instead of 5)\n",
      "    Line #636 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #646 (got 1 columns instead of 5)\n",
      "    Line #651 (got 1 columns instead of 5)\n",
      "    Line #656 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #666 (got 1 columns instead of 5)\n",
      "    Line #671 (got 1 columns instead of 5)\n",
      "    Line #676 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #686 (got 1 columns instead of 5)\n",
      "    Line #691 (got 1 columns instead of 5)\n",
      "    Line #696 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #706 (got 1 columns instead of 5)\n",
      "    Line #711 (got 1 columns instead of 5)\n",
      "    Line #716 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #726 (got 1 columns instead of 5)\n",
      "    Line #731 (got 1 columns instead of 5)\n",
      "    Line #736 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #746 (got 1 columns instead of 5)\n",
      "    Line #751 (got 1 columns instead of 5)\n",
      "    Line #756 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #766 (got 1 columns instead of 5)\n",
      "    Line #771 (got 1 columns instead of 5)\n",
      "    Line #776 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #786 (got 1 columns instead of 5)\n",
      "    Line #791 (got 1 columns instead of 5)\n",
      "    Line #796 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #806 (got 1 columns instead of 5)\n",
      "    Line #811 (got 1 columns instead of 5)\n",
      "    Line #816 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #826 (got 1 columns instead of 5)\n",
      "    Line #831 (got 1 columns instead of 5)\n",
      "    Line #836 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #846 (got 1 columns instead of 5)\n",
      "    Line #851 (got 1 columns instead of 5)\n",
      "    Line #856 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #866 (got 1 columns instead of 5)\n",
      "    Line #871 (got 1 columns instead of 5)\n",
      "    Line #876 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #886 (got 1 columns instead of 5)\n",
      "    Line #891 (got 1 columns instead of 5)\n",
      "    Line #896 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #906 (got 1 columns instead of 5)\n",
      "    Line #911 (got 1 columns instead of 5)\n",
      "    Line #916 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #926 (got 1 columns instead of 5)\n",
      "    Line #931 (got 1 columns instead of 5)\n",
      "    Line #936 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #946 (got 1 columns instead of 5)\n",
      "    Line #951 (got 1 columns instead of 5)\n",
      "    Line #956 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #966 (got 1 columns instead of 5)\n",
      "    Line #971 (got 1 columns instead of 5)\n",
      "    Line #976 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #986 (got 1 columns instead of 5)\n",
      "    Line #991 (got 1 columns instead of 5)\n",
      "    Line #996 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1006 (got 1 columns instead of 5)\n",
      "    Line #1011 (got 1 columns instead of 5)\n",
      "    Line #1016 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1026 (got 1 columns instead of 5)\n",
      "    Line #1031 (got 1 columns instead of 5)\n",
      "    Line #1036 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1046 (got 1 columns instead of 5)\n",
      "    Line #1051 (got 1 columns instead of 5)\n",
      "    Line #1056 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1066 (got 1 columns instead of 5)\n",
      "    Line #1071 (got 1 columns instead of 5)\n",
      "    Line #1076 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1086 (got 1 columns instead of 5)\n",
      "    Line #1091 (got 1 columns instead of 5)\n",
      "    Line #1096 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1106 (got 1 columns instead of 5)\n",
      "    Line #1111 (got 1 columns instead of 5)\n",
      "    Line #1116 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1126 (got 1 columns instead of 5)\n",
      "    Line #1131 (got 1 columns instead of 5)\n",
      "    Line #1136 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1146 (got 1 columns instead of 5)\n",
      "    Line #1151 (got 1 columns instead of 5)\n",
      "    Line #1156 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1166 (got 1 columns instead of 5)\n",
      "    Line #1171 (got 1 columns instead of 5)\n",
      "    Line #1176 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1186 (got 1 columns instead of 5)\n",
      "    Line #1191 (got 1 columns instead of 5)\n",
      "    Line #1196 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1206 (got 1 columns instead of 5)\n",
      "    Line #1211 (got 1 columns instead of 5)\n",
      "    Line #1216 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1226 (got 1 columns instead of 5)\n",
      "    Line #1231 (got 1 columns instead of 5)\n",
      "    Line #1236 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1246 (got 1 columns instead of 5)\n",
      "    Line #1251 (got 1 columns instead of 5)\n",
      "    Line #1256 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1266 (got 1 columns instead of 5)\n",
      "    Line #1271 (got 1 columns instead of 5)\n",
      "    Line #1276 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1286 (got 1 columns instead of 5)\n",
      "    Line #1291 (got 1 columns instead of 5)\n",
      "    Line #1296 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1306 (got 1 columns instead of 5)\n",
      "    Line #1311 (got 1 columns instead of 5)\n",
      "    Line #1316 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1326 (got 1 columns instead of 5)\n",
      "    Line #1331 (got 1 columns instead of 5)\n",
      "    Line #1336 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1346 (got 1 columns instead of 5)\n",
      "    Line #1351 (got 1 columns instead of 5)\n",
      "    Line #1356 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1366 (got 1 columns instead of 5)\n",
      "    Line #1371 (got 1 columns instead of 5)\n",
      "    Line #1376 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1386 (got 1 columns instead of 5)\n",
      "    Line #1391 (got 1 columns instead of 5)\n",
      "    Line #1396 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1406 (got 1 columns instead of 5)\n",
      "    Line #1411 (got 1 columns instead of 5)\n",
      "    Line #1416 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1426 (got 1 columns instead of 5)\n",
      "    Line #1431 (got 1 columns instead of 5)\n",
      "    Line #1436 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1446 (got 1 columns instead of 5)\n",
      "    Line #1451 (got 1 columns instead of 5)\n",
      "    Line #1456 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1466 (got 1 columns instead of 5)\n",
      "    Line #1471 (got 1 columns instead of 5)\n",
      "    Line #1476 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1486 (got 1 columns instead of 5)\n",
      "    Line #1491 (got 1 columns instead of 5)\n",
      "    Line #1496 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1506 (got 1 columns instead of 5)\n",
      "    Line #1511 (got 1 columns instead of 5)\n",
      "    Line #1516 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1526 (got 1 columns instead of 5)\n",
      "    Line #1531 (got 1 columns instead of 5)\n",
      "    Line #1536 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1546 (got 1 columns instead of 5)\n",
      "    Line #1551 (got 1 columns instead of 5)\n",
      "    Line #1556 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1566 (got 1 columns instead of 5)\n",
      "    Line #1571 (got 1 columns instead of 5)\n",
      "    Line #1576 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1586 (got 1 columns instead of 5)\n",
      "    Line #1591 (got 1 columns instead of 5)\n",
      "    Line #1596 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1606 (got 1 columns instead of 5)\n",
      "    Line #1611 (got 1 columns instead of 5)\n",
      "    Line #1616 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1626 (got 1 columns instead of 5)\n",
      "    Line #1631 (got 1 columns instead of 5)\n",
      "    Line #1636 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1646 (got 1 columns instead of 5)\n",
      "    Line #1651 (got 1 columns instead of 5)\n",
      "    Line #1656 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1666 (got 1 columns instead of 5)\n",
      "    Line #1671 (got 1 columns instead of 5)\n",
      "    Line #1676 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1686 (got 1 columns instead of 5)\n",
      "    Line #1691 (got 1 columns instead of 5)\n",
      "    Line #1696 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1706 (got 1 columns instead of 5)\n",
      "    Line #1711 (got 1 columns instead of 5)\n",
      "    Line #1716 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1726 (got 1 columns instead of 5)\n",
      "    Line #1731 (got 1 columns instead of 5)\n",
      "    Line #1736 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1746 (got 1 columns instead of 5)\n",
      "    Line #1751 (got 1 columns instead of 5)\n",
      "    Line #1756 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1766 (got 1 columns instead of 5)\n",
      "    Line #1771 (got 1 columns instead of 5)\n",
      "    Line #1776 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1786 (got 1 columns instead of 5)\n",
      "    Line #1791 (got 1 columns instead of 5)\n",
      "    Line #1796 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1806 (got 1 columns instead of 5)\n",
      "    Line #1811 (got 1 columns instead of 5)\n",
      "    Line #1816 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1826 (got 1 columns instead of 5)\n",
      "    Line #1831 (got 1 columns instead of 5)\n",
      "    Line #1836 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1846 (got 1 columns instead of 5)\n",
      "    Line #1851 (got 1 columns instead of 5)\n",
      "    Line #1856 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1866 (got 1 columns instead of 5)\n",
      "    Line #1871 (got 1 columns instead of 5)\n",
      "    Line #1876 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1886 (got 1 columns instead of 5)\n",
      "    Line #1891 (got 1 columns instead of 5)\n",
      "    Line #1896 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1906 (got 1 columns instead of 5)\n",
      "    Line #1911 (got 1 columns instead of 5)\n",
      "    Line #1916 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1926 (got 1 columns instead of 5)\n",
      "    Line #1931 (got 1 columns instead of 5)\n",
      "    Line #1936 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1946 (got 1 columns instead of 5)\n",
      "    Line #1951 (got 1 columns instead of 5)\n",
      "    Line #1956 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1966 (got 1 columns instead of 5)\n",
      "    Line #1971 (got 1 columns instead of 5)\n",
      "    Line #1976 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1986 (got 1 columns instead of 5)\n",
      "    Line #1991 (got 1 columns instead of 5)\n",
      "    Line #1996 (got 1 columns instead of 5)\n",
      "    Line #2001 (got 1 columns instead of 5)\n",
      "    Line #2006 (got 1 columns instead of 5)\n",
      "    Line #2011 (got 1 columns instead of 5)\n",
      "    Line #2016 (got 1 columns instead of 5)\n",
      "    Line #2021 (got 1 columns instead of 5)\n",
      "    Line #2026 (got 1 columns instead of 5)\n",
      "    Line #2031 (got 1 columns instead of 5)\n",
      "    Line #2036 (got 1 columns instead of 5)\n",
      "    Line #2041 (got 1 columns instead of 5)\n",
      "    Line #2046 (got 1 columns instead of 5)\n",
      "    Line #2051 (got 1 columns instead of 5)\n",
      "    Line #2056 (got 1 columns instead of 5)\n",
      "    Line #2061 (got 1 columns instead of 5)\n",
      "    Line #2066 (got 1 columns instead of 5)\n",
      "    Line #2071 (got 1 columns instead of 5)\n",
      "    Line #2076 (got 1 columns instead of 5)\n",
      "    Line #2081 (got 1 columns instead of 5)\n",
      "    Line #2086 (got 1 columns instead of 5)\n",
      "    Line #2091 (got 1 columns instead of 5)\n",
      "    Line #2096 (got 1 columns instead of 5)\n",
      "    Line #2101 (got 1 columns instead of 5)\n",
      "    Line #2106 (got 1 columns instead of 5)\n",
      "    Line #2111 (got 1 columns instead of 5)\n",
      "    Line #2116 (got 1 columns instead of 5)\n",
      "    Line #2121 (got 1 columns instead of 5)\n",
      "    Line #2126 (got 1 columns instead of 5)\n",
      "    Line #2131 (got 1 columns instead of 5)\n",
      "    Line #2136 (got 1 columns instead of 5)\n",
      "    Line #2141 (got 1 columns instead of 5)\n",
      "    Line #2146 (got 1 columns instead of 5)\n",
      "    Line #2151 (got 1 columns instead of 5)\n",
      "    Line #2156 (got 1 columns instead of 5)\n",
      "    Line #2161 (got 1 columns instead of 5)\n",
      "    Line #2166 (got 1 columns instead of 5)\n",
      "    Line #2171 (got 1 columns instead of 5)\n",
      "    Line #2176 (got 1 columns instead of 5)\n",
      "    Line #2181 (got 1 columns instead of 5)\n",
      "    Line #2186 (got 1 columns instead of 5)\n",
      "    Line #2191 (got 1 columns instead of 5)\n",
      "    Line #2196 (got 1 columns instead of 5)\n",
      "    Line #2201 (got 1 columns instead of 5)\n",
      "    Line #2206 (got 1 columns instead of 5)\n",
      "    Line #2211 (got 1 columns instead of 5)\n",
      "    Line #2216 (got 1 columns instead of 5)\n",
      "    Line #2221 (got 1 columns instead of 5)\n",
      "    Line #2226 (got 1 columns instead of 5)\n",
      "    Line #2231 (got 1 columns instead of 5)\n",
      "    Line #2236 (got 1 columns instead of 5)\n",
      "    Line #2241 (got 1 columns instead of 5)\n",
      "    Line #2246 (got 1 columns instead of 5)\n",
      "    Line #2251 (got 1 columns instead of 5)\n",
      "    Line #2256 (got 1 columns instead of 5)\n",
      "    Line #2261 (got 1 columns instead of 5)\n",
      "    Line #2266 (got 1 columns instead of 5)\n",
      "    Line #2271 (got 1 columns instead of 5)\n",
      "    Line #2276 (got 1 columns instead of 5)\n",
      "    Line #2281 (got 1 columns instead of 5)\n",
      "    Line #2286 (got 1 columns instead of 5)\n",
      "    Line #2291 (got 1 columns instead of 5)\n",
      "    Line #2296 (got 1 columns instead of 5)\n",
      "    Line #2301 (got 1 columns instead of 5)\n",
      "    Line #2306 (got 1 columns instead of 5)\n",
      "    Line #2311 (got 1 columns instead of 5)\n",
      "    Line #2316 (got 1 columns instead of 5)\n",
      "    Line #2321 (got 1 columns instead of 5)\n",
      "    Line #2326 (got 1 columns instead of 5)\n",
      "    Line #2331 (got 1 columns instead of 5)\n",
      "    Line #2336 (got 1 columns instead of 5)\n",
      "    Line #2341 (got 1 columns instead of 5)\n",
      "    Line #2346 (got 1 columns instead of 5)\n",
      "    Line #2351 (got 1 columns instead of 5)\n",
      "    Line #2356 (got 1 columns instead of 5)\n",
      "    Line #2361 (got 1 columns instead of 5)\n",
      "    Line #2366 (got 1 columns instead of 5)\n",
      "    Line #2371 (got 1 columns instead of 5)\n",
      "    Line #2376 (got 1 columns instead of 5)\n",
      "    Line #2381 (got 1 columns instead of 5)\n",
      "    Line #2386 (got 1 columns instead of 5)\n",
      "    Line #2391 (got 1 columns instead of 5)\n",
      "    Line #2396 (got 1 columns instead of 5)\n",
      "    Line #2401 (got 1 columns instead of 5)\n",
      "    Line #2406 (got 1 columns instead of 5)\n",
      "    Line #2411 (got 1 columns instead of 5)\n",
      "    Line #2416 (got 1 columns instead of 5)\n",
      "    Line #2421 (got 1 columns instead of 5)\n",
      "    Line #2426 (got 1 columns instead of 5)\n",
      "    Line #2431 (got 1 columns instead of 5)\n",
      "    Line #2436 (got 1 columns instead of 5)\n",
      "    Line #2441 (got 1 columns instead of 5)\n",
      "    Line #2446 (got 1 columns instead of 5)\n",
      "    Line #2451 (got 1 columns instead of 5)\n",
      "    Line #2456 (got 1 columns instead of 5)\n",
      "    Line #2461 (got 1 columns instead of 5)\n",
      "    Line #2466 (got 1 columns instead of 5)\n",
      "    Line #2471 (got 1 columns instead of 5)\n",
      "    Line #2476 (got 1 columns instead of 5)\n",
      "    Line #2481 (got 1 columns instead of 5)\n",
      "    Line #2486 (got 1 columns instead of 5)\n",
      "    Line #2491 (got 1 columns instead of 5)\n",
      "    Line #2496 (got 1 columns instead of 5)\n",
      "    Line #2501 (got 1 columns instead of 5)\n",
      "    Line #2506 (got 1 columns instead of 5)\n",
      "    Line #2511 (got 1 columns instead of 5)\n",
      "    Line #2516 (got 1 columns instead of 5)\n",
      "    Line #2521 (got 1 columns instead of 5)\n",
      "    Line #2526 (got 1 columns instead of 5)\n",
      "    Line #2531 (got 1 columns instead of 5)\n",
      "    Line #2536 (got 1 columns instead of 5)\n",
      "    Line #2541 (got 1 columns instead of 5)\n",
      "    Line #2546 (got 1 columns instead of 5)\n",
      "    Line #2551 (got 1 columns instead of 5)\n",
      "    Line #2556 (got 1 columns instead of 5)\n",
      "    Line #2561 (got 1 columns instead of 5)\n",
      "    Line #2566 (got 1 columns instead of 5)\n",
      "    Line #2571 (got 1 columns instead of 5)\n",
      "    Line #2576 (got 1 columns instead of 5)\n",
      "    Line #2581 (got 1 columns instead of 5)\n",
      "    Line #2586 (got 1 columns instead of 5)\n",
      "    Line #2591 (got 1 columns instead of 5)\n",
      "    Line #2596 (got 1 columns instead of 5)\n",
      "    Line #2601 (got 1 columns instead of 5)\n",
      "    Line #2606 (got 1 columns instead of 5)\n",
      "    Line #2611 (got 1 columns instead of 5)\n",
      "    Line #2616 (got 1 columns instead of 5)\n",
      "    Line #2621 (got 1 columns instead of 5)\n",
      "    Line #2626 (got 1 columns instead of 5)\n",
      "    Line #2631 (got 1 columns instead of 5)\n",
      "    Line #2636 (got 1 columns instead of 5)\n",
      "    Line #2641 (got 1 columns instead of 5)\n",
      "    Line #2646 (got 1 columns instead of 5)\n",
      "    Line #2651 (got 1 columns instead of 5)\n",
      "    Line #2656 (got 1 columns instead of 5)\n",
      "    Line #2661 (got 1 columns instead of 5)\n",
      "    Line #2666 (got 1 columns instead of 5)\n",
      "    Line #2671 (got 1 columns instead of 5)\n",
      "    Line #2676 (got 1 columns instead of 5)\n",
      "    Line #2681 (got 1 columns instead of 5)\n",
      "    Line #2686 (got 1 columns instead of 5)\n",
      "    Line #2691 (got 1 columns instead of 5)\n",
      "    Line #2696 (got 1 columns instead of 5)\n",
      "    Line #2701 (got 1 columns instead of 5)\n",
      "    Line #2706 (got 1 columns instead of 5)\n",
      "    Line #2711 (got 1 columns instead of 5)\n",
      "    Line #2716 (got 1 columns instead of 5)\n",
      "    Line #2721 (got 1 columns instead of 5)\n",
      "    Line #2726 (got 1 columns instead of 5)\n",
      "    Line #2731 (got 1 columns instead of 5)\n",
      "    Line #2736 (got 1 columns instead of 5)\n",
      "    Line #2741 (got 1 columns instead of 5)\n",
      "    Line #2746 (got 1 columns instead of 5)\n",
      "    Line #2751 (got 1 columns instead of 5)\n",
      "    Line #2756 (got 1 columns instead of 5)\n",
      "    Line #2761 (got 1 columns instead of 5)\n",
      "    Line #2766 (got 1 columns instead of 5)\n",
      "    Line #2771 (got 1 columns instead of 5)\n",
      "    Line #2776 (got 1 columns instead of 5)\n",
      "    Line #2781 (got 1 columns instead of 5)\n",
      "    Line #2786 (got 1 columns instead of 5)\n",
      "    Line #2791 (got 1 columns instead of 5)\n",
      "    Line #2796 (got 1 columns instead of 5)\n",
      "    Line #2801 (got 1 columns instead of 5)\n",
      "    Line #2806 (got 1 columns instead of 5)\n",
      "    Line #2811 (got 1 columns instead of 5)\n",
      "    Line #2816 (got 1 columns instead of 5)\n",
      "    Line #2821 (got 1 columns instead of 5)\n",
      "    Line #2826 (got 1 columns instead of 5)\n",
      "    Line #2831 (got 1 columns instead of 5)\n",
      "    Line #2836 (got 1 columns instead of 5)\n",
      "    Line #2841 (got 1 columns instead of 5)\n",
      "    Line #2846 (got 1 columns instead of 5)\n",
      "    Line #2851 (got 1 columns instead of 5)\n",
      "    Line #2856 (got 1 columns instead of 5)\n",
      "    Line #2861 (got 1 columns instead of 5)\n",
      "    Line #2866 (got 1 columns instead of 5)\n",
      "    Line #2871 (got 1 columns instead of 5)\n",
      "    Line #2876 (got 1 columns instead of 5)\n",
      "    Line #2881 (got 1 columns instead of 5)\n",
      "    Line #2886 (got 1 columns instead of 5)\n",
      "    Line #2891 (got 1 columns instead of 5)\n",
      "    Line #2896 (got 1 columns instead of 5)\n",
      "    Line #2901 (got 1 columns instead of 5)\n",
      "    Line #2906 (got 1 columns instead of 5)\n",
      "    Line #2911 (got 1 columns instead of 5)\n",
      "    Line #2916 (got 1 columns instead of 5)\n",
      "    Line #2921 (got 1 columns instead of 5)\n",
      "    Line #2926 (got 1 columns instead of 5)\n",
      "    Line #2931 (got 1 columns instead of 5)\n",
      "    Line #2936 (got 1 columns instead of 5)\n",
      "    Line #2941 (got 1 columns instead of 5)\n",
      "    Line #2946 (got 1 columns instead of 5)\n",
      "    Line #2951 (got 1 columns instead of 5)\n",
      "    Line #2956 (got 1 columns instead of 5)\n",
      "    Line #2961 (got 1 columns instead of 5)\n",
      "    Line #2966 (got 1 columns instead of 5)\n",
      "    Line #2971 (got 1 columns instead of 5)\n",
      "    Line #2976 (got 1 columns instead of 5)\n",
      "    Line #2981 (got 1 columns instead of 5)\n",
      "    Line #2986 (got 1 columns instead of 5)\n",
      "    Line #2991 (got 1 columns instead of 5)\n",
      "    Line #2996 (got 1 columns instead of 5)\n",
      "    Line #3001 (got 1 columns instead of 5)\n",
      "    Line #3006 (got 1 columns instead of 5)\n",
      "    Line #3011 (got 1 columns instead of 5)\n",
      "    Line #3016 (got 1 columns instead of 5)\n",
      "    Line #3021 (got 1 columns instead of 5)\n",
      "    Line #3026 (got 1 columns instead of 5)\n",
      "    Line #3031 (got 1 columns instead of 5)\n",
      "    Line #3036 (got 1 columns instead of 5)\n",
      "    Line #3041 (got 1 columns instead of 5)\n",
      "    Line #3046 (got 1 columns instead of 5)\n",
      "    Line #3051 (got 1 columns instead of 5)\n",
      "    Line #3056 (got 1 columns instead of 5)\n",
      "    Line #3061 (got 1 columns instead of 5)\n",
      "    Line #3066 (got 1 columns instead of 5)\n",
      "    Line #3071 (got 1 columns instead of 5)\n",
      "    Line #3076 (got 1 columns instead of 5)\n",
      "    Line #3081 (got 1 columns instead of 5)\n",
      "    Line #3086 (got 1 columns instead of 5)\n",
      "    Line #3091 (got 1 columns instead of 5)\n",
      "    Line #3096 (got 1 columns instead of 5)\n",
      "    Line #3101 (got 1 columns instead of 5)\n",
      "    Line #3106 (got 1 columns instead of 5)\n",
      "    Line #3111 (got 1 columns instead of 5)\n",
      "    Line #3116 (got 1 columns instead of 5)\n",
      "    Line #3121 (got 1 columns instead of 5)\n",
      "    Line #3126 (got 1 columns instead of 5)\n",
      "    Line #3131 (got 1 columns instead of 5)\n",
      "    Line #3136 (got 1 columns instead of 5)\n",
      "    Line #3141 (got 1 columns instead of 5)\n",
      "    Line #3146 (got 1 columns instead of 5)\n",
      "    Line #3151 (got 1 columns instead of 5)\n",
      "    Line #3156 (got 1 columns instead of 5)\n",
      "    Line #3161 (got 1 columns instead of 5)\n",
      "    Line #3166 (got 1 columns instead of 5)\n",
      "    Line #3171 (got 1 columns instead of 5)\n",
      "    Line #3176 (got 1 columns instead of 5)\n",
      "    Line #3181 (got 1 columns instead of 5)\n",
      "    Line #3186 (got 1 columns instead of 5)\n",
      "    Line #3191 (got 1 columns instead of 5)\n",
      "    Line #3196 (got 1 columns instead of 5)\n",
      "    Line #3201 (got 1 columns instead of 5)\n",
      "    Line #3206 (got 1 columns instead of 5)\n",
      "    Line #3211 (got 1 columns instead of 5)\n",
      "    Line #3216 (got 1 columns instead of 5)\n",
      "    Line #3221 (got 1 columns instead of 5)\n",
      "    Line #3226 (got 1 columns instead of 5)\n",
      "    Line #3231 (got 1 columns instead of 5)\n",
      "    Line #3236 (got 1 columns instead of 5)\n",
      "    Line #3241 (got 1 columns instead of 5)\n",
      "    Line #3246 (got 1 columns instead of 5)\n",
      "    Line #3251 (got 1 columns instead of 5)\n",
      "    Line #3256 (got 1 columns instead of 5)\n",
      "    Line #3261 (got 1 columns instead of 5)\n",
      "    Line #3266 (got 1 columns instead of 5)\n",
      "    Line #3271 (got 1 columns instead of 5)\n",
      "    Line #3276 (got 1 columns instead of 5)\n",
      "    Line #3281 (got 1 columns instead of 5)\n",
      "    Line #3286 (got 1 columns instead of 5)\n",
      "    Line #3291 (got 1 columns instead of 5)\n",
      "    Line #3296 (got 1 columns instead of 5)\n",
      "    Line #3301 (got 1 columns instead of 5)\n",
      "    Line #3306 (got 1 columns instead of 5)\n",
      "    Line #3311 (got 1 columns instead of 5)\n",
      "    Line #3316 (got 1 columns instead of 5)\n",
      "    Line #3321 (got 1 columns instead of 5)\n",
      "    Line #3326 (got 1 columns instead of 5)\n",
      "    Line #3331 (got 1 columns instead of 5)\n",
      "    Line #3336 (got 1 columns instead of 5)\n",
      "    Line #3341 (got 1 columns instead of 5)\n",
      "    Line #3346 (got 1 columns instead of 5)\n",
      "    Line #3351 (got 1 columns instead of 5)\n",
      "    Line #3356 (got 1 columns instead of 5)\n",
      "    Line #3361 (got 1 columns instead of 5)\n",
      "    Line #3366 (got 1 columns instead of 5)\n",
      "    Line #3371 (got 1 columns instead of 5)\n",
      "    Line #3376 (got 1 columns instead of 5)\n",
      "    Line #3381 (got 1 columns instead of 5)\n",
      "    Line #3386 (got 1 columns instead of 5)\n",
      "    Line #3391 (got 1 columns instead of 5)\n",
      "    Line #3396 (got 1 columns instead of 5)\n",
      "    Line #3401 (got 1 columns instead of 5)\n",
      "    Line #3406 (got 1 columns instead of 5)\n",
      "    Line #3411 (got 1 columns instead of 5)\n",
      "    Line #3416 (got 1 columns instead of 5)\n",
      "    Line #3421 (got 1 columns instead of 5)\n",
      "    Line #3426 (got 1 columns instead of 5)\n",
      "    Line #3431 (got 1 columns instead of 5)\n",
      "    Line #3436 (got 1 columns instead of 5)\n",
      "    Line #3441 (got 1 columns instead of 5)\n",
      "    Line #3446 (got 1 columns instead of 5)\n",
      "    Line #3451 (got 1 columns instead of 5)\n",
      "    Line #3456 (got 1 columns instead of 5)\n",
      "    Line #3461 (got 1 columns instead of 5)\n",
      "    Line #3466 (got 1 columns instead of 5)\n",
      "    Line #3471 (got 1 columns instead of 5)\n",
      "    Line #3476 (got 1 columns instead of 5)\n",
      "    Line #3481 (got 1 columns instead of 5)\n",
      "    Line #3486 (got 1 columns instead of 5)\n",
      "    Line #3491 (got 1 columns instead of 5)\n",
      "    Line #3496 (got 1 columns instead of 5)\n",
      "    Line #3501 (got 1 columns instead of 5)\n",
      "    Line #3506 (got 1 columns instead of 5)\n",
      "    Line #3511 (got 1 columns instead of 5)\n",
      "    Line #3516 (got 1 columns instead of 5)\n",
      "    Line #3521 (got 1 columns instead of 5)\n",
      "    Line #3526 (got 1 columns instead of 5)\n",
      "    Line #3531 (got 1 columns instead of 5)\n",
      "    Line #3536 (got 1 columns instead of 5)\n",
      "    Line #3541 (got 1 columns instead of 5)\n",
      "    Line #3546 (got 1 columns instead of 5)\n",
      "    Line #3551 (got 1 columns instead of 5)\n",
      "    Line #3556 (got 1 columns instead of 5)\n",
      "    Line #3561 (got 1 columns instead of 5)\n",
      "    Line #3566 (got 1 columns instead of 5)\n",
      "    Line #3571 (got 1 columns instead of 5)\n",
      "    Line #3576 (got 1 columns instead of 5)\n",
      "    Line #3581 (got 1 columns instead of 5)\n",
      "    Line #3586 (got 1 columns instead of 5)\n",
      "    Line #3591 (got 1 columns instead of 5)\n",
      "    Line #3596 (got 1 columns instead of 5)\n",
      "    Line #3601 (got 1 columns instead of 5)\n",
      "    Line #3606 (got 1 columns instead of 5)\n",
      "    Line #3611 (got 1 columns instead of 5)\n",
      "    Line #3616 (got 1 columns instead of 5)\n",
      "    Line #3621 (got 1 columns instead of 5)\n",
      "    Line #3626 (got 1 columns instead of 5)\n",
      "    Line #3631 (got 1 columns instead of 5)\n",
      "    Line #3636 (got 1 columns instead of 5)\n",
      "    Line #3641 (got 1 columns instead of 5)\n",
      "    Line #3646 (got 1 columns instead of 5)\n",
      "    Line #3651 (got 1 columns instead of 5)\n",
      "    Line #3656 (got 1 columns instead of 5)\n",
      "    Line #3661 (got 1 columns instead of 5)\n",
      "    Line #3666 (got 1 columns instead of 5)\n",
      "    Line #3671 (got 1 columns instead of 5)\n",
      "    Line #3676 (got 1 columns instead of 5)\n",
      "    Line #3681 (got 1 columns instead of 5)\n",
      "    Line #3686 (got 1 columns instead of 5)\n",
      "    Line #3691 (got 1 columns instead of 5)\n",
      "    Line #3696 (got 1 columns instead of 5)\n",
      "    Line #3701 (got 1 columns instead of 5)\n",
      "    Line #3706 (got 1 columns instead of 5)\n",
      "    Line #3711 (got 1 columns instead of 5)\n",
      "    Line #3716 (got 1 columns instead of 5)\n",
      "    Line #3721 (got 1 columns instead of 5)\n",
      "    Line #3726 (got 1 columns instead of 5)\n",
      "    Line #3731 (got 1 columns instead of 5)\n",
      "    Line #3736 (got 1 columns instead of 5)\n",
      "    Line #3741 (got 1 columns instead of 5)\n",
      "    Line #3746 (got 1 columns instead of 5)\n",
      "    Line #3751 (got 1 columns instead of 5)\n",
      "    Line #3756 (got 1 columns instead of 5)\n",
      "    Line #3761 (got 1 columns instead of 5)\n",
      "    Line #3766 (got 1 columns instead of 5)\n",
      "    Line #3771 (got 1 columns instead of 5)\n",
      "    Line #3776 (got 1 columns instead of 5)\n",
      "    Line #3781 (got 1 columns instead of 5)\n",
      "    Line #3786 (got 1 columns instead of 5)\n",
      "    Line #3791 (got 1 columns instead of 5)\n",
      "    Line #3796 (got 1 columns instead of 5)\n",
      "    Line #3801 (got 1 columns instead of 5)\n",
      "    Line #3806 (got 1 columns instead of 5)\n",
      "    Line #3811 (got 1 columns instead of 5)\n",
      "    Line #3816 (got 1 columns instead of 5)\n",
      "    Line #3821 (got 1 columns instead of 5)\n",
      "    Line #3826 (got 1 columns instead of 5)\n",
      "    Line #3831 (got 1 columns instead of 5)\n",
      "    Line #3836 (got 1 columns instead of 5)\n",
      "    Line #3841 (got 1 columns instead of 5)\n",
      "    Line #3846 (got 1 columns instead of 5)\n",
      "    Line #3851 (got 1 columns instead of 5)\n",
      "    Line #3856 (got 1 columns instead of 5)\n",
      "    Line #3861 (got 1 columns instead of 5)\n",
      "    Line #3866 (got 1 columns instead of 5)\n",
      "    Line #3871 (got 1 columns instead of 5)\n",
      "    Line #3876 (got 1 columns instead of 5)\n",
      "    Line #3881 (got 1 columns instead of 5)\n",
      "    Line #3886 (got 1 columns instead of 5)\n",
      "    Line #3891 (got 1 columns instead of 5)\n",
      "    Line #3896 (got 1 columns instead of 5)\n",
      "    Line #3901 (got 1 columns instead of 5)\n",
      "    Line #3906 (got 1 columns instead of 5)\n",
      "    Line #3911 (got 1 columns instead of 5)\n",
      "    Line #3916 (got 1 columns instead of 5)\n",
      "    Line #3921 (got 1 columns instead of 5)\n",
      "    Line #3926 (got 1 columns instead of 5)\n",
      "    Line #3931 (got 1 columns instead of 5)\n",
      "    Line #3936 (got 1 columns instead of 5)\n",
      "    Line #3941 (got 1 columns instead of 5)\n",
      "    Line #3946 (got 1 columns instead of 5)\n",
      "    Line #3951 (got 1 columns instead of 5)\n",
      "    Line #3956 (got 1 columns instead of 5)\n",
      "    Line #3961 (got 1 columns instead of 5)\n",
      "    Line #3966 (got 1 columns instead of 5)\n",
      "    Line #3971 (got 1 columns instead of 5)\n",
      "    Line #3976 (got 1 columns instead of 5)\n",
      "    Line #3981 (got 1 columns instead of 5)\n",
      "    Line #3986 (got 1 columns instead of 5)\n",
      "    Line #3991 (got 1 columns instead of 5)\n",
      "    Line #3996 (got 1 columns instead of 5)\n",
      "    Line #4001 (got 1 columns instead of 5)\n",
      "    Line #4006 (got 1 columns instead of 5)\n",
      "    Line #4011 (got 1 columns instead of 5)\n",
      "    Line #4016 (got 1 columns instead of 5)\n",
      "    Line #4021 (got 1 columns instead of 5)\n",
      "    Line #4026 (got 1 columns instead of 5)\n",
      "    Line #4031 (got 1 columns instead of 5)\n",
      "    Line #4036 (got 1 columns instead of 5)\n",
      "    Line #4041 (got 1 columns instead of 5)\n",
      "    Line #4046 (got 1 columns instead of 5)\n",
      "    Line #4051 (got 1 columns instead of 5)\n",
      "    Line #4056 (got 1 columns instead of 5)\n",
      "    Line #4061 (got 1 columns instead of 5)\n",
      "    Line #4066 (got 1 columns instead of 5)\n",
      "    Line #4071 (got 1 columns instead of 5)\n",
      "    Line #4076 (got 1 columns instead of 5)\n",
      "    Line #4081 (got 1 columns instead of 5)\n",
      "    Line #4086 (got 1 columns instead of 5)\n",
      "    Line #4091 (got 1 columns instead of 5)\n",
      "    Line #4096 (got 1 columns instead of 5)\n",
      "    Line #4101 (got 1 columns instead of 5)\n",
      "    Line #4106 (got 1 columns instead of 5)\n",
      "    Line #4111 (got 1 columns instead of 5)\n",
      "    Line #4116 (got 1 columns instead of 5)\n",
      "    Line #4121 (got 1 columns instead of 5)\n",
      "    Line #4126 (got 1 columns instead of 5)\n",
      "    Line #4131 (got 1 columns instead of 5)\n",
      "    Line #4136 (got 1 columns instead of 5)\n",
      "    Line #4141 (got 1 columns instead of 5)\n",
      "    Line #4146 (got 1 columns instead of 5)\n",
      "    Line #4151 (got 1 columns instead of 5)\n",
      "    Line #4156 (got 1 columns instead of 5)\n",
      "    Line #4161 (got 1 columns instead of 5)\n",
      "    Line #4166 (got 1 columns instead of 5)\n",
      "    Line #4171 (got 1 columns instead of 5)\n",
      "    Line #4176 (got 1 columns instead of 5)\n",
      "    Line #4181 (got 1 columns instead of 5)\n",
      "    Line #4186 (got 1 columns instead of 5)\n",
      "    Line #4191 (got 1 columns instead of 5)\n",
      "    Line #4196 (got 1 columns instead of 5)\n",
      "    Line #4201 (got 1 columns instead of 5)\n",
      "    Line #4206 (got 1 columns instead of 5)\n",
      "    Line #4211 (got 1 columns instead of 5)\n",
      "    Line #4216 (got 1 columns instead of 5)\n",
      "    Line #4221 (got 1 columns instead of 5)\n",
      "    Line #4226 (got 1 columns instead of 5)\n",
      "    Line #4231 (got 1 columns instead of 5)\n",
      "    Line #4236 (got 1 columns instead of 5)\n",
      "    Line #4241 (got 1 columns instead of 5)\n",
      "    Line #4246 (got 1 columns instead of 5)\n",
      "    Line #4251 (got 1 columns instead of 5)\n",
      "    Line #4256 (got 1 columns instead of 5)\n",
      "    Line #4261 (got 1 columns instead of 5)\n",
      "    Line #4266 (got 1 columns instead of 5)\n",
      "    Line #4271 (got 1 columns instead of 5)\n",
      "    Line #4276 (got 1 columns instead of 5)\n",
      "    Line #4281 (got 1 columns instead of 5)\n",
      "    Line #4286 (got 1 columns instead of 5)\n",
      "    Line #4291 (got 1 columns instead of 5)\n",
      "    Line #4296 (got 1 columns instead of 5)\n",
      "    Line #4301 (got 1 columns instead of 5)\n",
      "    Line #4306 (got 1 columns instead of 5)\n",
      "    Line #4311 (got 1 columns instead of 5)\n",
      "    Line #4316 (got 1 columns instead of 5)\n",
      "    Line #4321 (got 1 columns instead of 5)\n",
      "    Line #4326 (got 1 columns instead of 5)\n",
      "    Line #4331 (got 1 columns instead of 5)\n",
      "    Line #4336 (got 1 columns instead of 5)\n",
      "    Line #4341 (got 1 columns instead of 5)\n",
      "    Line #4346 (got 1 columns instead of 5)\n",
      "    Line #4351 (got 1 columns instead of 5)\n",
      "    Line #4356 (got 1 columns instead of 5)\n",
      "    Line #4361 (got 1 columns instead of 5)\n",
      "    Line #4366 (got 1 columns instead of 5)\n",
      "    Line #4371 (got 1 columns instead of 5)\n",
      "    Line #4376 (got 1 columns instead of 5)\n",
      "    Line #4381 (got 1 columns instead of 5)\n",
      "    Line #4386 (got 1 columns instead of 5)\n",
      "    Line #4391 (got 1 columns instead of 5)\n",
      "    Line #4396 (got 1 columns instead of 5)\n",
      "    Line #4401 (got 1 columns instead of 5)\n",
      "    Line #4406 (got 1 columns instead of 5)\n",
      "    Line #4411 (got 1 columns instead of 5)\n",
      "    Line #4416 (got 1 columns instead of 5)\n",
      "    Line #4421 (got 1 columns instead of 5)\n",
      "    Line #4426 (got 1 columns instead of 5)\n",
      "    Line #4431 (got 1 columns instead of 5)\n",
      "    Line #4436 (got 1 columns instead of 5)\n",
      "    Line #4441 (got 1 columns instead of 5)\n",
      "    Line #4446 (got 1 columns instead of 5)\n",
      "    Line #4451 (got 1 columns instead of 5)\n",
      "    Line #4456 (got 1 columns instead of 5)\n",
      "    Line #4461 (got 1 columns instead of 5)\n",
      "    Line #4466 (got 1 columns instead of 5)\n",
      "    Line #4471 (got 1 columns instead of 5)\n",
      "    Line #4476 (got 1 columns instead of 5)\n",
      "    Line #4481 (got 1 columns instead of 5)\n",
      "    Line #4486 (got 1 columns instead of 5)\n",
      "    Line #4491 (got 1 columns instead of 5)\n",
      "    Line #4496 (got 1 columns instead of 5)\n",
      "    Line #4501 (got 1 columns instead of 5)\n",
      "    Line #4506 (got 1 columns instead of 5)\n",
      "    Line #4511 (got 1 columns instead of 5)\n",
      "    Line #4516 (got 1 columns instead of 5)\n",
      "    Line #4521 (got 1 columns instead of 5)\n",
      "    Line #4526 (got 1 columns instead of 5)\n",
      "    Line #4531 (got 1 columns instead of 5)\n",
      "    Line #4536 (got 1 columns instead of 5)\n",
      "    Line #4541 (got 1 columns instead of 5)\n",
      "    Line #4546 (got 1 columns instead of 5)\n",
      "    Line #4551 (got 1 columns instead of 5)\n",
      "    Line #4556 (got 1 columns instead of 5)\n",
      "    Line #4561 (got 1 columns instead of 5)\n",
      "    Line #4566 (got 1 columns instead of 5)\n",
      "    Line #4571 (got 1 columns instead of 5)\n",
      "    Line #4576 (got 1 columns instead of 5)\n",
      "    Line #4581 (got 1 columns instead of 5)\n",
      "    Line #4586 (got 1 columns instead of 5)\n",
      "    Line #4591 (got 1 columns instead of 5)\n",
      "    Line #4596 (got 1 columns instead of 5)\n",
      "    Line #4601 (got 1 columns instead of 5)\n",
      "    Line #4606 (got 1 columns instead of 5)\n",
      "    Line #4611 (got 1 columns instead of 5)\n",
      "    Line #4616 (got 1 columns instead of 5)\n",
      "    Line #4621 (got 1 columns instead of 5)\n",
      "    Line #4626 (got 1 columns instead of 5)\n",
      "    Line #4631 (got 1 columns instead of 5)\n",
      "    Line #4636 (got 1 columns instead of 5)\n",
      "    Line #4641 (got 1 columns instead of 5)\n",
      "    Line #4646 (got 1 columns instead of 5)\n",
      "    Line #4651 (got 1 columns instead of 5)\n",
      "    Line #4656 (got 1 columns instead of 5)\n",
      "    Line #4661 (got 1 columns instead of 5)\n",
      "    Line #4666 (got 1 columns instead of 5)\n",
      "    Line #4671 (got 1 columns instead of 5)\n",
      "    Line #4676 (got 1 columns instead of 5)\n",
      "    Line #4681 (got 1 columns instead of 5)\n",
      "    Line #4686 (got 1 columns instead of 5)\n",
      "    Line #4691 (got 1 columns instead of 5)\n",
      "    Line #4696 (got 1 columns instead of 5)\n",
      "    Line #4701 (got 1 columns instead of 5)\n",
      "    Line #4706 (got 1 columns instead of 5)\n",
      "    Line #4711 (got 1 columns instead of 5)\n",
      "    Line #4716 (got 1 columns instead of 5)\n",
      "    Line #4721 (got 1 columns instead of 5)\n",
      "    Line #4726 (got 1 columns instead of 5)\n",
      "    Line #4731 (got 1 columns instead of 5)\n",
      "    Line #4736 (got 1 columns instead of 5)\n",
      "    Line #4741 (got 1 columns instead of 5)\n",
      "    Line #4746 (got 1 columns instead of 5)\n",
      "    Line #4751 (got 1 columns instead of 5)\n",
      "    Line #4756 (got 1 columns instead of 5)\n",
      "    Line #4761 (got 1 columns instead of 5)\n",
      "    Line #4766 (got 1 columns instead of 5)\n",
      "    Line #4771 (got 1 columns instead of 5)\n",
      "    Line #4776 (got 1 columns instead of 5)\n",
      "    Line #4781 (got 1 columns instead of 5)\n",
      "    Line #4786 (got 1 columns instead of 5)\n",
      "    Line #4791 (got 1 columns instead of 5)\n",
      "    Line #4796 (got 1 columns instead of 5)\n",
      "    Line #4801 (got 1 columns instead of 5)\n",
      "    Line #4806 (got 1 columns instead of 5)\n",
      "    Line #4811 (got 1 columns instead of 5)\n",
      "    Line #4816 (got 1 columns instead of 5)\n",
      "    Line #4821 (got 1 columns instead of 5)\n",
      "    Line #4826 (got 1 columns instead of 5)\n",
      "    Line #4831 (got 1 columns instead of 5)\n",
      "    Line #4836 (got 1 columns instead of 5)\n",
      "    Line #4841 (got 1 columns instead of 5)\n",
      "    Line #4846 (got 1 columns instead of 5)\n",
      "    Line #4851 (got 1 columns instead of 5)\n",
      "    Line #4856 (got 1 columns instead of 5)\n",
      "    Line #4861 (got 1 columns instead of 5)\n",
      "    Line #4866 (got 1 columns instead of 5)\n",
      "    Line #4871 (got 1 columns instead of 5)\n",
      "    Line #4876 (got 1 columns instead of 5)\n",
      "    Line #4881 (got 1 columns instead of 5)\n",
      "    Line #4886 (got 1 columns instead of 5)\n",
      "    Line #4891 (got 1 columns instead of 5)\n",
      "    Line #4896 (got 1 columns instead of 5)\n",
      "    Line #4901 (got 1 columns instead of 5)\n",
      "    Line #4906 (got 1 columns instead of 5)\n",
      "    Line #4911 (got 1 columns instead of 5)\n",
      "    Line #4916 (got 1 columns instead of 5)\n",
      "    Line #4921 (got 1 columns instead of 5)\n",
      "    Line #4926 (got 1 columns instead of 5)\n",
      "    Line #4931 (got 1 columns instead of 5)\n",
      "    Line #4936 (got 1 columns instead of 5)\n",
      "    Line #4941 (got 1 columns instead of 5)\n",
      "    Line #4946 (got 1 columns instead of 5)\n",
      "    Line #4951 (got 1 columns instead of 5)\n",
      "    Line #4956 (got 1 columns instead of 5)\n",
      "    Line #4961 (got 1 columns instead of 5)\n",
      "    Line #4966 (got 1 columns instead of 5)\n",
      "    Line #4971 (got 1 columns instead of 5)\n",
      "    Line #4976 (got 1 columns instead of 5)\n",
      "    Line #4981 (got 1 columns instead of 5)\n",
      "    Line #4986 (got 1 columns instead of 5)\n",
      "    Line #4991 (got 1 columns instead of 5)\n",
      "    Line #4996 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates = (xyz_file[:,1:-1])\n",
    "coordinates = coordinates.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[0.         0.769767   0.55746937]\n",
      "[ 0.         -0.71017975  0.50340914]\n",
      "[ 0.         -0.0037242  -0.06630491]\n",
      "gfjkhgfjhgfkgh\n",
      "[0.         0.77715107 0.59586089]\n",
      "[ 0.         -0.77642641  0.59515778]\n",
      "[ 0.000000e+00 -4.529000e-05 -7.443867e-02]\n",
      "-13815.2523726009\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(coordinates))\n",
    "print(type(coordinates))\n",
    "print(coordinates[0])\n",
    "print(coordinates[1])\n",
    "print(coordinates[2])\n",
    "\n",
    "print('gfjkhgfjhgfkgh')\n",
    "print(coordinates[3])\n",
    "print(coordinates[4])\n",
    "print(coordinates[5])\n",
    "\n",
    "print(energies[0])\n",
    "# There is 1000 water molecules and each of them consists of 3 atoms, so we have 3000 atoms in total and each\n",
    "# of them has 3 coordinates.\n",
    "# Thus the coordinates array has 3000 lines, each of them corresponding to one atom (the first three lines\n",
    "# correspopnd to the first water molecule) and 3 columns corresponding to the x, y and z coordinates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.769767    0.55746937]\n",
      " [ 0.         -0.71017975  0.50340914]\n",
      " [ 0.         -0.0037242  -0.06630491]\n",
      " ...\n",
      " [ 0.          0.81441381  0.59863567]\n",
      " [ 0.         -0.76145415  0.54978922]\n",
      " [ 0.         -0.00330998 -0.07177656]]\n"
     ]
    }
   ],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4809337903171818\n",
      "0.9936714692829608\n",
      "0.9075536033167435\n",
      "[[0.         1.48093379 0.99367147]\n",
      " [1.48093379 0.         0.9075536 ]\n",
      " [0.99367147 0.9075536  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                # i-th water molecule\n",
    "N = 3                                # 3 atoms per molecule\n",
    "coord = coordinates[N*i:N*(i+1),:]  # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                     # pairwise distances between all of its 3 atom\n",
    "    \n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "#        print('i=',i)\n",
    "        for j in range(i+1,N):\n",
    "#            print(j)\n",
    "#            pairwise_dist_matrix[i][j] = \\\n",
    "#            np.sqrt(  (coord[i][0] - coord[j][0] )**2 + (coord[i][1] - coord[j][1] )**2 +(coord[i][2] - coord[j][2] )**2   )\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "            print(pairwise_dist_matrix[i][j])\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65865495 1.67618952 1.79537284]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = len(coord)\n",
    "\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.19473151 5.24520024 5.28298916]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     #1\n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4809337903171818\n",
      "0.9936714692829608\n",
      "0.9075536033167435\n",
      "1.5535776391049219\n",
      "1.0263214321709564\n",
      "1.0252448728694317\n",
      "1.641368955251778\n",
      "1.0296241445528371\n",
      "0.9245425561246101\n",
      "1.562310202650181\n",
      "1.0444362369102793\n",
      "0.9408131662300395\n",
      "1.5534064631206874\n",
      "1.0117901180626663\n",
      "1.0059422892679348\n",
      "1.4097927336451104\n",
      "0.981488947698239\n",
      "0.9698663397992994\n",
      "1.5919719259635425\n",
      "1.0457779476389586\n",
      "1.0238150317336627\n",
      "1.4460928508359974\n",
      "0.9923289507886108\n",
      "0.884325958533155\n",
      "1.5898529914539519\n",
      "1.055877732322894\n",
      "0.9995253500458467\n",
      "1.5580038885371763\n",
      "1.026995739510787\n",
      "0.9326169288303807\n",
      "1.64450333465209\n",
      "1.0430751892542738\n",
      "0.9564843903217655\n",
      "1.5484755704371989\n",
      "1.0101019030778409\n",
      "0.8980030723132179\n",
      "1.534972856109294\n",
      "1.0542232363755162\n",
      "0.9130081322388126\n",
      "1.4452059088962874\n",
      "1.0511128942080972\n",
      "0.8812115616889616\n",
      "1.5039871588034366\n",
      "1.0042258972723037\n",
      "0.8787207649978057\n",
      "1.4726051808003189\n",
      "0.9759117438815379\n",
      "0.8779145167183795\n",
      "1.402078601872584\n",
      "0.9698430808210092\n",
      "0.9351833947496675\n",
      "1.4235979309765876\n",
      "0.9906884558655636\n",
      "0.9824386458497598\n",
      "1.67826296710936\n",
      "1.0516742882790227\n",
      "0.9927380452841285\n",
      "1.7474378439433444\n",
      "1.0676659965009325\n",
      "1.0610100610762987\n",
      "1.5052527818133659\n",
      "0.9942330515859805\n",
      "0.9262999043774867\n",
      "1.3499876149055674\n",
      "0.9457132013070922\n",
      "0.9083847446494033\n",
      "1.4953550740423542\n",
      "1.0656506537583172\n",
      "0.9595793830065271\n",
      "1.5238764981324113\n",
      "1.016954323190743\n",
      "0.8931258877553345\n",
      "1.3944292541763168\n",
      "0.9635762159053215\n",
      "0.8906750687787309\n",
      "1.58373554910371\n",
      "1.0492427412965633\n",
      "0.9526004179323846\n",
      "1.4013490149683778\n",
      "0.9293780465990986\n",
      "0.8704914130346157\n",
      "1.639061329737379\n",
      "1.0528893193360576\n",
      "0.9916795060070416\n",
      "1.558899399939691\n",
      "1.0104492170631016\n",
      "0.8992164551821861\n",
      "1.5816553220665515\n",
      "1.0354511026116482\n",
      "0.9959948617387812\n",
      "1.4875483696503706\n",
      "0.9829316237029618\n",
      "0.8997642644007356\n",
      "1.5935952028107896\n",
      "1.0595354620238264\n",
      "0.9653215498917969\n",
      "1.4644354911555226\n",
      "1.0330901266865515\n",
      "0.9766253433204951\n",
      "1.6433946578134158\n",
      "1.0226730388521315\n",
      "1.0031340526624952\n",
      "1.688638554548731\n",
      "1.028623146577906\n",
      "0.9808018693688627\n",
      "1.4149748412610683\n",
      "0.8986810850392645\n",
      "0.8751791123573894\n",
      "1.4951389033582951\n",
      "1.0594116790478183\n",
      "0.8920628467508483\n",
      "1.5612643446715304\n",
      "0.9608120409710537\n",
      "0.9079734453619632\n",
      "1.5092280929053667\n",
      "1.0450635335281067\n",
      "0.9197728635504712\n",
      "1.5880887961435728\n",
      "1.0496089177423764\n",
      "0.9057674555992625\n",
      "1.6913108010966456\n",
      "1.0492672742876878\n",
      "1.0145813972550035\n",
      "1.4211760130645983\n",
      "0.9890806660001281\n",
      "0.9274747561507882\n",
      "1.4159733530376375\n",
      "1.0007600650113748\n",
      "0.9362846387594177\n",
      "1.4466478881127778\n",
      "0.9338886678177836\n",
      "0.8802509954598139\n",
      "1.4654026381382008\n",
      "1.040029425735224\n",
      "0.9266621907285038\n",
      "1.4678004716066844\n",
      "1.0681804702133448\n",
      "0.8960539489275157\n",
      "1.3832506928036097\n",
      "0.9829079881499538\n",
      "0.932759072015445\n",
      "1.6768197627412538\n",
      "1.0646979956378277\n",
      "1.0493794379929808\n",
      "1.4826663566198552\n",
      "0.9358502003215252\n",
      "0.8862044717740328\n",
      "1.4228670914042367\n",
      "0.9797434977261885\n",
      "0.8977367804416332\n",
      "1.561167183095516\n",
      "1.0641487366225817\n",
      "1.0028070927825523\n",
      "1.5721369621848869\n",
      "1.0244858513548551\n",
      "0.8909120062570051\n",
      "1.6702198242204402\n",
      "1.0521303326040414\n",
      "1.0215704960967424\n",
      "1.5336609196778406\n",
      "0.9829232228202153\n",
      "0.9096008505000531\n",
      "1.7766909659248782\n",
      "1.059688787369392\n",
      "1.047242370874686\n",
      "1.5693866382290933\n",
      "1.0329904092122948\n",
      "0.9782741389635243\n",
      "1.6839376020686283\n",
      "1.063902668482795\n",
      "1.0473611429879202\n",
      "1.5571256163705074\n",
      "0.9424646096652353\n",
      "0.9338686516324114\n",
      "1.485135352121878\n",
      "1.016238748962593\n",
      "0.9123765214062644\n",
      "1.6657092769353294\n",
      "1.0295702275067162\n",
      "1.0257968310388907\n",
      "1.621310298714397\n",
      "0.9938050292399928\n",
      "0.9679862425016641\n",
      "1.4281961195983623\n",
      "1.0116691094191395\n",
      "0.9580315060650031\n",
      "1.533525880656837\n",
      "1.0153940693677792\n",
      "0.941897663082867\n",
      "1.4286565728722305\n",
      "0.9974257961680273\n",
      "0.9381331032642497\n",
      "1.4190397314910783\n",
      "1.0166670126024084\n",
      "0.9035427619087628\n",
      "1.420738877395513\n",
      "0.9901165145744405\n",
      "0.9431796896912495\n",
      "1.7575854656765062\n",
      "1.058360355465517\n",
      "1.0297245520945044\n",
      "1.706730258708487\n",
      "1.0454961579917543\n",
      "0.9947861029647161\n",
      "1.4719030925841388\n",
      "1.0142077046823517\n",
      "1.0125852893362222\n",
      "1.4411624278895458\n",
      "0.9571460630858586\n",
      "0.9256636649569439\n",
      "1.5040346536942224\n",
      "1.0214195417280643\n",
      "0.8859551128942151\n",
      "1.5374422127039802\n",
      "1.0014591830003903\n",
      "0.9953125987259103\n",
      "1.4976393943246211\n",
      "0.9585977393168023\n",
      "0.9522713538402217\n",
      "1.4540997859964389\n",
      "1.065914428528814\n",
      "0.9439344544063696\n",
      "1.6098872616183084\n",
      "1.0536303859723477\n",
      "1.043904926453287\n",
      "1.4513598860373798\n",
      "0.9952763268566303\n",
      "0.9408150993934085\n",
      "1.3643197689319437\n",
      "0.9531624750166133\n",
      "0.9139135480234193\n",
      "1.6897726606292465\n",
      "1.0203869414968614\n",
      "0.9994175253048811\n",
      "1.4425441501314475\n",
      "0.9445305448191796\n",
      "0.9286351667478001\n",
      "1.6197250696496575\n",
      "1.0494776099994307\n",
      "1.0347895828799942\n",
      "1.292897344889538\n",
      "0.8776427552607171\n",
      "0.8696875996607012\n",
      "1.4306717673330374\n",
      "0.9888971492241095\n",
      "0.890541806613878\n",
      "1.358735174111176\n",
      "0.9935146819806854\n",
      "0.8809281080348218\n",
      "1.6115529344229182\n",
      "1.0402606568978874\n",
      "0.9305171540770852\n",
      "1.4776374308409561\n",
      "0.981904731507618\n",
      "0.8768825435924217\n",
      "1.5675238629548893\n",
      "1.001024410899191\n",
      "0.9059029852538434\n",
      "1.5814738390277023\n",
      "0.9917231885099416\n",
      "0.9803803845145243\n",
      "1.5791959488942726\n",
      "1.006485720309222\n",
      "0.91732622209044\n",
      "1.6635933887280914\n",
      "1.040545198884942\n",
      "1.014808917056797\n",
      "1.6163569460179257\n",
      "1.0406243035092102\n",
      "0.9637198905660243\n",
      "1.4756631275867897\n",
      "0.9728788125023171\n",
      "0.9359249000393053\n",
      "1.540439207943732\n",
      "0.9846176812857022\n",
      "0.8754596206012739\n",
      "1.5103053812642833\n",
      "1.0595474756444458\n",
      "1.005845467110647\n",
      "1.3620696740210885\n",
      "0.9512652974157905\n",
      "0.8776299929255003\n",
      "1.499006168146363\n",
      "0.9823301044622772\n",
      "0.9038279485753643\n",
      "1.6380485101153748\n",
      "1.0430238828370233\n",
      "0.9720505642937475\n",
      "1.4935407601658617\n",
      "1.0499679780107074\n",
      "0.9247727041576321\n",
      "1.7340003436201448\n",
      "1.0495634894864239\n",
      "1.0091903630206176\n",
      "1.545014766537375\n",
      "0.9974322690501206\n",
      "0.9352511851339482\n",
      "1.4946483762786915\n",
      "1.0336816717043904\n",
      "0.9948427227939317\n",
      "1.5524239809684253\n",
      "1.0665991556570298\n",
      "0.9264252407316967\n",
      "1.4639982063451962\n",
      "0.9840965942783247\n",
      "0.9746981238927301\n",
      "1.5781714751338531\n",
      "0.9694477971503592\n",
      "0.9023913927878487\n",
      "1.4599079091900733\n",
      "1.0120523641615982\n",
      "0.9480480754932082\n",
      "1.324926277834566\n",
      "0.90077432973134\n",
      "0.8833889505853768\n",
      "1.5580123155159027\n",
      "0.9765909127029262\n",
      "0.9259771334356953\n",
      "1.690558150211333\n",
      "1.045318430881954\n",
      "0.9967279989358385\n",
      "1.5790573614825645\n",
      "1.0080190437334549\n",
      "0.9888407855103425\n",
      "1.5826816619094741\n",
      "1.047095728446856\n",
      "0.8797029877676569\n",
      "1.2965773322805174\n",
      "0.8982843611428172\n",
      "0.8809893933102251\n",
      "1.435260018138356\n",
      "0.9724469239159249\n",
      "0.8796439497954518\n",
      "1.5026853787627736\n",
      "1.069179619089493\n",
      "0.8909637163145445\n",
      "1.6789127103184376\n",
      "1.0688157049001061\n",
      "0.9494725860213997\n",
      "1.6991230298873532\n",
      "1.0598978822026206\n",
      "0.9830774287162519\n",
      "1.5984942806177629\n",
      "1.0251068321969026\n",
      "0.8756113197151207\n",
      "1.5646262428822661\n",
      "0.9731767989202096\n",
      "0.9108079074689039\n",
      "1.472803166219061\n",
      "0.9140800082082606\n",
      "0.8969815684912295\n",
      "1.5910049826178883\n",
      "1.0640074885658677\n",
      "0.9763460839132061\n",
      "1.567414746180109\n",
      "0.9904473777564324\n",
      "0.8736719401215718\n",
      "1.5793424529521165\n",
      "1.0627679997570398\n",
      "1.0552554755148162\n",
      "1.4963878773779136\n",
      "0.9938421408100468\n",
      "0.9518422393631819\n",
      "1.5522261429307291\n",
      "1.0428794554638492\n",
      "0.8765702997739666\n",
      "1.58766244734766\n",
      "1.0466428966716086\n",
      "0.8867440298259976\n",
      "1.548391705004694\n",
      "1.0329124645980328\n",
      "0.932710593744985\n",
      "1.4269750378432773\n",
      "0.9559540658519841\n",
      "0.873488363921329\n",
      "1.5935320144050797\n",
      "1.0352988575716393\n",
      "0.9170137019665271\n",
      "1.5334607927527915\n",
      "0.9865455083418886\n",
      "0.9678232782105837\n",
      "1.5623097945245867\n",
      "0.9902519279063922\n",
      "0.9225978568416567\n",
      "1.392961599204907\n",
      "0.9737103697590253\n",
      "0.9233665600991079\n",
      "1.6350177912850214\n",
      "1.0460578258376612\n",
      "1.0119770454332682\n",
      "1.4295434606862207\n",
      "1.068146027928467\n",
      "0.9049976580363824\n",
      "1.668348945595803\n",
      "1.0556000980557039\n",
      "0.9606494070285351\n",
      "1.5136235655763104\n",
      "1.0497607169897434\n",
      "0.9639156798435965\n",
      "1.579981962341275\n",
      "1.0122484852775477\n",
      "0.930080463027444\n",
      "1.4327230548237064\n",
      "0.9264123253543557\n",
      "0.872984996825518\n",
      "1.4706054615873712\n",
      "1.0554070389584111\n",
      "0.9370490920288902\n",
      "1.5074999956834014\n",
      "1.0090911186766065\n",
      "0.9725042467810291\n",
      "1.7117144437109446\n",
      "1.0504111549907258\n",
      "1.041571627268636\n",
      "1.658197160215434\n",
      "1.0569408317073812\n",
      "0.9214960358382152\n",
      "1.5578279168926434\n",
      "1.0513967143466132\n",
      "1.0182586090779213\n",
      "1.677975462997262\n",
      "1.0312253820674284\n",
      "0.9682431724662788\n",
      "1.6579678273861997\n",
      "1.051419662923872\n",
      "0.9967463591464156\n",
      "1.4767700116099096\n",
      "1.035967086687379\n",
      "0.8778383066517378\n",
      "1.49320236342748\n",
      "1.0565611112084243\n",
      "0.965860278868786\n",
      "1.5663492821488079\n",
      "1.0621405502207812\n",
      "0.8726548809843512\n",
      "1.4458620003809308\n",
      "0.9003122854798473\n",
      "0.8973377060387934\n",
      "1.6955671546978925\n",
      "1.0655805516424932\n",
      "1.0082111999265642\n",
      "1.3887257489844185\n",
      "0.9542915536135049\n",
      "0.8970284020496432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4958533307985287\n",
      "1.0221897674221927\n",
      "0.8801708857932625\n",
      "1.5073404318449897\n",
      "0.9954346731429243\n",
      "0.9053877487811005\n",
      "1.6879461229928094\n",
      "1.0329187058369063\n",
      "0.9826659556795618\n",
      "1.556145581907922\n",
      "1.0118185169521245\n",
      "0.9078299901146193\n",
      "1.561316742411757\n",
      "1.0524112729799517\n",
      "0.9209090093001352\n",
      "1.4665940421510013\n",
      "0.9808425936350222\n",
      "0.9338041582861301\n",
      "1.4992568274782745\n",
      "1.0650133489910205\n",
      "0.9357100258182879\n",
      "1.44593219516878\n",
      "1.002654930241362\n",
      "0.9842065618768889\n",
      "1.3852523304741347\n",
      "0.9778173335894711\n",
      "0.9108395269076578\n",
      "1.61473358469588\n",
      "1.0432208489078931\n",
      "0.8747236676162726\n",
      "1.3939908502066145\n",
      "0.9513053016128954\n",
      "0.9347496926579574\n",
      "1.4786412233788682\n",
      "0.9520465102030099\n",
      "0.8946629683477847\n",
      "1.4646471656479765\n",
      "1.0641459523756298\n",
      "0.8957030937498677\n",
      "1.6260085803257667\n",
      "1.0213196868133794\n",
      "0.9656820700051714\n",
      "1.4090372108599392\n",
      "0.9298293964100707\n",
      "0.8722847279513767\n",
      "1.5282704678903805\n",
      "1.00991712581757\n",
      "0.8824578964462434\n",
      "1.447801117251626\n",
      "0.9875940109150331\n",
      "0.9383455192661826\n",
      "1.633637867447388\n",
      "1.0558151074462676\n",
      "0.922763374491611\n",
      "1.5778922806100328\n",
      "0.9986504323519029\n",
      "0.898338349408921\n",
      "1.470942009099404\n",
      "1.0175218269666375\n",
      "0.972391873671075\n",
      "1.4134286491343144\n",
      "1.0361140730078329\n",
      "0.8772264427861062\n",
      "1.4654551275157426\n",
      "1.0438613991945533\n",
      "0.9738701713721354\n",
      "1.4668486354077597\n",
      "1.0077278717453462\n",
      "0.9645034043871845\n",
      "1.3240087776292069\n",
      "0.881884087385378\n",
      "0.8756945259218177\n",
      "1.338584956998862\n",
      "0.9502477173599738\n",
      "0.8917910779378347\n",
      "1.7372523009960095\n",
      "1.0563022838532576\n",
      "1.0545628010542398\n",
      "1.5788376263207569\n",
      "0.9867133480139961\n",
      "0.9069183264709897\n",
      "1.4891614424931217\n",
      "1.0627181590717691\n",
      "0.8827575566527492\n",
      "1.6093457951601366\n",
      "1.0086223073803935\n",
      "0.9973027825864015\n",
      "1.5667840452350856\n",
      "1.034999839306357\n",
      "1.0118717947735287\n",
      "1.4812725085895835\n",
      "1.0221876714355194\n",
      "0.9027886402360588\n",
      "1.5820815970426132\n",
      "1.0151632375758246\n",
      "0.8868506608708343\n",
      "1.3830765804852594\n",
      "0.9042232643310883\n",
      "0.877502479319053\n",
      "1.336900999709613\n",
      "0.9373710250535796\n",
      "0.8782035020447428\n",
      "1.5163807430609626\n",
      "0.9568580032579321\n",
      "0.8834643557219214\n",
      "1.4505589771216938\n",
      "1.042472844473173\n",
      "0.8954736767875509\n",
      "1.6553170318782313\n",
      "1.063434775959532\n",
      "0.9567482623572248\n",
      "1.6114474444048164\n",
      "1.0546856784057619\n",
      "0.8956050028450832\n",
      "1.3432440537335015\n",
      "0.9477960457413519\n",
      "0.897363836319989\n",
      "1.4420948979282302\n",
      "1.0466476371594287\n",
      "0.8877955705115275\n",
      "1.5519178034891055\n",
      "1.000106082199841\n",
      "0.9418270384071472\n",
      "1.3800295403899012\n",
      "0.948248281769605\n",
      "0.9441707948995883\n",
      "1.6106761405903711\n",
      "1.013193897411675\n",
      "0.9550415060317458\n",
      "1.4157807034025376\n",
      "1.041823858689452\n",
      "0.9081233387053683\n",
      "1.4541360045337388\n",
      "1.0496754723818533\n",
      "0.9165588883810469\n",
      "1.6288095672758895\n",
      "1.0390019623955609\n",
      "0.9328248711499136\n",
      "1.4670731137470057\n",
      "0.9383799821940402\n",
      "0.9172439414866527\n",
      "1.4429502731118127\n",
      "0.923543095348733\n",
      "0.8761358823374695\n",
      "1.5131102775216438\n",
      "0.9732898004994783\n",
      "0.8776408806451643\n",
      "1.6071320531640034\n",
      "1.0669196608431615\n",
      "1.0295299075726283\n",
      "1.4614565281671923\n",
      "0.9395639816513212\n",
      "0.9363279511356436\n",
      "1.4728708661923606\n",
      "0.9927618921781347\n",
      "0.985148390241576\n",
      "1.5275483797320522\n",
      "1.042246061318392\n",
      "1.0113038488280295\n",
      "1.5404520463673987\n",
      "1.0402662365729731\n",
      "0.873221032747594\n",
      "1.39668081321528\n",
      "0.920759805228741\n",
      "0.901716581326193\n",
      "1.5020165577979612\n",
      "0.9569599301880285\n",
      "0.8704785360142111\n",
      "1.574390141109777\n",
      "1.0043826962403786\n",
      "0.978254325492118\n",
      "1.5713637726522605\n",
      "0.9910451322511963\n",
      "0.8720465429489077\n",
      "1.5701722089737498\n",
      "0.9866932495422184\n",
      "0.9140711137775291\n",
      "1.5090372590298722\n",
      "1.0277492237856725\n",
      "0.9350953112723794\n",
      "1.483155444046165\n",
      "1.0612723091751135\n",
      "0.9140088278291365\n",
      "1.4128679491472407\n",
      "0.9684288784799682\n",
      "0.9000327399866404\n",
      "1.4384965903996036\n",
      "0.9803869934223067\n",
      "0.883832912366637\n",
      "1.6146909267762888\n",
      "1.0249578023670198\n",
      "0.9614675144601454\n",
      "1.3519424896983503\n",
      "0.9154906061961062\n",
      "0.8741585864552324\n",
      "1.4270157535094137\n",
      "0.9512729187619642\n",
      "0.8709295067190769\n",
      "1.5741419920699569\n",
      "1.0538856490721926\n",
      "1.0397341330990946\n",
      "1.4886476711348204\n",
      "1.067440692980189\n",
      "0.8704492877078643\n",
      "1.4951078783650735\n",
      "1.0271013329904566\n",
      "0.888952012297454\n",
      "1.608888426155104\n",
      "1.0357694048759778\n",
      "0.9453835696389873\n",
      "1.4692884023723354\n",
      "0.9402498723332258\n",
      "0.8741769785139348\n",
      "1.5871707531916417\n",
      "1.0409196034427293\n",
      "0.953492091630503\n",
      "1.6420647040323475\n",
      "1.0572621763304373\n",
      "1.0453008168058286\n",
      "1.340489164550555\n",
      "0.8829806414495152\n",
      "0.8727117275278808\n",
      "1.558853704187045\n",
      "0.9912601927838828\n",
      "0.9877100834103437\n",
      "1.5851189938933206\n",
      "1.0446114425025372\n",
      "0.8982010033508665\n",
      "1.451751076083657\n",
      "1.0349869805906105\n",
      "0.943913782105047\n",
      "1.5507767732040794\n",
      "0.9704291000572691\n",
      "0.8820503525178267\n",
      "1.372917337875325\n",
      "0.9831454698680181\n",
      "0.8990420911960451\n",
      "1.4891883165833377\n",
      "0.9031052988611225\n",
      "0.8830553755430649\n",
      "1.4913595730154057\n",
      "0.9587317265529053\n",
      "0.8905267520845871\n",
      "1.5626589486860913\n",
      "0.9697043869347115\n",
      "0.9510909893904911\n",
      "1.3997181537142205\n",
      "0.9461418930958261\n",
      "0.9036677921404145\n",
      "1.439232147248836\n",
      "0.9308938859808592\n",
      "0.9047102757762429\n",
      "1.4042241265343207\n",
      "0.9951931422696908\n",
      "0.8811603708139977\n",
      "1.4808014882802334\n",
      "1.0108138307266692\n",
      "0.9250913946123793\n",
      "1.458625365131462\n",
      "1.0102032190770724\n",
      "0.9543085215112632\n",
      "1.6318406141434558\n",
      "0.9951157065648129\n",
      "0.9571304892884437\n",
      "1.5862228646221048\n",
      "1.0330763988014613\n",
      "0.9406473045435919\n",
      "1.7061879659539567\n",
      "1.0169342115084403\n",
      "1.0122602112931856\n",
      "1.354646531039084\n",
      "0.9184595556163919\n",
      "0.8777660220270751\n",
      "1.5612923979659994\n",
      "1.0103420343942666\n",
      "0.8782530363070962\n",
      "1.5917733466893946\n",
      "1.056076883363716\n",
      "0.9634083774676525\n",
      "1.4880897455313111\n",
      "0.9218458125092843\n",
      "0.889029057632964\n",
      "1.603595383240199\n",
      "1.0303828664585872\n",
      "0.8978102609705807\n",
      "1.5678710103538431\n",
      "1.0521000932080393\n",
      "0.9692337466265198\n",
      "1.476626008574834\n",
      "0.9979639286323022\n",
      "0.9093244308574258\n",
      "1.5544103003161414\n",
      "0.9920960160384072\n",
      "0.956711983662826\n",
      "1.7003405441016353\n",
      "1.0595377143386187\n",
      "0.9798737930762298\n",
      "1.4131618799052166\n",
      "0.8969506599386088\n",
      "0.8695584656964932\n",
      "1.6511866336687573\n",
      "1.0253758373570636\n",
      "0.9775902895004491\n",
      "1.558596237019823\n",
      "1.0599316623627992\n",
      "0.9912409364094775\n",
      "1.5831509611634025\n",
      "1.0346113026280424\n",
      "0.9444530969340487\n",
      "1.4584953017643025\n",
      "0.9932540230237066\n",
      "0.9575497028518005\n",
      "1.6246692488630419\n",
      "1.0542031705240378\n",
      "0.9580404352917112\n",
      "1.3794336591488698\n",
      "0.983781559194046\n",
      "0.8855543355243417\n",
      "1.7309836665648457\n",
      "1.0692424561150566\n",
      "1.0687257760884066\n",
      "1.5826829879735014\n",
      "0.9495518613915651\n",
      "0.9440256819135548\n",
      "1.5061034160225368\n",
      "1.0020522415857913\n",
      "0.918221478255245\n",
      "1.4716699388854744\n",
      "0.8914288096647583\n",
      "0.8762384221722292\n",
      "1.5997304000133474\n",
      "1.052281906829154\n",
      "1.0476699333567911\n",
      "1.540378556161599\n",
      "1.001100084599152\n",
      "0.928933637938516\n",
      "1.5275022757420367\n",
      "1.066123512139196\n",
      "0.9923741575766984\n",
      "1.5161926274661957\n",
      "1.0355435995176807\n",
      "1.0323891229488629\n",
      "1.5941873413288936\n",
      "1.0468076901332262\n",
      "0.872537514749774\n",
      "1.680485468082226\n",
      "1.0490038505164843\n",
      "1.0393230243660745\n",
      "1.4404793314157578\n",
      "0.978325564467663\n",
      "0.9350007532336547\n",
      "1.5025896099879077\n",
      "1.0436066387257827\n",
      "0.9045964586700541\n",
      "1.562886465394615\n",
      "1.0211666143044678\n",
      "0.9304864916691901\n",
      "1.5754382987463411\n",
      "1.043754450874974\n",
      "0.9131079450451588\n",
      "1.5524114647452936\n",
      "0.9638503026965514\n",
      "0.9160877765483083\n",
      "1.3536112565881582\n",
      "0.9102550918851704\n",
      "0.8889529729484809\n",
      "1.6703401248015917\n",
      "0.9949432184409722\n",
      "0.988644462856726\n",
      "1.436766697393808\n",
      "1.029245188477959\n",
      "0.9045130440732078\n",
      "1.4801440666230021\n",
      "1.0602647579453446\n",
      "0.941448101338983\n",
      "1.6436089377649759\n",
      "1.0310732834636582\n",
      "0.9238190311765598\n",
      "1.452197489979119\n",
      "0.9859345598844654\n",
      "0.8877341490426649\n",
      "1.446271271744804\n",
      "1.0491145429886384\n",
      "0.886206398771553\n",
      "1.3505613900965712\n",
      "0.9295054620261551\n",
      "0.8793828080098437\n",
      "1.2997039893754423\n",
      "0.8947542838784783\n",
      "0.8709707914121648\n",
      "1.611081284728422\n",
      "1.0153325976507717\n",
      "0.9171247161578631\n",
      "1.4910047818081602\n",
      "1.0440582508512215\n",
      "0.9349524885318425\n",
      "1.5154183296057828\n",
      "0.9777078236297896\n",
      "0.9449406276045497\n",
      "1.4547434819864422\n",
      "0.952010132703893\n",
      "0.947023031607615\n",
      "1.4738633608664138\n",
      "0.9606716916981398\n",
      "0.9496966366392121\n",
      "1.6272331942229237\n",
      "1.0454925512154791\n",
      "1.000867651651406\n",
      "1.5554880168243452\n",
      "0.954857805593385\n",
      "0.9209252943977867\n",
      "1.537466121108988\n",
      "0.9852402783237401\n",
      "0.9039053720535047\n",
      "1.4485722716024283\n",
      "1.0153301765961753\n",
      "0.9500488706843075\n",
      "1.6242245230084738\n",
      "1.0219686321898145\n",
      "0.9759819344395269\n",
      "1.6255523039628932\n",
      "0.9887761756566679\n",
      "0.9538229017240232\n",
      "1.6665200249210632\n",
      "1.068043908661062\n",
      "0.9788174797982816\n",
      "1.6198890258922274\n",
      "1.0419679644054711\n",
      "0.8748465686398769\n",
      "1.579673153511093\n",
      "1.0393018737650082\n",
      "0.9343116713073232\n",
      "1.5834620132680592\n",
      "0.9746906275111954\n",
      "0.9574723243516329\n",
      "1.5563548202689\n",
      "0.9618281747493439\n",
      "0.9195197501295546\n",
      "1.4407347739095835\n",
      "1.0048544358241622\n",
      "0.9428221653576245\n",
      "1.6197019288287904\n",
      "1.0555091249527995\n",
      "0.9498116040007294\n",
      "1.4063422268001273\n",
      "0.982983027424997\n",
      "0.953095669468279\n",
      "1.6898326007361733\n",
      "1.0683765462369683\n",
      "0.9930505509803075\n",
      "1.6418875097393932\n",
      "1.0555174338546116\n",
      "0.9375734297133479\n",
      "1.631094137327105\n",
      "1.0437916735621984\n",
      "1.0024473545351968\n",
      "1.5134543448098519\n",
      "1.0479201615277993\n",
      "0.9589665188849935\n",
      "1.5814046947483145\n",
      "1.0320843058358922\n",
      "1.0086795923004845\n",
      "1.5466541608247155\n",
      "1.0600525859338943\n",
      "0.87039272395187\n",
      "1.4695139119287668\n",
      "0.9689484056949216\n",
      "0.8705589709102356\n",
      "1.4856687416859025\n",
      "0.9616012098012297\n",
      "0.9292792004422913\n",
      "1.6756538659996036\n",
      "1.0431772996432698\n",
      "0.9863815401094312\n",
      "1.6033453054316575\n",
      "1.0249623120465259\n",
      "0.978936332044423\n",
      "1.636956132154225\n",
      "1.0161036621100146\n",
      "0.9825235268687909\n",
      "1.6752971608535212\n",
      "1.0665588309222156\n",
      "1.0270908583697067\n",
      "1.4159232029756506\n",
      "0.9645269704267123\n",
      "0.9009066167175945\n",
      "1.4050856548232646\n",
      "1.0246937965180185\n",
      "0.9000121594500974\n",
      "1.490810892695215\n",
      "1.0020973995538611\n",
      "0.8850742090801007\n",
      "1.365881466776329\n",
      "0.9585310857638332\n",
      "0.9306157898055071\n",
      "1.3429626391166052\n",
      "0.9279550763477337\n",
      "0.9228085102404153\n",
      "1.636892999780037\n",
      "1.0300576238404695\n",
      "0.9360096185516676\n",
      "1.7572255289724015\n",
      "1.062069953683508\n",
      "1.0313203559666924\n",
      "1.6028429246388445\n",
      "0.987482806324443\n",
      "0.9315503763285186\n",
      "1.3794378384727433\n",
      "0.9143982458409656\n",
      "0.8793798033652864\n",
      "1.6009323509797813\n",
      "1.0581739696195753\n",
      "0.9517008489793864\n",
      "1.5864267329271926\n",
      "0.9885466358172261\n",
      "0.9066017393029586\n",
      "1.4591957591731568\n",
      "1.0657064107223768\n",
      "0.8960773420122186\n",
      "1.5779939015117543\n",
      "1.0333953348541138\n",
      "1.0047570049116337\n",
      "1.6032665636636005\n",
      "1.0239666539190106\n",
      "0.9127071130425715\n",
      "1.3930187860486023\n",
      "0.9751896264746134\n",
      "0.9511696657143757\n",
      "1.499568171123719\n",
      "1.0301167518876888\n",
      "0.8989816274296851\n",
      "1.5156917626247686\n",
      "0.9370932231681851\n",
      "0.9286788818965475\n",
      "1.7048753862017556\n",
      "1.068811146528668\n",
      "0.9796301660996302\n",
      "1.5819726748750251\n",
      "1.045962560445434\n",
      "0.9029433632192924\n",
      "1.5024787903633485\n",
      "1.0419126983162157\n",
      "0.9536714400933293\n",
      "1.4332735520315825\n",
      "0.9819998390207139\n",
      "0.8745617607123265\n",
      "1.3299136342399276\n",
      "0.9416167316348397\n",
      "0.8874984106800206\n",
      "1.5841513020262856\n",
      "0.9403325042756819\n",
      "0.9377400060340282\n",
      "1.543513546382931\n",
      "1.058090627061826\n",
      "0.8954785290818856\n",
      "1.6010712939846643\n",
      "0.9646987786921256\n",
      "0.9494709925356128\n",
      "1.665268313082216\n",
      "1.0311661149340892\n",
      "0.9723169391803511\n",
      "1.4502640461612373\n",
      "1.0217832717696609\n",
      "0.9621338510800083\n",
      "1.4028176268986283\n",
      "0.963349582485051\n",
      "0.9093641053144778\n",
      "1.5393718230487994\n",
      "0.9400980017881954\n",
      "0.9047576339008394\n",
      "1.519363770947837\n",
      "0.9559538976028286\n",
      "0.9049088853383658\n",
      "1.2812770659386041\n",
      "0.8857834206534061\n",
      "0.8793438218004578\n",
      "1.4551410165314416\n",
      "0.9900154452442217\n",
      "0.9623054300437882\n",
      "1.6586441852370126\n",
      "1.011280357280319\n",
      "0.9730556522277256\n",
      "1.6154039576459538\n",
      "1.0634868559284536\n",
      "0.9750760460117642\n",
      "1.4482427645344447\n",
      "0.9638050618224198\n",
      "0.8705372972363282\n",
      "1.4044230269080877\n",
      "1.000096534801404\n",
      "0.89969148934596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4980962592739637\n",
      "1.0657219996167888\n",
      "0.9600794870409546\n",
      "1.2944299012358822\n",
      "0.8828241637578633\n",
      "0.881305492224931\n",
      "1.6632350665214224\n",
      "1.033118192152689\n",
      "0.9951710048263613\n",
      "1.622394902890017\n",
      "1.0636493679841552\n",
      "0.8846767866085702\n",
      "1.586275198062356\n",
      "1.0550643252094398\n",
      "0.9953363409550626\n",
      "1.4969578070171659\n",
      "1.0064103552097357\n",
      "0.9670349275245715\n",
      "1.6195739478097937\n",
      "1.0045897005242808\n",
      "0.968396047827692\n",
      "1.5089237766324632\n",
      "1.018488880575834\n",
      "0.9130164170775609\n",
      "1.4873166298341771\n",
      "1.0537784143446904\n",
      "0.9509021731569117\n",
      "1.4572328170009192\n",
      "0.9456277231913873\n",
      "0.895446284505566\n",
      "1.4644312422165613\n",
      "1.0581237403650778\n",
      "0.9339014554322776\n",
      "1.5203222072620328\n",
      "1.058727167434596\n",
      "1.0406566788104517\n",
      "1.4288879059340247\n",
      "1.03512499481514\n",
      "0.8928284481530189\n",
      "1.483799839137243\n",
      "0.9831439363527745\n",
      "0.9648047898468316\n",
      "1.6175559246020588\n",
      "1.0566167524463514\n",
      "0.8805711270374408\n",
      "1.55599872199667\n",
      "1.0136967946765818\n",
      "1.0000653465796263\n",
      "1.6301956601524867\n",
      "1.0639846613977912\n",
      "1.011451228532115\n",
      "1.5365209438295049\n",
      "0.9345380056226668\n",
      "0.892420324248421\n",
      "1.372613700484236\n",
      "0.9241353609349309\n",
      "0.9067396444206878\n",
      "1.6355241150632074\n",
      "1.0582275735444144\n",
      "1.0214205101800629\n",
      "1.4329139799938355\n",
      "1.00166642836767\n",
      "0.9387305543452892\n",
      "1.4148888762874188\n",
      "1.005508962426675\n",
      "0.9475063768289166\n",
      "1.4597133500326525\n",
      "0.974875131026852\n",
      "0.9693696424935381\n",
      "1.4394964239978447\n",
      "0.958562410930253\n",
      "0.8695876449715928\n",
      "1.4485201371067187\n",
      "1.0337976933892163\n",
      "0.8854534778320895\n",
      "1.3074950059865795\n",
      "0.9142580559021991\n",
      "0.8711877625499072\n",
      "1.5424011415750638\n",
      "1.0344426282048582\n",
      "1.0122031590089986\n",
      "1.4900422963856035\n",
      "0.9546005574679446\n",
      "0.8820170775332715\n",
      "1.5665087072281854\n",
      "1.012880428544121\n",
      "0.90793658791777\n",
      "1.5815511243589235\n",
      "1.060595689375873\n",
      "0.9724285000066838\n",
      "1.4927595137521552\n",
      "0.9707149045214775\n",
      "0.8738570600129029\n",
      "1.3817657202896112\n",
      "1.0037342820125459\n",
      "0.8968847561905263\n",
      "1.5505441284605308\n",
      "1.040104883009464\n",
      "0.9556050194364398\n",
      "1.2986407362785217\n",
      "0.886939534890595\n",
      "0.8805887038443652\n",
      "1.4446720399232933\n",
      "1.0041811448983085\n",
      "0.9663128454096606\n",
      "1.5018853536944081\n",
      "1.0356921710303326\n",
      "1.0245424213209098\n",
      "1.4773011155655371\n",
      "0.944677644674277\n",
      "0.9337332804707199\n",
      "1.4124786908191431\n",
      "0.9627221188832484\n",
      "0.9592427759161324\n",
      "1.417957835587257\n",
      "0.9439998720988296\n",
      "0.8736041656356195\n",
      "1.5629934026311163\n",
      "1.047872672149318\n",
      "1.024425959467537\n",
      "1.5312749551661176\n",
      "1.015558604037823\n",
      "0.9322773040800221\n",
      "1.41649660020282\n",
      "0.9857537093005296\n",
      "0.8793348866025066\n",
      "1.5168654317017205\n",
      "0.9279169050602344\n",
      "0.8944605283944177\n",
      "1.6121172479419565\n",
      "0.9994239927349466\n",
      "0.9595043363447358\n",
      "1.4277406898257066\n",
      "0.9258422826205445\n",
      "0.923926067054257\n",
      "1.6675510728262135\n",
      "1.0556827924469314\n",
      "0.9535713343616803\n",
      "1.3491652653000754\n",
      "0.9582845911190712\n",
      "0.8750875993783956\n",
      "1.5052338128397496\n",
      "0.9168969745772433\n",
      "0.9085015329713496\n",
      "1.5329955041756347\n",
      "1.0485468207660673\n",
      "0.874827154480482\n",
      "1.5664489848366658\n",
      "1.0303783282402073\n",
      "0.9533521973947024\n",
      "1.4136477957237885\n",
      "1.0009789609656967\n",
      "0.9264782282505448\n",
      "1.5412839767152013\n",
      "1.0337027788512696\n",
      "1.0307325644848522\n",
      "1.5905953861652997\n",
      "1.0357426340084723\n",
      "0.9969103138497052\n",
      "1.4675624541294785\n",
      "0.9853203143728192\n",
      "0.8910302656423686\n",
      "1.5716958906053984\n",
      "1.0465200693797196\n",
      "0.9622462636933583\n",
      "1.428987329886295\n",
      "0.9084506413789161\n",
      "0.8769954938697898\n",
      "1.5276781671230975\n",
      "0.9821759720208997\n",
      "0.890895870361784\n",
      "1.4461169630880393\n",
      "0.9389779306627165\n",
      "0.9059461160212218\n",
      "1.6278875578208933\n",
      "0.9980350000173723\n",
      "0.9935749863367099\n",
      "1.531045717185058\n",
      "0.967726748002497\n",
      "0.9265799247691188\n",
      "1.5175573582705082\n",
      "1.0228919854153982\n",
      "0.9712436878196624\n",
      "1.4783881528290255\n",
      "0.9786258552212734\n",
      "0.9181786923727394\n",
      "1.4471682659878868\n",
      "1.0312525991829011\n",
      "0.8900230954862685\n",
      "1.6584376618805703\n",
      "1.0276025921168035\n",
      "0.9825579689706687\n",
      "1.7831096339391115\n",
      "1.0642364568923375\n",
      "1.062294600502799\n",
      "1.5227805530906688\n",
      "0.9796070884794023\n",
      "0.8877569310291393\n",
      "1.561839085567172\n",
      "1.0184212494186404\n",
      "0.9821092057408605\n",
      "1.6405280321558924\n",
      "1.0684402733719134\n",
      "0.9003823137910683\n",
      "1.4859709642144836\n",
      "1.0299995892652185\n",
      "0.9515448444423527\n",
      "1.4499470329795712\n",
      "0.993835973886123\n",
      "0.9281376659286963\n",
      "1.4939921168940866\n",
      "1.029185893794691\n",
      "0.8730261554232072\n",
      "1.5698022214453478\n",
      "1.0160527666355275\n",
      "1.0059632955811517\n",
      "1.467455848268808\n",
      "1.007890377348115\n",
      "0.9426892575986966\n",
      "1.6551765882655514\n",
      "1.0262598256511606\n",
      "1.0145907411709607\n",
      "1.3806214750274344\n",
      "0.917137453737327\n",
      "0.8755980165997044\n",
      "1.4095182269896367\n",
      "0.9856254537966584\n",
      "0.886847446648811\n",
      "1.687131771969897\n",
      "1.0317529048613818\n",
      "1.0197977877731015\n",
      "1.4684625196213774\n",
      "1.0253735080280966\n",
      "0.9281383948459726\n",
      "1.6046091042934123\n",
      "1.0390460824385803\n",
      "0.9272195432511745\n",
      "1.5545813606271526\n",
      "0.9827165834069445\n",
      "0.9357003429270233\n",
      "1.465224720880626\n",
      "0.8876368002630757\n",
      "0.8800101586763426\n",
      "1.4937052408095945\n",
      "1.0431803439989438\n",
      "0.9952437811587047\n",
      "1.6141337239175537\n",
      "1.0424304785852656\n",
      "0.8774992313112123\n",
      "1.4981810192516598\n",
      "1.0196221293272334\n",
      "0.9147233288288664\n",
      "1.4716632151472337\n",
      "0.9498647962978513\n",
      "0.919907096307405\n",
      "1.3749725897957483\n",
      "0.9061617379029088\n",
      "0.8806665109522814\n",
      "1.6523401043512398\n",
      "1.0132171725100123\n",
      "0.9640846125567024\n",
      "1.5910329419578715\n",
      "1.0129260665760507\n",
      "0.9032741682432953\n",
      "1.602765365289114\n",
      "1.064060873670333\n",
      "0.8951116648943408\n",
      "1.4688488664327486\n",
      "1.0525852031810201\n",
      "0.8767902709676275\n",
      "1.4268095188451377\n",
      "1.033831289742025\n",
      "0.9241964397672291\n",
      "1.4522779622995088\n",
      "0.8988383596715545\n",
      "0.8725482444918715\n",
      "1.4269191728057495\n",
      "1.0191852775325534\n",
      "0.9390740327452195\n",
      "1.4748480466410807\n",
      "1.0271178154610756\n",
      "0.8848893563454281\n",
      "1.4821120615588672\n",
      "0.9502929110370695\n",
      "0.8914422013642406\n",
      "1.4544611885380885\n",
      "0.9565073506641151\n",
      "0.9097447367048853\n",
      "1.5034209863971235\n",
      "1.0176793239523605\n",
      "0.8884527243129597\n",
      "1.6891909826427738\n",
      "1.0679584405491938\n",
      "1.037972991959036\n",
      "1.3155290496402248\n",
      "0.9263443718886029\n",
      "0.8944540660080393\n",
      "1.5069934035225048\n",
      "0.9661137511254649\n",
      "0.9368808324481762\n",
      "1.2723301677522094\n",
      "0.8814945945877264\n",
      "0.8764575813951191\n",
      "1.475882367273172\n",
      "0.9707276018482631\n",
      "0.9225679104379467\n",
      "1.5880981531470768\n",
      "1.0639715629939952\n",
      "1.0288049685326255\n",
      "1.551072156728567\n",
      "0.9804069839158134\n",
      "0.908767313701889\n",
      "1.4521381562253346\n",
      "1.0368348041678437\n",
      "0.8867736377573949\n",
      "1.4036896887686345\n",
      "0.9561021679916699\n",
      "0.9548220216074049\n",
      "1.6282903615959774\n",
      "1.0510124539643013\n",
      "0.8990519744306591\n",
      "1.4669595869779088\n",
      "0.9039565903581196\n",
      "0.8713404202104813\n",
      "1.6449362133272816\n",
      "1.0390468358723948\n",
      "0.9313006388620041\n",
      "1.7281391232346457\n",
      "1.0492299343364875\n",
      "1.0426954343074746\n",
      "1.5688245408543977\n",
      "1.0378513317869777\n",
      "0.9199782816116199\n",
      "1.6559319296587727\n",
      "1.0660851032896501\n",
      "0.9913577385766993\n",
      "1.4766491645131652\n",
      "1.0001427446198747\n",
      "0.9797151696855569\n",
      "1.5974137323231987\n",
      "1.0654449845873704\n",
      "0.8884370371705199\n",
      "1.4292422190372454\n",
      "0.9741733914825558\n",
      "0.9274016542556616\n",
      "1.6117275499676325\n",
      "1.0556411463536433\n",
      "0.9699975414270349\n",
      "1.5704239459214702\n",
      "1.0671986369351403\n",
      "0.9552043154750369\n",
      "1.4939595436586046\n",
      "1.0507233730804442\n",
      "0.9948251725169307\n",
      "1.5276602158186678\n",
      "0.9793303441025024\n",
      "0.9001708777576002\n",
      "1.3970144917104912\n",
      "0.896331084971952\n",
      "0.8701590009132361\n",
      "1.3457890786586553\n",
      "0.9763526193908859\n",
      "0.8725682668830252\n",
      "1.5940794527322981\n",
      "1.037851797079244\n",
      "0.9000455882176369\n",
      "1.465354440390834\n",
      "0.9489483843471255\n",
      "0.9299685721055964\n",
      "1.5360807065779911\n",
      "0.9927605388023806\n",
      "0.8733567536784805\n",
      "1.4651005084727315\n",
      "0.94927538565519\n",
      "0.9160841175756186\n",
      "1.4007988905322808\n",
      "0.969625932045007\n",
      "0.9359473518718104\n",
      "1.4186344946031473\n",
      "0.9974112810560796\n",
      "0.8995435282758044\n",
      "1.5981760403512653\n",
      "1.0521986173994722\n",
      "0.8807279974708419\n",
      "1.5324480180134188\n",
      "1.0427910463895418\n",
      "0.9010505645427368\n",
      "1.341379630126993\n",
      "0.8949099828759678\n",
      "0.8918732665235571\n",
      "1.6260676252915753\n",
      "1.0472171055085355\n",
      "0.9594341252163235\n",
      "1.5767477559239953\n",
      "0.9937744588932959\n",
      "0.9473077357947804\n",
      "1.6436050562375035\n",
      "1.0212659520589102\n",
      "0.9694215541569995\n",
      "1.5613859102491263\n",
      "0.9812384366337173\n",
      "0.978020220683086\n",
      "1.4238579696128861\n",
      "1.0670700804035327\n",
      "0.8742805666017406\n",
      "1.4334503411693815\n",
      "1.0327727420905952\n",
      "0.8966568635881452\n",
      "1.3962023792953793\n",
      "0.9410980514685238\n",
      "0.9199421938022043\n",
      "1.4279735753005562\n",
      "0.9819264093819264\n",
      "0.9652621861147326\n",
      "1.4745363452008902\n",
      "0.9720310253443661\n",
      "0.9087332738453358\n",
      "1.4940156431098526\n",
      "0.9246537610259239\n",
      "0.8713464537887513\n",
      "1.5751821128109027\n",
      "0.952493108128515\n",
      "0.9440294121524038\n",
      "1.6670102577434718\n",
      "1.021403895006943\n",
      "0.9788721867252151\n",
      "1.6018893150689044\n",
      "1.0533899745653617\n",
      "0.9623378626137314\n",
      "1.3712102547663294\n",
      "0.8973158279027114\n",
      "0.8749613092679993\n",
      "1.6155539444680183\n",
      "1.0346692478124855\n",
      "0.9603279845372138\n",
      "1.6343827457332085\n",
      "1.0004263156764097\n",
      "0.9968457727398782\n",
      "1.4568728207247823\n",
      "0.9293887998841729\n",
      "0.8692951580765196\n",
      "1.5502693604724656\n",
      "0.9739792152221903\n",
      "0.8874271938614242\n",
      "1.6948404330206974\n",
      "1.0619908842484933\n",
      "1.0103873440176794\n",
      "1.4981607695686954\n",
      "0.9381712846111473\n",
      "0.9196197742491315\n",
      "1.3485316161580605\n",
      "0.9634330521420144\n",
      "0.8991107088035756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5975028352220113\n",
      "1.0211919217583532\n",
      "0.9103811224731931\n",
      "1.604780482596879\n",
      "1.0242205767258699\n",
      "0.9044825132858672\n",
      "1.3844531841379975\n",
      "0.9096874016819451\n",
      "0.8878625006130606\n",
      "1.4637920634443358\n",
      "0.9382964650341575\n",
      "0.9039808499044448\n",
      "1.3986393493221685\n",
      "1.024397795248976\n",
      "0.9006401128967333\n",
      "1.425269607724355\n",
      "0.9332743815500215\n",
      "0.8714414164737437\n",
      "1.6396159884783708\n",
      "1.0455524748975251\n",
      "0.9951950234714156\n",
      "1.5664405146652156\n",
      "1.0082877721331316\n",
      "0.8737872913950874\n",
      "1.5292782359636725\n",
      "1.0195285241747738\n",
      "0.9218653010892787\n",
      "1.526988321417722\n",
      "0.9909948489705951\n",
      "0.8940879070505092\n",
      "1.446143541187612\n",
      "0.9500660030897695\n",
      "0.8848069564733074\n",
      "1.5052669405544026\n",
      "1.0306629741787512\n",
      "0.8890429627688854\n",
      "1.4607761785571793\n",
      "0.9827180778325147\n",
      "0.9764464519294097\n",
      "1.6986107350892905\n",
      "1.026431918583825\n",
      "0.9866063231374088\n",
      "1.3880295877406916\n",
      "0.8797956238845609\n",
      "0.8777854863214863\n",
      "1.5231181114835408\n",
      "0.9899255988019368\n",
      "0.8857003941390096\n",
      "1.4760141829270776\n",
      "0.908710205286805\n",
      "0.8924773849320761\n",
      "1.4556758257313152\n",
      "0.9604254997653306\n",
      "0.9310196378679402\n",
      "1.4271236271828291\n",
      "1.0219112439680242\n",
      "0.9053392055207257\n",
      "1.4138073078216975\n",
      "0.9694218822780237\n",
      "0.8722302141874669\n",
      "1.4975594304824247\n",
      "0.9701759690847863\n",
      "0.9367971540216586\n",
      "1.3727916471659383\n",
      "0.9393088352166428\n",
      "0.9097277084503921\n",
      "1.6064546468734011\n",
      "0.9814530136325343\n",
      "0.9584848393454889\n",
      "1.761137151188326\n",
      "1.0514858346226403\n",
      "1.0445626724757238\n",
      "1.530916095816641\n",
      "1.014524874336337\n",
      "0.909711968569583\n",
      "1.5500169260038241\n",
      "0.9860351738329801\n",
      "0.9605940885144643\n",
      "1.3714145579969286\n",
      "0.8827993650002656\n",
      "0.8704398169762654\n",
      "1.6912544303038803\n",
      "1.0538394838597283\n",
      "1.0272952593046505\n",
      "1.5281166456835493\n",
      "1.0130681946143225\n",
      "0.8944359111776284\n",
      "1.450006197067614\n",
      "1.0262925143686858\n",
      "0.9465933946550148\n",
      "1.4218939207449224\n",
      "1.0177290510153008\n",
      "0.9498372884400706\n",
      "1.4734736221604159\n",
      "1.0548862369381906\n",
      "0.9612944057359496\n",
      "1.621621562615488\n",
      "1.0512358885045185\n",
      "0.8939106060233503\n",
      "1.547658574050404\n",
      "1.0556474672576697\n",
      "0.910261712010768\n",
      "1.4989002615216784\n",
      "0.9412505247807595\n",
      "0.9014239444434624\n",
      "1.6582944112064144\n",
      "1.0363046776355247\n",
      "0.9330254908551385\n",
      "1.441422555090249\n",
      "0.9824785928091568\n",
      "0.9637919467150593\n",
      "1.4497744648124093\n",
      "0.8891985868036115\n",
      "0.8865531350946408\n",
      "1.6133113729833644\n",
      "1.017707576120641\n",
      "0.932350977823383\n",
      "1.5011072308363094\n",
      "1.035763782183392\n",
      "0.9761590429273193\n",
      "1.5980300076603569\n",
      "0.9858612269291205\n",
      "0.9244403218353131\n",
      "1.4254350045810482\n",
      "1.032909177494694\n",
      "0.8807087933644105\n",
      "1.386108383133828\n",
      "0.9571909400889712\n",
      "0.9425538630150773\n",
      "1.4886029716891658\n",
      "1.0268114821116536\n",
      "0.9717273202078461\n",
      "1.528074075631947\n",
      "0.9874515159398519\n",
      "0.9734159530853563\n",
      "1.5237748590073892\n",
      "1.0615025266047664\n",
      "0.8978513087814318\n",
      "1.5112347271978857\n",
      "0.9994483239862044\n",
      "0.8952547369318515\n",
      "1.5189586251349858\n",
      "0.9267362802009914\n",
      "0.9201972150519019\n",
      "1.5114556598304116\n",
      "0.9248880497867351\n",
      "0.8804912350846104\n",
      "1.6655309740954285\n",
      "1.0508844778466406\n",
      "1.0369310283264512\n",
      "1.514899798272544\n",
      "1.0239499782909813\n",
      "0.9380697072820144\n",
      "1.5404550651871678\n",
      "0.9544425438061479\n",
      "0.944837951968456\n",
      "1.366986153819858\n",
      "0.9180838446445188\n",
      "0.9048059831630528\n",
      "1.650369380551082\n",
      "1.0141277875646255\n",
      "0.9955471155921244\n",
      "1.4126322147132928\n",
      "1.0263934049363899\n",
      "0.8982451789630326\n",
      "1.524872609724905\n",
      "0.9972731854481546\n",
      "0.9206010967450411\n",
      "1.5568267088757786\n",
      "1.052878081925912\n",
      "1.0471183522497398\n",
      "1.3377905470015592\n",
      "0.8930467115351228\n",
      "0.8712604367122143\n",
      "1.3845542140074483\n",
      "0.9607769224091872\n",
      "0.8938341328112088\n",
      "1.6220795228763083\n",
      "1.0485173063018909\n",
      "0.8707720103518941\n",
      "1.509517706300964\n",
      "1.030450519390097\n",
      "0.8752832997224321\n",
      "1.5739891595348565\n",
      "1.039534434406186\n",
      "0.9912498177113048\n",
      "1.5947745963603586\n",
      "1.0202079082237263\n",
      "0.92152911547398\n",
      "1.6136588842649977\n",
      "1.0588060816951765\n",
      "1.0267076526714802\n",
      "1.5328071306925812\n",
      "1.0008929869432301\n",
      "0.9743769503671788\n",
      "1.5661577945723584\n",
      "0.9507567682800857\n",
      "0.9435034973565644\n",
      "1.448923727837972\n",
      "0.9862296347165826\n",
      "0.9634149293309944\n",
      "1.5393614637818236\n",
      "0.9786813623778482\n",
      "0.9770134107123641\n",
      "1.5081705393576739\n",
      "0.9978658781546308\n",
      "0.9781864547605051\n",
      "1.558512393297165\n",
      "1.0268879809029077\n",
      "0.9821347814274813\n",
      "1.6642198327492652\n",
      "1.0533793390168\n",
      "0.9348358377813779\n",
      "1.2999328514089972\n",
      "0.8878661582764031\n",
      "0.871388062066313\n",
      "1.5265372712307075\n",
      "0.9947171189038952\n",
      "0.9215880533658324\n",
      "1.5220435311630922\n",
      "0.9802301100574643\n",
      "0.9093178424804499\n",
      "1.4510505943552292\n",
      "1.0578910916929898\n",
      "0.9406032483799368\n",
      "1.4525241091295884\n",
      "0.9639551157365949\n",
      "0.8827412323181426\n",
      "1.42455681707796\n",
      "1.0076784544221287\n",
      "0.8844732307372135\n",
      "1.5291052533979952\n",
      "0.977807853423327\n",
      "0.9687232775364037\n",
      "1.444700583405986\n",
      "1.0295063044925474\n",
      "0.9308415696233485\n",
      "1.559823367077704\n",
      "1.023071482837521\n",
      "0.9813059203078849\n",
      "1.5517047426650536\n",
      "0.988968473505654\n",
      "0.885956491176573\n",
      "1.441891825877119\n",
      "1.0518435421838943\n",
      "0.9059008418467424\n",
      "1.4472081269260357\n",
      "0.8946144685580583\n",
      "0.8700645937805996\n",
      "1.56413002526062\n",
      "1.0013537375215233\n",
      "0.9813454022407341\n",
      "1.5402081486458892\n",
      "1.0283989353773744\n",
      "0.919578533800486\n",
      "1.3151004102718902\n",
      "0.8967162211804125\n",
      "0.8805105548330081\n",
      "1.5475600305659027\n",
      "1.002003300961668\n",
      "0.8908176083614877\n",
      "1.5121588722117771\n",
      "1.0080752396984687\n",
      "0.9601006962293958\n",
      "1.7184540810074629\n",
      "1.032715252429727\n",
      "1.0228532573543887\n",
      "1.3792352369018388\n",
      "0.9894410189112385\n",
      "0.9106572278871389\n",
      "1.3255399472576674\n",
      "0.9278476642245886\n",
      "0.8777099077068146\n",
      "1.4422675098823097\n",
      "0.9294330104442686\n",
      "0.8813030354564819\n",
      "1.647242578997483\n",
      "1.0194033725013545\n",
      "1.0141330220719433\n",
      "1.3748355541424018\n",
      "0.9266415927296945\n",
      "0.8919214621028524\n",
      "1.3261460414690478\n",
      "0.8824953268396228\n",
      "0.8696850994545611\n",
      "1.4558447358182\n",
      "0.9628222047376445\n",
      "0.8760118732567955\n",
      "1.4688518288769108\n",
      "0.915009399978031\n",
      "0.9102048074800173\n",
      "1.4129828129080462\n",
      "0.9291329303626\n",
      "0.8731740896499127\n",
      "1.6636311442221696\n",
      "1.0610493215259031\n",
      "0.9735044545442681\n",
      "1.5157187773995198\n",
      "1.0299349894548993\n",
      "0.935161840068841\n",
      "1.4842640309878377\n",
      "1.0107002781328314\n",
      "1.0066610320141975\n",
      "1.3917103120378287\n",
      "0.9228002588797088\n",
      "0.8753562686950843\n",
      "1.5559988312876287\n",
      "1.0030240445959682\n",
      "0.8704741544695298\n",
      "1.373410106374897\n",
      "0.9444143177240313\n",
      "0.9254717957706564\n",
      "1.6583950821414666\n",
      "1.0303928599532945\n",
      "0.9661045327136082\n",
      "1.4759677336248729\n",
      "0.9131929804458819\n",
      "0.8996940536937843\n",
      "1.5886903193308666\n",
      "1.032351965181662\n",
      "0.8823538204358913\n",
      "1.3763683379866123\n",
      "0.8826103585765906\n",
      "0.8755718236756188\n",
      "1.4196220258395857\n",
      "0.914338456470383\n",
      "0.9103732660242598\n",
      "1.6245739213999733\n",
      "1.0349315014576894\n",
      "0.9158317053577194\n",
      "1.687550301535364\n",
      "1.0615097078735611\n",
      "1.0321110634171156\n",
      "1.6134100330193484\n",
      "1.0443604292696094\n",
      "0.878441091514578\n",
      "1.7422279120617694\n",
      "1.0630708708861725\n",
      "1.0191099722598682\n",
      "1.567611696433623\n",
      "1.0361190553482698\n",
      "0.8869320276865726\n",
      "1.4813085168197544\n",
      "1.0615388701456907\n",
      "0.8921831315708786\n",
      "1.3677529644824147\n",
      "0.9087352412686099\n",
      "0.898367603016467\n",
      "1.5042621825137321\n",
      "0.9083984355641405\n",
      "0.886030745413316\n",
      "1.447174265908265\n",
      "1.0631971160658027\n",
      "0.8700290365407971\n",
      "1.6977792140012524\n",
      "1.0548429291893846\n",
      "1.0065497705396844\n",
      "1.556614668983923\n",
      "1.0672654835416808\n",
      "0.8853877250883494\n",
      "1.565897446488859\n",
      "1.0425533941227132\n",
      "0.9185307395204958\n",
      "1.5023267739985076\n",
      "1.0644221095878381\n",
      "0.9080119413947833\n",
      "1.5733123503636979\n",
      "1.0000001126184777\n",
      "0.9685336607632435\n",
      "1.5151817278992645\n",
      "1.0108245634273265\n",
      "0.9320898844641138\n",
      "1.3724212703428993\n",
      "0.9567064780471393\n",
      "0.9294676766163489\n",
      "1.4404977154748777\n",
      "1.0599519904675487\n",
      "0.9050283829222464\n",
      "1.760254053094474\n",
      "1.0566070963855774\n",
      "1.0364146858304952\n",
      "1.5206889747388728\n",
      "1.0338846803700446\n",
      "0.8758704313712755\n",
      "1.3804084015694191\n",
      "0.9852605202175668\n",
      "0.8987527177453621\n",
      "1.5302327089623986\n",
      "1.0410811304813723\n",
      "0.9603071622535774\n",
      "1.598857334833258\n",
      "1.0528770193317076\n",
      "0.9367687165951588\n",
      "1.4313401029873203\n",
      "1.0440743357660551\n",
      "0.9024398415393036\n",
      "1.517169943250262\n",
      "0.9957996498732223\n",
      "0.9872742413192813\n",
      "1.4674050022429406\n",
      "0.997404617668264\n",
      "0.9014857377540064\n",
      "1.571652856631236\n",
      "1.0325552781854055\n",
      "0.9151136543496764\n",
      "1.3597011230603469\n",
      "0.9843712064407621\n",
      "0.8851240996798391\n",
      "1.4281164768782055\n",
      "1.040654211911292\n",
      "0.8987581424675768\n",
      "1.4365220804918788\n",
      "0.9279000230725696\n",
      "0.8905194612543145\n",
      "1.601925420931228\n",
      "1.0430064613859202\n",
      "1.0425148125657102\n",
      "1.5476882525251436\n",
      "0.9568951713355426\n",
      "0.8888751971986628\n",
      "1.48331958114247\n",
      "0.9314192245004953\n",
      "0.8775687143466537\n",
      "1.6370741187011832\n",
      "1.0397658388139284\n",
      "1.0283517388182364\n",
      "1.3682928086837216\n",
      "0.9873754251881318\n",
      "0.8970713017244106\n",
      "1.480514622602925\n",
      "1.0206336033290995\n",
      "0.9003016227582721\n",
      "1.5297979364661978\n",
      "0.9258121899399279\n",
      "0.9176094503818436\n",
      "1.6692330448381802\n",
      "1.0468568657412534\n",
      "0.9426685950715661\n",
      "1.5987619444788568\n",
      "1.0438014949778953\n",
      "0.9436587799361931\n",
      "1.59728792359494\n",
      "1.0153697943228985\n",
      "0.9557878143310932\n",
      "1.5214269492565786\n",
      "0.9553033826830679\n",
      "0.9500240644647182\n",
      "1.534738435174129\n",
      "0.9990268129363449\n",
      "0.8920656154348092\n",
      "1.4451497798134492\n",
      "0.9538349851006857\n",
      "0.9411955270603077\n",
      "1.5652185001475607\n",
      "0.9829109235909204\n",
      "0.8759627131469603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6038518218830173\n",
      "1.063367542560589\n",
      "0.9257042402244118\n",
      "1.6800808797329787\n",
      "1.0583487089254064\n",
      "1.0246669082257482\n",
      "1.6465854682596526\n",
      "1.0373003103199794\n",
      "0.9593537467397191\n",
      "1.6973580028981476\n",
      "1.0552862118888133\n",
      "1.0096612732041756\n",
      "1.6355388469104766\n",
      "1.0610387202979206\n",
      "0.9464264836324733\n",
      "1.5005347588965217\n",
      "0.9103165378028225\n",
      "0.8949546545914173\n",
      "1.5549109015137452\n",
      "1.0114303222696648\n",
      "1.0084144246487756\n",
      "1.4935760792871295\n",
      "0.9849045482382012\n",
      "0.886011490216791\n",
      "1.3994449822615502\n",
      "0.9534231492170598\n",
      "0.9046443625763156\n",
      "1.6230930133962214\n",
      "1.048370109134751\n",
      "0.8947553794316734\n",
      "1.543821056017721\n",
      "0.995025548644164\n",
      "0.9007618587649382\n",
      "1.7612956306664074\n",
      "1.0670689840063883\n",
      "1.0251992647528252\n",
      "1.5514663286122152\n",
      "1.0302393671022365\n",
      "0.9531962152090282\n",
      "1.5180705506091612\n",
      "1.0312797455849643\n",
      "1.0205287216371184\n",
      "1.5353950551353852\n",
      "0.9407598841653363\n",
      "0.8854820135856\n",
      "1.4126864408206843\n",
      "1.0075953152644896\n",
      "0.873776206138256\n",
      "1.465075572580378\n",
      "1.0144990440847197\n",
      "1.0038933425477876\n",
      "1.6262694365054426\n",
      "1.0194779582698932\n",
      "0.94098717360187\n",
      "1.7458307794752355\n",
      "1.0600930090443443\n",
      "1.0451626667912381\n",
      "1.5540143758394769\n",
      "1.0116611327037441\n",
      "1.0020039217063348\n",
      "1.574657637275017\n",
      "1.0181561215938044\n",
      "0.9051105426039099\n",
      "1.3966026676004242\n",
      "0.9005592015668046\n",
      "0.869710090930791\n",
      "1.646010609824244\n",
      "1.020963937240381\n",
      "0.9574834984300253\n",
      "1.5778765143358742\n",
      "1.0462480230138322\n",
      "0.9751127696904035\n",
      "1.539993454628535\n",
      "1.006032284681524\n",
      "0.8814896341203341\n",
      "1.5154278808374098\n",
      "1.003769059049782\n",
      "0.8713288775434759\n",
      "1.7395152304204828\n",
      "1.0354046318746304\n",
      "1.027234667031714\n",
      "1.4844443428976666\n",
      "1.0415620305278936\n",
      "0.9826608070104159\n",
      "1.542619104354514\n",
      "0.9353711666377326\n",
      "0.9030323551176704\n",
      "1.5670214183938909\n",
      "0.9686963310225934\n",
      "0.8899893673163335\n",
      "1.4849992940785939\n",
      "0.9923564394834568\n",
      "0.9806380526415371\n",
      "1.5290357210766474\n",
      "0.9630192675687478\n",
      "0.9629248987667086\n",
      "1.3765322909325197\n",
      "0.9515817069613806\n",
      "0.9047570993935729\n",
      "1.6164087425178193\n",
      "1.019176623567939\n",
      "0.9670208159056949\n",
      "1.5692725764400046\n",
      "1.0143749577188585\n",
      "0.9542433862177829\n",
      "1.4292332062709412\n",
      "0.9821990623274761\n",
      "0.8966639211049061\n",
      "1.5804432816700076\n",
      "0.9978967389150383\n",
      "0.9960486934046681\n",
      "1.537266480684179\n",
      "0.9472539455534954\n",
      "0.9226741109270996\n",
      "1.5163392272768208\n",
      "1.0491768855370474\n",
      "0.8895672785033587\n",
      "1.5475823378097069\n",
      "1.0035783925294224\n",
      "0.9480542937871657\n",
      "1.5372360480445628\n",
      "1.0378314525061147\n",
      "0.8704426443933606\n",
      "1.439756519658994\n",
      "1.020524759107803\n",
      "0.8922210290216573\n",
      "1.361286742136675\n",
      "0.9515177920826896\n",
      "0.9129572838786204\n",
      "1.3930318809379287\n",
      "0.9630001562179957\n",
      "0.9401373751355618\n",
      "1.6191952612312441\n",
      "1.0517910484452868\n",
      "0.8964154291781411\n",
      "1.6443761959387346\n",
      "1.0259825613960647\n",
      "1.012216981975405\n",
      "1.5900079853112257\n",
      "0.9960880060227192\n",
      "0.9473844654661316\n",
      "1.466474631839819\n",
      "1.0106012007597878\n",
      "0.9135916401109647\n",
      "1.6735824904016487\n",
      "1.02599414519007\n",
      "0.990449286841994\n",
      "1.557505663396856\n",
      "0.9783800819903746\n",
      "0.8864372633710756\n",
      "1.5084373254322831\n",
      "0.9917820073217506\n",
      "0.9811160583269917\n",
      "1.4856346270543226\n",
      "1.0357706010803407\n",
      "0.9182830185328745\n",
      "1.5017436341326642\n",
      "0.9897713443225424\n",
      "0.9471097406913321\n",
      "1.652497589650645\n",
      "1.0444351128010765\n",
      "0.9758202861541433\n",
      "1.64477347577248\n",
      "1.0395932981116485\n",
      "1.0372434488946582\n",
      "1.4821354076649218\n",
      "0.9781980468977963\n",
      "0.8710127663991412\n",
      "1.502239182799227\n",
      "1.0517237110741433\n",
      "0.9117689099792434\n",
      "1.6251687077171864\n",
      "1.0479220850695412\n",
      "0.9215789308417394\n",
      "1.4464022978740663\n",
      "0.9215798384581241\n",
      "0.9106264914473288\n",
      "1.3387710914956785\n",
      "0.9527268610527095\n",
      "0.872735518154773\n",
      "1.7354013409904168\n",
      "1.0667056897082965\n",
      "1.0591474237488003\n",
      "1.419789527346911\n",
      "0.9973025854178806\n",
      "0.9109252668359816\n",
      "1.3404828968360936\n",
      "0.9598189555392473\n",
      "0.8979937341945167\n",
      "1.6224173238235384\n",
      "0.9913406404354371\n",
      "0.9322059605033663\n",
      "1.3573191310686905\n",
      "0.973565754366276\n",
      "0.8897850934014387\n",
      "1.5831556060543819\n",
      "0.9812020434510157\n",
      "0.9349665839410117\n",
      "1.6084947497882973\n",
      "1.0482337313224306\n",
      "0.8804724228818716\n",
      "1.4915114273220673\n",
      "1.0502352745760732\n",
      "0.9597014047292204\n",
      "1.666344862538454\n",
      "1.0165419476378463\n",
      "0.964190519615737\n",
      "1.6609159833150595\n",
      "1.0408719660185457\n",
      "0.9743861399124433\n",
      "1.6262485996853382\n",
      "1.036961072061739\n",
      "0.9069263656914659\n",
      "1.5739416255430192\n",
      "1.0105135442447142\n",
      "0.88100733829804\n",
      "1.5691480095306387\n",
      "1.068409929958828\n",
      "0.9367955409788599\n",
      "1.7141769497444292\n",
      "1.0314334518029775\n",
      "1.0099342487625926\n",
      "1.6662386672633462\n",
      "1.0122358827011717\n",
      "0.9682924924704159\n",
      "1.5170783298511668\n",
      "1.014639112467193\n",
      "0.9686288009293204\n",
      "1.4956630496888674\n",
      "0.9430953533222587\n",
      "0.8846226014458288\n",
      "1.585410969141163\n",
      "1.0547059172642526\n",
      "0.9537826833053317\n",
      "1.3553058879540583\n",
      "0.9553151850630568\n",
      "0.8710405273767007\n",
      "1.5193045023416394\n",
      "0.9696945397634954\n",
      "0.9655275362255463\n",
      "1.5512400403175275\n",
      "1.0278529396899796\n",
      "0.8791499302407754\n",
      "1.6063826117716054\n",
      "0.9795028656102944\n",
      "0.9381432228498922\n",
      "1.6428546007775529\n",
      "1.0294912360341772\n",
      "1.0199123189542718\n",
      "1.5730199387136692\n",
      "1.0127713528936648\n",
      "0.9789066673677294\n",
      "1.5383816100294694\n",
      "0.9731119967821518\n",
      "0.9170438924659573\n",
      "1.647599064202656\n",
      "1.068464750226425\n",
      "1.0036245291158852\n",
      "1.4789860284724743\n",
      "0.9406695861953599\n",
      "0.8821044060544617\n",
      "1.4945970331903617\n",
      "1.0263045588525765\n",
      "0.9162082641613228\n",
      "1.4775380089010637\n",
      "1.0262907025629533\n",
      "0.8835141064724581\n",
      "1.5793660661263773\n",
      "0.9724683590549658\n",
      "0.940603203204964\n",
      "1.4569980747113351\n",
      "0.9775228749568834\n",
      "0.9295786965244439\n",
      "1.3150076044449373\n",
      "0.9031264957346317\n",
      "0.8942143347142617\n",
      "1.472785463740192\n",
      "1.0409763910059167\n",
      "0.9618530152139485\n",
      "1.693580135176038\n",
      "1.0398472246830888\n",
      "0.9886743088936683\n",
      "1.588855985263296\n",
      "1.065670225799812\n",
      "0.9047929616536275\n",
      "1.3814451125641034\n",
      "0.8976532587762863\n",
      "0.886590685674422\n",
      "1.6010102487543376\n",
      "1.034799113867019\n",
      "0.9545673138845517\n",
      "1.5230711624644533\n",
      "1.050186130011032\n",
      "0.8730262603033659\n",
      "1.47118104868901\n",
      "0.9941838045478981\n",
      "0.88523376180819\n",
      "1.5570035931547481\n",
      "0.9326456109648075\n",
      "0.9245768013237496\n",
      "1.4844460028045643\n",
      "1.0126058009809307\n",
      "0.9209026773739929\n",
      "1.4010508387297114\n",
      "0.9634033966003409\n",
      "0.8768831743912244\n",
      "1.4034889344823414\n",
      "0.9935836538992031\n",
      "0.9436028207658731\n",
      "1.4594997232917624\n",
      "1.0531593928818086\n",
      "0.9170229003654013\n",
      "1.7426005419934394\n",
      "1.0567340817386668\n",
      "1.0204935426395791\n",
      "1.6131171919695777\n",
      "1.0419942494354757\n",
      "1.0411815321959672\n",
      "1.6032398121842104\n",
      "1.0085512197796387\n",
      "0.8939484839283647\n",
      "1.5136272088657179\n",
      "0.9279315520560469\n",
      "0.8805224707357799\n",
      "1.533875164089235\n",
      "0.9990056208761752\n",
      "0.9383662043187775\n",
      "1.7569917280874925\n",
      "1.0447479837147804\n",
      "1.0433686942115303\n",
      "1.4822114340617434\n",
      "1.0655226867763525\n",
      "0.956429006965632\n",
      "1.5813862981357438\n",
      "1.058309263189111\n",
      "0.9446744228855382\n",
      "1.6075865623564827\n",
      "1.04156302416393\n",
      "1.0263878015190022\n",
      "1.4417792142691568\n",
      "1.0063377716972115\n",
      "0.9222025343663068\n",
      "1.5512822854171104\n",
      "1.0333532126360916\n",
      "0.9733065446797017\n",
      "1.43440420776227\n",
      "0.8934386637804879\n",
      "0.8774661998795583\n",
      "1.5483410750239677\n",
      "0.9345251575929443\n",
      "0.9304271964088185\n",
      "1.5600615770096968\n",
      "1.0421920304522012\n",
      "1.016202850227635\n",
      "1.4985445117596583\n",
      "0.9407574027075059\n",
      "0.9247200122481919\n",
      "1.4334764846361812\n",
      "0.943685674760973\n",
      "0.893612753806588\n",
      "1.6658123059143242\n",
      "1.0323864130916527\n",
      "0.9473618801335583\n",
      "1.435428869114949\n",
      "1.0533725220992822\n",
      "0.8890456042979745\n",
      "1.638974346610714\n",
      "1.06534156367106\n",
      "0.8981333852849044\n",
      "1.4139905710321568\n",
      "1.0184327130078414\n",
      "0.938941527434078\n",
      "1.6937255879246633\n",
      "1.0244772654680987\n",
      "1.0014991335329524\n",
      "1.4757430884502878\n",
      "0.8911329507365446\n",
      "0.8832269570395832\n",
      "1.5641039321737307\n",
      "1.0104300490211333\n",
      "0.9124140430499605\n",
      "1.5840943839835497\n",
      "1.065059111218253\n",
      "0.8813085085711614\n",
      "1.5631666880554178\n",
      "1.0416517583546576\n",
      "0.9050436381958619\n",
      "1.4939288360461083\n",
      "0.932260399830964\n",
      "0.9160361347347424\n",
      "1.5967654171165657\n",
      "0.9961239175795398\n",
      "0.9880476980224209\n",
      "1.5173495165334068\n",
      "1.047107955955769\n",
      "0.9010834686849526\n",
      "1.6168951328804313\n",
      "1.0539848485591439\n",
      "0.961090565637133\n",
      "1.3086919330032438\n",
      "0.9153893027396945\n",
      "0.893611604619074\n",
      "1.3709318970694295\n",
      "0.9016626053355883\n",
      "0.8858347811155577\n",
      "1.535627769397882\n",
      "1.012572609309832\n",
      "0.9459497536729014\n",
      "1.6970456378036285\n",
      "1.0438070244966908\n",
      "0.9708579346868659\n",
      "1.6161730995285393\n",
      "0.9736519931068365\n",
      "0.9544927774770748\n",
      "1.5202504920437043\n",
      "0.9722133327510069\n",
      "0.9442962410822378\n",
      "1.6475214114669277\n",
      "1.0581689831370924\n",
      "0.9603035462758801\n",
      "1.6935880578701143\n",
      "1.0561686242210715\n",
      "0.9878009253818206\n",
      "1.6571838598579705\n",
      "1.0226871320915993\n",
      "0.9941401775607901\n",
      "1.390716595494381\n",
      "1.0285359619681085\n",
      "0.8873676424345686\n",
      "1.3588216166191478\n",
      "0.9400603732476625\n",
      "0.9258100185391768\n",
      "1.41860563748333\n",
      "0.8895856698232952\n",
      "0.882814578438814\n",
      "1.6507877598632086\n",
      "1.0452161611554482\n",
      "0.9096627955369858\n",
      "1.548940906583661\n",
      "1.0259934659866505\n",
      "1.0142885719493473\n",
      "1.4869211914711895\n",
      "1.0126698946569548\n",
      "0.9993963735622156\n",
      "1.4676105687433494\n",
      "0.9302880058045888\n",
      "0.8822118950178685\n",
      "1.6601326241178354\n",
      "1.0127111087044698\n",
      "0.9870011378050297\n",
      "1.4648800272195772\n",
      "1.0303147155630035\n",
      "0.8890244294118631\n",
      "1.4762152028288253\n",
      "0.9972878372925008\n",
      "0.930997247412361\n",
      "1.4799912407150528\n",
      "0.9122956487893393\n",
      "0.8729299080751273\n",
      "1.5271245854528643\n",
      "0.9196634701458785\n",
      "0.8914907158197404\n",
      "1.4239644083373948\n",
      "1.034824902845493\n",
      "0.9182608328744661\n",
      "1.5627584218952493\n",
      "0.9977454778657181\n",
      "0.9118909368225676\n",
      "1.5233646412893442\n",
      "1.019454202662687\n",
      "0.8804363549416434\n",
      "1.4313242166706888\n",
      "0.9385137256279593\n",
      "0.905618538683226\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4897732765161076\n",
      "0.9250000694353694\n",
      "0.8790929218846355\n",
      "1.6095125836998516\n",
      "1.032010243848672\n",
      "0.938640677003351\n",
      "1.398467429048943\n",
      "1.0129487328199023\n",
      "0.8730773051188867\n",
      "1.6010494319831516\n",
      "1.0003582534844515\n",
      "0.9562937877188883\n",
      "1.4543927016569558\n",
      "0.936808933307579\n",
      "0.9111318607981578\n",
      "1.3991898278334898\n",
      "1.0035768131959335\n",
      "0.9274897402788974\n",
      "1.3587213344587308\n",
      "0.9340130783896426\n",
      "0.8878291620077572\n",
      "1.5170762952489847\n",
      "1.0351237280917185\n",
      "0.8844807415397187\n",
      "1.622899096891369\n",
      "1.066062977933567\n",
      "0.9540975316907621\n",
      "1.5879029500630502\n",
      "0.979815490974897\n",
      "0.9563633827709347\n",
      "1.501107714756661\n",
      "0.9858177877538544\n",
      "0.9017503133312397\n",
      "1.359866303329431\n",
      "0.9190265701516583\n",
      "0.9002528380827357\n",
      "1.5208215976475796\n",
      "0.98479362463361\n",
      "0.9266075579443543\n",
      "1.5930471847909355\n",
      "1.0236698757981488\n",
      "0.9512111186458756\n",
      "1.4417966767596178\n",
      "1.0429130112138387\n",
      "0.9351522518117811\n",
      "1.579283085542821\n",
      "1.028058494960882\n",
      "0.9979261918265877\n",
      "1.3240487009843807\n",
      "0.9462293154427155\n",
      "0.8719339161235996\n",
      "1.429489177547735\n",
      "0.9753559359370255\n",
      "0.9111526231766583\n",
      "1.3304076353302945\n",
      "0.9345924845365102\n",
      "0.911637799391434\n",
      "1.540194565196452\n",
      "1.059274999034892\n",
      "0.9226102423044167\n",
      "1.4329163384184014\n",
      "0.9914099822378553\n",
      "0.9782316520133972\n",
      "1.4883861373959186\n",
      "0.9705090581228641\n",
      "0.8935746466515896\n",
      "1.4951722212326386\n",
      "0.9240739130516272\n",
      "0.8750800248038064\n",
      "1.5509932020101826\n",
      "1.05158328712561\n",
      "0.8873464776944143\n",
      "1.3847699493876775\n",
      "1.0068826798566872\n",
      "0.880772333306285\n",
      "1.6085050114473214\n",
      "0.9873646227686926\n",
      "0.9359131045626415\n",
      "1.5008579565101283\n",
      "0.9471249405504442\n",
      "0.8903345940940085\n",
      "1.5094034158054144\n",
      "1.027215377745769\n",
      "0.8712788914444133\n",
      "1.597887235821251\n",
      "0.9654558208052718\n",
      "0.929987472003044\n",
      "1.5850263313104425\n",
      "1.0363948050849225\n",
      "0.9923238025899348\n",
      "1.6411902505583218\n",
      "1.038102083565328\n",
      "0.9957336579383821\n",
      "1.5049611134902434\n",
      "0.9052952662401692\n",
      "0.8986250979778242\n",
      "1.6253824409788942\n",
      "1.0517439393779764\n",
      "0.998181880337792\n",
      "1.4560942827884367\n",
      "1.0228690773806495\n",
      "0.8780182813895296\n",
      "1.6577993454185602\n",
      "1.0535861523313896\n",
      "0.9856299758465921\n",
      "1.6824245975723557\n",
      "1.0210935555917815\n",
      "0.99268397250428\n",
      "1.6093006959588105\n",
      "1.017187665116848\n",
      "0.9108402390576749\n",
      "1.5498930926587087\n",
      "0.9998342451313805\n",
      "0.9774175438832043\n",
      "1.6075422459117166\n",
      "1.0347392507092221\n",
      "0.9819354051289078\n",
      "1.4555500429358652\n",
      "0.9860679708087613\n",
      "0.9632562264417476\n",
      "1.6596391327889906\n",
      "1.0613633029473328\n",
      "0.950901508952774\n",
      "1.5007751760828294\n",
      "0.9372210651824071\n",
      "0.8783085457177757\n",
      "1.316717522714181\n",
      "0.8954526330135393\n",
      "0.8727066313830444\n",
      "1.394050448216351\n",
      "0.953372440881079\n",
      "0.8992861689268675\n",
      "1.4885548010455925\n",
      "0.927876766091867\n",
      "0.8922821018830568\n",
      "1.3971184668501706\n",
      "0.9949203415358623\n",
      "0.8841174455434107\n",
      "1.4844780489668767\n",
      "1.0570637891705972\n",
      "0.8732183355009974\n",
      "1.5688822215967664\n",
      "1.0256076468267845\n",
      "0.8924039027168641\n",
      "1.4996114092153523\n",
      "0.9647328001600579\n",
      "0.8756151287454235\n",
      "1.5200341119270946\n",
      "0.9951596077726883\n",
      "0.901258736109666\n",
      "1.5015292905359017\n",
      "1.0482704526307094\n",
      "1.015461034366357\n",
      "1.3684026950935901\n",
      "0.9825580371526429\n",
      "0.900140180466844\n",
      "1.5315629513968723\n",
      "0.9527335828649115\n",
      "0.8765973555254578\n",
      "1.6869058819526208\n",
      "1.067677561512236\n",
      "0.9792762072043727\n",
      "1.4050480859856846\n",
      "0.9430801374883194\n",
      "0.9247533728125727\n",
      "1.6669258818334884\n",
      "1.0522640431682988\n",
      "1.0310503808218345\n",
      "1.6116166167927648\n",
      "1.030033821875315\n",
      "1.0249831464312775\n",
      "1.4789870746911529\n",
      "0.8947274197274211\n",
      "0.8743587632122874\n",
      "1.4884640687368829\n",
      "0.9970979382865288\n",
      "0.8928361144163337\n",
      "1.3781400277411893\n",
      "0.9501853698881221\n",
      "0.8984630943938446\n",
      "1.6027451087431828\n",
      "1.0380613176428284\n",
      "0.9892613764504735\n",
      "1.4558667114493484\n",
      "1.0199734349594047\n",
      "0.9327776262583851\n",
      "1.6214798674915094\n",
      "0.998801135613243\n",
      "0.981949545404279\n",
      "1.6118028639635766\n",
      "1.03841368037653\n",
      "1.0353483924183442\n",
      "1.5639218776358856\n",
      "0.9782046414002351\n",
      "0.9023747477649556\n",
      "1.6013686959684237\n",
      "1.0116108295415684\n",
      "0.9805237873575593\n",
      "1.4867747500992803\n",
      "0.9086178348537388\n",
      "0.8757867166050376\n",
      "1.5160891770810123\n",
      "0.9062544579773903\n",
      "0.8943485386495522\n",
      "1.5419512719061275\n",
      "1.0569934891303194\n",
      "0.9167939320285134\n",
      "1.5387859752171342\n",
      "0.9987278135807919\n",
      "0.9929149852179052\n",
      "1.6657086617946901\n",
      "1.0652335825197237\n",
      "0.9581168301598052\n",
      "1.6484235964917624\n",
      "1.063176988884775\n",
      "0.9859549195203322\n",
      "1.6417783337195386\n",
      "0.9978404438021918\n",
      "0.9915169230525317\n",
      "1.3659802408160708\n",
      "0.925728464339439\n",
      "0.881433834390371\n",
      "1.7060554108063282\n",
      "1.068425650135517\n",
      "1.0018933692976764\n",
      "1.4778875404038787\n",
      "0.9276333935666509\n",
      "0.9235353321852058\n",
      "1.4580811088451633\n",
      "0.9888455662053963\n",
      "0.9378250843257316\n",
      "1.3489374288774212\n",
      "0.9026721317422557\n",
      "0.8900154509435281\n",
      "1.5805569002015096\n",
      "1.059415296416511\n",
      "0.985693347894921\n",
      "1.4245824378731065\n",
      "0.9690060128793229\n",
      "0.9526223517331083\n",
      "1.3904310353795906\n",
      "1.0116113890388116\n",
      "0.8902535328344834\n",
      "1.7298631531039854\n",
      "1.0579822307534494\n",
      "1.027851305571781\n",
      "1.5394828356885122\n",
      "0.9605763328380555\n",
      "0.9480470232311378\n",
      "1.645903251351189\n",
      "1.0252853865431681\n",
      "0.9644260099484047\n",
      "1.430293362610314\n",
      "1.0324071634310859\n",
      "0.9088454162748221\n",
      "1.654380594929709\n",
      "1.0121016795758562\n",
      "0.9549268784377813\n",
      "1.6543582763293558\n",
      "1.0415774724468057\n",
      "0.947294236879653\n",
      "1.5089342481715708\n",
      "1.0041650141399263\n",
      "0.8903753950946423\n",
      "1.541841650295417\n",
      "1.062470672916377\n",
      "1.0001121809279296\n",
      "1.5081652134592431\n",
      "0.9646208394644388\n",
      "0.9512679195536421\n",
      "1.4122360608452917\n",
      "0.9949217235763693\n",
      "0.9468712821913146\n",
      "1.5039707937439106\n",
      "1.0381410920470528\n",
      "1.0023848705361087\n",
      "1.5391870719917904\n",
      "1.0104177153928338\n",
      "0.9859998094106578\n",
      "1.6977774419991944\n",
      "1.0325396926420556\n",
      "0.9969499764334918\n",
      "1.4175548231145503\n",
      "0.9593501867647581\n",
      "0.9452405728502684\n",
      "1.4469617562846846\n",
      "0.9627803051687878\n",
      "0.9378209195187663\n",
      "1.6259600632837587\n",
      "1.0521793621826976\n",
      "0.9574407515680208\n",
      "1.477338536477865\n",
      "1.0227774948892032\n",
      "0.9474009536760373\n",
      "1.6704166656495447\n",
      "1.0066868225757302\n",
      "1.0050946949177522\n",
      "1.4374180955836944\n",
      "0.9791937110154016\n",
      "0.8939405135965947\n",
      "1.5063778751033627\n",
      "0.9463164630449538\n",
      "0.8816443824426228\n",
      "1.4695851712479544\n",
      "1.0654842912408633\n",
      "0.9397098256541615\n",
      "1.4910506228589637\n",
      "1.0304011520294147\n",
      "0.9002305493902737\n",
      "1.4972694899770576\n",
      "1.0657836023114269\n",
      "0.9233638739970307\n",
      "1.54529051781405\n",
      "0.998851546372521\n",
      "0.9779667364794337\n",
      "1.553666951846496\n",
      "1.0252774045201438\n",
      "0.9976295067651739\n",
      "1.4191338540632081\n",
      "0.945469897496837\n",
      "0.9078831582229215\n",
      "1.6467295347914102\n",
      "0.9946831552259733\n",
      "0.9767329148087173\n",
      "1.597443683187043\n",
      "1.011299512839303\n",
      "0.9764955779119965\n",
      "1.5463196991822765\n",
      "0.9461928842197769\n",
      "0.8966272506702493\n",
      "1.4240971451838924\n",
      "1.014945250125092\n",
      "0.9247526141097695\n",
      "1.5926081656331894\n",
      "1.0490706386637212\n",
      "0.9153039393130215\n",
      "1.5439116424473842\n",
      "0.9701611362918555\n",
      "0.9037587317819097\n",
      "1.5529999946335178\n",
      "0.9770411529412237\n",
      "0.9719098401376136\n",
      "1.343247274752993\n",
      "0.9493168342473023\n",
      "0.8771794694635725\n",
      "1.4588769236310102\n",
      "0.9901983267795\n",
      "0.8842006258202564\n",
      "1.601661573942437\n",
      "1.0377535630409056\n",
      "1.0345203322143226\n",
      "1.3550777154589209\n",
      "0.9603113322411146\n",
      "0.9025922297613204\n",
      "1.5910882882175348\n",
      "0.9815214583169884\n",
      "0.9462694168310659\n",
      "1.5059609855994243\n",
      "0.9944366004721279\n",
      "0.8893178317660961\n",
      "1.5636043992975552\n",
      "0.9488769577104624\n",
      "0.9363105657423915\n",
      "1.6812371982071734\n",
      "1.0687954040086685\n",
      "0.9979635512677936\n",
      "1.606717793497234\n",
      "1.0349110277582225\n",
      "0.9717999399656878\n",
      "1.6493576670327306\n",
      "1.063173473903346\n",
      "0.9291429960627058\n",
      "1.3622726083873606\n",
      "0.9786985182371918\n",
      "0.8775833481511311\n",
      "1.46753733888186\n",
      "1.066827341302604\n",
      "0.915469172420823\n",
      "1.317794646493778\n",
      "0.9235288106942582\n",
      "0.8871944346052015\n",
      "1.6440434196446767\n",
      "1.0107922536283787\n",
      "0.9555911257510924\n",
      "1.4802050818533063\n",
      "1.0490606612209414\n",
      "0.9344104016656549\n",
      "1.6027811605425522\n",
      "1.0250481776564138\n",
      "0.9971327556466576\n",
      "1.6063122745263865\n",
      "1.0214156766082543\n",
      "0.9825770233140475\n",
      "1.4312620753475962\n",
      "0.9263540375120287\n",
      "0.9169899521029511\n",
      "1.4498256498461208\n",
      "1.0170181475046678\n",
      "0.8864308250984674\n",
      "1.4450713269797477\n",
      "0.9798653856053061\n",
      "0.939706826814272\n",
      "1.5316887842196167\n",
      "0.9439453647340076\n",
      "0.9018947568683833\n",
      "1.516947651645933\n",
      "0.9691545282423976\n",
      "0.9349756356944156\n",
      "1.6150114748492068\n",
      "1.0580814286620253\n",
      "0.8781198372369033\n",
      "1.5764810295442773\n",
      "1.0455310109695362\n",
      "0.9402726564180481\n",
      "1.4604771425136118\n",
      "1.0255623813514212\n",
      "0.9990911555829121\n",
      "1.5592834563252909\n",
      "1.011729625725533\n",
      "0.8760281032136701\n",
      "1.4293690161422972\n",
      "1.0114638582416897\n",
      "0.9679340314375859\n",
      "1.3867010409582952\n",
      "0.9846286922937024\n",
      "0.897699775475768\n",
      "1.4466199936262776\n",
      "1.018503537142703\n",
      "0.9078573205670315\n",
      "1.5745218090235378\n",
      "1.010119938951482\n",
      "0.8925887617369805\n",
      "1.5276285960328093\n",
      "0.9657547988773807\n",
      "0.8871912642911337\n",
      "1.4649702408698717\n",
      "0.9291245266970539\n",
      "0.9095400082639399\n",
      "1.6493141660470356\n",
      "1.0413986551729182\n",
      "0.9657361619302786\n",
      "1.671561655205925\n",
      "1.0337997274052968\n",
      "1.0120711175587374\n",
      "1.6286164580749884\n",
      "1.0654399764267066\n",
      "0.9848434378600975\n",
      "1.452404016305535\n",
      "0.9319924727214735\n",
      "0.8918337561128056\n",
      "1.5415317578718877\n",
      "1.0081010892704452\n",
      "0.9631537457287801\n",
      "1.5899773988154264\n",
      "1.0525640041051019\n",
      "0.9005970642626095\n",
      "1.5311873473960547\n",
      "0.9811518549632999\n",
      "0.9059171753076596\n",
      "1.4909201468149464\n",
      "0.9861736595599505\n",
      "0.9621254118717893\n",
      "1.5350195590258786\n",
      "0.9811392464241399\n",
      "0.8880520998324645\n",
      "1.510866132469722\n",
      "0.9714335613620164\n",
      "0.9072773927545683\n",
      "1.6403201017764328\n",
      "1.055874304784153\n",
      "0.9088733736753075\n",
      "1.536124772877385\n",
      "0.9719024338779267\n",
      "0.9505471593143512\n",
      "1.4735738188740972\n",
      "1.0287750669047948\n",
      "0.9093952183903788\n",
      "1.6972204457047677\n",
      "1.0502564506590697\n",
      "1.0349336939172007\n",
      "1.3800959062128844\n",
      "0.9636446250116342\n",
      "0.9229135223864804\n",
      "1.5552469768531776\n",
      "0.9809157508056056\n",
      "0.8710349605088754\n",
      "1.3994281702327267\n",
      "0.8822776105574656\n",
      "0.876161540402432\n",
      "1.440149724039298\n",
      "0.9650917338076686\n",
      "0.9230516092797265\n",
      "1.4681056222221824\n",
      "1.0437062023014632\n",
      "0.9506537408152363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.483280887148162\n",
      "1.065698480936326\n",
      "0.964654426651193\n",
      "1.579702914602594\n",
      "1.0663594953233395\n",
      "1.0396996303102528\n",
      "1.5219105018172954\n",
      "1.0652715038453147\n",
      "0.8782157665788936\n",
      "1.4364196991361498\n",
      "1.0461844147745556\n",
      "0.88042347195475\n",
      "1.4513862290534767\n",
      "1.0260860254541573\n",
      "0.8964000458007252\n",
      "1.532666498629559\n",
      "1.0522466913249469\n",
      "0.9323077939044888\n",
      "1.5589325478470506\n",
      "0.9597709371831505\n",
      "0.8998091799401845\n",
      "1.4584356295426133\n",
      "0.9959949751677285\n",
      "0.9855139159738076\n",
      "1.6262578201343425\n",
      "0.9980639434245268\n",
      "0.9533906598111581\n",
      "1.5933391578848177\n",
      "1.0542807292139884\n",
      "1.0265100486907441\n",
      "1.60131873358559\n",
      "1.0339940781654318\n",
      "1.013268417535698\n",
      "1.6674666311922537\n",
      "1.053316352982641\n",
      "0.9369901177553938\n",
      "1.5903902265902048\n",
      "1.0035054703741735\n",
      "0.8970506238872803\n",
      "1.4357802698407318\n",
      "1.0266512398755423\n",
      "0.9612244294459907\n",
      "1.551200529900174\n",
      "1.0233259156872705\n",
      "0.9353719882864305\n",
      "1.5035666325429755\n",
      "1.045818184097252\n",
      "1.0160731583342373\n",
      "1.4899377361511401\n",
      "0.9756856283111892\n",
      "0.9211211235713489\n",
      "1.627655784628275\n",
      "1.0557412715446455\n",
      "0.8803340561703351\n",
      "1.5531966543391662\n",
      "0.9820859686866332\n",
      "0.9372693428503993\n",
      "1.52254634389428\n",
      "0.9686576165074078\n",
      "0.8726801419581485\n",
      "1.5361736767892387\n",
      "1.0216893591895782\n",
      "0.9367474588831677\n",
      "1.548451859265094\n",
      "0.9640655601098302\n",
      "0.9447618181188434\n",
      "1.44940481743382\n",
      "0.9976808955421838\n",
      "0.952716595390854\n",
      "1.584427881758073\n",
      "0.980160612756084\n",
      "0.9355942473977233\n",
      "1.3134407936474666\n",
      "0.9032005018862921\n",
      "0.9003589065884909\n",
      "1.5595123384148533\n",
      "1.062649278657936\n",
      "0.9789137181662019\n",
      "1.5535791292563963\n",
      "0.9537035705757585\n",
      "0.8961167489732577\n",
      "1.6513482569740043\n",
      "1.0578904291031213\n",
      "0.944947265398996\n",
      "1.5766248136548418\n",
      "1.0574141832156108\n",
      "0.9803706448950811\n",
      "-13813.419735561924\n",
      "torch.Size([900, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "N                    = 3           # number of atoms per molecule\n",
    "number_of_features   = 20           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "    \n",
    "    \n",
    "heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "random.shuffle(heta)\n",
    "\n",
    "Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "random.shuffle(Rs)\n",
    "\n",
    "lambdaa = np.ones(number_of_features)\n",
    "random.shuffle(lambdaa)\n",
    "\n",
    "zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "random.shuffle(zeta)\n",
    "\n",
    "\n",
    "# heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "# zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "# Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "# lambdaa = [1., 1., 1., 1., 1., 1.]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "training_set_size    = data_size - 100\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features,2):\n",
    "        G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])     \n",
    "        G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.var(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "\n",
    "G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set = np.vsplit(G_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.var(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "print(mean_lab)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.94011721 0.77829821 1.44344043 ... 0.01999619 0.87112445 0.1158069 ]\n",
      " [0.9995811  0.88641472 1.4803309  ... 0.04266943 0.95848254 0.18135498]\n",
      " [1.31439656 1.21935437 1.74117052 ... 0.06213325 1.40252565 0.27946167]\n",
      " ...\n",
      " [0.8459288  0.61544427 1.35899814 ... 0.01076776 0.74853903 0.07520604]\n",
      " [0.89953042 0.70742078 1.39482273 ... 0.02236074 0.82882568 0.11724208]\n",
      " [1.21981677 1.02123465 1.68039475 ... 0.03298874 1.26241196 0.18493328]]\n"
     ]
    }
   ],
   "source": [
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n",
      "[[0.33333333 2.        ]\n",
      " [3.         4.        ]\n",
      " [5.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "# for i in range(3):\n",
    "#     a[i,0] = a[i,0]/(sum(a[:,0])/3)\n",
    "# print(a)\n",
    "\n",
    "b = (sum(a[:,0])/3)\n",
    "print(b)\n",
    "print(type(b))\n",
    "a[0,0] = a[0,0]/b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([  -3.3669,   -2.6794,   -3.4957,   -0.8164,   -9.5576,  -52.3066,\n",
      "          -3.4165,   -4.9550,   -2.0192, -484.8375,   -8.1867, -256.4972,\n",
      "          -1.5849, -186.4589,   -1.8482, -135.9458,   -2.2246,  -38.0778,\n",
      "          -2.6052,   -9.8786])\n",
      "x2 tensor([-1.5537e+00, -4.1569e-01, -1.9123e+00,  1.3752e+00, -6.6221e-01,\n",
      "         2.1518e+01, -1.3910e+00,  1.9488e-01, -1.0067e+00,  2.8188e+02,\n",
      "         3.5793e-02,  1.4202e+02, -1.5373e+00,  9.9566e+01, -1.1600e+00,\n",
      "         6.9494e+01, -1.0954e+00,  1.3931e+01, -1.2021e+00,  1.2635e+00])\n",
      "x3 tensor([  8.0458,   6.5554,   9.2833,   5.3343,  17.9228,  77.6209,   7.6480,\n",
      "         10.1241,   4.3637, 562.7095,  14.9666, 319.0089,   4.3268, 240.1537,\n",
      "          4.2952, 181.4388,   4.9861,  58.5775,   5.9301,  17.9401])\n",
      "output\n",
      "tensor([-0.0576], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets, self).__init__()\n",
    "        self.fc1 = nn.Linear(20, 10)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(10, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "#         self.fc6 = nn.Linear(6, 4)\n",
    "#         self.fc7 = nn.Linear(4, 2)\n",
    "#         self.fc8 = nn.Linear(2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = torch.tanh(self.fc4(x))\n",
    "#         x = torch.tanh(self.fc5(x))\n",
    "#         x = torch.tanh(self.fc6(x))\n",
    "#         x = torch.tanh(self.fc7(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN, self).__init__()\n",
    "        self.network1 = Subnets()\n",
    "        self.network2 = Subnets()\n",
    "        self.network3 = Subnets()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "#         print(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN()\n",
    "N = 1\n",
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class simplenn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(simplenn, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 3)        # where fc stands for fully connected \n",
    "#         self.fc2 = nn.Linear(3, 1)        \n",
    "   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "#         x = self.fc2(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "#         return x\n",
    "\n",
    "# mod = simplenn()\n",
    "\n",
    "# print(mod.fc1.weight)\n",
    "# print(mod.fc1.bias)\n",
    "\n",
    "# print(mod.fc2.weight)\n",
    "# print(mod.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# x1 = x1[:2]\n",
    "# x2 = x2[:2]\n",
    "\n",
    "# x1[0] = -18650\n",
    "# x1[1] = 109075\n",
    "# print('x1',x1)\n",
    "\n",
    "# x2[0] = -6\n",
    "# x2[1] = 7\n",
    "# print('x2',x2)\n",
    "\n",
    "# output1 = mod(x1)\n",
    "# print('output1')\n",
    "# print(output1)\n",
    "\n",
    "# output2 = mod(x2)\n",
    "# print('output2')\n",
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([  -3.3669,   -2.6794,   -3.4957,   -0.8164,   -9.5576,  -52.3066,\n",
      "          -3.4165,   -4.9550,   -2.0192, -484.8375,   -8.1867, -256.4972,\n",
      "          -1.5849, -186.4589,   -1.8482, -135.9458,   -2.2246,  -38.0778,\n",
      "          -2.6052,   -9.8786])\n",
      "x2 tensor([-1.5537e+00, -4.1569e-01, -1.9123e+00,  1.3752e+00, -6.6221e-01,\n",
      "         2.1518e+01, -1.3910e+00,  1.9488e-01, -1.0067e+00,  2.8188e+02,\n",
      "         3.5793e-02,  1.4202e+02, -1.5373e+00,  9.9566e+01, -1.1600e+00,\n",
      "         6.9494e+01, -1.0954e+00,  1.3931e+01, -1.2021e+00,  1.2635e+00])\n",
      "x3 tensor([  8.0458,   6.5554,   9.2833,   5.3343,  17.9228,  77.6209,   7.6480,\n",
      "         10.1241,   4.3637, 562.7095,  14.9666, 319.0089,   4.3268, 240.1537,\n",
      "          4.2952, 181.4388,   4.9861,  58.5775,   5.9301,  17.9401])\n",
      "output\n",
      "tensor([-0.0576], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.0767,  0.1162, -0.0293, -0.0024, -0.0759,  0.2108,  0.0159,  0.1640,\n",
      "          0.0895, -0.0396,  0.0339,  0.1994,  0.1056, -0.1322, -0.0169, -0.0172,\n",
      "          0.1570,  0.1106, -0.0036,  0.1732],\n",
      "        [ 0.0625,  0.1281, -0.1736,  0.1375, -0.1028, -0.0405, -0.0381,  0.0583,\n",
      "          0.1039, -0.1018,  0.1559, -0.1421,  0.0069, -0.0959,  0.0958,  0.1842,\n",
      "         -0.2151, -0.1892,  0.1577,  0.0893],\n",
      "        [-0.1498,  0.0088, -0.1616, -0.1947, -0.2171,  0.1426,  0.2182,  0.2200,\n",
      "          0.1337, -0.1738, -0.0770, -0.1743,  0.1575, -0.0698,  0.0360,  0.2077,\n",
      "          0.0186,  0.0501,  0.0462,  0.1930],\n",
      "        [-0.0144, -0.1148,  0.2065,  0.1097, -0.1476, -0.0114,  0.1569, -0.1213,\n",
      "         -0.2101,  0.0546,  0.0444,  0.2202, -0.0645, -0.1021, -0.1016, -0.1810,\n",
      "         -0.1953, -0.1505, -0.0650, -0.0320],\n",
      "        [ 0.0264, -0.1856,  0.0253, -0.0507,  0.1399,  0.1035, -0.1313, -0.1046,\n",
      "         -0.0735, -0.0945,  0.0649, -0.1370, -0.1427, -0.0569,  0.1674, -0.2034,\n",
      "         -0.1846,  0.2070, -0.1609, -0.1392],\n",
      "        [-0.1435, -0.0815, -0.0644, -0.0032,  0.0284,  0.1979, -0.1374, -0.0268,\n",
      "          0.0244,  0.1270,  0.1852, -0.1996, -0.1439, -0.0870,  0.0693,  0.1568,\n",
      "          0.1772, -0.0544,  0.1083, -0.2213],\n",
      "        [-0.1358, -0.0691, -0.0391,  0.2058,  0.0577,  0.1138,  0.1453, -0.0846,\n",
      "         -0.0712, -0.1896,  0.0118, -0.1049,  0.0714, -0.1042,  0.0487,  0.1591,\n",
      "          0.2206, -0.1665, -0.1768,  0.2004],\n",
      "        [-0.1802,  0.0310, -0.0320, -0.0371,  0.1412, -0.0219, -0.1550, -0.1078,\n",
      "          0.0404, -0.1602, -0.0875,  0.0922, -0.0901, -0.0121,  0.1957,  0.1188,\n",
      "         -0.1673,  0.0458,  0.0667, -0.0312],\n",
      "        [-0.1258,  0.0952, -0.1239,  0.0286,  0.1314, -0.2043, -0.0568, -0.0054,\n",
      "         -0.1012, -0.1734, -0.1152,  0.1873,  0.1313,  0.2221, -0.2133,  0.0309,\n",
      "          0.1554, -0.0819,  0.0105, -0.2224],\n",
      "        [ 0.0413,  0.0710,  0.0553,  0.1851, -0.1673,  0.0879, -0.1951, -0.0754,\n",
      "          0.1551, -0.0586,  0.1384, -0.1135, -0.0209,  0.1076,  0.0211,  0.0350,\n",
      "         -0.1202,  0.0225, -0.0794,  0.1257]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([ 0.1095, -0.1971, -0.0908, -0.2215,  0.0108,  0.1732, -0.1897, -0.0428,\n",
      "        -0.1126, -0.0934], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.2429,  0.2656, -0.1816, -0.1514,  0.2015, -0.0588,  0.1610, -0.0161,\n",
      "         -0.1347,  0.0081],\n",
      "        [ 0.0931,  0.2407, -0.0055,  0.0758,  0.2203,  0.2448, -0.1389, -0.1965,\n",
      "          0.0814, -0.2405],\n",
      "        [-0.3020,  0.3004, -0.0938,  0.2770, -0.2033,  0.0947, -0.0135, -0.2239,\n",
      "          0.2653, -0.2309],\n",
      "        [ 0.2119, -0.2217, -0.2926, -0.1028,  0.3071, -0.2487, -0.2963, -0.1019,\n",
      "          0.0508, -0.2977],\n",
      "        [-0.1654, -0.1701,  0.2405, -0.1755, -0.2983,  0.1302,  0.2040,  0.1015,\n",
      "          0.2820, -0.1947],\n",
      "        [ 0.1896, -0.0303,  0.0376,  0.0231, -0.1849,  0.0673,  0.0321, -0.1856,\n",
      "         -0.0106,  0.1550],\n",
      "        [-0.2353, -0.2105, -0.2062,  0.1714,  0.2284,  0.1594, -0.2663,  0.1483,\n",
      "          0.1999,  0.3057],\n",
      "        [ 0.0252,  0.0055, -0.1583, -0.1720,  0.2197,  0.1456, -0.2610,  0.0276,\n",
      "         -0.0507,  0.3001],\n",
      "        [-0.2787,  0.0977, -0.1190,  0.2809,  0.2376,  0.1190,  0.0400,  0.0174,\n",
      "         -0.3036, -0.2130],\n",
      "        [ 0.1644,  0.0150, -0.2508,  0.3031, -0.0819,  0.0358, -0.0093, -0.0554,\n",
      "          0.1182,  0.1654]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.1178,  0.1022,  0.1334,  0.2813,  0.0531, -0.2545,  0.1917,  0.1697,\n",
      "        -0.1814,  0.2597], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(model.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(model.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BPNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 0.43901\n",
      "[1,    20] loss: 0.19859\n",
      "[1,    30] loss: 0.12531\n",
      "[2,    10] loss: 0.05845\n",
      "[2,    20] loss: 0.05323\n",
      "[2,    30] loss: 0.05006\n",
      "[3,    10] loss: 0.04503\n",
      "[3,    20] loss: 0.04004\n",
      "[3,    30] loss: 0.03580\n",
      "[4,    10] loss: 0.03626\n",
      "[4,    20] loss: 0.03599\n",
      "[4,    30] loss: 0.03278\n",
      "[5,    10] loss: 0.03602\n",
      "[5,    20] loss: 0.03122\n",
      "[5,    30] loss: 0.03051\n",
      "[6,    10] loss: 0.03061\n",
      "[6,    20] loss: 0.03119\n",
      "[6,    30] loss: 0.02590\n",
      "[7,    10] loss: 0.02532\n",
      "[7,    20] loss: 0.02642\n",
      "[7,    30] loss: 0.02083\n",
      "[8,    10] loss: 0.02393\n",
      "[8,    20] loss: 0.02686\n",
      "[8,    30] loss: 0.02101\n",
      "[9,    10] loss: 0.02055\n",
      "[9,    20] loss: 0.02460\n",
      "[9,    30] loss: 0.01984\n",
      "[10,    10] loss: 0.01903\n",
      "[10,    20] loss: 0.02354\n",
      "[10,    30] loss: 0.02142\n",
      "[11,    10] loss: 0.02169\n",
      "[11,    20] loss: 0.02821\n",
      "[11,    30] loss: 0.02031\n",
      "[12,    10] loss: 0.01703\n",
      "[12,    20] loss: 0.02075\n",
      "[12,    30] loss: 0.01932\n",
      "[13,    10] loss: 0.01910\n",
      "[13,    20] loss: 0.02106\n",
      "[13,    30] loss: 0.02140\n",
      "[14,    10] loss: 0.02179\n",
      "[14,    20] loss: 0.01940\n",
      "[14,    30] loss: 0.01696\n",
      "[15,    10] loss: 0.01878\n",
      "[15,    20] loss: 0.01959\n",
      "[15,    30] loss: 0.01735\n",
      "[16,    10] loss: 0.02020\n",
      "[16,    20] loss: 0.02164\n",
      "[16,    30] loss: 0.01558\n",
      "[17,    10] loss: 0.01564\n",
      "[17,    20] loss: 0.01856\n",
      "[17,    30] loss: 0.01521\n",
      "[18,    10] loss: 0.01799\n",
      "[18,    20] loss: 0.01756\n",
      "[18,    30] loss: 0.01464\n",
      "[19,    10] loss: 0.01628\n",
      "[19,    20] loss: 0.01471\n",
      "[19,    30] loss: 0.01458\n",
      "[20,    10] loss: 0.01612\n",
      "[20,    20] loss: 0.01643\n",
      "[20,    30] loss: 0.01743\n",
      "[21,    10] loss: 0.01264\n",
      "[21,    20] loss: 0.01411\n",
      "[21,    30] loss: 0.01493\n",
      "[22,    10] loss: 0.01943\n",
      "[22,    20] loss: 0.02180\n",
      "[22,    30] loss: 0.01845\n",
      "[23,    10] loss: 0.01608\n",
      "[23,    20] loss: 0.01710\n",
      "[23,    30] loss: 0.01200\n",
      "[24,    10] loss: 0.01399\n",
      "[24,    20] loss: 0.01578\n",
      "[24,    30] loss: 0.01188\n",
      "[25,    10] loss: 0.01456\n",
      "[25,    20] loss: 0.01649\n",
      "[25,    30] loss: 0.01150\n",
      "[26,    10] loss: 0.01408\n",
      "[26,    20] loss: 0.01052\n",
      "[26,    30] loss: 0.01150\n",
      "[27,    10] loss: 0.01562\n",
      "[27,    20] loss: 0.01272\n",
      "[27,    30] loss: 0.01173\n",
      "[28,    10] loss: 0.01342\n",
      "[28,    20] loss: 0.01329\n",
      "[28,    30] loss: 0.01102\n",
      "[29,    10] loss: 0.01155\n",
      "[29,    20] loss: 0.01471\n",
      "[29,    30] loss: 0.01490\n",
      "[30,    10] loss: 0.01063\n",
      "[30,    20] loss: 0.01042\n",
      "[30,    30] loss: 0.00995\n",
      "[31,    10] loss: 0.01392\n",
      "[31,    20] loss: 0.01234\n",
      "[31,    30] loss: 0.01096\n",
      "[32,    10] loss: 0.01163\n",
      "[32,    20] loss: 0.01270\n",
      "[32,    30] loss: 0.01052\n",
      "[33,    10] loss: 0.01159\n",
      "[33,    20] loss: 0.01172\n",
      "[33,    30] loss: 0.01071\n",
      "[34,    10] loss: 0.01056\n",
      "[34,    20] loss: 0.01315\n",
      "[34,    30] loss: 0.01141\n",
      "[35,    10] loss: 0.01107\n",
      "[35,    20] loss: 0.00980\n",
      "[35,    30] loss: 0.01130\n",
      "[36,    10] loss: 0.01032\n",
      "[36,    20] loss: 0.00957\n",
      "[36,    30] loss: 0.00895\n",
      "[37,    10] loss: 0.01080\n",
      "[37,    20] loss: 0.01192\n",
      "[37,    30] loss: 0.00915\n",
      "[38,    10] loss: 0.00952\n",
      "[38,    20] loss: 0.00893\n",
      "[38,    30] loss: 0.00854\n",
      "[39,    10] loss: 0.00979\n",
      "[39,    20] loss: 0.01365\n",
      "[39,    30] loss: 0.01516\n",
      "[40,    10] loss: 0.00970\n",
      "[40,    20] loss: 0.00970\n",
      "[40,    30] loss: 0.00837\n",
      "[41,    10] loss: 0.01188\n",
      "[41,    20] loss: 0.01046\n",
      "[41,    30] loss: 0.01001\n",
      "[42,    10] loss: 0.00851\n",
      "[42,    20] loss: 0.01169\n",
      "[42,    30] loss: 0.01361\n",
      "[43,    10] loss: 0.01068\n",
      "[43,    20] loss: 0.01149\n",
      "[43,    30] loss: 0.00914\n",
      "[44,    10] loss: 0.01080\n",
      "[44,    20] loss: 0.01190\n",
      "[44,    30] loss: 0.01019\n",
      "[45,    10] loss: 0.00823\n",
      "[45,    20] loss: 0.00991\n",
      "[45,    30] loss: 0.01011\n",
      "[46,    10] loss: 0.00914\n",
      "[46,    20] loss: 0.01062\n",
      "[46,    30] loss: 0.00892\n",
      "[47,    10] loss: 0.00832\n",
      "[47,    20] loss: 0.00948\n",
      "[47,    30] loss: 0.00890\n",
      "[48,    10] loss: 0.00756\n",
      "[48,    20] loss: 0.01064\n",
      "[48,    30] loss: 0.00950\n",
      "[49,    10] loss: 0.00862\n",
      "[49,    20] loss: 0.00961\n",
      "[49,    30] loss: 0.00834\n",
      "[50,    10] loss: 0.00823\n",
      "[50,    20] loss: 0.01293\n",
      "[50,    30] loss: 0.01179\n",
      "[51,    10] loss: 0.00857\n",
      "[51,    20] loss: 0.00898\n",
      "[51,    30] loss: 0.00778\n",
      "[52,    10] loss: 0.00812\n",
      "[52,    20] loss: 0.00819\n",
      "[52,    30] loss: 0.00831\n",
      "[53,    10] loss: 0.00813\n",
      "[53,    20] loss: 0.00891\n",
      "[53,    30] loss: 0.01043\n",
      "[54,    10] loss: 0.00704\n",
      "[54,    20] loss: 0.00895\n",
      "[54,    30] loss: 0.00964\n",
      "[55,    10] loss: 0.00757\n",
      "[55,    20] loss: 0.00890\n",
      "[55,    30] loss: 0.00949\n",
      "[56,    10] loss: 0.00851\n",
      "[56,    20] loss: 0.00887\n",
      "[56,    30] loss: 0.01029\n",
      "[57,    10] loss: 0.00770\n",
      "[57,    20] loss: 0.01593\n",
      "[57,    30] loss: 0.01259\n",
      "[58,    10] loss: 0.01100\n",
      "[58,    20] loss: 0.01353\n",
      "[58,    30] loss: 0.01218\n",
      "[59,    10] loss: 0.01174\n",
      "[59,    20] loss: 0.01158\n",
      "[59,    30] loss: 0.01040\n",
      "[60,    10] loss: 0.00864\n",
      "[60,    20] loss: 0.01238\n",
      "[60,    30] loss: 0.01218\n",
      "[61,    10] loss: 0.00748\n",
      "[61,    20] loss: 0.00797\n",
      "[61,    30] loss: 0.00852\n",
      "[62,    10] loss: 0.00861\n",
      "[62,    20] loss: 0.00765\n",
      "[62,    30] loss: 0.00787\n",
      "[63,    10] loss: 0.00871\n",
      "[63,    20] loss: 0.00980\n",
      "[63,    30] loss: 0.00982\n",
      "[64,    10] loss: 0.00909\n",
      "[64,    20] loss: 0.01322\n",
      "[64,    30] loss: 0.01889\n",
      "[65,    10] loss: 0.01521\n",
      "[65,    20] loss: 0.01818\n",
      "[65,    30] loss: 0.01943\n",
      "[66,    10] loss: 0.01175\n",
      "[66,    20] loss: 0.01113\n",
      "[66,    30] loss: 0.00928\n",
      "[67,    10] loss: 0.01099\n",
      "[67,    20] loss: 0.00945\n",
      "[67,    30] loss: 0.00950\n",
      "[68,    10] loss: 0.00768\n",
      "[68,    20] loss: 0.00908\n",
      "[68,    30] loss: 0.00993\n",
      "[69,    10] loss: 0.00820\n",
      "[69,    20] loss: 0.00883\n",
      "[69,    30] loss: 0.00786\n",
      "[70,    10] loss: 0.00780\n",
      "[70,    20] loss: 0.01138\n",
      "[70,    30] loss: 0.01490\n",
      "[71,    10] loss: 0.01195\n",
      "[71,    20] loss: 0.01624\n",
      "[71,    30] loss: 0.01294\n",
      "[72,    10] loss: 0.00819\n",
      "[72,    20] loss: 0.00849\n",
      "[72,    30] loss: 0.00967\n",
      "[73,    10] loss: 0.00855\n",
      "[73,    20] loss: 0.00965\n",
      "[73,    30] loss: 0.00947\n",
      "[74,    10] loss: 0.00731\n",
      "[74,    20] loss: 0.01110\n",
      "[74,    30] loss: 0.01137\n",
      "[75,    10] loss: 0.00850\n",
      "[75,    20] loss: 0.00832\n",
      "[75,    30] loss: 0.00838\n",
      "[76,    10] loss: 0.00869\n",
      "[76,    20] loss: 0.00830\n",
      "[76,    30] loss: 0.00963\n",
      "[77,    10] loss: 0.01041\n",
      "[77,    20] loss: 0.00950\n",
      "[77,    30] loss: 0.00918\n",
      "[78,    10] loss: 0.00835\n",
      "[78,    20] loss: 0.00963\n",
      "[78,    30] loss: 0.00882\n",
      "[79,    10] loss: 0.00888\n",
      "[79,    20] loss: 0.01019\n",
      "[79,    30] loss: 0.01113\n",
      "[80,    10] loss: 0.00893\n",
      "[80,    20] loss: 0.00827\n",
      "[80,    30] loss: 0.00774\n",
      "[81,    10] loss: 0.00839\n",
      "[81,    20] loss: 0.00859\n",
      "[81,    30] loss: 0.00946\n",
      "[82,    10] loss: 0.00799\n",
      "[82,    20] loss: 0.00893\n",
      "[82,    30] loss: 0.00913\n",
      "[83,    10] loss: 0.00773\n",
      "[83,    20] loss: 0.00911\n",
      "[83,    30] loss: 0.00850\n",
      "[84,    10] loss: 0.01031\n",
      "[84,    20] loss: 0.01286\n",
      "[84,    30] loss: 0.01487\n",
      "[85,    10] loss: 0.01504\n",
      "[85,    20] loss: 0.01702\n",
      "[85,    30] loss: 0.01967\n",
      "[86,    10] loss: 0.01060\n",
      "[86,    20] loss: 0.00864\n",
      "[86,    30] loss: 0.00832\n",
      "[87,    10] loss: 0.00969\n",
      "[87,    20] loss: 0.00725\n",
      "[87,    30] loss: 0.00824\n",
      "[88,    10] loss: 0.00653\n",
      "[88,    20] loss: 0.00768\n",
      "[88,    30] loss: 0.00825\n",
      "[89,    10] loss: 0.00933\n",
      "[89,    20] loss: 0.00893\n",
      "[89,    30] loss: 0.00820\n",
      "[90,    10] loss: 0.01090\n",
      "[90,    20] loss: 0.00983\n",
      "[90,    30] loss: 0.00899\n",
      "[91,    10] loss: 0.01031\n",
      "[91,    20] loss: 0.00882\n",
      "[91,    30] loss: 0.00744\n",
      "[92,    10] loss: 0.00931\n",
      "[92,    20] loss: 0.00749\n",
      "[92,    30] loss: 0.00726\n",
      "[93,    10] loss: 0.00866\n",
      "[93,    20] loss: 0.01085\n",
      "[93,    30] loss: 0.01575\n",
      "[94,    10] loss: 0.01703\n",
      "[94,    20] loss: 0.01642\n",
      "[94,    30] loss: 0.02152\n",
      "[95,    10] loss: 0.01007\n",
      "[95,    20] loss: 0.00887\n",
      "[95,    30] loss: 0.00897\n",
      "[96,    10] loss: 0.00953\n",
      "[96,    20] loss: 0.00873\n",
      "[96,    30] loss: 0.01200\n",
      "[97,    10] loss: 0.00699\n",
      "[97,    20] loss: 0.00905\n",
      "[97,    30] loss: 0.00889\n",
      "[98,    10] loss: 0.01157\n",
      "[98,    20] loss: 0.00838\n",
      "[98,    30] loss: 0.00767\n",
      "[99,    10] loss: 0.00674\n",
      "[99,    20] loss: 0.00890\n",
      "[99,    30] loss: 0.00903\n",
      "[100,    10] loss: 0.01131\n",
      "[100,    20] loss: 0.00840\n",
      "[100,    30] loss: 0.00872\n",
      "[101,    10] loss: 0.00980\n",
      "[101,    20] loss: 0.00973\n",
      "[101,    30] loss: 0.01093\n",
      "[102,    10] loss: 0.00705\n",
      "[102,    20] loss: 0.00848\n",
      "[102,    30] loss: 0.00895\n",
      "[103,    10] loss: 0.00878\n",
      "[103,    20] loss: 0.00688\n",
      "[103,    30] loss: 0.00732\n",
      "[104,    10] loss: 0.00973\n",
      "[104,    20] loss: 0.01018\n",
      "[104,    30] loss: 0.00798\n",
      "[105,    10] loss: 0.01036\n",
      "[105,    20] loss: 0.00738\n",
      "[105,    30] loss: 0.00610\n",
      "[106,    10] loss: 0.00681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106,    20] loss: 0.00559\n",
      "[106,    30] loss: 0.00525\n",
      "[107,    10] loss: 0.00511\n",
      "[107,    20] loss: 0.00585\n",
      "[107,    30] loss: 0.00592\n",
      "[108,    10] loss: 0.00512\n",
      "[108,    20] loss: 0.00648\n",
      "[108,    30] loss: 0.00595\n",
      "[109,    10] loss: 0.00638\n",
      "[109,    20] loss: 0.00724\n",
      "[109,    30] loss: 0.00690\n",
      "[110,    10] loss: 0.00821\n",
      "[110,    20] loss: 0.00901\n",
      "[110,    30] loss: 0.00645\n",
      "[111,    10] loss: 0.00845\n",
      "[111,    20] loss: 0.00909\n",
      "[111,    30] loss: 0.00605\n",
      "[112,    10] loss: 0.00715\n",
      "[112,    20] loss: 0.00665\n",
      "[112,    30] loss: 0.00716\n",
      "[113,    10] loss: 0.00447\n",
      "[113,    20] loss: 0.00569\n",
      "[113,    30] loss: 0.00752\n",
      "[114,    10] loss: 0.00654\n",
      "[114,    20] loss: 0.00774\n",
      "[114,    30] loss: 0.00665\n",
      "[115,    10] loss: 0.00676\n",
      "[115,    20] loss: 0.00765\n",
      "[115,    30] loss: 0.00536\n",
      "[116,    10] loss: 0.00663\n",
      "[116,    20] loss: 0.00645\n",
      "[116,    30] loss: 0.00707\n",
      "[117,    10] loss: 0.00547\n",
      "[117,    20] loss: 0.00542\n",
      "[117,    30] loss: 0.00475\n",
      "[118,    10] loss: 0.00461\n",
      "[118,    20] loss: 0.00553\n",
      "[118,    30] loss: 0.00596\n",
      "[119,    10] loss: 0.00811\n",
      "[119,    20] loss: 0.00905\n",
      "[119,    30] loss: 0.00760\n",
      "[120,    10] loss: 0.00954\n",
      "[120,    20] loss: 0.01072\n",
      "[120,    30] loss: 0.00904\n",
      "[121,    10] loss: 0.00940\n",
      "[121,    20] loss: 0.00860\n",
      "[121,    30] loss: 0.00694\n",
      "[122,    10] loss: 0.00802\n",
      "[122,    20] loss: 0.00675\n",
      "[122,    30] loss: 0.00572\n",
      "[123,    10] loss: 0.00552\n",
      "[123,    20] loss: 0.00579\n",
      "[123,    30] loss: 0.00501\n",
      "[124,    10] loss: 0.00400\n",
      "[124,    20] loss: 0.00604\n",
      "[124,    30] loss: 0.00659\n",
      "[125,    10] loss: 0.00787\n",
      "[125,    20] loss: 0.00803\n",
      "[125,    30] loss: 0.00941\n",
      "[126,    10] loss: 0.00989\n",
      "[126,    20] loss: 0.00793\n",
      "[126,    30] loss: 0.00603\n",
      "[127,    10] loss: 0.00669\n",
      "[127,    20] loss: 0.00654\n",
      "[127,    30] loss: 0.00543\n",
      "[128,    10] loss: 0.00523\n",
      "[128,    20] loss: 0.00632\n",
      "[128,    30] loss: 0.00688\n",
      "[129,    10] loss: 0.00825\n",
      "[129,    20] loss: 0.00779\n",
      "[129,    30] loss: 0.00829\n",
      "[130,    10] loss: 0.00696\n",
      "[130,    20] loss: 0.00603\n",
      "[130,    30] loss: 0.00653\n",
      "[131,    10] loss: 0.00733\n",
      "[131,    20] loss: 0.00770\n",
      "[131,    30] loss: 0.00635\n",
      "[132,    10] loss: 0.00757\n",
      "[132,    20] loss: 0.00856\n",
      "[132,    30] loss: 0.00635\n",
      "[133,    10] loss: 0.00879\n",
      "[133,    20] loss: 0.00957\n",
      "[133,    30] loss: 0.00676\n",
      "[134,    10] loss: 0.00915\n",
      "[134,    20] loss: 0.00998\n",
      "[134,    30] loss: 0.00676\n",
      "[135,    10] loss: 0.00561\n",
      "[135,    20] loss: 0.00640\n",
      "[135,    30] loss: 0.00831\n",
      "[136,    10] loss: 0.01073\n",
      "[136,    20] loss: 0.00940\n",
      "[136,    30] loss: 0.00649\n",
      "[137,    10] loss: 0.00630\n",
      "[137,    20] loss: 0.00709\n",
      "[137,    30] loss: 0.00659\n",
      "[138,    10] loss: 0.00521\n",
      "[138,    20] loss: 0.00620\n",
      "[138,    30] loss: 0.00694\n",
      "[139,    10] loss: 0.00811\n",
      "[139,    20] loss: 0.00659\n",
      "[139,    30] loss: 0.00623\n",
      "[140,    10] loss: 0.00479\n",
      "[140,    20] loss: 0.00546\n",
      "[140,    30] loss: 0.00729\n",
      "[141,    10] loss: 0.00878\n",
      "[141,    20] loss: 0.00830\n",
      "[141,    30] loss: 0.00879\n",
      "[142,    10] loss: 0.00822\n",
      "[142,    20] loss: 0.00772\n",
      "[142,    30] loss: 0.00597\n",
      "[143,    10] loss: 0.00453\n",
      "[143,    20] loss: 0.00540\n",
      "[143,    30] loss: 0.00484\n",
      "[144,    10] loss: 0.00633\n",
      "[144,    20] loss: 0.00682\n",
      "[144,    30] loss: 0.00497\n",
      "[145,    10] loss: 0.00685\n",
      "[145,    20] loss: 0.00644\n",
      "[145,    30] loss: 0.00448\n",
      "[146,    10] loss: 0.00573\n",
      "[146,    20] loss: 0.00535\n",
      "[146,    30] loss: 0.00577\n",
      "[147,    10] loss: 0.00547\n",
      "[147,    20] loss: 0.00599\n",
      "[147,    30] loss: 0.00759\n",
      "[148,    10] loss: 0.00964\n",
      "[148,    20] loss: 0.00819\n",
      "[148,    30] loss: 0.00529\n",
      "[149,    10] loss: 0.00657\n",
      "[149,    20] loss: 0.00742\n",
      "[149,    30] loss: 0.00647\n",
      "[150,    10] loss: 0.00620\n",
      "[150,    20] loss: 0.00650\n",
      "[150,    30] loss: 0.00542\n",
      "[151,    10] loss: 0.00727\n",
      "[151,    20] loss: 0.00737\n",
      "[151,    30] loss: 0.00464\n",
      "[152,    10] loss: 0.00486\n",
      "[152,    20] loss: 0.00596\n",
      "[152,    30] loss: 0.00601\n",
      "[153,    10] loss: 0.00628\n",
      "[153,    20] loss: 0.00574\n",
      "[153,    30] loss: 0.00606\n",
      "[154,    10] loss: 0.00803\n",
      "[154,    20] loss: 0.00877\n",
      "[154,    30] loss: 0.00548\n",
      "[155,    10] loss: 0.00543\n",
      "[155,    20] loss: 0.00578\n",
      "[155,    30] loss: 0.00414\n",
      "[156,    10] loss: 0.00493\n",
      "[156,    20] loss: 0.00627\n",
      "[156,    30] loss: 0.00572\n",
      "[157,    10] loss: 0.00647\n",
      "[157,    20] loss: 0.00730\n",
      "[157,    30] loss: 0.00875\n",
      "[158,    10] loss: 0.00845\n",
      "[158,    20] loss: 0.00778\n",
      "[158,    30] loss: 0.00484\n",
      "[159,    10] loss: 0.00485\n",
      "[159,    20] loss: 0.00552\n",
      "[159,    30] loss: 0.00631\n",
      "[160,    10] loss: 0.00791\n",
      "[160,    20] loss: 0.00804\n",
      "[160,    30] loss: 0.00488\n",
      "[161,    10] loss: 0.00498\n",
      "[161,    20] loss: 0.00537\n",
      "[161,    30] loss: 0.00681\n",
      "[162,    10] loss: 0.00947\n",
      "[162,    20] loss: 0.00909\n",
      "[162,    30] loss: 0.00504\n",
      "[163,    10] loss: 0.00530\n",
      "[163,    20] loss: 0.00655\n",
      "[163,    30] loss: 0.00509\n",
      "[164,    10] loss: 0.00555\n",
      "[164,    20] loss: 0.00582\n",
      "[164,    30] loss: 0.00594\n",
      "[165,    10] loss: 0.00716\n",
      "[165,    20] loss: 0.00849\n",
      "[165,    30] loss: 0.00671\n",
      "[166,    10] loss: 0.00845\n",
      "[166,    20] loss: 0.00863\n",
      "[166,    30] loss: 0.00472\n",
      "[167,    10] loss: 0.00466\n",
      "[167,    20] loss: 0.00543\n",
      "[167,    30] loss: 0.00581\n",
      "[168,    10] loss: 0.00730\n",
      "[168,    20] loss: 0.00784\n",
      "[168,    30] loss: 0.00539\n",
      "[169,    10] loss: 0.00540\n",
      "[169,    20] loss: 0.00575\n",
      "[169,    30] loss: 0.00570\n",
      "[170,    10] loss: 0.00585\n",
      "[170,    20] loss: 0.00688\n",
      "[170,    30] loss: 0.00568\n",
      "[171,    10] loss: 0.00696\n",
      "[171,    20] loss: 0.00801\n",
      "[171,    30] loss: 0.00628\n",
      "[172,    10] loss: 0.00652\n",
      "[172,    20] loss: 0.00750\n",
      "[172,    30] loss: 0.00399\n",
      "[173,    10] loss: 0.00437\n",
      "[173,    20] loss: 0.00531\n",
      "[173,    30] loss: 0.00548\n",
      "[174,    10] loss: 0.00588\n",
      "[174,    20] loss: 0.00715\n",
      "[174,    30] loss: 0.00887\n",
      "[175,    10] loss: 0.01282\n",
      "[175,    20] loss: 0.01237\n",
      "[175,    30] loss: 0.01262\n",
      "[176,    10] loss: 0.00651\n",
      "[176,    20] loss: 0.00630\n",
      "[176,    30] loss: 0.00553\n",
      "[177,    10] loss: 0.00614\n",
      "[177,    20] loss: 0.00596\n",
      "[177,    30] loss: 0.00502\n",
      "[178,    10] loss: 0.00525\n",
      "[178,    20] loss: 0.00525\n",
      "[178,    30] loss: 0.00563\n",
      "[179,    10] loss: 0.00663\n",
      "[179,    20] loss: 0.00660\n",
      "[179,    30] loss: 0.00734\n",
      "[180,    10] loss: 0.00987\n",
      "[180,    20] loss: 0.00862\n",
      "[180,    30] loss: 0.00848\n",
      "[181,    10] loss: 0.00516\n",
      "[181,    20] loss: 0.00430\n",
      "[181,    30] loss: 0.00453\n",
      "[182,    10] loss: 0.00549\n",
      "[182,    20] loss: 0.00521\n",
      "[182,    30] loss: 0.00518\n",
      "[183,    10] loss: 0.00411\n",
      "[183,    20] loss: 0.00389\n",
      "[183,    30] loss: 0.00353\n",
      "[184,    10] loss: 0.00395\n",
      "[184,    20] loss: 0.00442\n",
      "[184,    30] loss: 0.00450\n",
      "[185,    10] loss: 0.00536\n",
      "[185,    20] loss: 0.00494\n",
      "[185,    30] loss: 0.00454\n",
      "[186,    10] loss: 0.00484\n",
      "[186,    20] loss: 0.00443\n",
      "[186,    30] loss: 0.00446\n",
      "[187,    10] loss: 0.00465\n",
      "[187,    20] loss: 0.00508\n",
      "[187,    30] loss: 0.00469\n",
      "[188,    10] loss: 0.00450\n",
      "[188,    20] loss: 0.00483\n",
      "[188,    30] loss: 0.00506\n",
      "[189,    10] loss: 0.00509\n",
      "[189,    20] loss: 0.00586\n",
      "[189,    30] loss: 0.00460\n",
      "[190,    10] loss: 0.00493\n",
      "[190,    20] loss: 0.00439\n",
      "[190,    30] loss: 0.00461\n",
      "[191,    10] loss: 0.00476\n",
      "[191,    20] loss: 0.00505\n",
      "[191,    30] loss: 0.00557\n",
      "[192,    10] loss: 0.00493\n",
      "[192,    20] loss: 0.00591\n",
      "[192,    30] loss: 0.00415\n",
      "[193,    10] loss: 0.00386\n",
      "[193,    20] loss: 0.00416\n",
      "[193,    30] loss: 0.00530\n",
      "[194,    10] loss: 0.00665\n",
      "[194,    20] loss: 0.00703\n",
      "[194,    30] loss: 0.00499\n",
      "[195,    10] loss: 0.00549\n",
      "[195,    20] loss: 0.00495\n",
      "[195,    30] loss: 0.00557\n",
      "[196,    10] loss: 0.00568\n",
      "[196,    20] loss: 0.00629\n",
      "[196,    30] loss: 0.00646\n",
      "[197,    10] loss: 0.00619\n",
      "[197,    20] loss: 0.00662\n",
      "[197,    30] loss: 0.00581\n",
      "[198,    10] loss: 0.00573\n",
      "[198,    20] loss: 0.00613\n",
      "[198,    30] loss: 0.00357\n",
      "[199,    10] loss: 0.00411\n",
      "[199,    20] loss: 0.00443\n",
      "[199,    30] loss: 0.00460\n",
      "[200,    10] loss: 0.00464\n",
      "[200,    20] loss: 0.00489\n",
      "[200,    30] loss: 0.00529\n",
      "[201,    10] loss: 0.00605\n",
      "[201,    20] loss: 0.00566\n",
      "[201,    30] loss: 0.00639\n",
      "[202,    10] loss: 0.00635\n",
      "[202,    20] loss: 0.00625\n",
      "[202,    30] loss: 0.00476\n",
      "[203,    10] loss: 0.00601\n",
      "[203,    20] loss: 0.00645\n",
      "[203,    30] loss: 0.00898\n",
      "[204,    10] loss: 0.00381\n",
      "[204,    20] loss: 0.00431\n",
      "[204,    30] loss: 0.00495\n",
      "[205,    10] loss: 0.00566\n",
      "[205,    20] loss: 0.00404\n",
      "[205,    30] loss: 0.00389\n",
      "[206,    10] loss: 0.00313\n",
      "[206,    20] loss: 0.00337\n",
      "[206,    30] loss: 0.00414\n",
      "[207,    10] loss: 0.00473\n",
      "[207,    20] loss: 0.00485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207,    30] loss: 0.00563\n",
      "[208,    10] loss: 0.00603\n",
      "[208,    20] loss: 0.00414\n",
      "[208,    30] loss: 0.00460\n",
      "[209,    10] loss: 0.00597\n",
      "[209,    20] loss: 0.00592\n",
      "[209,    30] loss: 0.00475\n",
      "[210,    10] loss: 0.00446\n",
      "[210,    20] loss: 0.00423\n",
      "[210,    30] loss: 0.00366\n",
      "[211,    10] loss: 0.00407\n",
      "[211,    20] loss: 0.00399\n",
      "[211,    30] loss: 0.00439\n",
      "[212,    10] loss: 0.00436\n",
      "[212,    20] loss: 0.00428\n",
      "[212,    30] loss: 0.00421\n",
      "[213,    10] loss: 0.00411\n",
      "[213,    20] loss: 0.00407\n",
      "[213,    30] loss: 0.00483\n",
      "[214,    10] loss: 0.00502\n",
      "[214,    20] loss: 0.00521\n",
      "[214,    30] loss: 0.00767\n",
      "[215,    10] loss: 0.00921\n",
      "[215,    20] loss: 0.00817\n",
      "[215,    30] loss: 0.00470\n",
      "[216,    10] loss: 0.00517\n",
      "[216,    20] loss: 0.00535\n",
      "[216,    30] loss: 0.00696\n",
      "[217,    10] loss: 0.00703\n",
      "[217,    20] loss: 0.00700\n",
      "[217,    30] loss: 0.00511\n",
      "[218,    10] loss: 0.00491\n",
      "[218,    20] loss: 0.00563\n",
      "[218,    30] loss: 0.00541\n",
      "[219,    10] loss: 0.00507\n",
      "[219,    20] loss: 0.00561\n",
      "[219,    30] loss: 0.00508\n",
      "[220,    10] loss: 0.00474\n",
      "[220,    20] loss: 0.00482\n",
      "[220,    30] loss: 0.00467\n",
      "[221,    10] loss: 0.00476\n",
      "[221,    20] loss: 0.00478\n",
      "[221,    30] loss: 0.00473\n",
      "[222,    10] loss: 0.00480\n",
      "[222,    20] loss: 0.00474\n",
      "[222,    30] loss: 0.00411\n",
      "[223,    10] loss: 0.00417\n",
      "[223,    20] loss: 0.00467\n",
      "[223,    30] loss: 0.00444\n",
      "[224,    10] loss: 0.00482\n",
      "[224,    20] loss: 0.00479\n",
      "[224,    30] loss: 0.00498\n",
      "[225,    10] loss: 0.00526\n",
      "[225,    20] loss: 0.00604\n",
      "[225,    30] loss: 0.00529\n",
      "[226,    10] loss: 0.00534\n",
      "[226,    20] loss: 0.00425\n",
      "[226,    30] loss: 0.00383\n",
      "[227,    10] loss: 0.00558\n",
      "[227,    20] loss: 0.00569\n",
      "[227,    30] loss: 0.00433\n",
      "[228,    10] loss: 0.00600\n",
      "[228,    20] loss: 0.00640\n",
      "[228,    30] loss: 0.01038\n",
      "[229,    10] loss: 0.01437\n",
      "[229,    20] loss: 0.01175\n",
      "[229,    30] loss: 0.00677\n",
      "[230,    10] loss: 0.00564\n",
      "[230,    20] loss: 0.00533\n",
      "[230,    30] loss: 0.00437\n",
      "[231,    10] loss: 0.00500\n",
      "[231,    20] loss: 0.00420\n",
      "[231,    30] loss: 0.00420\n",
      "[232,    10] loss: 0.00808\n",
      "[232,    20] loss: 0.00743\n",
      "[232,    30] loss: 0.00645\n",
      "[233,    10] loss: 0.00741\n",
      "[233,    20] loss: 0.00666\n",
      "[233,    30] loss: 0.00735\n",
      "[234,    10] loss: 0.00742\n",
      "[234,    20] loss: 0.00802\n",
      "[234,    30] loss: 0.00615\n",
      "[235,    10] loss: 0.01135\n",
      "[235,    20] loss: 0.00694\n",
      "[235,    30] loss: 0.00591\n",
      "[236,    10] loss: 0.00547\n",
      "[236,    20] loss: 0.00510\n",
      "[236,    30] loss: 0.00813\n",
      "[237,    10] loss: 0.01473\n",
      "[237,    20] loss: 0.01373\n",
      "[237,    30] loss: 0.01148\n",
      "[238,    10] loss: 0.01158\n",
      "[238,    20] loss: 0.01072\n",
      "[238,    30] loss: 0.01279\n",
      "[239,    10] loss: 0.01290\n",
      "[239,    20] loss: 0.00996\n",
      "[239,    30] loss: 0.00698\n",
      "[240,    10] loss: 0.00668\n",
      "[240,    20] loss: 0.00689\n",
      "[240,    30] loss: 0.00526\n",
      "[241,    10] loss: 0.00389\n",
      "[241,    20] loss: 0.00429\n",
      "[241,    30] loss: 0.00544\n",
      "[242,    10] loss: 0.00572\n",
      "[242,    20] loss: 0.00495\n",
      "[242,    30] loss: 0.00489\n",
      "[243,    10] loss: 0.00596\n",
      "[243,    20] loss: 0.00565\n",
      "[243,    30] loss: 0.00878\n",
      "[244,    10] loss: 0.00876\n",
      "[244,    20] loss: 0.00755\n",
      "[244,    30] loss: 0.00725\n",
      "[245,    10] loss: 0.00450\n",
      "[245,    20] loss: 0.00443\n",
      "[245,    30] loss: 0.00479\n",
      "[246,    10] loss: 0.00420\n",
      "[246,    20] loss: 0.00453\n",
      "[246,    30] loss: 0.00435\n",
      "[247,    10] loss: 0.00455\n",
      "[247,    20] loss: 0.00553\n",
      "[247,    30] loss: 0.00531\n",
      "[248,    10] loss: 0.00574\n",
      "[248,    20] loss: 0.00458\n",
      "[248,    30] loss: 0.00491\n",
      "[249,    10] loss: 0.00509\n",
      "[249,    20] loss: 0.00479\n",
      "[249,    30] loss: 0.00581\n",
      "[250,    10] loss: 0.00506\n",
      "[250,    20] loss: 0.00382\n",
      "[250,    30] loss: 0.00414\n",
      "[251,    10] loss: 0.00374\n",
      "[251,    20] loss: 0.00378\n",
      "[251,    30] loss: 0.00489\n",
      "[252,    10] loss: 0.00495\n",
      "[252,    20] loss: 0.00392\n",
      "[252,    30] loss: 0.00406\n",
      "[253,    10] loss: 0.00374\n",
      "[253,    20] loss: 0.00360\n",
      "[253,    30] loss: 0.00394\n",
      "[254,    10] loss: 0.00413\n",
      "[254,    20] loss: 0.00359\n",
      "[254,    30] loss: 0.00513\n",
      "[255,    10] loss: 0.00476\n",
      "[255,    20] loss: 0.00458\n",
      "[255,    30] loss: 0.00634\n",
      "[256,    10] loss: 0.00689\n",
      "[256,    20] loss: 0.00543\n",
      "[256,    30] loss: 0.00474\n",
      "[257,    10] loss: 0.00322\n",
      "[257,    20] loss: 0.00324\n",
      "[257,    30] loss: 0.00415\n",
      "[258,    10] loss: 0.00655\n",
      "[258,    20] loss: 0.00334\n",
      "[258,    30] loss: 0.00411\n",
      "[259,    10] loss: 0.00335\n",
      "[259,    20] loss: 0.00327\n",
      "[259,    30] loss: 0.00370\n",
      "[260,    10] loss: 0.00474\n",
      "[260,    20] loss: 0.00363\n",
      "[260,    30] loss: 0.00380\n",
      "[261,    10] loss: 0.00326\n",
      "[261,    20] loss: 0.00268\n",
      "[261,    30] loss: 0.00348\n",
      "[262,    10] loss: 0.00468\n",
      "[262,    20] loss: 0.00407\n",
      "[262,    30] loss: 0.00352\n",
      "[263,    10] loss: 0.00403\n",
      "[263,    20] loss: 0.00289\n",
      "[263,    30] loss: 0.00359\n",
      "[264,    10] loss: 0.00385\n",
      "[264,    20] loss: 0.00401\n",
      "[264,    30] loss: 0.00344\n",
      "[265,    10] loss: 0.00400\n",
      "[265,    20] loss: 0.00439\n",
      "[265,    30] loss: 0.00775\n",
      "[266,    10] loss: 0.00991\n",
      "[266,    20] loss: 0.01095\n",
      "[266,    30] loss: 0.00902\n",
      "[267,    10] loss: 0.00693\n",
      "[267,    20] loss: 0.00527\n",
      "[267,    30] loss: 0.00563\n",
      "[268,    10] loss: 0.00678\n",
      "[268,    20] loss: 0.00818\n",
      "[268,    30] loss: 0.00912\n",
      "[269,    10] loss: 0.00553\n",
      "[269,    20] loss: 0.00721\n",
      "[269,    30] loss: 0.00483\n",
      "[270,    10] loss: 0.00375\n",
      "[270,    20] loss: 0.00373\n",
      "[270,    30] loss: 0.00473\n",
      "[271,    10] loss: 0.00713\n",
      "[271,    20] loss: 0.00556\n",
      "[271,    30] loss: 0.00718\n",
      "[272,    10] loss: 0.00445\n",
      "[272,    20] loss: 0.00431\n",
      "[272,    30] loss: 0.00639\n",
      "[273,    10] loss: 0.00568\n",
      "[273,    20] loss: 0.00491\n",
      "[273,    30] loss: 0.00447\n",
      "[274,    10] loss: 0.00361\n",
      "[274,    20] loss: 0.00540\n",
      "[274,    30] loss: 0.00918\n",
      "[275,    10] loss: 0.00831\n",
      "[275,    20] loss: 0.00615\n",
      "[275,    30] loss: 0.00614\n",
      "[276,    10] loss: 0.00335\n",
      "[276,    20] loss: 0.00439\n",
      "[276,    30] loss: 0.00527\n",
      "[277,    10] loss: 0.00398\n",
      "[277,    20] loss: 0.00618\n",
      "[277,    30] loss: 0.00756\n",
      "[278,    10] loss: 0.00607\n",
      "[278,    20] loss: 0.00445\n",
      "[278,    30] loss: 0.00493\n",
      "[279,    10] loss: 0.00369\n",
      "[279,    20] loss: 0.00522\n",
      "[279,    30] loss: 0.00715\n",
      "[280,    10] loss: 0.00558\n",
      "[280,    20] loss: 0.00491\n",
      "[280,    30] loss: 0.00636\n",
      "[281,    10] loss: 0.00431\n",
      "[281,    20] loss: 0.00488\n",
      "[281,    30] loss: 0.00629\n",
      "[282,    10] loss: 0.00450\n",
      "[282,    20] loss: 0.00420\n",
      "[282,    30] loss: 0.00535\n",
      "[283,    10] loss: 0.00405\n",
      "[283,    20] loss: 0.00450\n",
      "[283,    30] loss: 0.00517\n",
      "[284,    10] loss: 0.00447\n",
      "[284,    20] loss: 0.00477\n",
      "[284,    30] loss: 0.00564\n",
      "[285,    10] loss: 0.00412\n",
      "[285,    20] loss: 0.00431\n",
      "[285,    30] loss: 0.00504\n",
      "[286,    10] loss: 0.00373\n",
      "[286,    20] loss: 0.00514\n",
      "[286,    30] loss: 0.00788\n",
      "[287,    10] loss: 0.00669\n",
      "[287,    20] loss: 0.00712\n",
      "[287,    30] loss: 0.00860\n",
      "[288,    10] loss: 0.00554\n",
      "[288,    20] loss: 0.00576\n",
      "[288,    30] loss: 0.00602\n",
      "[289,    10] loss: 0.00437\n",
      "[289,    20] loss: 0.00571\n",
      "[289,    30] loss: 0.00664\n",
      "[290,    10] loss: 0.00555\n",
      "[290,    20] loss: 0.00649\n",
      "[290,    30] loss: 0.00734\n",
      "[291,    10] loss: 0.00443\n",
      "[291,    20] loss: 0.00562\n",
      "[291,    30] loss: 0.00544\n",
      "[292,    10] loss: 0.00313\n",
      "[292,    20] loss: 0.00513\n",
      "[292,    30] loss: 0.00579\n",
      "[293,    10] loss: 0.00487\n",
      "[293,    20] loss: 0.00555\n",
      "[293,    30] loss: 0.00551\n",
      "[294,    10] loss: 0.00302\n",
      "[294,    20] loss: 0.00462\n",
      "[294,    30] loss: 0.00496\n",
      "[295,    10] loss: 0.00439\n",
      "[295,    20] loss: 0.00512\n",
      "[295,    30] loss: 0.00502\n",
      "[296,    10] loss: 0.00300\n",
      "[296,    20] loss: 0.00513\n",
      "[296,    30] loss: 0.00607\n",
      "[297,    10] loss: 0.00434\n",
      "[297,    20] loss: 0.00583\n",
      "[297,    30] loss: 0.00606\n",
      "[298,    10] loss: 0.00318\n",
      "[298,    20] loss: 0.00517\n",
      "[298,    30] loss: 0.00587\n",
      "[299,    10] loss: 0.00319\n",
      "[299,    20] loss: 0.00406\n",
      "[299,    30] loss: 0.00440\n",
      "[300,    10] loss: 0.00297\n",
      "[300,    20] loss: 0.00517\n",
      "[300,    30] loss: 0.00630\n",
      "[301,    10] loss: 0.00382\n",
      "[301,    20] loss: 0.00448\n",
      "[301,    30] loss: 0.00510\n",
      "[302,    10] loss: 0.00381\n",
      "[302,    20] loss: 0.00552\n",
      "[302,    30] loss: 0.00623\n",
      "[303,    10] loss: 0.00326\n",
      "[303,    20] loss: 0.00410\n",
      "[303,    30] loss: 0.00464\n",
      "[304,    10] loss: 0.00279\n",
      "[304,    20] loss: 0.00427\n",
      "[304,    30] loss: 0.00572\n",
      "[305,    10] loss: 0.00425\n",
      "[305,    20] loss: 0.00405\n",
      "[305,    30] loss: 0.00442\n",
      "[306,    10] loss: 0.00360\n",
      "[306,    20] loss: 0.00421\n",
      "[306,    30] loss: 0.00525\n",
      "[307,    10] loss: 0.00337\n",
      "[307,    20] loss: 0.00357\n",
      "[307,    30] loss: 0.00410\n",
      "[308,    10] loss: 0.00537\n",
      "[308,    20] loss: 0.00577\n",
      "[308,    30] loss: 0.00481\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[309,    10] loss: 0.00370\n",
      "[309,    20] loss: 0.00447\n",
      "[309,    30] loss: 0.00322\n",
      "[310,    10] loss: 0.00280\n",
      "[310,    20] loss: 0.00328\n",
      "[310,    30] loss: 0.00396\n",
      "[311,    10] loss: 0.00387\n",
      "[311,    20] loss: 0.00402\n",
      "[311,    30] loss: 0.00491\n",
      "[312,    10] loss: 0.00694\n",
      "[312,    20] loss: 0.00577\n",
      "[312,    30] loss: 0.00384\n",
      "[313,    10] loss: 0.00308\n",
      "[313,    20] loss: 0.00406\n",
      "[313,    30] loss: 0.00398\n",
      "[314,    10] loss: 0.00437\n",
      "[314,    20] loss: 0.00460\n",
      "[314,    30] loss: 0.00448\n",
      "[315,    10] loss: 0.00402\n",
      "[315,    20] loss: 0.00413\n",
      "[315,    30] loss: 0.00362\n",
      "[316,    10] loss: 0.00334\n",
      "[316,    20] loss: 0.00324\n",
      "[316,    30] loss: 0.00344\n",
      "[317,    10] loss: 0.00373\n",
      "[317,    20] loss: 0.00351\n",
      "[317,    30] loss: 0.00426\n",
      "[318,    10] loss: 0.00434\n",
      "[318,    20] loss: 0.00422\n",
      "[318,    30] loss: 0.00419\n",
      "[319,    10] loss: 0.00367\n",
      "[319,    20] loss: 0.00402\n",
      "[319,    30] loss: 0.00401\n",
      "[320,    10] loss: 0.00377\n",
      "[320,    20] loss: 0.00375\n",
      "[320,    30] loss: 0.00362\n",
      "[321,    10] loss: 0.00365\n",
      "[321,    20] loss: 0.00357\n",
      "[321,    30] loss: 0.00344\n",
      "[322,    10] loss: 0.00297\n",
      "[322,    20] loss: 0.00276\n",
      "[322,    30] loss: 0.00288\n",
      "[323,    10] loss: 0.00402\n",
      "[323,    20] loss: 0.00389\n",
      "[323,    30] loss: 0.00415\n",
      "[324,    10] loss: 0.00349\n",
      "[324,    20] loss: 0.00322\n",
      "[324,    30] loss: 0.00417\n",
      "[325,    10] loss: 0.00433\n",
      "[325,    20] loss: 0.00389\n",
      "[325,    30] loss: 0.00371\n",
      "[326,    10] loss: 0.00371\n",
      "[326,    20] loss: 0.00353\n",
      "[326,    30] loss: 0.00426\n",
      "[327,    10] loss: 0.00416\n",
      "[327,    20] loss: 0.00432\n",
      "[327,    30] loss: 0.00403\n",
      "[328,    10] loss: 0.00394\n",
      "[328,    20] loss: 0.00367\n",
      "[328,    30] loss: 0.00380\n",
      "[329,    10] loss: 0.00358\n",
      "[329,    20] loss: 0.00297\n",
      "[329,    30] loss: 0.00401\n",
      "[330,    10] loss: 0.00440\n",
      "[330,    20] loss: 0.00324\n",
      "[330,    30] loss: 0.00379\n",
      "[331,    10] loss: 0.00504\n",
      "[331,    20] loss: 0.00541\n",
      "[331,    30] loss: 0.00419\n",
      "[332,    10] loss: 0.00278\n",
      "[332,    20] loss: 0.00294\n",
      "[332,    30] loss: 0.00306\n",
      "[333,    10] loss: 0.00380\n",
      "[333,    20] loss: 0.00384\n",
      "[333,    30] loss: 0.00633\n",
      "[334,    10] loss: 0.00549\n",
      "[334,    20] loss: 0.00501\n",
      "[334,    30] loss: 0.00407\n",
      "[335,    10] loss: 0.00330\n",
      "[335,    20] loss: 0.00304\n",
      "[335,    30] loss: 0.00361\n",
      "[336,    10] loss: 0.00469\n",
      "[336,    20] loss: 0.00436\n",
      "[336,    30] loss: 0.00382\n",
      "[337,    10] loss: 0.00508\n",
      "[337,    20] loss: 0.00500\n",
      "[337,    30] loss: 0.00472\n",
      "[338,    10] loss: 0.00377\n",
      "[338,    20] loss: 0.00391\n",
      "[338,    30] loss: 0.00557\n",
      "[339,    10] loss: 0.00403\n",
      "[339,    20] loss: 0.00459\n",
      "[339,    30] loss: 0.00667\n",
      "[340,    10] loss: 0.00511\n",
      "[340,    20] loss: 0.00538\n",
      "[340,    30] loss: 0.00571\n",
      "[341,    10] loss: 0.00418\n",
      "[341,    20] loss: 0.00403\n",
      "[341,    30] loss: 0.00464\n",
      "[342,    10] loss: 0.00410\n",
      "[342,    20] loss: 0.00457\n",
      "[342,    30] loss: 0.00596\n",
      "[343,    10] loss: 0.00358\n",
      "[343,    20] loss: 0.00424\n",
      "[343,    30] loss: 0.00536\n",
      "[344,    10] loss: 0.00430\n",
      "[344,    20] loss: 0.00360\n",
      "[344,    30] loss: 0.00514\n",
      "[345,    10] loss: 0.00339\n",
      "[345,    20] loss: 0.00396\n",
      "[345,    30] loss: 0.00410\n",
      "[346,    10] loss: 0.00324\n",
      "[346,    20] loss: 0.00341\n",
      "[346,    30] loss: 0.00449\n",
      "[347,    10] loss: 0.00392\n",
      "[347,    20] loss: 0.00416\n",
      "[347,    30] loss: 0.00443\n",
      "[348,    10] loss: 0.00396\n",
      "[348,    20] loss: 0.00410\n",
      "[348,    30] loss: 0.00388\n",
      "[349,    10] loss: 0.00433\n",
      "[349,    20] loss: 0.00488\n",
      "[349,    30] loss: 0.00656\n",
      "[350,    10] loss: 0.00547\n",
      "[350,    20] loss: 0.00728\n",
      "[350,    30] loss: 0.00739\n",
      "[351,    10] loss: 0.00391\n",
      "[351,    20] loss: 0.00413\n",
      "[351,    30] loss: 0.00450\n",
      "[352,    10] loss: 0.00334\n",
      "[352,    20] loss: 0.00336\n",
      "[352,    30] loss: 0.00395\n",
      "[353,    10] loss: 0.00295\n",
      "[353,    20] loss: 0.00316\n",
      "[353,    30] loss: 0.00409\n",
      "[354,    10] loss: 0.00336\n",
      "[354,    20] loss: 0.00339\n",
      "[354,    30] loss: 0.00397\n",
      "[355,    10] loss: 0.00320\n",
      "[355,    20] loss: 0.00299\n",
      "[355,    30] loss: 0.00406\n",
      "[356,    10] loss: 0.00368\n",
      "[356,    20] loss: 0.00409\n",
      "[356,    30] loss: 0.00561\n",
      "[357,    10] loss: 0.00472\n",
      "[357,    20] loss: 0.00456\n",
      "[357,    30] loss: 0.00470\n",
      "[358,    10] loss: 0.00403\n",
      "[358,    20] loss: 0.00287\n",
      "[358,    30] loss: 0.00309\n",
      "[359,    10] loss: 0.00306\n",
      "[359,    20] loss: 0.00351\n",
      "[359,    30] loss: 0.00468\n",
      "[360,    10] loss: 0.00527\n",
      "[360,    20] loss: 0.00412\n",
      "[360,    30] loss: 0.00527\n",
      "[361,    10] loss: 0.00409\n",
      "[361,    20] loss: 0.00490\n",
      "[361,    30] loss: 0.00480\n",
      "[362,    10] loss: 0.00272\n",
      "[362,    20] loss: 0.00317\n",
      "[362,    30] loss: 0.00458\n",
      "[363,    10] loss: 0.00266\n",
      "[363,    20] loss: 0.00301\n",
      "[363,    30] loss: 0.00400\n",
      "[364,    10] loss: 0.00287\n",
      "[364,    20] loss: 0.00314\n",
      "[364,    30] loss: 0.00399\n",
      "[365,    10] loss: 0.00408\n",
      "[365,    20] loss: 0.00443\n",
      "[365,    30] loss: 0.00544\n",
      "[366,    10] loss: 0.00471\n",
      "[366,    20] loss: 0.00405\n",
      "[366,    30] loss: 0.00435\n",
      "[367,    10] loss: 0.00393\n",
      "[367,    20] loss: 0.00379\n",
      "[367,    30] loss: 0.00409\n",
      "[368,    10] loss: 0.00393\n",
      "[368,    20] loss: 0.00403\n",
      "[368,    30] loss: 0.00400\n",
      "[369,    10] loss: 0.00302\n",
      "[369,    20] loss: 0.00350\n",
      "[369,    30] loss: 0.00434\n",
      "[370,    10] loss: 0.00298\n",
      "[370,    20] loss: 0.00258\n",
      "[370,    30] loss: 0.00375\n",
      "[371,    10] loss: 0.00323\n",
      "[371,    20] loss: 0.00343\n",
      "[371,    30] loss: 0.00442\n",
      "[372,    10] loss: 0.00378\n",
      "[372,    20] loss: 0.00321\n",
      "[372,    30] loss: 0.00341\n",
      "[373,    10] loss: 0.00356\n",
      "[373,    20] loss: 0.00358\n",
      "[373,    30] loss: 0.00472\n",
      "[374,    10] loss: 0.00382\n",
      "[374,    20] loss: 0.00297\n",
      "[374,    30] loss: 0.00359\n",
      "[375,    10] loss: 0.00325\n",
      "[375,    20] loss: 0.00410\n",
      "[375,    30] loss: 0.00476\n",
      "[376,    10] loss: 0.00460\n",
      "[376,    20] loss: 0.00452\n",
      "[376,    30] loss: 0.00419\n",
      "[377,    10] loss: 0.00299\n",
      "[377,    20] loss: 0.00359\n",
      "[377,    30] loss: 0.00386\n",
      "[378,    10] loss: 0.00329\n",
      "[378,    20] loss: 0.00437\n",
      "[378,    30] loss: 0.00619\n",
      "[379,    10] loss: 0.00540\n",
      "[379,    20] loss: 0.00576\n",
      "[379,    30] loss: 0.00604\n",
      "[380,    10] loss: 0.00409\n",
      "[380,    20] loss: 0.00396\n",
      "[380,    30] loss: 0.00477\n",
      "[381,    10] loss: 0.00381\n",
      "[381,    20] loss: 0.00346\n",
      "[381,    30] loss: 0.00410\n",
      "[382,    10] loss: 0.00348\n",
      "[382,    20] loss: 0.00349\n",
      "[382,    30] loss: 0.00472\n",
      "[383,    10] loss: 0.00382\n",
      "[383,    20] loss: 0.00365\n",
      "[383,    30] loss: 0.00528\n",
      "[384,    10] loss: 0.00378\n",
      "[384,    20] loss: 0.00392\n",
      "[384,    30] loss: 0.00450\n",
      "[385,    10] loss: 0.00306\n",
      "[385,    20] loss: 0.00314\n",
      "[385,    30] loss: 0.00365\n",
      "[386,    10] loss: 0.00375\n",
      "[386,    20] loss: 0.00373\n",
      "[386,    30] loss: 0.00473\n",
      "[387,    10] loss: 0.00407\n",
      "[387,    20] loss: 0.00421\n",
      "[387,    30] loss: 0.00381\n",
      "[388,    10] loss: 0.00435\n",
      "[388,    20] loss: 0.00494\n",
      "[388,    30] loss: 0.00563\n",
      "[389,    10] loss: 0.00445\n",
      "[389,    20] loss: 0.00406\n",
      "[389,    30] loss: 0.00390\n",
      "[390,    10] loss: 0.00261\n",
      "[390,    20] loss: 0.00277\n",
      "[390,    30] loss: 0.00313\n",
      "[391,    10] loss: 0.00341\n",
      "[391,    20] loss: 0.00394\n",
      "[391,    30] loss: 0.00417\n",
      "[392,    10] loss: 0.00387\n",
      "[392,    20] loss: 0.00470\n",
      "[392,    30] loss: 0.00557\n",
      "[393,    10] loss: 0.00388\n",
      "[393,    20] loss: 0.00376\n",
      "[393,    30] loss: 0.00353\n",
      "[394,    10] loss: 0.00297\n",
      "[394,    20] loss: 0.00325\n",
      "[394,    30] loss: 0.00411\n",
      "[395,    10] loss: 0.00438\n",
      "[395,    20] loss: 0.00423\n",
      "[395,    30] loss: 0.00436\n",
      "[396,    10] loss: 0.00250\n",
      "[396,    20] loss: 0.00282\n",
      "[396,    30] loss: 0.00364\n",
      "[397,    10] loss: 0.00371\n",
      "[397,    20] loss: 0.00489\n",
      "[397,    30] loss: 0.00585\n",
      "[398,    10] loss: 0.00343\n",
      "[398,    20] loss: 0.00361\n",
      "[398,    30] loss: 0.00324\n",
      "[399,    10] loss: 0.00301\n",
      "[399,    20] loss: 0.00320\n",
      "[399,    30] loss: 0.00330\n",
      "[400,    10] loss: 0.00242\n",
      "[400,    20] loss: 0.00305\n",
      "[400,    30] loss: 0.00368\n",
      "[401,    10] loss: 0.00325\n",
      "[401,    20] loss: 0.00341\n",
      "[401,    30] loss: 0.00311\n",
      "[402,    10] loss: 0.00252\n",
      "[402,    20] loss: 0.00374\n",
      "[402,    30] loss: 0.00638\n",
      "[403,    10] loss: 0.00522\n",
      "[403,    20] loss: 0.00392\n",
      "[403,    30] loss: 0.00498\n",
      "[404,    10] loss: 0.00430\n",
      "[404,    20] loss: 0.00471\n",
      "[404,    30] loss: 0.00484\n",
      "[405,    10] loss: 0.00304\n",
      "[405,    20] loss: 0.00285\n",
      "[405,    30] loss: 0.00333\n",
      "[406,    10] loss: 0.00378\n",
      "[406,    20] loss: 0.00397\n",
      "[406,    30] loss: 0.00568\n",
      "[407,    10] loss: 0.00646\n",
      "[407,    20] loss: 0.00648\n",
      "[407,    30] loss: 0.00566\n",
      "[408,    10] loss: 0.00512\n",
      "[408,    20] loss: 0.00461\n",
      "[408,    30] loss: 0.00531\n",
      "[409,    10] loss: 0.00430\n",
      "[409,    20] loss: 0.00470\n",
      "[409,    30] loss: 0.00491\n",
      "[410,    10] loss: 0.00417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410,    20] loss: 0.00417\n",
      "[410,    30] loss: 0.00451\n",
      "[411,    10] loss: 0.00387\n",
      "[411,    20] loss: 0.00381\n",
      "[411,    30] loss: 0.00401\n",
      "[412,    10] loss: 0.00470\n",
      "[412,    20] loss: 0.00614\n",
      "[412,    30] loss: 0.00881\n",
      "[413,    10] loss: 0.00687\n",
      "[413,    20] loss: 0.00482\n",
      "[413,    30] loss: 0.00443\n",
      "[414,    10] loss: 0.00350\n",
      "[414,    20] loss: 0.00343\n",
      "[414,    30] loss: 0.00427\n",
      "[415,    10] loss: 0.00408\n",
      "[415,    20] loss: 0.00378\n",
      "[415,    30] loss: 0.00421\n",
      "[416,    10] loss: 0.00410\n",
      "[416,    20] loss: 0.00419\n",
      "[416,    30] loss: 0.00492\n",
      "[417,    10] loss: 0.00478\n",
      "[417,    20] loss: 0.00406\n",
      "[417,    30] loss: 0.00425\n",
      "[418,    10] loss: 0.00260\n",
      "[418,    20] loss: 0.00362\n",
      "[418,    30] loss: 0.00527\n",
      "[419,    10] loss: 0.00332\n",
      "[419,    20] loss: 0.00321\n",
      "[419,    30] loss: 0.00349\n",
      "[420,    10] loss: 0.00295\n",
      "[420,    20] loss: 0.00378\n",
      "[420,    30] loss: 0.00561\n",
      "[421,    10] loss: 0.00357\n",
      "[421,    20] loss: 0.00365\n",
      "[421,    30] loss: 0.00476\n",
      "[422,    10] loss: 0.00265\n",
      "[422,    20] loss: 0.00349\n",
      "[422,    30] loss: 0.00571\n",
      "[423,    10] loss: 0.00355\n",
      "[423,    20] loss: 0.00347\n",
      "[423,    30] loss: 0.00397\n",
      "[424,    10] loss: 0.00272\n",
      "[424,    20] loss: 0.00398\n",
      "[424,    30] loss: 0.00507\n",
      "[425,    10] loss: 0.00310\n",
      "[425,    20] loss: 0.00346\n",
      "[425,    30] loss: 0.00445\n",
      "[426,    10] loss: 0.00240\n",
      "[426,    20] loss: 0.00376\n",
      "[426,    30] loss: 0.00502\n",
      "[427,    10] loss: 0.00339\n",
      "[427,    20] loss: 0.00330\n",
      "[427,    30] loss: 0.00405\n",
      "[428,    10] loss: 0.00279\n",
      "[428,    20] loss: 0.00389\n",
      "[428,    30] loss: 0.00554\n",
      "[429,    10] loss: 0.00294\n",
      "[429,    20] loss: 0.00340\n",
      "[429,    30] loss: 0.00467\n",
      "[430,    10] loss: 0.00295\n",
      "[430,    20] loss: 0.00340\n",
      "[430,    30] loss: 0.00429\n",
      "[431,    10] loss: 0.00284\n",
      "[431,    20] loss: 0.00395\n",
      "[431,    30] loss: 0.00567\n",
      "[432,    10] loss: 0.00296\n",
      "[432,    20] loss: 0.00269\n",
      "[432,    30] loss: 0.00343\n",
      "[433,    10] loss: 0.00371\n",
      "[433,    20] loss: 0.00406\n",
      "[433,    30] loss: 0.00358\n",
      "[434,    10] loss: 0.00265\n",
      "[434,    20] loss: 0.00421\n",
      "[434,    30] loss: 0.00366\n",
      "[435,    10] loss: 0.00368\n",
      "[435,    20] loss: 0.00467\n",
      "[435,    30] loss: 0.00489\n",
      "[436,    10] loss: 0.00328\n",
      "[436,    20] loss: 0.00350\n",
      "[436,    30] loss: 0.00448\n",
      "[437,    10] loss: 0.00310\n",
      "[437,    20] loss: 0.00325\n",
      "[437,    30] loss: 0.00432\n",
      "[438,    10] loss: 0.00524\n",
      "[438,    20] loss: 0.00518\n",
      "[438,    30] loss: 0.00371\n",
      "[439,    10] loss: 0.00401\n",
      "[439,    20] loss: 0.00465\n",
      "[439,    30] loss: 0.00377\n",
      "[440,    10] loss: 0.00476\n",
      "[440,    20] loss: 0.00502\n",
      "[440,    30] loss: 0.00417\n",
      "[441,    10] loss: 0.00329\n",
      "[441,    20] loss: 0.00430\n",
      "[441,    30] loss: 0.00524\n",
      "[442,    10] loss: 0.00318\n",
      "[442,    20] loss: 0.00332\n",
      "[442,    30] loss: 0.00349\n",
      "[443,    10] loss: 0.00250\n",
      "[443,    20] loss: 0.00340\n",
      "[443,    30] loss: 0.00675\n",
      "[444,    10] loss: 0.00403\n",
      "[444,    20] loss: 0.00322\n",
      "[444,    30] loss: 0.00327\n",
      "[445,    10] loss: 0.00286\n",
      "[445,    20] loss: 0.00359\n",
      "[445,    30] loss: 0.00383\n",
      "[446,    10] loss: 0.00298\n",
      "[446,    20] loss: 0.00411\n",
      "[446,    30] loss: 0.00653\n",
      "[447,    10] loss: 0.00441\n",
      "[447,    20] loss: 0.00393\n",
      "[447,    30] loss: 0.00473\n",
      "[448,    10] loss: 0.00245\n",
      "[448,    20] loss: 0.00247\n",
      "[448,    30] loss: 0.00292\n",
      "[449,    10] loss: 0.00311\n",
      "[449,    20] loss: 0.00362\n",
      "[449,    30] loss: 0.00473\n",
      "[450,    10] loss: 0.00341\n",
      "[450,    20] loss: 0.00381\n",
      "[450,    30] loss: 0.00393\n",
      "[451,    10] loss: 0.00254\n",
      "[451,    20] loss: 0.00436\n",
      "[451,    30] loss: 0.00735\n",
      "[452,    10] loss: 0.00349\n",
      "[452,    20] loss: 0.00455\n",
      "[452,    30] loss: 0.00558\n",
      "[453,    10] loss: 0.00299\n",
      "[453,    20] loss: 0.00452\n",
      "[453,    30] loss: 0.00485\n",
      "[454,    10] loss: 0.00387\n",
      "[454,    20] loss: 0.00572\n",
      "[454,    30] loss: 0.00457\n",
      "[455,    10] loss: 0.00332\n",
      "[455,    20] loss: 0.00397\n",
      "[455,    30] loss: 0.00479\n",
      "[456,    10] loss: 0.00309\n",
      "[456,    20] loss: 0.00388\n",
      "[456,    30] loss: 0.00502\n",
      "[457,    10] loss: 0.00333\n",
      "[457,    20] loss: 0.00344\n",
      "[457,    30] loss: 0.00481\n",
      "[458,    10] loss: 0.00400\n",
      "[458,    20] loss: 0.00360\n",
      "[458,    30] loss: 0.00380\n",
      "[459,    10] loss: 0.00263\n",
      "[459,    20] loss: 0.00390\n",
      "[459,    30] loss: 0.00476\n",
      "[460,    10] loss: 0.00327\n",
      "[460,    20] loss: 0.00327\n",
      "[460,    30] loss: 0.00390\n",
      "[461,    10] loss: 0.00314\n",
      "[461,    20] loss: 0.00313\n",
      "[461,    30] loss: 0.00380\n",
      "[462,    10] loss: 0.00312\n",
      "[462,    20] loss: 0.00384\n",
      "[462,    30] loss: 0.00404\n",
      "[463,    10] loss: 0.00303\n",
      "[463,    20] loss: 0.00324\n",
      "[463,    30] loss: 0.00410\n",
      "[464,    10] loss: 0.00279\n",
      "[464,    20] loss: 0.00290\n",
      "[464,    30] loss: 0.00328\n",
      "[465,    10] loss: 0.00282\n",
      "[465,    20] loss: 0.00355\n",
      "[465,    30] loss: 0.00394\n",
      "[466,    10] loss: 0.00291\n",
      "[466,    20] loss: 0.00327\n",
      "[466,    30] loss: 0.00442\n",
      "[467,    10] loss: 0.00409\n",
      "[467,    20] loss: 0.00372\n",
      "[467,    30] loss: 0.00364\n",
      "[468,    10] loss: 0.00282\n",
      "[468,    20] loss: 0.00329\n",
      "[468,    30] loss: 0.00427\n",
      "[469,    10] loss: 0.00352\n",
      "[469,    20] loss: 0.00313\n",
      "[469,    30] loss: 0.00308\n",
      "[470,    10] loss: 0.00328\n",
      "[470,    20] loss: 0.00371\n",
      "[470,    30] loss: 0.00484\n",
      "[471,    10] loss: 0.00440\n",
      "[471,    20] loss: 0.00651\n",
      "[471,    30] loss: 0.01008\n",
      "[472,    10] loss: 0.00934\n",
      "[472,    20] loss: 0.00863\n",
      "[472,    30] loss: 0.00597\n",
      "[473,    10] loss: 0.00389\n",
      "[473,    20] loss: 0.00385\n",
      "[473,    30] loss: 0.00338\n",
      "[474,    10] loss: 0.00268\n",
      "[474,    20] loss: 0.00310\n",
      "[474,    30] loss: 0.00424\n",
      "[475,    10] loss: 0.00273\n",
      "[475,    20] loss: 0.00293\n",
      "[475,    30] loss: 0.00302\n",
      "[476,    10] loss: 0.00245\n",
      "[476,    20] loss: 0.00399\n",
      "[476,    30] loss: 0.00499\n",
      "[477,    10] loss: 0.00394\n",
      "[477,    20] loss: 0.00326\n",
      "[477,    30] loss: 0.00379\n",
      "[478,    10] loss: 0.00267\n",
      "[478,    20] loss: 0.00347\n",
      "[478,    30] loss: 0.00478\n",
      "[479,    10] loss: 0.00339\n",
      "[479,    20] loss: 0.00316\n",
      "[479,    30] loss: 0.00337\n",
      "[480,    10] loss: 0.00267\n",
      "[480,    20] loss: 0.00368\n",
      "[480,    30] loss: 0.00554\n",
      "[481,    10] loss: 0.00323\n",
      "[481,    20] loss: 0.00357\n",
      "[481,    30] loss: 0.00478\n",
      "[482,    10] loss: 0.00394\n",
      "[482,    20] loss: 0.00325\n",
      "[482,    30] loss: 0.00335\n",
      "[483,    10] loss: 0.00275\n",
      "[483,    20] loss: 0.00468\n",
      "[483,    30] loss: 0.00485\n",
      "[484,    10] loss: 0.00291\n",
      "[484,    20] loss: 0.00292\n",
      "[484,    30] loss: 0.00275\n",
      "[485,    10] loss: 0.00235\n",
      "[485,    20] loss: 0.00349\n",
      "[485,    30] loss: 0.00449\n",
      "[486,    10] loss: 0.00306\n",
      "[486,    20] loss: 0.00318\n",
      "[486,    30] loss: 0.00291\n",
      "[487,    10] loss: 0.00254\n",
      "[487,    20] loss: 0.00312\n",
      "[487,    30] loss: 0.00499\n",
      "[488,    10] loss: 0.00480\n",
      "[488,    20] loss: 0.00669\n",
      "[488,    30] loss: 0.00590\n",
      "[489,    10] loss: 0.00379\n",
      "[489,    20] loss: 0.00419\n",
      "[489,    30] loss: 0.00545\n",
      "[490,    10] loss: 0.00276\n",
      "[490,    20] loss: 0.00380\n",
      "[490,    30] loss: 0.00484\n",
      "[491,    10] loss: 0.00277\n",
      "[491,    20] loss: 0.00327\n",
      "[491,    30] loss: 0.00438\n",
      "[492,    10] loss: 0.00296\n",
      "[492,    20] loss: 0.00435\n",
      "[492,    30] loss: 0.00546\n",
      "[493,    10] loss: 0.00252\n",
      "[493,    20] loss: 0.00352\n",
      "[493,    30] loss: 0.00446\n",
      "[494,    10] loss: 0.00281\n",
      "[494,    20] loss: 0.00391\n",
      "[494,    30] loss: 0.00559\n",
      "[495,    10] loss: 0.00268\n",
      "[495,    20] loss: 0.00418\n",
      "[495,    30] loss: 0.00457\n",
      "[496,    10] loss: 0.00268\n",
      "[496,    20] loss: 0.00358\n",
      "[496,    30] loss: 0.00513\n",
      "[497,    10] loss: 0.00260\n",
      "[497,    20] loss: 0.00347\n",
      "[497,    30] loss: 0.00427\n",
      "[498,    10] loss: 0.00261\n",
      "[498,    20] loss: 0.00373\n",
      "[498,    30] loss: 0.00554\n",
      "[499,    10] loss: 0.00262\n",
      "[499,    20] loss: 0.00335\n",
      "[499,    30] loss: 0.00502\n",
      "[500,    10] loss: 0.00377\n",
      "[500,    20] loss: 0.00430\n",
      "[500,    30] loss: 0.00500\n",
      "[501,    10] loss: 0.00308\n",
      "[501,    20] loss: 0.00411\n",
      "[501,    30] loss: 0.00581\n",
      "[502,    10] loss: 0.00344\n",
      "[502,    20] loss: 0.00382\n",
      "[502,    30] loss: 0.00513\n",
      "[503,    10] loss: 0.00266\n",
      "[503,    20] loss: 0.00330\n",
      "[503,    30] loss: 0.00516\n",
      "[504,    10] loss: 0.00246\n",
      "[504,    20] loss: 0.00300\n",
      "[504,    30] loss: 0.00413\n",
      "[505,    10] loss: 0.00274\n",
      "[505,    20] loss: 0.00325\n",
      "[505,    30] loss: 0.00399\n",
      "[506,    10] loss: 0.00368\n",
      "[506,    20] loss: 0.00395\n",
      "[506,    30] loss: 0.00390\n",
      "[507,    10] loss: 0.00304\n",
      "[507,    20] loss: 0.00386\n",
      "[507,    30] loss: 0.00423\n",
      "[508,    10] loss: 0.00248\n",
      "[508,    20] loss: 0.00294\n",
      "[508,    30] loss: 0.00356\n",
      "[509,    10] loss: 0.00259\n",
      "[509,    20] loss: 0.00399\n",
      "[509,    30] loss: 0.00373\n",
      "[510,    10] loss: 0.00255\n",
      "[510,    20] loss: 0.00330\n",
      "[510,    30] loss: 0.00412\n",
      "[511,    10] loss: 0.00280\n",
      "[511,    20] loss: 0.00347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[511,    30] loss: 0.00357\n",
      "[512,    10] loss: 0.00249\n",
      "[512,    20] loss: 0.00380\n",
      "[512,    30] loss: 0.00380\n",
      "[513,    10] loss: 0.00422\n",
      "[513,    20] loss: 0.00546\n",
      "[513,    30] loss: 0.00612\n",
      "[514,    10] loss: 0.00397\n",
      "[514,    20] loss: 0.00489\n",
      "[514,    30] loss: 0.00662\n",
      "[515,    10] loss: 0.00358\n",
      "[515,    20] loss: 0.00294\n",
      "[515,    30] loss: 0.00315\n",
      "[516,    10] loss: 0.00334\n",
      "[516,    20] loss: 0.00359\n",
      "[516,    30] loss: 0.00289\n",
      "[517,    10] loss: 0.00236\n",
      "[517,    20] loss: 0.00324\n",
      "[517,    30] loss: 0.00361\n",
      "[518,    10] loss: 0.00291\n",
      "[518,    20] loss: 0.00321\n",
      "[518,    30] loss: 0.00327\n",
      "[519,    10] loss: 0.00240\n",
      "[519,    20] loss: 0.00311\n",
      "[519,    30] loss: 0.00338\n",
      "[520,    10] loss: 0.00322\n",
      "[520,    20] loss: 0.00396\n",
      "[520,    30] loss: 0.00370\n",
      "[521,    10] loss: 0.00254\n",
      "[521,    20] loss: 0.00385\n",
      "[521,    30] loss: 0.00444\n",
      "[522,    10] loss: 0.00238\n",
      "[522,    20] loss: 0.00261\n",
      "[522,    30] loss: 0.00250\n",
      "[523,    10] loss: 0.00301\n",
      "[523,    20] loss: 0.00401\n",
      "[523,    30] loss: 0.00355\n",
      "[524,    10] loss: 0.00304\n",
      "[524,    20] loss: 0.00366\n",
      "[524,    30] loss: 0.00270\n",
      "[525,    10] loss: 0.00226\n",
      "[525,    20] loss: 0.00398\n",
      "[525,    30] loss: 0.00469\n",
      "[526,    10] loss: 0.00246\n",
      "[526,    20] loss: 0.00321\n",
      "[526,    30] loss: 0.00361\n",
      "[527,    10] loss: 0.00277\n",
      "[527,    20] loss: 0.00358\n",
      "[527,    30] loss: 0.00329\n",
      "[528,    10] loss: 0.00258\n",
      "[528,    20] loss: 0.00344\n",
      "[528,    30] loss: 0.00347\n",
      "[529,    10] loss: 0.00266\n",
      "[529,    20] loss: 0.00330\n",
      "[529,    30] loss: 0.00253\n",
      "[530,    10] loss: 0.00238\n",
      "[530,    20] loss: 0.00342\n",
      "[530,    30] loss: 0.00301\n",
      "[531,    10] loss: 0.00334\n",
      "[531,    20] loss: 0.00492\n",
      "[531,    30] loss: 0.00429\n",
      "[532,    10] loss: 0.00328\n",
      "[532,    20] loss: 0.00415\n",
      "[532,    30] loss: 0.00388\n",
      "[533,    10] loss: 0.00230\n",
      "[533,    20] loss: 0.00324\n",
      "[533,    30] loss: 0.00311\n",
      "[534,    10] loss: 0.00285\n",
      "[534,    20] loss: 0.00372\n",
      "[534,    30] loss: 0.00318\n",
      "[535,    10] loss: 0.00274\n",
      "[535,    20] loss: 0.00358\n",
      "[535,    30] loss: 0.00318\n",
      "[536,    10] loss: 0.00307\n",
      "[536,    20] loss: 0.00399\n",
      "[536,    30] loss: 0.00285\n",
      "[537,    10] loss: 0.00247\n",
      "[537,    20] loss: 0.00366\n",
      "[537,    30] loss: 0.00310\n",
      "[538,    10] loss: 0.00255\n",
      "[538,    20] loss: 0.00320\n",
      "[538,    30] loss: 0.00360\n",
      "[539,    10] loss: 0.00239\n",
      "[539,    20] loss: 0.00328\n",
      "[539,    30] loss: 0.00285\n",
      "[540,    10] loss: 0.00260\n",
      "[540,    20] loss: 0.00353\n",
      "[540,    30] loss: 0.00395\n",
      "[541,    10] loss: 0.00223\n",
      "[541,    20] loss: 0.00293\n",
      "[541,    30] loss: 0.00268\n",
      "[542,    10] loss: 0.00270\n",
      "[542,    20] loss: 0.00323\n",
      "[542,    30] loss: 0.00302\n",
      "[543,    10] loss: 0.00279\n",
      "[543,    20] loss: 0.00398\n",
      "[543,    30] loss: 0.00380\n",
      "[544,    10] loss: 0.00258\n",
      "[544,    20] loss: 0.00340\n",
      "[544,    30] loss: 0.00318\n",
      "[545,    10] loss: 0.00235\n",
      "[545,    20] loss: 0.00349\n",
      "[545,    30] loss: 0.00301\n",
      "[546,    10] loss: 0.00296\n",
      "[546,    20] loss: 0.00409\n",
      "[546,    30] loss: 0.00351\n",
      "[547,    10] loss: 0.00241\n",
      "[547,    20] loss: 0.00303\n",
      "[547,    30] loss: 0.00282\n",
      "[548,    10] loss: 0.00277\n",
      "[548,    20] loss: 0.00319\n",
      "[548,    30] loss: 0.00294\n",
      "[549,    10] loss: 0.00262\n",
      "[549,    20] loss: 0.00311\n",
      "[549,    30] loss: 0.00396\n",
      "[550,    10] loss: 0.00291\n",
      "[550,    20] loss: 0.00352\n",
      "[550,    30] loss: 0.00340\n",
      "[551,    10] loss: 0.00261\n",
      "[551,    20] loss: 0.00353\n",
      "[551,    30] loss: 0.00299\n",
      "[552,    10] loss: 0.00254\n",
      "[552,    20] loss: 0.00291\n",
      "[552,    30] loss: 0.00282\n",
      "[553,    10] loss: 0.00282\n",
      "[553,    20] loss: 0.00353\n",
      "[553,    30] loss: 0.00417\n",
      "[554,    10] loss: 0.00268\n",
      "[554,    20] loss: 0.00307\n",
      "[554,    30] loss: 0.00369\n",
      "[555,    10] loss: 0.00301\n",
      "[555,    20] loss: 0.00393\n",
      "[555,    30] loss: 0.00469\n",
      "[556,    10] loss: 0.00329\n",
      "[556,    20] loss: 0.00365\n",
      "[556,    30] loss: 0.00343\n",
      "[557,    10] loss: 0.00458\n",
      "[557,    20] loss: 0.00448\n",
      "[557,    30] loss: 0.00572\n",
      "[558,    10] loss: 0.00373\n",
      "[558,    20] loss: 0.00400\n",
      "[558,    30] loss: 0.00435\n",
      "[559,    10] loss: 0.00304\n",
      "[559,    20] loss: 0.00417\n",
      "[559,    30] loss: 0.00499\n",
      "[560,    10] loss: 0.00328\n",
      "[560,    20] loss: 0.00432\n",
      "[560,    30] loss: 0.00345\n",
      "[561,    10] loss: 0.00318\n",
      "[561,    20] loss: 0.00396\n",
      "[561,    30] loss: 0.00359\n",
      "[562,    10] loss: 0.00225\n",
      "[562,    20] loss: 0.00382\n",
      "[562,    30] loss: 0.00489\n",
      "[563,    10] loss: 0.00442\n",
      "[563,    20] loss: 0.00383\n",
      "[563,    30] loss: 0.00353\n",
      "[564,    10] loss: 0.00346\n",
      "[564,    20] loss: 0.00356\n",
      "[564,    30] loss: 0.00364\n",
      "[565,    10] loss: 0.00253\n",
      "[565,    20] loss: 0.00449\n",
      "[565,    30] loss: 0.00468\n",
      "[566,    10] loss: 0.00369\n",
      "[566,    20] loss: 0.00371\n",
      "[566,    30] loss: 0.00317\n",
      "[567,    10] loss: 0.00364\n",
      "[567,    20] loss: 0.00439\n",
      "[567,    30] loss: 0.00451\n",
      "[568,    10] loss: 0.00294\n",
      "[568,    20] loss: 0.00352\n",
      "[568,    30] loss: 0.00454\n",
      "[569,    10] loss: 0.00470\n",
      "[569,    20] loss: 0.00333\n",
      "[569,    30] loss: 0.00373\n",
      "[570,    10] loss: 0.00453\n",
      "[570,    20] loss: 0.00499\n",
      "[570,    30] loss: 0.00399\n",
      "[571,    10] loss: 0.00378\n",
      "[571,    20] loss: 0.00330\n",
      "[571,    30] loss: 0.00296\n",
      "[572,    10] loss: 0.00313\n",
      "[572,    20] loss: 0.00354\n",
      "[572,    30] loss: 0.00303\n",
      "[573,    10] loss: 0.00252\n",
      "[573,    20] loss: 0.00385\n",
      "[573,    30] loss: 0.00508\n",
      "[574,    10] loss: 0.00299\n",
      "[574,    20] loss: 0.00356\n",
      "[574,    30] loss: 0.00446\n",
      "[575,    10] loss: 0.00379\n",
      "[575,    20] loss: 0.00378\n",
      "[575,    30] loss: 0.00331\n",
      "[576,    10] loss: 0.00373\n",
      "[576,    20] loss: 0.00517\n",
      "[576,    30] loss: 0.00439\n",
      "[577,    10] loss: 0.00240\n",
      "[577,    20] loss: 0.00338\n",
      "[577,    30] loss: 0.00298\n",
      "[578,    10] loss: 0.00306\n",
      "[578,    20] loss: 0.00354\n",
      "[578,    30] loss: 0.00332\n",
      "[579,    10] loss: 0.00257\n",
      "[579,    20] loss: 0.00370\n",
      "[579,    30] loss: 0.00376\n",
      "[580,    10] loss: 0.00227\n",
      "[580,    20] loss: 0.00335\n",
      "[580,    30] loss: 0.00458\n",
      "[581,    10] loss: 0.00474\n",
      "[581,    20] loss: 0.00421\n",
      "[581,    30] loss: 0.00302\n",
      "[582,    10] loss: 0.00271\n",
      "[582,    20] loss: 0.00344\n",
      "[582,    30] loss: 0.00368\n",
      "[583,    10] loss: 0.00274\n",
      "[583,    20] loss: 0.00332\n",
      "[583,    30] loss: 0.00438\n",
      "[584,    10] loss: 0.00411\n",
      "[584,    20] loss: 0.00378\n",
      "[584,    30] loss: 0.00296\n",
      "[585,    10] loss: 0.00253\n",
      "[585,    20] loss: 0.00340\n",
      "[585,    30] loss: 0.00341\n",
      "[586,    10] loss: 0.00268\n",
      "[586,    20] loss: 0.00319\n",
      "[586,    30] loss: 0.00253\n",
      "[587,    10] loss: 0.00285\n",
      "[587,    20] loss: 0.00359\n",
      "[587,    30] loss: 0.00400\n",
      "[588,    10] loss: 0.00303\n",
      "[588,    20] loss: 0.00367\n",
      "[588,    30] loss: 0.00339\n",
      "[589,    10] loss: 0.00296\n",
      "[589,    20] loss: 0.00319\n",
      "[589,    30] loss: 0.00387\n",
      "[590,    10] loss: 0.00325\n",
      "[590,    20] loss: 0.00391\n",
      "[590,    30] loss: 0.00301\n",
      "[591,    10] loss: 0.00322\n",
      "[591,    20] loss: 0.00403\n",
      "[591,    30] loss: 0.00414\n",
      "[592,    10] loss: 0.00381\n",
      "[592,    20] loss: 0.00357\n",
      "[592,    30] loss: 0.00336\n",
      "[593,    10] loss: 0.00387\n",
      "[593,    20] loss: 0.00395\n",
      "[593,    30] loss: 0.00262\n",
      "[594,    10] loss: 0.00253\n",
      "[594,    20] loss: 0.00383\n",
      "[594,    30] loss: 0.00322\n",
      "[595,    10] loss: 0.00400\n",
      "[595,    20] loss: 0.00472\n",
      "[595,    30] loss: 0.00431\n",
      "[596,    10] loss: 0.00296\n",
      "[596,    20] loss: 0.00336\n",
      "[596,    30] loss: 0.00487\n",
      "[597,    10] loss: 0.00389\n",
      "[597,    20] loss: 0.00429\n",
      "[597,    30] loss: 0.00268\n",
      "[598,    10] loss: 0.00243\n",
      "[598,    20] loss: 0.00336\n",
      "[598,    30] loss: 0.00390\n",
      "[599,    10] loss: 0.00274\n",
      "[599,    20] loss: 0.00320\n",
      "[599,    30] loss: 0.00412\n",
      "[600,    10] loss: 0.00324\n",
      "[600,    20] loss: 0.00375\n",
      "[600,    30] loss: 0.00384\n",
      "[601,    10] loss: 0.00258\n",
      "[601,    20] loss: 0.00303\n",
      "[601,    30] loss: 0.00423\n",
      "[602,    10] loss: 0.00341\n",
      "[602,    20] loss: 0.00402\n",
      "[602,    30] loss: 0.00346\n",
      "[603,    10] loss: 0.00305\n",
      "[603,    20] loss: 0.00317\n",
      "[603,    30] loss: 0.00329\n",
      "[604,    10] loss: 0.00364\n",
      "[604,    20] loss: 0.00406\n",
      "[604,    30] loss: 0.00428\n",
      "[605,    10] loss: 0.00309\n",
      "[605,    20] loss: 0.00343\n",
      "[605,    30] loss: 0.00292\n",
      "[606,    10] loss: 0.00313\n",
      "[606,    20] loss: 0.00411\n",
      "[606,    30] loss: 0.00465\n",
      "[607,    10] loss: 0.00324\n",
      "[607,    20] loss: 0.00351\n",
      "[607,    30] loss: 0.00470\n",
      "[608,    10] loss: 0.00486\n",
      "[608,    20] loss: 0.00443\n",
      "[608,    30] loss: 0.00349\n",
      "[609,    10] loss: 0.00337\n",
      "[609,    20] loss: 0.00468\n",
      "[609,    30] loss: 0.00382\n",
      "[610,    10] loss: 0.00223\n",
      "[610,    20] loss: 0.00342\n",
      "[610,    30] loss: 0.00268\n",
      "[611,    10] loss: 0.00255\n",
      "[611,    20] loss: 0.00408\n",
      "[611,    30] loss: 0.00376\n",
      "[612,    10] loss: 0.00304\n",
      "[612,    20] loss: 0.00339\n",
      "[612,    30] loss: 0.00340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[613,    10] loss: 0.00390\n",
      "[613,    20] loss: 0.00429\n",
      "[613,    30] loss: 0.00480\n",
      "[614,    10] loss: 0.00267\n",
      "[614,    20] loss: 0.00293\n",
      "[614,    30] loss: 0.00376\n",
      "[615,    10] loss: 0.00431\n",
      "[615,    20] loss: 0.00425\n",
      "[615,    30] loss: 0.00276\n",
      "[616,    10] loss: 0.00266\n",
      "[616,    20] loss: 0.00387\n",
      "[616,    30] loss: 0.00489\n",
      "[617,    10] loss: 0.00321\n",
      "[617,    20] loss: 0.00348\n",
      "[617,    30] loss: 0.00374\n",
      "[618,    10] loss: 0.00447\n",
      "[618,    20] loss: 0.00527\n",
      "[618,    30] loss: 0.00506\n",
      "[619,    10] loss: 0.00345\n",
      "[619,    20] loss: 0.00438\n",
      "[619,    30] loss: 0.00388\n",
      "[620,    10] loss: 0.00273\n",
      "[620,    20] loss: 0.00287\n",
      "[620,    30] loss: 0.00283\n",
      "[621,    10] loss: 0.00269\n",
      "[621,    20] loss: 0.00350\n",
      "[621,    30] loss: 0.00309\n",
      "[622,    10] loss: 0.00396\n",
      "[622,    20] loss: 0.00426\n",
      "[622,    30] loss: 0.00310\n",
      "[623,    10] loss: 0.00242\n",
      "[623,    20] loss: 0.00290\n",
      "[623,    30] loss: 0.00304\n",
      "[624,    10] loss: 0.00335\n",
      "[624,    20] loss: 0.00398\n",
      "[624,    30] loss: 0.00357\n",
      "[625,    10] loss: 0.00312\n",
      "[625,    20] loss: 0.00372\n",
      "[625,    30] loss: 0.00302\n",
      "[626,    10] loss: 0.00238\n",
      "[626,    20] loss: 0.00297\n",
      "[626,    30] loss: 0.00285\n",
      "[627,    10] loss: 0.00384\n",
      "[627,    20] loss: 0.00556\n",
      "[627,    30] loss: 0.00342\n",
      "[628,    10] loss: 0.00247\n",
      "[628,    20] loss: 0.00279\n",
      "[628,    30] loss: 0.00278\n",
      "[629,    10] loss: 0.00315\n",
      "[629,    20] loss: 0.00346\n",
      "[629,    30] loss: 0.00350\n",
      "[630,    10] loss: 0.00317\n",
      "[630,    20] loss: 0.00389\n",
      "[630,    30] loss: 0.00271\n",
      "[631,    10] loss: 0.00219\n",
      "[631,    20] loss: 0.00275\n",
      "[631,    30] loss: 0.00265\n",
      "[632,    10] loss: 0.00393\n",
      "[632,    20] loss: 0.00480\n",
      "[632,    30] loss: 0.00425\n",
      "[633,    10] loss: 0.00317\n",
      "[633,    20] loss: 0.00462\n",
      "[633,    30] loss: 0.00325\n",
      "[634,    10] loss: 0.00291\n",
      "[634,    20] loss: 0.00322\n",
      "[634,    30] loss: 0.00252\n",
      "[635,    10] loss: 0.00267\n",
      "[635,    20] loss: 0.00312\n",
      "[635,    30] loss: 0.00268\n",
      "[636,    10] loss: 0.00281\n",
      "[636,    20] loss: 0.00346\n",
      "[636,    30] loss: 0.00276\n",
      "[637,    10] loss: 0.00387\n",
      "[637,    20] loss: 0.00455\n",
      "[637,    30] loss: 0.00291\n",
      "[638,    10] loss: 0.00258\n",
      "[638,    20] loss: 0.00361\n",
      "[638,    30] loss: 0.00260\n",
      "[639,    10] loss: 0.00262\n",
      "[639,    20] loss: 0.00314\n",
      "[639,    30] loss: 0.00239\n",
      "[640,    10] loss: 0.00283\n",
      "[640,    20] loss: 0.00325\n",
      "[640,    30] loss: 0.00250\n",
      "[641,    10] loss: 0.00294\n",
      "[641,    20] loss: 0.00344\n",
      "[641,    30] loss: 0.00273\n",
      "[642,    10] loss: 0.00340\n",
      "[642,    20] loss: 0.00361\n",
      "[642,    30] loss: 0.00245\n",
      "[643,    10] loss: 0.00284\n",
      "[643,    20] loss: 0.00318\n",
      "[643,    30] loss: 0.00246\n",
      "[644,    10] loss: 0.00284\n",
      "[644,    20] loss: 0.00351\n",
      "[644,    30] loss: 0.00351\n",
      "[645,    10] loss: 0.00294\n",
      "[645,    20] loss: 0.00376\n",
      "[645,    30] loss: 0.00338\n",
      "[646,    10] loss: 0.00270\n",
      "[646,    20] loss: 0.00315\n",
      "[646,    30] loss: 0.00245\n",
      "[647,    10] loss: 0.00330\n",
      "[647,    20] loss: 0.00429\n",
      "[647,    30] loss: 0.00339\n",
      "[648,    10] loss: 0.00280\n",
      "[648,    20] loss: 0.00317\n",
      "[648,    30] loss: 0.00228\n",
      "[649,    10] loss: 0.00357\n",
      "[649,    20] loss: 0.00468\n",
      "[649,    30] loss: 0.00348\n",
      "[650,    10] loss: 0.00300\n",
      "[650,    20] loss: 0.00389\n",
      "[650,    30] loss: 0.00297\n",
      "[651,    10] loss: 0.00284\n",
      "[651,    20] loss: 0.00357\n",
      "[651,    30] loss: 0.00263\n",
      "[652,    10] loss: 0.00298\n",
      "[652,    20] loss: 0.00341\n",
      "[652,    30] loss: 0.00286\n",
      "[653,    10] loss: 0.00458\n",
      "[653,    20] loss: 0.00663\n",
      "[653,    30] loss: 0.00633\n",
      "[654,    10] loss: 0.00799\n",
      "[654,    20] loss: 0.00708\n",
      "[654,    30] loss: 0.00529\n",
      "[655,    10] loss: 0.00413\n",
      "[655,    20] loss: 0.00332\n",
      "[655,    30] loss: 0.00266\n",
      "[656,    10] loss: 0.00302\n",
      "[656,    20] loss: 0.00401\n",
      "[656,    30] loss: 0.00370\n",
      "[657,    10] loss: 0.00428\n",
      "[657,    20] loss: 0.00390\n",
      "[657,    30] loss: 0.00369\n",
      "[658,    10] loss: 0.00272\n",
      "[658,    20] loss: 0.00334\n",
      "[658,    30] loss: 0.00244\n",
      "[659,    10] loss: 0.00258\n",
      "[659,    20] loss: 0.00362\n",
      "[659,    30] loss: 0.00335\n",
      "[660,    10] loss: 0.00321\n",
      "[660,    20] loss: 0.00393\n",
      "[660,    30] loss: 0.00332\n",
      "[661,    10] loss: 0.00274\n",
      "[661,    20] loss: 0.00329\n",
      "[661,    30] loss: 0.00306\n",
      "[662,    10] loss: 0.00407\n",
      "[662,    20] loss: 0.00435\n",
      "[662,    30] loss: 0.00301\n",
      "[663,    10] loss: 0.00262\n",
      "[663,    20] loss: 0.00363\n",
      "[663,    30] loss: 0.00307\n",
      "[664,    10] loss: 0.00300\n",
      "[664,    20] loss: 0.00373\n",
      "[664,    30] loss: 0.00352\n",
      "[665,    10] loss: 0.00271\n",
      "[665,    20] loss: 0.00357\n",
      "[665,    30] loss: 0.00528\n",
      "[666,    10] loss: 0.00332\n",
      "[666,    20] loss: 0.00333\n",
      "[666,    30] loss: 0.00246\n",
      "[667,    10] loss: 0.00325\n",
      "[667,    20] loss: 0.00401\n",
      "[667,    30] loss: 0.00271\n",
      "[668,    10] loss: 0.00247\n",
      "[668,    20] loss: 0.00300\n",
      "[668,    30] loss: 0.00281\n",
      "[669,    10] loss: 0.00282\n",
      "[669,    20] loss: 0.00350\n",
      "[669,    30] loss: 0.00411\n",
      "[670,    10] loss: 0.00340\n",
      "[670,    20] loss: 0.00334\n",
      "[670,    30] loss: 0.00286\n",
      "[671,    10] loss: 0.00336\n",
      "[671,    20] loss: 0.00384\n",
      "[671,    30] loss: 0.00400\n",
      "[672,    10] loss: 0.00298\n",
      "[672,    20] loss: 0.00332\n",
      "[672,    30] loss: 0.00295\n",
      "[673,    10] loss: 0.00302\n",
      "[673,    20] loss: 0.00311\n",
      "[673,    30] loss: 0.00257\n",
      "[674,    10] loss: 0.00287\n",
      "[674,    20] loss: 0.00335\n",
      "[674,    30] loss: 0.00362\n",
      "[675,    10] loss: 0.00373\n",
      "[675,    20] loss: 0.00381\n",
      "[675,    30] loss: 0.00301\n",
      "[676,    10] loss: 0.00258\n",
      "[676,    20] loss: 0.00314\n",
      "[676,    30] loss: 0.00248\n",
      "[677,    10] loss: 0.00322\n",
      "[677,    20] loss: 0.00358\n",
      "[677,    30] loss: 0.00379\n",
      "[678,    10] loss: 0.00373\n",
      "[678,    20] loss: 0.00441\n",
      "[678,    30] loss: 0.00409\n",
      "[679,    10] loss: 0.00294\n",
      "[679,    20] loss: 0.00313\n",
      "[679,    30] loss: 0.00383\n",
      "[680,    10] loss: 0.00373\n",
      "[680,    20] loss: 0.00344\n",
      "[680,    30] loss: 0.00304\n",
      "[681,    10] loss: 0.00485\n",
      "[681,    20] loss: 0.00496\n",
      "[681,    30] loss: 0.00487\n",
      "[682,    10] loss: 0.00362\n",
      "[682,    20] loss: 0.00453\n",
      "[682,    30] loss: 0.00710\n",
      "[683,    10] loss: 0.00638\n",
      "[683,    20] loss: 0.00498\n",
      "[683,    30] loss: 0.00421\n",
      "[684,    10] loss: 0.00311\n",
      "[684,    20] loss: 0.00390\n",
      "[684,    30] loss: 0.00287\n",
      "[685,    10] loss: 0.00251\n",
      "[685,    20] loss: 0.00365\n",
      "[685,    30] loss: 0.00343\n",
      "[686,    10] loss: 0.00286\n",
      "[686,    20] loss: 0.00402\n",
      "[686,    30] loss: 0.00460\n",
      "[687,    10] loss: 0.00318\n",
      "[687,    20] loss: 0.00359\n",
      "[687,    30] loss: 0.00352\n",
      "[688,    10] loss: 0.00308\n",
      "[688,    20] loss: 0.00418\n",
      "[688,    30] loss: 0.00467\n",
      "[689,    10] loss: 0.00327\n",
      "[689,    20] loss: 0.00379\n",
      "[689,    30] loss: 0.00365\n",
      "[690,    10] loss: 0.00300\n",
      "[690,    20] loss: 0.00339\n",
      "[690,    30] loss: 0.00354\n",
      "[691,    10] loss: 0.00431\n",
      "[691,    20] loss: 0.00613\n",
      "[691,    30] loss: 0.00349\n",
      "[692,    10] loss: 0.00358\n",
      "[692,    20] loss: 0.00413\n",
      "[692,    30] loss: 0.00366\n",
      "[693,    10] loss: 0.00419\n",
      "[693,    20] loss: 0.00332\n",
      "[693,    30] loss: 0.00253\n",
      "[694,    10] loss: 0.00247\n",
      "[694,    20] loss: 0.00309\n",
      "[694,    30] loss: 0.00285\n",
      "[695,    10] loss: 0.00215\n",
      "[695,    20] loss: 0.00336\n",
      "[695,    30] loss: 0.00295\n",
      "[696,    10] loss: 0.00252\n",
      "[696,    20] loss: 0.00365\n",
      "[696,    30] loss: 0.00372\n",
      "[697,    10] loss: 0.00417\n",
      "[697,    20] loss: 0.00353\n",
      "[697,    30] loss: 0.00246\n",
      "[698,    10] loss: 0.00312\n",
      "[698,    20] loss: 0.00350\n",
      "[698,    30] loss: 0.00304\n",
      "[699,    10] loss: 0.00276\n",
      "[699,    20] loss: 0.00332\n",
      "[699,    30] loss: 0.00243\n",
      "[700,    10] loss: 0.00237\n",
      "[700,    20] loss: 0.00283\n",
      "[700,    30] loss: 0.00246\n",
      "[701,    10] loss: 0.00247\n",
      "[701,    20] loss: 0.00354\n",
      "[701,    30] loss: 0.00359\n",
      "[702,    10] loss: 0.00241\n",
      "[702,    20] loss: 0.00311\n",
      "[702,    30] loss: 0.00285\n",
      "[703,    10] loss: 0.00280\n",
      "[703,    20] loss: 0.00370\n",
      "[703,    30] loss: 0.00265\n",
      "[704,    10] loss: 0.00266\n",
      "[704,    20] loss: 0.00381\n",
      "[704,    30] loss: 0.00318\n",
      "[705,    10] loss: 0.00363\n",
      "[705,    20] loss: 0.00424\n",
      "[705,    30] loss: 0.00555\n",
      "[706,    10] loss: 0.00321\n",
      "[706,    20] loss: 0.00393\n",
      "[706,    30] loss: 0.00306\n",
      "[707,    10] loss: 0.00340\n",
      "[707,    20] loss: 0.00439\n",
      "[707,    30] loss: 0.00319\n",
      "[708,    10] loss: 0.00291\n",
      "[708,    20] loss: 0.00310\n",
      "[708,    30] loss: 0.00233\n",
      "[709,    10] loss: 0.00274\n",
      "[709,    20] loss: 0.00334\n",
      "[709,    30] loss: 0.00323\n",
      "[710,    10] loss: 0.00344\n",
      "[710,    20] loss: 0.00438\n",
      "[710,    30] loss: 0.00404\n",
      "[711,    10] loss: 0.00239\n",
      "[711,    20] loss: 0.00302\n",
      "[711,    30] loss: 0.00343\n",
      "[712,    10] loss: 0.00272\n",
      "[712,    20] loss: 0.00363\n",
      "[712,    30] loss: 0.00322\n",
      "[713,    10] loss: 0.00267\n",
      "[713,    20] loss: 0.00359\n",
      "[713,    30] loss: 0.00342\n",
      "[714,    10] loss: 0.00269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[714,    20] loss: 0.00389\n",
      "[714,    30] loss: 0.00394\n",
      "[715,    10] loss: 0.00279\n",
      "[715,    20] loss: 0.00371\n",
      "[715,    30] loss: 0.00252\n",
      "[716,    10] loss: 0.00304\n",
      "[716,    20] loss: 0.00440\n",
      "[716,    30] loss: 0.00435\n",
      "[717,    10] loss: 0.00250\n",
      "[717,    20] loss: 0.00358\n",
      "[717,    30] loss: 0.00285\n",
      "[718,    10] loss: 0.00230\n",
      "[718,    20] loss: 0.00341\n",
      "[718,    30] loss: 0.00273\n",
      "[719,    10] loss: 0.00261\n",
      "[719,    20] loss: 0.00369\n",
      "[719,    30] loss: 0.00342\n",
      "[720,    10] loss: 0.00280\n",
      "[720,    20] loss: 0.00375\n",
      "[720,    30] loss: 0.00330\n",
      "[721,    10] loss: 0.00290\n",
      "[721,    20] loss: 0.00325\n",
      "[721,    30] loss: 0.00231\n",
      "[722,    10] loss: 0.00269\n",
      "[722,    20] loss: 0.00358\n",
      "[722,    30] loss: 0.00336\n",
      "[723,    10] loss: 0.00259\n",
      "[723,    20] loss: 0.00345\n",
      "[723,    30] loss: 0.00279\n",
      "[724,    10] loss: 0.00242\n",
      "[724,    20] loss: 0.00317\n",
      "[724,    30] loss: 0.00318\n",
      "[725,    10] loss: 0.00238\n",
      "[725,    20] loss: 0.00308\n",
      "[725,    30] loss: 0.00218\n",
      "[726,    10] loss: 0.00242\n",
      "[726,    20] loss: 0.00331\n",
      "[726,    30] loss: 0.00303\n",
      "[727,    10] loss: 0.00374\n",
      "[727,    20] loss: 0.00462\n",
      "[727,    30] loss: 0.00471\n",
      "[728,    10] loss: 0.00568\n",
      "[728,    20] loss: 0.00588\n",
      "[728,    30] loss: 0.00659\n",
      "[729,    10] loss: 0.00672\n",
      "[729,    20] loss: 0.00738\n",
      "[729,    30] loss: 0.00512\n",
      "[730,    10] loss: 0.00511\n",
      "[730,    20] loss: 0.00678\n",
      "[730,    30] loss: 0.00502\n",
      "[731,    10] loss: 0.00455\n",
      "[731,    20] loss: 0.00395\n",
      "[731,    30] loss: 0.00330\n",
      "[732,    10] loss: 0.00271\n",
      "[732,    20] loss: 0.00273\n",
      "[732,    30] loss: 0.00237\n",
      "[733,    10] loss: 0.00299\n",
      "[733,    20] loss: 0.00345\n",
      "[733,    30] loss: 0.00293\n",
      "[734,    10] loss: 0.00266\n",
      "[734,    20] loss: 0.00321\n",
      "[734,    30] loss: 0.00276\n",
      "[735,    10] loss: 0.00275\n",
      "[735,    20] loss: 0.00337\n",
      "[735,    30] loss: 0.00327\n",
      "[736,    10] loss: 0.00375\n",
      "[736,    20] loss: 0.00383\n",
      "[736,    30] loss: 0.00326\n",
      "[737,    10] loss: 0.00242\n",
      "[737,    20] loss: 0.00300\n",
      "[737,    30] loss: 0.00270\n",
      "[738,    10] loss: 0.00305\n",
      "[738,    20] loss: 0.00359\n",
      "[738,    30] loss: 0.00274\n",
      "[739,    10] loss: 0.00296\n",
      "[739,    20] loss: 0.00407\n",
      "[739,    30] loss: 0.00509\n",
      "[740,    10] loss: 0.00277\n",
      "[740,    20] loss: 0.00275\n",
      "[740,    30] loss: 0.00225\n",
      "[741,    10] loss: 0.00316\n",
      "[741,    20] loss: 0.00346\n",
      "[741,    30] loss: 0.00322\n",
      "[742,    10] loss: 0.00290\n",
      "[742,    20] loss: 0.00326\n",
      "[742,    30] loss: 0.00270\n",
      "[743,    10] loss: 0.00340\n",
      "[743,    20] loss: 0.00443\n",
      "[743,    30] loss: 0.00295\n",
      "[744,    10] loss: 0.00247\n",
      "[744,    20] loss: 0.00324\n",
      "[744,    30] loss: 0.00271\n",
      "[745,    10] loss: 0.00254\n",
      "[745,    20] loss: 0.00361\n",
      "[745,    30] loss: 0.00238\n",
      "[746,    10] loss: 0.00274\n",
      "[746,    20] loss: 0.00371\n",
      "[746,    30] loss: 0.00404\n",
      "[747,    10] loss: 0.00271\n",
      "[747,    20] loss: 0.00324\n",
      "[747,    30] loss: 0.00272\n",
      "[748,    10] loss: 0.00248\n",
      "[748,    20] loss: 0.00300\n",
      "[748,    30] loss: 0.00227\n",
      "[749,    10] loss: 0.00219\n",
      "[749,    20] loss: 0.00287\n",
      "[749,    30] loss: 0.00308\n",
      "[750,    10] loss: 0.00254\n",
      "[750,    20] loss: 0.00361\n",
      "[750,    30] loss: 0.00304\n",
      "[751,    10] loss: 0.00256\n",
      "[751,    20] loss: 0.00333\n",
      "[751,    30] loss: 0.00310\n",
      "[752,    10] loss: 0.00286\n",
      "[752,    20] loss: 0.00343\n",
      "[752,    30] loss: 0.00412\n",
      "[753,    10] loss: 0.00288\n",
      "[753,    20] loss: 0.00293\n",
      "[753,    30] loss: 0.00314\n",
      "[754,    10] loss: 0.00365\n",
      "[754,    20] loss: 0.00376\n",
      "[754,    30] loss: 0.00285\n",
      "[755,    10] loss: 0.00404\n",
      "[755,    20] loss: 0.00498\n",
      "[755,    30] loss: 0.00437\n",
      "[756,    10] loss: 0.00407\n",
      "[756,    20] loss: 0.00466\n",
      "[756,    30] loss: 0.00605\n",
      "[757,    10] loss: 0.00304\n",
      "[757,    20] loss: 0.00289\n",
      "[757,    30] loss: 0.00331\n",
      "[758,    10] loss: 0.00311\n",
      "[758,    20] loss: 0.00333\n",
      "[758,    30] loss: 0.00391\n",
      "[759,    10] loss: 0.00347\n",
      "[759,    20] loss: 0.00449\n",
      "[759,    30] loss: 0.00663\n",
      "[760,    10] loss: 0.00521\n",
      "[760,    20] loss: 0.00477\n",
      "[760,    30] loss: 0.00396\n",
      "[761,    10] loss: 0.00325\n",
      "[761,    20] loss: 0.00415\n",
      "[761,    30] loss: 0.00318\n",
      "[762,    10] loss: 0.00311\n",
      "[762,    20] loss: 0.00315\n",
      "[762,    30] loss: 0.00268\n",
      "[763,    10] loss: 0.00280\n",
      "[763,    20] loss: 0.00327\n",
      "[763,    30] loss: 0.00233\n",
      "[764,    10] loss: 0.00299\n",
      "[764,    20] loss: 0.00405\n",
      "[764,    30] loss: 0.00335\n",
      "[765,    10] loss: 0.00320\n",
      "[765,    20] loss: 0.00328\n",
      "[765,    30] loss: 0.00298\n",
      "[766,    10] loss: 0.00375\n",
      "[766,    20] loss: 0.00484\n",
      "[766,    30] loss: 0.00494\n",
      "[767,    10] loss: 0.00329\n",
      "[767,    20] loss: 0.00338\n",
      "[767,    30] loss: 0.00293\n",
      "[768,    10] loss: 0.00380\n",
      "[768,    20] loss: 0.00343\n",
      "[768,    30] loss: 0.00352\n",
      "[769,    10] loss: 0.00313\n",
      "[769,    20] loss: 0.00441\n",
      "[769,    30] loss: 0.00483\n",
      "[770,    10] loss: 0.00267\n",
      "[770,    20] loss: 0.00317\n",
      "[770,    30] loss: 0.00263\n",
      "[771,    10] loss: 0.00283\n",
      "[771,    20] loss: 0.00350\n",
      "[771,    30] loss: 0.00322\n",
      "[772,    10] loss: 0.00267\n",
      "[772,    20] loss: 0.00347\n",
      "[772,    30] loss: 0.00324\n",
      "[773,    10] loss: 0.00243\n",
      "[773,    20] loss: 0.00299\n",
      "[773,    30] loss: 0.00269\n",
      "[774,    10] loss: 0.00272\n",
      "[774,    20] loss: 0.00331\n",
      "[774,    30] loss: 0.00312\n",
      "[775,    10] loss: 0.00283\n",
      "[775,    20] loss: 0.00339\n",
      "[775,    30] loss: 0.00299\n",
      "[776,    10] loss: 0.00312\n",
      "[776,    20] loss: 0.00374\n",
      "[776,    30] loss: 0.00341\n",
      "[777,    10] loss: 0.00270\n",
      "[777,    20] loss: 0.00285\n",
      "[777,    30] loss: 0.00284\n",
      "[778,    10] loss: 0.00278\n",
      "[778,    20] loss: 0.00356\n",
      "[778,    30] loss: 0.00348\n",
      "[779,    10] loss: 0.00312\n",
      "[779,    20] loss: 0.00392\n",
      "[779,    30] loss: 0.00389\n",
      "[780,    10] loss: 0.00299\n",
      "[780,    20] loss: 0.00335\n",
      "[780,    30] loss: 0.00341\n",
      "[781,    10] loss: 0.00288\n",
      "[781,    20] loss: 0.00305\n",
      "[781,    30] loss: 0.00221\n",
      "[782,    10] loss: 0.00288\n",
      "[782,    20] loss: 0.00365\n",
      "[782,    30] loss: 0.00284\n",
      "[783,    10] loss: 0.00248\n",
      "[783,    20] loss: 0.00322\n",
      "[783,    30] loss: 0.00344\n",
      "[784,    10] loss: 0.00327\n",
      "[784,    20] loss: 0.00399\n",
      "[784,    30] loss: 0.00402\n",
      "[785,    10] loss: 0.00407\n",
      "[785,    20] loss: 0.00439\n",
      "[785,    30] loss: 0.00438\n",
      "[786,    10] loss: 0.00295\n",
      "[786,    20] loss: 0.00328\n",
      "[786,    30] loss: 0.00253\n",
      "[787,    10] loss: 0.00361\n",
      "[787,    20] loss: 0.00406\n",
      "[787,    30] loss: 0.00498\n",
      "[788,    10] loss: 0.00381\n",
      "[788,    20] loss: 0.00316\n",
      "[788,    30] loss: 0.00289\n",
      "[789,    10] loss: 0.00387\n",
      "[789,    20] loss: 0.00416\n",
      "[789,    30] loss: 0.00406\n",
      "[790,    10] loss: 0.00400\n",
      "[790,    20] loss: 0.00391\n",
      "[790,    30] loss: 0.00403\n",
      "[791,    10] loss: 0.00321\n",
      "[791,    20] loss: 0.00439\n",
      "[791,    30] loss: 0.00503\n",
      "[792,    10] loss: 0.00334\n",
      "[792,    20] loss: 0.00342\n",
      "[792,    30] loss: 0.00292\n",
      "[793,    10] loss: 0.00257\n",
      "[793,    20] loss: 0.00363\n",
      "[793,    30] loss: 0.00369\n",
      "[794,    10] loss: 0.00278\n",
      "[794,    20] loss: 0.00353\n",
      "[794,    30] loss: 0.00364\n",
      "[795,    10] loss: 0.00247\n",
      "[795,    20] loss: 0.00300\n",
      "[795,    30] loss: 0.00268\n",
      "[796,    10] loss: 0.00255\n",
      "[796,    20] loss: 0.00343\n",
      "[796,    30] loss: 0.00318\n",
      "[797,    10] loss: 0.00281\n",
      "[797,    20] loss: 0.00338\n",
      "[797,    30] loss: 0.00264\n",
      "[798,    10] loss: 0.00255\n",
      "[798,    20] loss: 0.00351\n",
      "[798,    30] loss: 0.00331\n",
      "[799,    10] loss: 0.00284\n",
      "[799,    20] loss: 0.00344\n",
      "[799,    30] loss: 0.00309\n",
      "[800,    10] loss: 0.00257\n",
      "[800,    20] loss: 0.00360\n",
      "[800,    30] loss: 0.00395\n",
      "[801,    10] loss: 0.00287\n",
      "[801,    20] loss: 0.00369\n",
      "[801,    30] loss: 0.00250\n",
      "[802,    10] loss: 0.00285\n",
      "[802,    20] loss: 0.00333\n",
      "[802,    30] loss: 0.00271\n",
      "[803,    10] loss: 0.00306\n",
      "[803,    20] loss: 0.00409\n",
      "[803,    30] loss: 0.00444\n",
      "[804,    10] loss: 0.00367\n",
      "[804,    20] loss: 0.00436\n",
      "[804,    30] loss: 0.00374\n",
      "[805,    10] loss: 0.00258\n",
      "[805,    20] loss: 0.00329\n",
      "[805,    30] loss: 0.00288\n",
      "[806,    10] loss: 0.00327\n",
      "[806,    20] loss: 0.00360\n",
      "[806,    30] loss: 0.00409\n",
      "[807,    10] loss: 0.00345\n",
      "[807,    20] loss: 0.00372\n",
      "[807,    30] loss: 0.00354\n",
      "[808,    10] loss: 0.00248\n",
      "[808,    20] loss: 0.00284\n",
      "[808,    30] loss: 0.00295\n",
      "[809,    10] loss: 0.00346\n",
      "[809,    20] loss: 0.00323\n",
      "[809,    30] loss: 0.00235\n",
      "[810,    10] loss: 0.00246\n",
      "[810,    20] loss: 0.00287\n",
      "[810,    30] loss: 0.00328\n",
      "[811,    10] loss: 0.00397\n",
      "[811,    20] loss: 0.00363\n",
      "[811,    30] loss: 0.00284\n",
      "[812,    10] loss: 0.00277\n",
      "[812,    20] loss: 0.00298\n",
      "[812,    30] loss: 0.00264\n",
      "[813,    10] loss: 0.00270\n",
      "[813,    20] loss: 0.00302\n",
      "[813,    30] loss: 0.00343\n",
      "[814,    10] loss: 0.00292\n",
      "[814,    20] loss: 0.00450\n",
      "[814,    30] loss: 0.00328\n",
      "[815,    10] loss: 0.00286\n",
      "[815,    20] loss: 0.00267\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[815,    30] loss: 0.00207\n",
      "[816,    10] loss: 0.00247\n",
      "[816,    20] loss: 0.00272\n",
      "[816,    30] loss: 0.00227\n",
      "[817,    10] loss: 0.00266\n",
      "[817,    20] loss: 0.00284\n",
      "[817,    30] loss: 0.00251\n",
      "[818,    10] loss: 0.00331\n",
      "[818,    20] loss: 0.00398\n",
      "[818,    30] loss: 0.00345\n",
      "[819,    10] loss: 0.00258\n",
      "[819,    20] loss: 0.00323\n",
      "[819,    30] loss: 0.00285\n",
      "[820,    10] loss: 0.00343\n",
      "[820,    20] loss: 0.00391\n",
      "[820,    30] loss: 0.00249\n",
      "[821,    10] loss: 0.00234\n",
      "[821,    20] loss: 0.00311\n",
      "[821,    30] loss: 0.00252\n",
      "[822,    10] loss: 0.00367\n",
      "[822,    20] loss: 0.00445\n",
      "[822,    30] loss: 0.00279\n",
      "[823,    10] loss: 0.00266\n",
      "[823,    20] loss: 0.00383\n",
      "[823,    30] loss: 0.00256\n",
      "[824,    10] loss: 0.00319\n",
      "[824,    20] loss: 0.00304\n",
      "[824,    30] loss: 0.00265\n",
      "[825,    10] loss: 0.00309\n",
      "[825,    20] loss: 0.00352\n",
      "[825,    30] loss: 0.00266\n",
      "[826,    10] loss: 0.00333\n",
      "[826,    20] loss: 0.00343\n",
      "[826,    30] loss: 0.00360\n",
      "[827,    10] loss: 0.00352\n",
      "[827,    20] loss: 0.00412\n",
      "[827,    30] loss: 0.00424\n",
      "[828,    10] loss: 0.00379\n",
      "[828,    20] loss: 0.00421\n",
      "[828,    30] loss: 0.00303\n",
      "[829,    10] loss: 0.00283\n",
      "[829,    20] loss: 0.00405\n",
      "[829,    30] loss: 0.00334\n",
      "[830,    10] loss: 0.00248\n",
      "[830,    20] loss: 0.00291\n",
      "[830,    30] loss: 0.00210\n",
      "[831,    10] loss: 0.00246\n",
      "[831,    20] loss: 0.00296\n",
      "[831,    30] loss: 0.00250\n",
      "[832,    10] loss: 0.00308\n",
      "[832,    20] loss: 0.00372\n",
      "[832,    30] loss: 0.00397\n",
      "[833,    10] loss: 0.00368\n",
      "[833,    20] loss: 0.00376\n",
      "[833,    30] loss: 0.00301\n",
      "[834,    10] loss: 0.00257\n",
      "[834,    20] loss: 0.00407\n",
      "[834,    30] loss: 0.00445\n",
      "[835,    10] loss: 0.00519\n",
      "[835,    20] loss: 0.00411\n",
      "[835,    30] loss: 0.00351\n",
      "[836,    10] loss: 0.00332\n",
      "[836,    20] loss: 0.00384\n",
      "[836,    30] loss: 0.00420\n",
      "[837,    10] loss: 0.00395\n",
      "[837,    20] loss: 0.00358\n",
      "[837,    30] loss: 0.00304\n",
      "[838,    10] loss: 0.00311\n",
      "[838,    20] loss: 0.00426\n",
      "[838,    30] loss: 0.00395\n",
      "[839,    10] loss: 0.00268\n",
      "[839,    20] loss: 0.00358\n",
      "[839,    30] loss: 0.00222\n",
      "[840,    10] loss: 0.00223\n",
      "[840,    20] loss: 0.00286\n",
      "[840,    30] loss: 0.00245\n",
      "[841,    10] loss: 0.00229\n",
      "[841,    20] loss: 0.00305\n",
      "[841,    30] loss: 0.00247\n",
      "[842,    10] loss: 0.00245\n",
      "[842,    20] loss: 0.00380\n",
      "[842,    30] loss: 0.00341\n",
      "[843,    10] loss: 0.00271\n",
      "[843,    20] loss: 0.00332\n",
      "[843,    30] loss: 0.00251\n",
      "[844,    10] loss: 0.00262\n",
      "[844,    20] loss: 0.00323\n",
      "[844,    30] loss: 0.00286\n",
      "[845,    10] loss: 0.00280\n",
      "[845,    20] loss: 0.00371\n",
      "[845,    30] loss: 0.00281\n",
      "[846,    10] loss: 0.00244\n",
      "[846,    20] loss: 0.00329\n",
      "[846,    30] loss: 0.00281\n",
      "[847,    10] loss: 0.00280\n",
      "[847,    20] loss: 0.00352\n",
      "[847,    30] loss: 0.00238\n",
      "[848,    10] loss: 0.00245\n",
      "[848,    20] loss: 0.00281\n",
      "[848,    30] loss: 0.00203\n",
      "[849,    10] loss: 0.00244\n",
      "[849,    20] loss: 0.00298\n",
      "[849,    30] loss: 0.00188\n",
      "[850,    10] loss: 0.00260\n",
      "[850,    20] loss: 0.00329\n",
      "[850,    30] loss: 0.00323\n",
      "[851,    10] loss: 0.00371\n",
      "[851,    20] loss: 0.00423\n",
      "[851,    30] loss: 0.00292\n",
      "[852,    10] loss: 0.00259\n",
      "[852,    20] loss: 0.00321\n",
      "[852,    30] loss: 0.00239\n",
      "[853,    10] loss: 0.00309\n",
      "[853,    20] loss: 0.00354\n",
      "[853,    30] loss: 0.00304\n",
      "[854,    10] loss: 0.00223\n",
      "[854,    20] loss: 0.00361\n",
      "[854,    30] loss: 0.00259\n",
      "[855,    10] loss: 0.00555\n",
      "[855,    20] loss: 0.00437\n",
      "[855,    30] loss: 0.00311\n",
      "[856,    10] loss: 0.00251\n",
      "[856,    20] loss: 0.00319\n",
      "[856,    30] loss: 0.00226\n",
      "[857,    10] loss: 0.00358\n",
      "[857,    20] loss: 0.00395\n",
      "[857,    30] loss: 0.00412\n",
      "[858,    10] loss: 0.00303\n",
      "[858,    20] loss: 0.00402\n",
      "[858,    30] loss: 0.00324\n",
      "[859,    10] loss: 0.00393\n",
      "[859,    20] loss: 0.00407\n",
      "[859,    30] loss: 0.00419\n",
      "[860,    10] loss: 0.00278\n",
      "[860,    20] loss: 0.00333\n",
      "[860,    30] loss: 0.00302\n",
      "[861,    10] loss: 0.00604\n",
      "[861,    20] loss: 0.00391\n",
      "[861,    30] loss: 0.00423\n",
      "[862,    10] loss: 0.00224\n",
      "[862,    20] loss: 0.00285\n",
      "[862,    30] loss: 0.00398\n",
      "[863,    10] loss: 0.00278\n",
      "[863,    20] loss: 0.00277\n",
      "[863,    30] loss: 0.00265\n",
      "[864,    10] loss: 0.00222\n",
      "[864,    20] loss: 0.00306\n",
      "[864,    30] loss: 0.00367\n",
      "[865,    10] loss: 0.00249\n",
      "[865,    20] loss: 0.00350\n",
      "[865,    30] loss: 0.00337\n",
      "[866,    10] loss: 0.00464\n",
      "[866,    20] loss: 0.00431\n",
      "[866,    30] loss: 0.00325\n",
      "[867,    10] loss: 0.00228\n",
      "[867,    20] loss: 0.00323\n",
      "[867,    30] loss: 0.00388\n",
      "[868,    10] loss: 0.00267\n",
      "[868,    20] loss: 0.00303\n",
      "[868,    30] loss: 0.00295\n",
      "[869,    10] loss: 0.00250\n",
      "[869,    20] loss: 0.00283\n",
      "[869,    30] loss: 0.00285\n",
      "[870,    10] loss: 0.00267\n",
      "[870,    20] loss: 0.00388\n",
      "[870,    30] loss: 0.00391\n",
      "[871,    10] loss: 0.00271\n",
      "[871,    20] loss: 0.00251\n",
      "[871,    30] loss: 0.00249\n",
      "[872,    10] loss: 0.00241\n",
      "[872,    20] loss: 0.00383\n",
      "[872,    30] loss: 0.00384\n",
      "[873,    10] loss: 0.00218\n",
      "[873,    20] loss: 0.00243\n",
      "[873,    30] loss: 0.00254\n",
      "[874,    10] loss: 0.00211\n",
      "[874,    20] loss: 0.00275\n",
      "[874,    30] loss: 0.00278\n",
      "[875,    10] loss: 0.00207\n",
      "[875,    20] loss: 0.00251\n",
      "[875,    30] loss: 0.00230\n",
      "[876,    10] loss: 0.00258\n",
      "[876,    20] loss: 0.00347\n",
      "[876,    30] loss: 0.00368\n",
      "[877,    10] loss: 0.00185\n",
      "[877,    20] loss: 0.00299\n",
      "[877,    30] loss: 0.00256\n",
      "[878,    10] loss: 0.00268\n",
      "[878,    20] loss: 0.00306\n",
      "[878,    30] loss: 0.00349\n",
      "[879,    10] loss: 0.00238\n",
      "[879,    20] loss: 0.00336\n",
      "[879,    30] loss: 0.00473\n",
      "[880,    10] loss: 0.00363\n",
      "[880,    20] loss: 0.00295\n",
      "[880,    30] loss: 0.00265\n",
      "[881,    10] loss: 0.00214\n",
      "[881,    20] loss: 0.00285\n",
      "[881,    30] loss: 0.00372\n",
      "[882,    10] loss: 0.00267\n",
      "[882,    20] loss: 0.00309\n",
      "[882,    30] loss: 0.00378\n",
      "[883,    10] loss: 0.00224\n",
      "[883,    20] loss: 0.00380\n",
      "[883,    30] loss: 0.00492\n",
      "[884,    10] loss: 0.00409\n",
      "[884,    20] loss: 0.00352\n",
      "[884,    30] loss: 0.00312\n",
      "[885,    10] loss: 0.00262\n",
      "[885,    20] loss: 0.00335\n",
      "[885,    30] loss: 0.00357\n",
      "[886,    10] loss: 0.00311\n",
      "[886,    20] loss: 0.00409\n",
      "[886,    30] loss: 0.00333\n",
      "[887,    10] loss: 0.00224\n",
      "[887,    20] loss: 0.00268\n",
      "[887,    30] loss: 0.00258\n",
      "[888,    10] loss: 0.00215\n",
      "[888,    20] loss: 0.00300\n",
      "[888,    30] loss: 0.00274\n",
      "[889,    10] loss: 0.00209\n",
      "[889,    20] loss: 0.00231\n",
      "[889,    30] loss: 0.00228\n",
      "[890,    10] loss: 0.00236\n",
      "[890,    20] loss: 0.00299\n",
      "[890,    30] loss: 0.00325\n",
      "[891,    10] loss: 0.00201\n",
      "[891,    20] loss: 0.00339\n",
      "[891,    30] loss: 0.00419\n",
      "[892,    10] loss: 0.00327\n",
      "[892,    20] loss: 0.00374\n",
      "[892,    30] loss: 0.00337\n",
      "[893,    10] loss: 0.00251\n",
      "[893,    20] loss: 0.00282\n",
      "[893,    30] loss: 0.00327\n",
      "[894,    10] loss: 0.00226\n",
      "[894,    20] loss: 0.00307\n",
      "[894,    30] loss: 0.00264\n",
      "[895,    10] loss: 0.00213\n",
      "[895,    20] loss: 0.00298\n",
      "[895,    30] loss: 0.00284\n",
      "[896,    10] loss: 0.00203\n",
      "[896,    20] loss: 0.00277\n",
      "[896,    30] loss: 0.00277\n",
      "[897,    10] loss: 0.00234\n",
      "[897,    20] loss: 0.00288\n",
      "[897,    30] loss: 0.00324\n",
      "[898,    10] loss: 0.00231\n",
      "[898,    20] loss: 0.00265\n",
      "[898,    30] loss: 0.00306\n",
      "[899,    10] loss: 0.00209\n",
      "[899,    20] loss: 0.00292\n",
      "[899,    30] loss: 0.00337\n",
      "[900,    10] loss: 0.00198\n",
      "[900,    20] loss: 0.00265\n",
      "[900,    30] loss: 0.00325\n",
      "[901,    10] loss: 0.00242\n",
      "[901,    20] loss: 0.00314\n",
      "[901,    30] loss: 0.00418\n",
      "[902,    10] loss: 0.00252\n",
      "[902,    20] loss: 0.00261\n",
      "[902,    30] loss: 0.00273\n",
      "[903,    10] loss: 0.00259\n",
      "[903,    20] loss: 0.00295\n",
      "[903,    30] loss: 0.00236\n",
      "[904,    10] loss: 0.00184\n",
      "[904,    20] loss: 0.00264\n",
      "[904,    30] loss: 0.00244\n",
      "[905,    10] loss: 0.00237\n",
      "[905,    20] loss: 0.00301\n",
      "[905,    30] loss: 0.00315\n",
      "[906,    10] loss: 0.00210\n",
      "[906,    20] loss: 0.00244\n",
      "[906,    30] loss: 0.00255\n",
      "[907,    10] loss: 0.00240\n",
      "[907,    20] loss: 0.00301\n",
      "[907,    30] loss: 0.00285\n",
      "[908,    10] loss: 0.00195\n",
      "[908,    20] loss: 0.00261\n",
      "[908,    30] loss: 0.00240\n",
      "[909,    10] loss: 0.00246\n",
      "[909,    20] loss: 0.00277\n",
      "[909,    30] loss: 0.00238\n",
      "[910,    10] loss: 0.00222\n",
      "[910,    20] loss: 0.00245\n",
      "[910,    30] loss: 0.00238\n",
      "[911,    10] loss: 0.00234\n",
      "[911,    20] loss: 0.00268\n",
      "[911,    30] loss: 0.00259\n",
      "[912,    10] loss: 0.00195\n",
      "[912,    20] loss: 0.00247\n",
      "[912,    30] loss: 0.00221\n",
      "[913,    10] loss: 0.00253\n",
      "[913,    20] loss: 0.00382\n",
      "[913,    30] loss: 0.00293\n",
      "[914,    10] loss: 0.00276\n",
      "[914,    20] loss: 0.00269\n",
      "[914,    30] loss: 0.00255\n",
      "[915,    10] loss: 0.00185\n",
      "[915,    20] loss: 0.00264\n",
      "[915,    30] loss: 0.00261\n",
      "[916,    10] loss: 0.00276\n",
      "[916,    20] loss: 0.00340\n",
      "[916,    30] loss: 0.00250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[917,    10] loss: 0.00304\n",
      "[917,    20] loss: 0.00324\n",
      "[917,    30] loss: 0.00247\n",
      "[918,    10] loss: 0.00232\n",
      "[918,    20] loss: 0.00384\n",
      "[918,    30] loss: 0.00300\n",
      "[919,    10] loss: 0.00213\n",
      "[919,    20] loss: 0.00389\n",
      "[919,    30] loss: 0.00309\n",
      "[920,    10] loss: 0.00331\n",
      "[920,    20] loss: 0.00369\n",
      "[920,    30] loss: 0.00431\n",
      "[921,    10] loss: 0.00230\n",
      "[921,    20] loss: 0.00314\n",
      "[921,    30] loss: 0.00275\n",
      "[922,    10] loss: 0.00247\n",
      "[922,    20] loss: 0.00363\n",
      "[922,    30] loss: 0.00586\n",
      "[923,    10] loss: 0.00542\n",
      "[923,    20] loss: 0.00448\n",
      "[923,    30] loss: 0.00272\n",
      "[924,    10] loss: 0.00259\n",
      "[924,    20] loss: 0.00417\n",
      "[924,    30] loss: 0.00370\n",
      "[925,    10] loss: 0.00417\n",
      "[925,    20] loss: 0.00340\n",
      "[925,    30] loss: 0.00284\n",
      "[926,    10] loss: 0.00298\n",
      "[926,    20] loss: 0.00383\n",
      "[926,    30] loss: 0.00356\n",
      "[927,    10] loss: 0.00236\n",
      "[927,    20] loss: 0.00326\n",
      "[927,    30] loss: 0.00315\n",
      "[928,    10] loss: 0.00277\n",
      "[928,    20] loss: 0.00251\n",
      "[928,    30] loss: 0.00213\n",
      "[929,    10] loss: 0.00195\n",
      "[929,    20] loss: 0.00314\n",
      "[929,    30] loss: 0.00273\n",
      "[930,    10] loss: 0.00333\n",
      "[930,    20] loss: 0.00389\n",
      "[930,    30] loss: 0.00267\n",
      "[931,    10] loss: 0.00243\n",
      "[931,    20] loss: 0.00348\n",
      "[931,    30] loss: 0.00345\n",
      "[932,    10] loss: 0.00323\n",
      "[932,    20] loss: 0.00274\n",
      "[932,    30] loss: 0.00217\n",
      "[933,    10] loss: 0.00264\n",
      "[933,    20] loss: 0.00359\n",
      "[933,    30] loss: 0.00383\n",
      "[934,    10] loss: 0.00254\n",
      "[934,    20] loss: 0.00299\n",
      "[934,    30] loss: 0.00241\n",
      "[935,    10] loss: 0.00296\n",
      "[935,    20] loss: 0.00474\n",
      "[935,    30] loss: 0.00441\n",
      "[936,    10] loss: 0.00306\n",
      "[936,    20] loss: 0.00337\n",
      "[936,    30] loss: 0.00250\n",
      "[937,    10] loss: 0.00247\n",
      "[937,    20] loss: 0.00350\n",
      "[937,    30] loss: 0.00283\n",
      "[938,    10] loss: 0.00240\n",
      "[938,    20] loss: 0.00302\n",
      "[938,    30] loss: 0.00244\n",
      "[939,    10] loss: 0.00257\n",
      "[939,    20] loss: 0.00379\n",
      "[939,    30] loss: 0.00310\n",
      "[940,    10] loss: 0.00251\n",
      "[940,    20] loss: 0.00360\n",
      "[940,    30] loss: 0.00215\n",
      "[941,    10] loss: 0.00237\n",
      "[941,    20] loss: 0.00352\n",
      "[941,    30] loss: 0.00315\n",
      "[942,    10] loss: 0.00367\n",
      "[942,    20] loss: 0.00499\n",
      "[942,    30] loss: 0.00373\n",
      "[943,    10] loss: 0.00393\n",
      "[943,    20] loss: 0.00486\n",
      "[943,    30] loss: 0.00368\n",
      "[944,    10] loss: 0.00303\n",
      "[944,    20] loss: 0.00334\n",
      "[944,    30] loss: 0.00220\n",
      "[945,    10] loss: 0.00209\n",
      "[945,    20] loss: 0.00353\n",
      "[945,    30] loss: 0.00265\n",
      "[946,    10] loss: 0.00250\n",
      "[946,    20] loss: 0.00278\n",
      "[946,    30] loss: 0.00224\n",
      "[947,    10] loss: 0.00334\n",
      "[947,    20] loss: 0.00411\n",
      "[947,    30] loss: 0.00333\n",
      "[948,    10] loss: 0.00318\n",
      "[948,    20] loss: 0.00364\n",
      "[948,    30] loss: 0.00185\n",
      "[949,    10] loss: 0.00213\n",
      "[949,    20] loss: 0.00317\n",
      "[949,    30] loss: 0.00217\n",
      "[950,    10] loss: 0.00221\n",
      "[950,    20] loss: 0.00293\n",
      "[950,    30] loss: 0.00190\n",
      "[951,    10] loss: 0.00252\n",
      "[951,    20] loss: 0.00360\n",
      "[951,    30] loss: 0.00290\n",
      "[952,    10] loss: 0.00255\n",
      "[952,    20] loss: 0.00303\n",
      "[952,    30] loss: 0.00249\n",
      "[953,    10] loss: 0.00232\n",
      "[953,    20] loss: 0.00382\n",
      "[953,    30] loss: 0.00262\n",
      "[954,    10] loss: 0.00240\n",
      "[954,    20] loss: 0.00302\n",
      "[954,    30] loss: 0.00245\n",
      "[955,    10] loss: 0.00245\n",
      "[955,    20] loss: 0.00364\n",
      "[955,    30] loss: 0.00242\n",
      "[956,    10] loss: 0.00227\n",
      "[956,    20] loss: 0.00326\n",
      "[956,    30] loss: 0.00242\n",
      "[957,    10] loss: 0.00245\n",
      "[957,    20] loss: 0.00354\n",
      "[957,    30] loss: 0.00224\n",
      "[958,    10] loss: 0.00235\n",
      "[958,    20] loss: 0.00328\n",
      "[958,    30] loss: 0.00232\n",
      "[959,    10] loss: 0.00217\n",
      "[959,    20] loss: 0.00289\n",
      "[959,    30] loss: 0.00214\n",
      "[960,    10] loss: 0.00287\n",
      "[960,    20] loss: 0.00362\n",
      "[960,    30] loss: 0.00286\n",
      "[961,    10] loss: 0.00211\n",
      "[961,    20] loss: 0.00381\n",
      "[961,    30] loss: 0.00215\n",
      "[962,    10] loss: 0.00227\n",
      "[962,    20] loss: 0.00307\n",
      "[962,    30] loss: 0.00223\n",
      "[963,    10] loss: 0.00236\n",
      "[963,    20] loss: 0.00364\n",
      "[963,    30] loss: 0.00262\n",
      "[964,    10] loss: 0.00239\n",
      "[964,    20] loss: 0.00301\n",
      "[964,    30] loss: 0.00215\n",
      "[965,    10] loss: 0.00224\n",
      "[965,    20] loss: 0.00372\n",
      "[965,    30] loss: 0.00227\n",
      "[966,    10] loss: 0.00241\n",
      "[966,    20] loss: 0.00311\n",
      "[966,    30] loss: 0.00248\n",
      "[967,    10] loss: 0.00214\n",
      "[967,    20] loss: 0.00329\n",
      "[967,    30] loss: 0.00231\n",
      "[968,    10] loss: 0.00293\n",
      "[968,    20] loss: 0.00337\n",
      "[968,    30] loss: 0.00212\n",
      "[969,    10] loss: 0.00234\n",
      "[969,    20] loss: 0.00326\n",
      "[969,    30] loss: 0.00271\n",
      "[970,    10] loss: 0.00262\n",
      "[970,    20] loss: 0.00345\n",
      "[970,    30] loss: 0.00244\n",
      "[971,    10] loss: 0.00236\n",
      "[971,    20] loss: 0.00297\n",
      "[971,    30] loss: 0.00210\n",
      "[972,    10] loss: 0.00259\n",
      "[972,    20] loss: 0.00342\n",
      "[972,    30] loss: 0.00279\n",
      "[973,    10] loss: 0.00236\n",
      "[973,    20] loss: 0.00321\n",
      "[973,    30] loss: 0.00215\n",
      "[974,    10] loss: 0.00251\n",
      "[974,    20] loss: 0.00331\n",
      "[974,    30] loss: 0.00267\n",
      "[975,    10] loss: 0.00217\n",
      "[975,    20] loss: 0.00347\n",
      "[975,    30] loss: 0.00288\n",
      "[976,    10] loss: 0.00279\n",
      "[976,    20] loss: 0.00320\n",
      "[976,    30] loss: 0.00214\n",
      "[977,    10] loss: 0.00223\n",
      "[977,    20] loss: 0.00284\n",
      "[977,    30] loss: 0.00216\n",
      "[978,    10] loss: 0.00276\n",
      "[978,    20] loss: 0.00367\n",
      "[978,    30] loss: 0.00239\n",
      "[979,    10] loss: 0.00299\n",
      "[979,    20] loss: 0.00369\n",
      "[979,    30] loss: 0.00313\n",
      "[980,    10] loss: 0.00216\n",
      "[980,    20] loss: 0.00313\n",
      "[980,    30] loss: 0.00221\n",
      "[981,    10] loss: 0.00271\n",
      "[981,    20] loss: 0.00352\n",
      "[981,    30] loss: 0.00310\n",
      "[982,    10] loss: 0.00234\n",
      "[982,    20] loss: 0.00274\n",
      "[982,    30] loss: 0.00233\n",
      "[983,    10] loss: 0.00263\n",
      "[983,    20] loss: 0.00358\n",
      "[983,    30] loss: 0.00279\n",
      "[984,    10] loss: 0.00242\n",
      "[984,    20] loss: 0.00251\n",
      "[984,    30] loss: 0.00229\n",
      "[985,    10] loss: 0.00265\n",
      "[985,    20] loss: 0.00372\n",
      "[985,    30] loss: 0.00298\n",
      "[986,    10] loss: 0.00223\n",
      "[986,    20] loss: 0.00330\n",
      "[986,    30] loss: 0.00228\n",
      "[987,    10] loss: 0.00301\n",
      "[987,    20] loss: 0.00366\n",
      "[987,    30] loss: 0.00242\n",
      "[988,    10] loss: 0.00285\n",
      "[988,    20] loss: 0.00289\n",
      "[988,    30] loss: 0.00230\n",
      "[989,    10] loss: 0.00249\n",
      "[989,    20] loss: 0.00367\n",
      "[989,    30] loss: 0.00274\n",
      "[990,    10] loss: 0.00240\n",
      "[990,    20] loss: 0.00276\n",
      "[990,    30] loss: 0.00204\n",
      "[991,    10] loss: 0.00241\n",
      "[991,    20] loss: 0.00355\n",
      "[991,    30] loss: 0.00263\n",
      "[992,    10] loss: 0.00227\n",
      "[992,    20] loss: 0.00315\n",
      "[992,    30] loss: 0.00234\n",
      "[993,    10] loss: 0.00236\n",
      "[993,    20] loss: 0.00345\n",
      "[993,    30] loss: 0.00236\n",
      "[994,    10] loss: 0.00245\n",
      "[994,    20] loss: 0.00282\n",
      "[994,    30] loss: 0.00224\n",
      "[995,    10] loss: 0.00267\n",
      "[995,    20] loss: 0.00369\n",
      "[995,    30] loss: 0.00268\n",
      "[996,    10] loss: 0.00283\n",
      "[996,    20] loss: 0.00312\n",
      "[996,    30] loss: 0.00236\n",
      "[997,    10] loss: 0.00272\n",
      "[997,    20] loss: 0.00351\n",
      "[997,    30] loss: 0.00234\n",
      "[998,    10] loss: 0.00246\n",
      "[998,    20] loss: 0.00344\n",
      "[998,    30] loss: 0.00275\n",
      "[999,    10] loss: 0.00273\n",
      "[999,    20] loss: 0.00358\n",
      "[999,    30] loss: 0.00222\n",
      "[1000,    10] loss: 0.00249\n",
      "[1000,    20] loss: 0.00328\n",
      "[1000,    30] loss: 0.00250\n",
      "[1001,    10] loss: 0.00246\n",
      "[1001,    20] loss: 0.00328\n",
      "[1001,    30] loss: 0.00212\n",
      "[1002,    10] loss: 0.00233\n",
      "[1002,    20] loss: 0.00319\n",
      "[1002,    30] loss: 0.00267\n",
      "[1003,    10] loss: 0.00262\n",
      "[1003,    20] loss: 0.00336\n",
      "[1003,    30] loss: 0.00218\n",
      "[1004,    10] loss: 0.00280\n",
      "[1004,    20] loss: 0.00363\n",
      "[1004,    30] loss: 0.00253\n",
      "[1005,    10] loss: 0.00252\n",
      "[1005,    20] loss: 0.00317\n",
      "[1005,    30] loss: 0.00235\n",
      "[1006,    10] loss: 0.00239\n",
      "[1006,    20] loss: 0.00315\n",
      "[1006,    30] loss: 0.00201\n",
      "[1007,    10] loss: 0.00250\n",
      "[1007,    20] loss: 0.00291\n",
      "[1007,    30] loss: 0.00245\n",
      "[1008,    10] loss: 0.00230\n",
      "[1008,    20] loss: 0.00359\n",
      "[1008,    30] loss: 0.00287\n",
      "[1009,    10] loss: 0.00267\n",
      "[1009,    20] loss: 0.00335\n",
      "[1009,    30] loss: 0.00227\n",
      "[1010,    10] loss: 0.00245\n",
      "[1010,    20] loss: 0.00332\n",
      "[1010,    30] loss: 0.00296\n",
      "[1011,    10] loss: 0.00229\n",
      "[1011,    20] loss: 0.00318\n",
      "[1011,    30] loss: 0.00229\n",
      "[1012,    10] loss: 0.00210\n",
      "[1012,    20] loss: 0.00298\n",
      "[1012,    30] loss: 0.00210\n",
      "[1013,    10] loss: 0.00273\n",
      "[1013,    20] loss: 0.00322\n",
      "[1013,    30] loss: 0.00245\n",
      "[1014,    10] loss: 0.00211\n",
      "[1014,    20] loss: 0.00295\n",
      "[1014,    30] loss: 0.00205\n",
      "[1015,    10] loss: 0.00250\n",
      "[1015,    20] loss: 0.00349\n",
      "[1015,    30] loss: 0.00253\n",
      "[1016,    10] loss: 0.00242\n",
      "[1016,    20] loss: 0.00292\n",
      "[1016,    30] loss: 0.00235\n",
      "[1017,    10] loss: 0.00244\n",
      "[1017,    20] loss: 0.00334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1017,    30] loss: 0.00241\n",
      "[1018,    10] loss: 0.00225\n",
      "[1018,    20] loss: 0.00303\n",
      "[1018,    30] loss: 0.00216\n",
      "[1019,    10] loss: 0.00225\n",
      "[1019,    20] loss: 0.00323\n",
      "[1019,    30] loss: 0.00229\n",
      "[1020,    10] loss: 0.00220\n",
      "[1020,    20] loss: 0.00330\n",
      "[1020,    30] loss: 0.00223\n",
      "[1021,    10] loss: 0.00311\n",
      "[1021,    20] loss: 0.00337\n",
      "[1021,    30] loss: 0.00255\n",
      "[1022,    10] loss: 0.00257\n",
      "[1022,    20] loss: 0.00385\n",
      "[1022,    30] loss: 0.00350\n",
      "[1023,    10] loss: 0.00254\n",
      "[1023,    20] loss: 0.00286\n",
      "[1023,    30] loss: 0.00233\n",
      "[1024,    10] loss: 0.00232\n",
      "[1024,    20] loss: 0.00292\n",
      "[1024,    30] loss: 0.00180\n",
      "[1025,    10] loss: 0.00242\n",
      "[1025,    20] loss: 0.00323\n",
      "[1025,    30] loss: 0.00265\n",
      "[1026,    10] loss: 0.00220\n",
      "[1026,    20] loss: 0.00300\n",
      "[1026,    30] loss: 0.00226\n",
      "[1027,    10] loss: 0.00287\n",
      "[1027,    20] loss: 0.00350\n",
      "[1027,    30] loss: 0.00271\n",
      "[1028,    10] loss: 0.00207\n",
      "[1028,    20] loss: 0.00302\n",
      "[1028,    30] loss: 0.00229\n",
      "[1029,    10] loss: 0.00265\n",
      "[1029,    20] loss: 0.00336\n",
      "[1029,    30] loss: 0.00269\n",
      "[1030,    10] loss: 0.00228\n",
      "[1030,    20] loss: 0.00328\n",
      "[1030,    30] loss: 0.00265\n",
      "[1031,    10] loss: 0.00249\n",
      "[1031,    20] loss: 0.00324\n",
      "[1031,    30] loss: 0.00311\n",
      "[1032,    10] loss: 0.00234\n",
      "[1032,    20] loss: 0.00284\n",
      "[1032,    30] loss: 0.00195\n",
      "[1033,    10] loss: 0.00283\n",
      "[1033,    20] loss: 0.00381\n",
      "[1033,    30] loss: 0.00305\n",
      "[1034,    10] loss: 0.00275\n",
      "[1034,    20] loss: 0.00270\n",
      "[1034,    30] loss: 0.00212\n",
      "[1035,    10] loss: 0.00263\n",
      "[1035,    20] loss: 0.00334\n",
      "[1035,    30] loss: 0.00266\n",
      "[1036,    10] loss: 0.00228\n",
      "[1036,    20] loss: 0.00400\n",
      "[1036,    30] loss: 0.00314\n",
      "[1037,    10] loss: 0.00248\n",
      "[1037,    20] loss: 0.00285\n",
      "[1037,    30] loss: 0.00197\n",
      "[1038,    10] loss: 0.00273\n",
      "[1038,    20] loss: 0.00339\n",
      "[1038,    30] loss: 0.00210\n",
      "[1039,    10] loss: 0.00231\n",
      "[1039,    20] loss: 0.00307\n",
      "[1039,    30] loss: 0.00250\n",
      "[1040,    10] loss: 0.00295\n",
      "[1040,    20] loss: 0.00319\n",
      "[1040,    30] loss: 0.00220\n",
      "[1041,    10] loss: 0.00263\n",
      "[1041,    20] loss: 0.00383\n",
      "[1041,    30] loss: 0.00350\n",
      "[1042,    10] loss: 0.00358\n",
      "[1042,    20] loss: 0.00262\n",
      "[1042,    30] loss: 0.00253\n",
      "[1043,    10] loss: 0.00368\n",
      "[1043,    20] loss: 0.00383\n",
      "[1043,    30] loss: 0.00375\n",
      "[1044,    10] loss: 0.00294\n",
      "[1044,    20] loss: 0.00314\n",
      "[1044,    30] loss: 0.00213\n",
      "[1045,    10] loss: 0.00265\n",
      "[1045,    20] loss: 0.00309\n",
      "[1045,    30] loss: 0.00334\n",
      "[1046,    10] loss: 0.00228\n",
      "[1046,    20] loss: 0.00311\n",
      "[1046,    30] loss: 0.00224\n",
      "[1047,    10] loss: 0.00274\n",
      "[1047,    20] loss: 0.00323\n",
      "[1047,    30] loss: 0.00198\n",
      "[1048,    10] loss: 0.00285\n",
      "[1048,    20] loss: 0.00324\n",
      "[1048,    30] loss: 0.00226\n",
      "[1049,    10] loss: 0.00244\n",
      "[1049,    20] loss: 0.00264\n",
      "[1049,    30] loss: 0.00185\n",
      "[1050,    10] loss: 0.00256\n",
      "[1050,    20] loss: 0.00342\n",
      "[1050,    30] loss: 0.00263\n",
      "[1051,    10] loss: 0.00357\n",
      "[1051,    20] loss: 0.00307\n",
      "[1051,    30] loss: 0.00240\n",
      "[1052,    10] loss: 0.00268\n",
      "[1052,    20] loss: 0.00441\n",
      "[1052,    30] loss: 0.00220\n",
      "[1053,    10] loss: 0.00350\n",
      "[1053,    20] loss: 0.00329\n",
      "[1053,    30] loss: 0.00239\n",
      "[1054,    10] loss: 0.00233\n",
      "[1054,    20] loss: 0.00415\n",
      "[1054,    30] loss: 0.00411\n",
      "[1055,    10] loss: 0.00312\n",
      "[1055,    20] loss: 0.00259\n",
      "[1055,    30] loss: 0.00215\n",
      "[1056,    10] loss: 0.00216\n",
      "[1056,    20] loss: 0.00388\n",
      "[1056,    30] loss: 0.00336\n",
      "[1057,    10] loss: 0.00269\n",
      "[1057,    20] loss: 0.00235\n",
      "[1057,    30] loss: 0.00171\n",
      "[1058,    10] loss: 0.00236\n",
      "[1058,    20] loss: 0.00340\n",
      "[1058,    30] loss: 0.00251\n",
      "[1059,    10] loss: 0.00234\n",
      "[1059,    20] loss: 0.00311\n",
      "[1059,    30] loss: 0.00195\n",
      "[1060,    10] loss: 0.00250\n",
      "[1060,    20] loss: 0.00296\n",
      "[1060,    30] loss: 0.00178\n",
      "[1061,    10] loss: 0.00258\n",
      "[1061,    20] loss: 0.00342\n",
      "[1061,    30] loss: 0.00336\n",
      "[1062,    10] loss: 0.00270\n",
      "[1062,    20] loss: 0.00378\n",
      "[1062,    30] loss: 0.00287\n",
      "[1063,    10] loss: 0.00365\n",
      "[1063,    20] loss: 0.00366\n",
      "[1063,    30] loss: 0.00474\n",
      "[1064,    10] loss: 0.00337\n",
      "[1064,    20] loss: 0.00354\n",
      "[1064,    30] loss: 0.00351\n",
      "[1065,    10] loss: 0.00301\n",
      "[1065,    20] loss: 0.00337\n",
      "[1065,    30] loss: 0.00285\n",
      "[1066,    10] loss: 0.00274\n",
      "[1066,    20] loss: 0.00387\n",
      "[1066,    30] loss: 0.00295\n",
      "[1067,    10] loss: 0.00369\n",
      "[1067,    20] loss: 0.00500\n",
      "[1067,    30] loss: 0.00395\n",
      "[1068,    10] loss: 0.00444\n",
      "[1068,    20] loss: 0.00398\n",
      "[1068,    30] loss: 0.00235\n",
      "[1069,    10] loss: 0.00267\n",
      "[1069,    20] loss: 0.00401\n",
      "[1069,    30] loss: 0.00280\n",
      "[1070,    10] loss: 0.00360\n",
      "[1070,    20] loss: 0.00367\n",
      "[1070,    30] loss: 0.00224\n",
      "[1071,    10] loss: 0.00351\n",
      "[1071,    20] loss: 0.00463\n",
      "[1071,    30] loss: 0.00293\n",
      "[1072,    10] loss: 0.00416\n",
      "[1072,    20] loss: 0.00456\n",
      "[1072,    30] loss: 0.00236\n",
      "[1073,    10] loss: 0.00298\n",
      "[1073,    20] loss: 0.00367\n",
      "[1073,    30] loss: 0.00266\n",
      "[1074,    10] loss: 0.00259\n",
      "[1074,    20] loss: 0.00298\n",
      "[1074,    30] loss: 0.00187\n",
      "[1075,    10] loss: 0.00243\n",
      "[1075,    20] loss: 0.00352\n",
      "[1075,    30] loss: 0.00241\n",
      "[1076,    10] loss: 0.00274\n",
      "[1076,    20] loss: 0.00318\n",
      "[1076,    30] loss: 0.00182\n",
      "[1077,    10] loss: 0.00260\n",
      "[1077,    20] loss: 0.00364\n",
      "[1077,    30] loss: 0.00249\n",
      "[1078,    10] loss: 0.00275\n",
      "[1078,    20] loss: 0.00395\n",
      "[1078,    30] loss: 0.00207\n",
      "[1079,    10] loss: 0.00291\n",
      "[1079,    20] loss: 0.00373\n",
      "[1079,    30] loss: 0.00289\n",
      "[1080,    10] loss: 0.00330\n",
      "[1080,    20] loss: 0.00376\n",
      "[1080,    30] loss: 0.00218\n",
      "[1081,    10] loss: 0.00289\n",
      "[1081,    20] loss: 0.00327\n",
      "[1081,    30] loss: 0.00212\n",
      "[1082,    10] loss: 0.00290\n",
      "[1082,    20] loss: 0.00352\n",
      "[1082,    30] loss: 0.00194\n",
      "[1083,    10] loss: 0.00253\n",
      "[1083,    20] loss: 0.00314\n",
      "[1083,    30] loss: 0.00216\n",
      "[1084,    10] loss: 0.00241\n",
      "[1084,    20] loss: 0.00228\n",
      "[1084,    30] loss: 0.00163\n",
      "[1085,    10] loss: 0.00273\n",
      "[1085,    20] loss: 0.00345\n",
      "[1085,    30] loss: 0.00292\n",
      "[1086,    10] loss: 0.00292\n",
      "[1086,    20] loss: 0.00273\n",
      "[1086,    30] loss: 0.00219\n",
      "[1087,    10] loss: 0.00235\n",
      "[1087,    20] loss: 0.00411\n",
      "[1087,    30] loss: 0.00344\n",
      "[1088,    10] loss: 0.00340\n",
      "[1088,    20] loss: 0.00369\n",
      "[1088,    30] loss: 0.00361\n",
      "[1089,    10] loss: 0.00452\n",
      "[1089,    20] loss: 0.00334\n",
      "[1089,    30] loss: 0.00349\n",
      "[1090,    10] loss: 0.00251\n",
      "[1090,    20] loss: 0.00291\n",
      "[1090,    30] loss: 0.00253\n",
      "[1091,    10] loss: 0.00322\n",
      "[1091,    20] loss: 0.00354\n",
      "[1091,    30] loss: 0.00247\n",
      "[1092,    10] loss: 0.00273\n",
      "[1092,    20] loss: 0.00281\n",
      "[1092,    30] loss: 0.00233\n",
      "[1093,    10] loss: 0.00318\n",
      "[1093,    20] loss: 0.00348\n",
      "[1093,    30] loss: 0.00301\n",
      "[1094,    10] loss: 0.00289\n",
      "[1094,    20] loss: 0.00293\n",
      "[1094,    30] loss: 0.00313\n",
      "[1095,    10] loss: 0.00262\n",
      "[1095,    20] loss: 0.00379\n",
      "[1095,    30] loss: 0.00517\n",
      "[1096,    10] loss: 0.00603\n",
      "[1096,    20] loss: 0.00363\n",
      "[1096,    30] loss: 0.00423\n",
      "[1097,    10] loss: 0.00416\n",
      "[1097,    20] loss: 0.00583\n",
      "[1097,    30] loss: 0.00546\n",
      "[1098,    10] loss: 0.00575\n",
      "[1098,    20] loss: 0.00356\n",
      "[1098,    30] loss: 0.00309\n",
      "[1099,    10] loss: 0.00446\n",
      "[1099,    20] loss: 0.00294\n",
      "[1099,    30] loss: 0.00301\n",
      "[1100,    10] loss: 0.00295\n",
      "[1100,    20] loss: 0.00348\n",
      "[1100,    30] loss: 0.00381\n",
      "[1101,    10] loss: 0.00302\n",
      "[1101,    20] loss: 0.00379\n",
      "[1101,    30] loss: 0.00340\n",
      "[1102,    10] loss: 0.00324\n",
      "[1102,    20] loss: 0.00300\n",
      "[1102,    30] loss: 0.00342\n",
      "[1103,    10] loss: 0.00319\n",
      "[1103,    20] loss: 0.00385\n",
      "[1103,    30] loss: 0.00445\n",
      "[1104,    10] loss: 0.00429\n",
      "[1104,    20] loss: 0.00386\n",
      "[1104,    30] loss: 0.00317\n",
      "[1105,    10] loss: 0.00295\n",
      "[1105,    20] loss: 0.00281\n",
      "[1105,    30] loss: 0.00339\n",
      "[1106,    10] loss: 0.00352\n",
      "[1106,    20] loss: 0.00382\n",
      "[1106,    30] loss: 0.00251\n",
      "[1107,    10] loss: 0.00248\n",
      "[1107,    20] loss: 0.00283\n",
      "[1107,    30] loss: 0.00291\n",
      "[1108,    10] loss: 0.00406\n",
      "[1108,    20] loss: 0.00420\n",
      "[1108,    30] loss: 0.00316\n",
      "[1109,    10] loss: 0.00279\n",
      "[1109,    20] loss: 0.00279\n",
      "[1109,    30] loss: 0.00269\n",
      "[1110,    10] loss: 0.00341\n",
      "[1110,    20] loss: 0.00406\n",
      "[1110,    30] loss: 0.00408\n",
      "[1111,    10] loss: 0.00354\n",
      "[1111,    20] loss: 0.00368\n",
      "[1111,    30] loss: 0.00304\n",
      "[1112,    10] loss: 0.00282\n",
      "[1112,    20] loss: 0.00326\n",
      "[1112,    30] loss: 0.00254\n",
      "[1113,    10] loss: 0.00303\n",
      "[1113,    20] loss: 0.00386\n",
      "[1113,    30] loss: 0.00306\n",
      "[1114,    10] loss: 0.00283\n",
      "[1114,    20] loss: 0.00302\n",
      "[1114,    30] loss: 0.00261\n",
      "[1115,    10] loss: 0.00295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1115,    20] loss: 0.00296\n",
      "[1115,    30] loss: 0.00236\n",
      "[1116,    10] loss: 0.00300\n",
      "[1116,    20] loss: 0.00399\n",
      "[1116,    30] loss: 0.00289\n",
      "[1117,    10] loss: 0.00235\n",
      "[1117,    20] loss: 0.00339\n",
      "[1117,    30] loss: 0.00304\n",
      "[1118,    10] loss: 0.00349\n",
      "[1118,    20] loss: 0.00387\n",
      "[1118,    30] loss: 0.00294\n",
      "[1119,    10] loss: 0.00269\n",
      "[1119,    20] loss: 0.00381\n",
      "[1119,    30] loss: 0.00330\n",
      "[1120,    10] loss: 0.00254\n",
      "[1120,    20] loss: 0.00321\n",
      "[1120,    30] loss: 0.00235\n",
      "[1121,    10] loss: 0.00235\n",
      "[1121,    20] loss: 0.00312\n",
      "[1121,    30] loss: 0.00346\n",
      "[1122,    10] loss: 0.00351\n",
      "[1122,    20] loss: 0.00359\n",
      "[1122,    30] loss: 0.00324\n",
      "[1123,    10] loss: 0.00425\n",
      "[1123,    20] loss: 0.00381\n",
      "[1123,    30] loss: 0.00263\n",
      "[1124,    10] loss: 0.00253\n",
      "[1124,    20] loss: 0.00298\n",
      "[1124,    30] loss: 0.00243\n",
      "[1125,    10] loss: 0.00316\n",
      "[1125,    20] loss: 0.00378\n",
      "[1125,    30] loss: 0.00277\n",
      "[1126,    10] loss: 0.00279\n",
      "[1126,    20] loss: 0.00320\n",
      "[1126,    30] loss: 0.00220\n",
      "[1127,    10] loss: 0.00299\n",
      "[1127,    20] loss: 0.00315\n",
      "[1127,    30] loss: 0.00257\n",
      "[1128,    10] loss: 0.00240\n",
      "[1128,    20] loss: 0.00239\n",
      "[1128,    30] loss: 0.00230\n",
      "[1129,    10] loss: 0.00375\n",
      "[1129,    20] loss: 0.00361\n",
      "[1129,    30] loss: 0.00307\n",
      "[1130,    10] loss: 0.00264\n",
      "[1130,    20] loss: 0.00239\n",
      "[1130,    30] loss: 0.00229\n",
      "[1131,    10] loss: 0.00276\n",
      "[1131,    20] loss: 0.00272\n",
      "[1131,    30] loss: 0.00274\n",
      "[1132,    10] loss: 0.00415\n",
      "[1132,    20] loss: 0.00515\n",
      "[1132,    30] loss: 0.00296\n",
      "[1133,    10] loss: 0.00275\n",
      "[1133,    20] loss: 0.00254\n",
      "[1133,    30] loss: 0.00224\n",
      "[1134,    10] loss: 0.00289\n",
      "[1134,    20] loss: 0.00357\n",
      "[1134,    30] loss: 0.00330\n",
      "[1135,    10] loss: 0.00335\n",
      "[1135,    20] loss: 0.00373\n",
      "[1135,    30] loss: 0.00289\n",
      "[1136,    10] loss: 0.00229\n",
      "[1136,    20] loss: 0.00226\n",
      "[1136,    30] loss: 0.00236\n",
      "[1137,    10] loss: 0.00304\n",
      "[1137,    20] loss: 0.00319\n",
      "[1137,    30] loss: 0.00261\n",
      "[1138,    10] loss: 0.00228\n",
      "[1138,    20] loss: 0.00266\n",
      "[1138,    30] loss: 0.00225\n",
      "[1139,    10] loss: 0.00216\n",
      "[1139,    20] loss: 0.00308\n",
      "[1139,    30] loss: 0.00271\n",
      "[1140,    10] loss: 0.00365\n",
      "[1140,    20] loss: 0.00447\n",
      "[1140,    30] loss: 0.00475\n",
      "[1141,    10] loss: 0.00435\n",
      "[1141,    20] loss: 0.00316\n",
      "[1141,    30] loss: 0.00323\n",
      "[1142,    10] loss: 0.00338\n",
      "[1142,    20] loss: 0.00328\n",
      "[1142,    30] loss: 0.00236\n",
      "[1143,    10] loss: 0.00271\n",
      "[1143,    20] loss: 0.00269\n",
      "[1143,    30] loss: 0.00283\n",
      "[1144,    10] loss: 0.00486\n",
      "[1144,    20] loss: 0.00370\n",
      "[1144,    30] loss: 0.00312\n",
      "[1145,    10] loss: 0.00275\n",
      "[1145,    20] loss: 0.00279\n",
      "[1145,    30] loss: 0.00255\n",
      "[1146,    10] loss: 0.00332\n",
      "[1146,    20] loss: 0.00273\n",
      "[1146,    30] loss: 0.00256\n",
      "[1147,    10] loss: 0.00273\n",
      "[1147,    20] loss: 0.00278\n",
      "[1147,    30] loss: 0.00296\n",
      "[1148,    10] loss: 0.00248\n",
      "[1148,    20] loss: 0.00282\n",
      "[1148,    30] loss: 0.00244\n",
      "[1149,    10] loss: 0.00356\n",
      "[1149,    20] loss: 0.00271\n",
      "[1149,    30] loss: 0.00327\n",
      "[1150,    10] loss: 0.00312\n",
      "[1150,    20] loss: 0.00239\n",
      "[1150,    30] loss: 0.00455\n",
      "[1151,    10] loss: 0.00349\n",
      "[1151,    20] loss: 0.00335\n",
      "[1151,    30] loss: 0.00310\n",
      "[1152,    10] loss: 0.00336\n",
      "[1152,    20] loss: 0.00388\n",
      "[1152,    30] loss: 0.00472\n",
      "[1153,    10] loss: 0.00278\n",
      "[1153,    20] loss: 0.00266\n",
      "[1153,    30] loss: 0.00269\n",
      "[1154,    10] loss: 0.00293\n",
      "[1154,    20] loss: 0.00390\n",
      "[1154,    30] loss: 0.00468\n",
      "[1155,    10] loss: 0.00296\n",
      "[1155,    20] loss: 0.00377\n",
      "[1155,    30] loss: 0.00355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-59db2644a1e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a single value, same as loss.item(), which is the mean loss for each mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# performs one back-propagation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# update the network parameters (perform an update step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 2000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[k][0],test_set[k][1],test_set[k][2])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "            \n",
    "        if i % 10 == 9:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[200:600],train_loss[200:600],'blue',label = 'Training loss')\n",
    "# plt.plot(x[200:600],test_loss[200:600],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:400],train_loss[:400],'blue',label = 'Training loss')\n",
    "plt.plot(x[:400],test_loss[:400],'red',label = 'Test loss')\n",
    "\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch for $H_2O$ training and test set',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('loss_graph_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  -4.5161,   -3.9995,   -5.1756,   -3.9153,  -10.4564,  -55.5737,\n",
      "          -4.3034,   -6.2949,   -2.3746, -507.3092,   -8.8254, -269.3573,\n",
      "          -2.3854, -196.1882,   -2.3572, -143.2192,   -2.7678,  -40.5771,\n",
      "          -3.3184,  -11.2665])\n",
      "output\n",
      "tensor([-0.4650], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = test_set[10]\n",
    "x1 = x1\n",
    "print(x1)\n",
    "\n",
    "output = net(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.42604542 -0.43418038 -0.15118572 -0.31769645 -0.35961902 -0.01564322\n",
      "  0.02764952  0.20080097  0.11060886  0.11521032 -0.46502477 -0.34780219\n",
      " -0.26343703 -0.43510413 -0.43951011 -0.04621547 -0.04857902  0.0695684\n",
      " -0.22981289 -0.48920506  0.49026838  0.04740983 -0.10733855  0.07519563\n",
      " -0.4293384   0.00969642 -0.44264609  0.06996229 -0.35209811  0.18484049\n",
      "  0.41726473  0.20929235  0.28769088 -0.34514558 -0.04204257 -0.35300216\n",
      " -0.41326371 -0.25680935  0.16224992 -0.4219451  -0.22199968 -0.44393384\n",
      "  0.64541876 -0.16648969 -0.12939724  0.47495341 -0.22555245  0.08305728\n",
      "  0.06208731  0.16106752 -0.00463289 -0.17523874 -0.20218149 -0.17733398\n",
      " -0.1068269   0.03721975 -0.46985215  0.3010478  -0.23280331 -0.48272923\n",
      "  0.01679234 -0.2748524   0.28917962 -0.44292784 -0.00902244  0.19349338\n",
      " -0.22982492  0.39547276  0.68447769 -0.28820837 -0.1150234  -0.00101949\n",
      "  0.18829334  0.76542294  0.58225548  0.17942345 -0.10223925 -0.18251713\n",
      " -0.38673723 -0.42873412  0.01694992 -0.2644797   0.06913005  0.12600425\n",
      " -0.16904707 -0.38786092  0.04105923 -0.29764253  0.58955741 -0.43779963\n",
      "  0.27391589 -0.39841679 -0.44268662 -0.40961665 -0.42741123  0.65800023\n",
      " -0.12019804 -0.03158747  0.05833963 -0.17997387]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900])\n",
      "-2.6490954e-09\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n"
     ]
    }
   ],
   "source": [
    "print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5770, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 2.6065e-01, -2.5736e-01,  2.0626e-01, -2.7914e-01, -2.1959e-01,\n",
      "         -1.6672e-01,  3.2766e-01,  1.4297e-01,  4.9485e-02,  8.5347e-02,\n",
      "          3.6335e-02, -2.1160e-01,  5.1101e-01, -4.4496e-03,  1.9313e-01,\n",
      "          1.5726e-01,  3.1445e-01, -5.5190e-02,  3.4995e-01,  1.1511e-01],\n",
      "        [ 9.4726e-01,  3.6160e-01,  1.6881e+00,  2.5591e-01, -1.5086e-02,\n",
      "         -1.0619e-01,  4.2495e-01,  7.0953e-01,  3.1453e-01, -6.6357e-02,\n",
      "         -2.4467e-02,  1.1275e-01,  1.9645e+00, -6.7396e-02,  3.5347e-01,\n",
      "          7.3393e-02,  5.1611e-01,  1.6257e-01,  3.8818e-01,  2.3179e-01],\n",
      "        [-7.5401e-01,  4.3655e-01, -8.9566e-01,  6.6575e-02, -1.1664e-01,\n",
      "          9.3890e-02, -7.9665e-01, -4.2790e-01, -3.2203e-01,  3.4547e-02,\n",
      "         -2.7067e-01,  6.4307e-02, -8.5870e-01, -1.5695e-01, -5.4232e-01,\n",
      "         -1.1773e-01, -2.8487e-01, -2.0509e-01, -4.0915e-01,  3.2380e-02],\n",
      "        [-3.0722e-02,  2.8992e-01, -5.0032e-01, -6.8952e-01,  2.5132e+00,\n",
      "          1.6915e-01,  4.4615e-01, -2.6814e-01,  1.1149e+00, -5.7298e-02,\n",
      "          1.6578e+00,  1.5142e-01,  2.8467e-02,  2.5059e-02,  5.2542e-01,\n",
      "         -2.0097e-01,  8.4628e-01,  2.4031e-01,  7.3134e-01,  3.5969e-01],\n",
      "        [-5.6043e-01,  1.5079e-01, -8.8335e-01, -5.8880e-02, -2.8639e-01,\n",
      "         -3.2687e-01, -2.7086e-01, -8.7540e-01, -4.4085e-02, -1.1808e-01,\n",
      "         -3.5180e-01, -1.9307e-02, -6.4366e-01,  1.5855e-02, -4.4064e-01,\n",
      "         -3.1500e-01, -5.6512e-01, -1.6225e-01, -3.4927e-01, -9.3950e-01],\n",
      "        [ 4.8784e-01, -1.9295e+00,  7.5453e-01,  2.4611e-02, -4.7710e-01,\n",
      "         -2.2928e-01, -1.2748e-02, -1.9986e-01,  1.0305e-01,  1.0815e-01,\n",
      "         -5.0811e-01, -1.7222e-01,  3.7557e-01, -7.8164e-02,  6.3545e-02,\n",
      "          7.6463e-02,  1.1573e-01, -1.3192e-01,  1.9611e-02, -1.9648e-01],\n",
      "        [-4.1654e-01, -3.6881e-01, -2.8109e-01, -2.3352e-01,  5.4851e-02,\n",
      "          1.1874e-01, -3.8712e-01, -4.5571e-01, -2.2733e-01,  3.1062e-01,\n",
      "         -6.3413e-02,  1.5456e-01, -4.6356e-01,  1.1281e-01, -2.6262e-01,\n",
      "          1.2277e-01, -1.9725e-01, -1.8806e-01, -1.2937e-01, -4.8117e-01],\n",
      "        [ 1.0202e-01, -7.2677e-01,  2.6189e-01,  6.6162e-01, -1.5945e+00,\n",
      "         -1.4358e-01, -2.7873e-01,  2.9278e-01, -6.3315e-01, -8.6310e-02,\n",
      "         -2.4754e+00,  7.7045e-02,  2.5207e-02,  3.1957e-03, -5.6352e-01,\n",
      "          1.4176e-01, -6.1055e-01, -3.9435e-01, -3.9437e-01, -2.1005e-01],\n",
      "        [ 6.6687e-01, -1.2319e+00,  7.1008e-01, -1.4920e+00, -1.4219e-01,\n",
      "         -4.4577e-02,  1.6971e-01, -6.9142e-01, -1.6079e-01,  1.8918e-02,\n",
      "         -2.2726e-01, -4.0994e-02,  5.1051e-01, -1.6847e-01,  2.3592e-01,\n",
      "          1.3422e-01,  2.3598e-01,  4.2553e-02,  1.6317e-01, -3.9888e-01],\n",
      "        [ 1.0316e-01, -9.8309e-01, -5.2994e-02, -4.8771e-01, -2.0171e-02,\n",
      "         -4.1912e-01,  8.9809e-02, -6.4619e-01,  5.5289e-02,  1.1068e-01,\n",
      "          1.9125e-01,  4.3858e-02,  9.2839e-04,  1.7199e-01, -7.5548e-02,\n",
      "          1.6904e-01, -8.2306e-02, -1.8874e-01, -1.0229e-01, -4.0187e-01]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.0741,  0.3683,  0.0035, -1.2087, -0.2046,  0.3047, -0.1402,  1.1478,\n",
      "        -0.6708, -0.3131], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.2812, -0.2296, -0.2077,  0.0725,  0.0836, -0.1970, -0.0339,  0.0051,\n",
      "         -0.0986,  0.3414],\n",
      "        [ 0.0121, -0.0464, -0.1293, -0.1328,  0.1426, -0.0279,  0.0297, -0.1017,\n",
      "         -0.0181,  0.0341],\n",
      "        [ 0.7905, -0.3040, -0.1594,  0.3108, -0.0024, -0.3657,  0.3137, -0.0782,\n",
      "         -0.0907,  0.2455],\n",
      "        [ 0.0288, -0.0014,  0.0105, -0.0412, -0.1590,  0.0116,  0.2019,  0.2030,\n",
      "          0.0217, -0.1416],\n",
      "        [ 0.2567,  0.4104, -0.1742,  0.0157, -0.0579,  0.0390, -0.2389,  0.0766,\n",
      "          0.0047, -0.2652],\n",
      "        [-0.1933,  0.3056, -0.3268,  0.2087,  0.2678,  0.2949,  0.1398, -0.0578,\n",
      "         -0.0899,  0.1024],\n",
      "        [ 0.0568,  0.4903,  0.3650,  0.2159, -0.2334, -0.2555, -0.0796, -0.3611,\n",
      "         -0.4363, -0.0602],\n",
      "        [ 0.0586,  0.2769,  0.3820,  0.1137,  0.1995, -0.2079,  0.1828, -0.0816,\n",
      "         -0.1477, -0.0533],\n",
      "        [ 0.3618,  0.0149, -0.2368, -0.0787,  0.2501, -0.0819,  0.3022, -0.1298,\n",
      "         -0.0894, -0.2513],\n",
      "        [ 0.3641,  0.1928, -0.0246, -0.1542,  0.1429, -0.0770, -0.1633, -0.1039,\n",
      "         -0.0131, -0.3122]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.0275,  0.0104, -0.4152, -0.0704, -0.1931,  0.0943, -0.0596,  0.3164,\n",
      "         0.1506, -0.0727], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(net.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotating test set molecules and checking performance**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Rotation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix():\n",
    "    theta = np.arccos(2*np.random.uniform(low = 0,high = 1)-1)\n",
    "    phi = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    u = np.array([np.sin(theta)*np.cos(phi),np.sin(theta)*np.sin(phi),np.cos(theta)])\n",
    "    theta = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    A = np.zeros((3,3))\n",
    "    A[0][0] = np.cos(theta) + (u[0]**2)*(1-np.cos(theta))\n",
    "    A[0][1] = u[0]*u[1]*(1-np.cos(theta)) - u[2]*np.sin(theta)\n",
    "    A[0][2] = u[0]*u[2]*(1-np.cos(theta)) + u[1]*np.sin(theta)\n",
    "    A[1][0] = u[1]*u[0]*(1-np.cos(theta)) + u[2]*np.sin(theta)\n",
    "    A[1][1] = np.cos(theta) + (u[1]**2)*(1-np.cos(theta))\n",
    "    A[1][2] = u[1]*u[2]*(1-np.cos(theta)) - u[0]*np.sin(theta)\n",
    "    A[2][0] = u[2]*u[0]*(1-np.cos(theta)) - u[1]*np.sin(theta)\n",
    "    A[2][1] = u[2]*u[1]*(1-np.cos(theta)) + u[0]*np.sin(theta)\n",
    "    A[2][2] = np.cos(theta) + (u[2]**2)*(1-np.cos(theta))\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(A,data):\n",
    "    data = np.array(data)\n",
    "    m = np.shape(data)[1]\n",
    "    for i in range(m):\n",
    "        data[:,i] = np.matmul(A,data[:,i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_molec_coord = np.zeros((np.shape(coordinates)))\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Computing symmetry functions for rotated test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.287083627615273\n",
      "0.9702538815523232\n",
      "0.9290961788883947\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-ed71674b9749>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mDp\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mpairwise_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnumber_of_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mG_rot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mradial_BP_symm_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mG_rot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mangular_BP_symm_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mRs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambdaa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mzeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "G_rot = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "N=3\n",
    "for i in range(data_size):\n",
    "    coord = rotated_molec_coord[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features,2):\n",
    "        G_rot[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j]) \n",
    "        G_rot[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_rot_train = G_rot[:training_set_size,:]\n",
    "var  = np.var(G_rot_train,axis=0)\n",
    "mean = np.mean(G_rot_train,axis=0)\n",
    "\n",
    "G_rot_norm = np.zeros((len(coordinates), number_of_features))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G_rot)[0]):\n",
    "    for j in range(np.shape(G_rot)[1]):\n",
    "        G_rot_norm[i,j] = (G_rot[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set_rot = np.vsplit(G_rot_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set_rot = torch.FloatTensor(data_set_rot)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_rot         = data_set_rot[:training_set_size]\n",
    "test_set_rot             = data_set_rot[training_set_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 20)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(G_rot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_set_rot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-205249772f52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprediction_rotated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_set_rot\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprediction_rotated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#[0]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_set_rot' is not defined"
     ]
    }
   ],
   "source": [
    "prediction_rotated = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x1,x2,x3 = test_set_rot[i]\n",
    "    prediction_rotated[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "\n",
    "prediction_rotated = torch.tensor(prediction_rotated)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_rotated),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'rotated test set',markersize=8)\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_rotated*var_lab+mean_lab, '*', color='yellow', label = 'Test set')\n",
    "\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for rotated $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('rotated_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
