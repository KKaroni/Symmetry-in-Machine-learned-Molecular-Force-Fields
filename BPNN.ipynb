{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (1000,) entries\n"
     ]
    }
   ],
   "source": [
    "energies_water = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_water),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-0a86150c1726>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #6 (got 1 columns instead of 5)\n",
      "    Line #11 (got 1 columns instead of 5)\n",
      "    Line #16 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #26 (got 1 columns instead of 5)\n",
      "    Line #31 (got 1 columns instead of 5)\n",
      "    Line #36 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #46 (got 1 columns instead of 5)\n",
      "    Line #51 (got 1 columns instead of 5)\n",
      "    Line #56 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #66 (got 1 columns instead of 5)\n",
      "    Line #71 (got 1 columns instead of 5)\n",
      "    Line #76 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #86 (got 1 columns instead of 5)\n",
      "    Line #91 (got 1 columns instead of 5)\n",
      "    Line #96 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #106 (got 1 columns instead of 5)\n",
      "    Line #111 (got 1 columns instead of 5)\n",
      "    Line #116 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #126 (got 1 columns instead of 5)\n",
      "    Line #131 (got 1 columns instead of 5)\n",
      "    Line #136 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #146 (got 1 columns instead of 5)\n",
      "    Line #151 (got 1 columns instead of 5)\n",
      "    Line #156 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #166 (got 1 columns instead of 5)\n",
      "    Line #171 (got 1 columns instead of 5)\n",
      "    Line #176 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #186 (got 1 columns instead of 5)\n",
      "    Line #191 (got 1 columns instead of 5)\n",
      "    Line #196 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #206 (got 1 columns instead of 5)\n",
      "    Line #211 (got 1 columns instead of 5)\n",
      "    Line #216 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #226 (got 1 columns instead of 5)\n",
      "    Line #231 (got 1 columns instead of 5)\n",
      "    Line #236 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #246 (got 1 columns instead of 5)\n",
      "    Line #251 (got 1 columns instead of 5)\n",
      "    Line #256 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #266 (got 1 columns instead of 5)\n",
      "    Line #271 (got 1 columns instead of 5)\n",
      "    Line #276 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #286 (got 1 columns instead of 5)\n",
      "    Line #291 (got 1 columns instead of 5)\n",
      "    Line #296 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #306 (got 1 columns instead of 5)\n",
      "    Line #311 (got 1 columns instead of 5)\n",
      "    Line #316 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #326 (got 1 columns instead of 5)\n",
      "    Line #331 (got 1 columns instead of 5)\n",
      "    Line #336 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #346 (got 1 columns instead of 5)\n",
      "    Line #351 (got 1 columns instead of 5)\n",
      "    Line #356 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #366 (got 1 columns instead of 5)\n",
      "    Line #371 (got 1 columns instead of 5)\n",
      "    Line #376 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #386 (got 1 columns instead of 5)\n",
      "    Line #391 (got 1 columns instead of 5)\n",
      "    Line #396 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #406 (got 1 columns instead of 5)\n",
      "    Line #411 (got 1 columns instead of 5)\n",
      "    Line #416 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #426 (got 1 columns instead of 5)\n",
      "    Line #431 (got 1 columns instead of 5)\n",
      "    Line #436 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #446 (got 1 columns instead of 5)\n",
      "    Line #451 (got 1 columns instead of 5)\n",
      "    Line #456 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #466 (got 1 columns instead of 5)\n",
      "    Line #471 (got 1 columns instead of 5)\n",
      "    Line #476 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #486 (got 1 columns instead of 5)\n",
      "    Line #491 (got 1 columns instead of 5)\n",
      "    Line #496 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #506 (got 1 columns instead of 5)\n",
      "    Line #511 (got 1 columns instead of 5)\n",
      "    Line #516 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #526 (got 1 columns instead of 5)\n",
      "    Line #531 (got 1 columns instead of 5)\n",
      "    Line #536 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #546 (got 1 columns instead of 5)\n",
      "    Line #551 (got 1 columns instead of 5)\n",
      "    Line #556 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #566 (got 1 columns instead of 5)\n",
      "    Line #571 (got 1 columns instead of 5)\n",
      "    Line #576 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #586 (got 1 columns instead of 5)\n",
      "    Line #591 (got 1 columns instead of 5)\n",
      "    Line #596 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #606 (got 1 columns instead of 5)\n",
      "    Line #611 (got 1 columns instead of 5)\n",
      "    Line #616 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #626 (got 1 columns instead of 5)\n",
      "    Line #631 (got 1 columns instead of 5)\n",
      "    Line #636 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #646 (got 1 columns instead of 5)\n",
      "    Line #651 (got 1 columns instead of 5)\n",
      "    Line #656 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #666 (got 1 columns instead of 5)\n",
      "    Line #671 (got 1 columns instead of 5)\n",
      "    Line #676 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #686 (got 1 columns instead of 5)\n",
      "    Line #691 (got 1 columns instead of 5)\n",
      "    Line #696 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #706 (got 1 columns instead of 5)\n",
      "    Line #711 (got 1 columns instead of 5)\n",
      "    Line #716 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #726 (got 1 columns instead of 5)\n",
      "    Line #731 (got 1 columns instead of 5)\n",
      "    Line #736 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #746 (got 1 columns instead of 5)\n",
      "    Line #751 (got 1 columns instead of 5)\n",
      "    Line #756 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #766 (got 1 columns instead of 5)\n",
      "    Line #771 (got 1 columns instead of 5)\n",
      "    Line #776 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #786 (got 1 columns instead of 5)\n",
      "    Line #791 (got 1 columns instead of 5)\n",
      "    Line #796 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #806 (got 1 columns instead of 5)\n",
      "    Line #811 (got 1 columns instead of 5)\n",
      "    Line #816 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #826 (got 1 columns instead of 5)\n",
      "    Line #831 (got 1 columns instead of 5)\n",
      "    Line #836 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #846 (got 1 columns instead of 5)\n",
      "    Line #851 (got 1 columns instead of 5)\n",
      "    Line #856 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #866 (got 1 columns instead of 5)\n",
      "    Line #871 (got 1 columns instead of 5)\n",
      "    Line #876 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #886 (got 1 columns instead of 5)\n",
      "    Line #891 (got 1 columns instead of 5)\n",
      "    Line #896 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #906 (got 1 columns instead of 5)\n",
      "    Line #911 (got 1 columns instead of 5)\n",
      "    Line #916 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #926 (got 1 columns instead of 5)\n",
      "    Line #931 (got 1 columns instead of 5)\n",
      "    Line #936 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #946 (got 1 columns instead of 5)\n",
      "    Line #951 (got 1 columns instead of 5)\n",
      "    Line #956 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #966 (got 1 columns instead of 5)\n",
      "    Line #971 (got 1 columns instead of 5)\n",
      "    Line #976 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #986 (got 1 columns instead of 5)\n",
      "    Line #991 (got 1 columns instead of 5)\n",
      "    Line #996 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1006 (got 1 columns instead of 5)\n",
      "    Line #1011 (got 1 columns instead of 5)\n",
      "    Line #1016 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1026 (got 1 columns instead of 5)\n",
      "    Line #1031 (got 1 columns instead of 5)\n",
      "    Line #1036 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1046 (got 1 columns instead of 5)\n",
      "    Line #1051 (got 1 columns instead of 5)\n",
      "    Line #1056 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1066 (got 1 columns instead of 5)\n",
      "    Line #1071 (got 1 columns instead of 5)\n",
      "    Line #1076 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1086 (got 1 columns instead of 5)\n",
      "    Line #1091 (got 1 columns instead of 5)\n",
      "    Line #1096 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1106 (got 1 columns instead of 5)\n",
      "    Line #1111 (got 1 columns instead of 5)\n",
      "    Line #1116 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1126 (got 1 columns instead of 5)\n",
      "    Line #1131 (got 1 columns instead of 5)\n",
      "    Line #1136 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1146 (got 1 columns instead of 5)\n",
      "    Line #1151 (got 1 columns instead of 5)\n",
      "    Line #1156 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1166 (got 1 columns instead of 5)\n",
      "    Line #1171 (got 1 columns instead of 5)\n",
      "    Line #1176 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1186 (got 1 columns instead of 5)\n",
      "    Line #1191 (got 1 columns instead of 5)\n",
      "    Line #1196 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1206 (got 1 columns instead of 5)\n",
      "    Line #1211 (got 1 columns instead of 5)\n",
      "    Line #1216 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1226 (got 1 columns instead of 5)\n",
      "    Line #1231 (got 1 columns instead of 5)\n",
      "    Line #1236 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1246 (got 1 columns instead of 5)\n",
      "    Line #1251 (got 1 columns instead of 5)\n",
      "    Line #1256 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1266 (got 1 columns instead of 5)\n",
      "    Line #1271 (got 1 columns instead of 5)\n",
      "    Line #1276 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1286 (got 1 columns instead of 5)\n",
      "    Line #1291 (got 1 columns instead of 5)\n",
      "    Line #1296 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1306 (got 1 columns instead of 5)\n",
      "    Line #1311 (got 1 columns instead of 5)\n",
      "    Line #1316 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1326 (got 1 columns instead of 5)\n",
      "    Line #1331 (got 1 columns instead of 5)\n",
      "    Line #1336 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1346 (got 1 columns instead of 5)\n",
      "    Line #1351 (got 1 columns instead of 5)\n",
      "    Line #1356 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1366 (got 1 columns instead of 5)\n",
      "    Line #1371 (got 1 columns instead of 5)\n",
      "    Line #1376 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1386 (got 1 columns instead of 5)\n",
      "    Line #1391 (got 1 columns instead of 5)\n",
      "    Line #1396 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1406 (got 1 columns instead of 5)\n",
      "    Line #1411 (got 1 columns instead of 5)\n",
      "    Line #1416 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1426 (got 1 columns instead of 5)\n",
      "    Line #1431 (got 1 columns instead of 5)\n",
      "    Line #1436 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1446 (got 1 columns instead of 5)\n",
      "    Line #1451 (got 1 columns instead of 5)\n",
      "    Line #1456 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1466 (got 1 columns instead of 5)\n",
      "    Line #1471 (got 1 columns instead of 5)\n",
      "    Line #1476 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1486 (got 1 columns instead of 5)\n",
      "    Line #1491 (got 1 columns instead of 5)\n",
      "    Line #1496 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1506 (got 1 columns instead of 5)\n",
      "    Line #1511 (got 1 columns instead of 5)\n",
      "    Line #1516 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1526 (got 1 columns instead of 5)\n",
      "    Line #1531 (got 1 columns instead of 5)\n",
      "    Line #1536 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1546 (got 1 columns instead of 5)\n",
      "    Line #1551 (got 1 columns instead of 5)\n",
      "    Line #1556 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1566 (got 1 columns instead of 5)\n",
      "    Line #1571 (got 1 columns instead of 5)\n",
      "    Line #1576 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1586 (got 1 columns instead of 5)\n",
      "    Line #1591 (got 1 columns instead of 5)\n",
      "    Line #1596 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1606 (got 1 columns instead of 5)\n",
      "    Line #1611 (got 1 columns instead of 5)\n",
      "    Line #1616 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1626 (got 1 columns instead of 5)\n",
      "    Line #1631 (got 1 columns instead of 5)\n",
      "    Line #1636 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1646 (got 1 columns instead of 5)\n",
      "    Line #1651 (got 1 columns instead of 5)\n",
      "    Line #1656 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1666 (got 1 columns instead of 5)\n",
      "    Line #1671 (got 1 columns instead of 5)\n",
      "    Line #1676 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1686 (got 1 columns instead of 5)\n",
      "    Line #1691 (got 1 columns instead of 5)\n",
      "    Line #1696 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1706 (got 1 columns instead of 5)\n",
      "    Line #1711 (got 1 columns instead of 5)\n",
      "    Line #1716 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1726 (got 1 columns instead of 5)\n",
      "    Line #1731 (got 1 columns instead of 5)\n",
      "    Line #1736 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1746 (got 1 columns instead of 5)\n",
      "    Line #1751 (got 1 columns instead of 5)\n",
      "    Line #1756 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1766 (got 1 columns instead of 5)\n",
      "    Line #1771 (got 1 columns instead of 5)\n",
      "    Line #1776 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1786 (got 1 columns instead of 5)\n",
      "    Line #1791 (got 1 columns instead of 5)\n",
      "    Line #1796 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1806 (got 1 columns instead of 5)\n",
      "    Line #1811 (got 1 columns instead of 5)\n",
      "    Line #1816 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1826 (got 1 columns instead of 5)\n",
      "    Line #1831 (got 1 columns instead of 5)\n",
      "    Line #1836 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1846 (got 1 columns instead of 5)\n",
      "    Line #1851 (got 1 columns instead of 5)\n",
      "    Line #1856 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1866 (got 1 columns instead of 5)\n",
      "    Line #1871 (got 1 columns instead of 5)\n",
      "    Line #1876 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1886 (got 1 columns instead of 5)\n",
      "    Line #1891 (got 1 columns instead of 5)\n",
      "    Line #1896 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1906 (got 1 columns instead of 5)\n",
      "    Line #1911 (got 1 columns instead of 5)\n",
      "    Line #1916 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1926 (got 1 columns instead of 5)\n",
      "    Line #1931 (got 1 columns instead of 5)\n",
      "    Line #1936 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1946 (got 1 columns instead of 5)\n",
      "    Line #1951 (got 1 columns instead of 5)\n",
      "    Line #1956 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1966 (got 1 columns instead of 5)\n",
      "    Line #1971 (got 1 columns instead of 5)\n",
      "    Line #1976 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1986 (got 1 columns instead of 5)\n",
      "    Line #1991 (got 1 columns instead of 5)\n",
      "    Line #1996 (got 1 columns instead of 5)\n",
      "    Line #2001 (got 1 columns instead of 5)\n",
      "    Line #2006 (got 1 columns instead of 5)\n",
      "    Line #2011 (got 1 columns instead of 5)\n",
      "    Line #2016 (got 1 columns instead of 5)\n",
      "    Line #2021 (got 1 columns instead of 5)\n",
      "    Line #2026 (got 1 columns instead of 5)\n",
      "    Line #2031 (got 1 columns instead of 5)\n",
      "    Line #2036 (got 1 columns instead of 5)\n",
      "    Line #2041 (got 1 columns instead of 5)\n",
      "    Line #2046 (got 1 columns instead of 5)\n",
      "    Line #2051 (got 1 columns instead of 5)\n",
      "    Line #2056 (got 1 columns instead of 5)\n",
      "    Line #2061 (got 1 columns instead of 5)\n",
      "    Line #2066 (got 1 columns instead of 5)\n",
      "    Line #2071 (got 1 columns instead of 5)\n",
      "    Line #2076 (got 1 columns instead of 5)\n",
      "    Line #2081 (got 1 columns instead of 5)\n",
      "    Line #2086 (got 1 columns instead of 5)\n",
      "    Line #2091 (got 1 columns instead of 5)\n",
      "    Line #2096 (got 1 columns instead of 5)\n",
      "    Line #2101 (got 1 columns instead of 5)\n",
      "    Line #2106 (got 1 columns instead of 5)\n",
      "    Line #2111 (got 1 columns instead of 5)\n",
      "    Line #2116 (got 1 columns instead of 5)\n",
      "    Line #2121 (got 1 columns instead of 5)\n",
      "    Line #2126 (got 1 columns instead of 5)\n",
      "    Line #2131 (got 1 columns instead of 5)\n",
      "    Line #2136 (got 1 columns instead of 5)\n",
      "    Line #2141 (got 1 columns instead of 5)\n",
      "    Line #2146 (got 1 columns instead of 5)\n",
      "    Line #2151 (got 1 columns instead of 5)\n",
      "    Line #2156 (got 1 columns instead of 5)\n",
      "    Line #2161 (got 1 columns instead of 5)\n",
      "    Line #2166 (got 1 columns instead of 5)\n",
      "    Line #2171 (got 1 columns instead of 5)\n",
      "    Line #2176 (got 1 columns instead of 5)\n",
      "    Line #2181 (got 1 columns instead of 5)\n",
      "    Line #2186 (got 1 columns instead of 5)\n",
      "    Line #2191 (got 1 columns instead of 5)\n",
      "    Line #2196 (got 1 columns instead of 5)\n",
      "    Line #2201 (got 1 columns instead of 5)\n",
      "    Line #2206 (got 1 columns instead of 5)\n",
      "    Line #2211 (got 1 columns instead of 5)\n",
      "    Line #2216 (got 1 columns instead of 5)\n",
      "    Line #2221 (got 1 columns instead of 5)\n",
      "    Line #2226 (got 1 columns instead of 5)\n",
      "    Line #2231 (got 1 columns instead of 5)\n",
      "    Line #2236 (got 1 columns instead of 5)\n",
      "    Line #2241 (got 1 columns instead of 5)\n",
      "    Line #2246 (got 1 columns instead of 5)\n",
      "    Line #2251 (got 1 columns instead of 5)\n",
      "    Line #2256 (got 1 columns instead of 5)\n",
      "    Line #2261 (got 1 columns instead of 5)\n",
      "    Line #2266 (got 1 columns instead of 5)\n",
      "    Line #2271 (got 1 columns instead of 5)\n",
      "    Line #2276 (got 1 columns instead of 5)\n",
      "    Line #2281 (got 1 columns instead of 5)\n",
      "    Line #2286 (got 1 columns instead of 5)\n",
      "    Line #2291 (got 1 columns instead of 5)\n",
      "    Line #2296 (got 1 columns instead of 5)\n",
      "    Line #2301 (got 1 columns instead of 5)\n",
      "    Line #2306 (got 1 columns instead of 5)\n",
      "    Line #2311 (got 1 columns instead of 5)\n",
      "    Line #2316 (got 1 columns instead of 5)\n",
      "    Line #2321 (got 1 columns instead of 5)\n",
      "    Line #2326 (got 1 columns instead of 5)\n",
      "    Line #2331 (got 1 columns instead of 5)\n",
      "    Line #2336 (got 1 columns instead of 5)\n",
      "    Line #2341 (got 1 columns instead of 5)\n",
      "    Line #2346 (got 1 columns instead of 5)\n",
      "    Line #2351 (got 1 columns instead of 5)\n",
      "    Line #2356 (got 1 columns instead of 5)\n",
      "    Line #2361 (got 1 columns instead of 5)\n",
      "    Line #2366 (got 1 columns instead of 5)\n",
      "    Line #2371 (got 1 columns instead of 5)\n",
      "    Line #2376 (got 1 columns instead of 5)\n",
      "    Line #2381 (got 1 columns instead of 5)\n",
      "    Line #2386 (got 1 columns instead of 5)\n",
      "    Line #2391 (got 1 columns instead of 5)\n",
      "    Line #2396 (got 1 columns instead of 5)\n",
      "    Line #2401 (got 1 columns instead of 5)\n",
      "    Line #2406 (got 1 columns instead of 5)\n",
      "    Line #2411 (got 1 columns instead of 5)\n",
      "    Line #2416 (got 1 columns instead of 5)\n",
      "    Line #2421 (got 1 columns instead of 5)\n",
      "    Line #2426 (got 1 columns instead of 5)\n",
      "    Line #2431 (got 1 columns instead of 5)\n",
      "    Line #2436 (got 1 columns instead of 5)\n",
      "    Line #2441 (got 1 columns instead of 5)\n",
      "    Line #2446 (got 1 columns instead of 5)\n",
      "    Line #2451 (got 1 columns instead of 5)\n",
      "    Line #2456 (got 1 columns instead of 5)\n",
      "    Line #2461 (got 1 columns instead of 5)\n",
      "    Line #2466 (got 1 columns instead of 5)\n",
      "    Line #2471 (got 1 columns instead of 5)\n",
      "    Line #2476 (got 1 columns instead of 5)\n",
      "    Line #2481 (got 1 columns instead of 5)\n",
      "    Line #2486 (got 1 columns instead of 5)\n",
      "    Line #2491 (got 1 columns instead of 5)\n",
      "    Line #2496 (got 1 columns instead of 5)\n",
      "    Line #2501 (got 1 columns instead of 5)\n",
      "    Line #2506 (got 1 columns instead of 5)\n",
      "    Line #2511 (got 1 columns instead of 5)\n",
      "    Line #2516 (got 1 columns instead of 5)\n",
      "    Line #2521 (got 1 columns instead of 5)\n",
      "    Line #2526 (got 1 columns instead of 5)\n",
      "    Line #2531 (got 1 columns instead of 5)\n",
      "    Line #2536 (got 1 columns instead of 5)\n",
      "    Line #2541 (got 1 columns instead of 5)\n",
      "    Line #2546 (got 1 columns instead of 5)\n",
      "    Line #2551 (got 1 columns instead of 5)\n",
      "    Line #2556 (got 1 columns instead of 5)\n",
      "    Line #2561 (got 1 columns instead of 5)\n",
      "    Line #2566 (got 1 columns instead of 5)\n",
      "    Line #2571 (got 1 columns instead of 5)\n",
      "    Line #2576 (got 1 columns instead of 5)\n",
      "    Line #2581 (got 1 columns instead of 5)\n",
      "    Line #2586 (got 1 columns instead of 5)\n",
      "    Line #2591 (got 1 columns instead of 5)\n",
      "    Line #2596 (got 1 columns instead of 5)\n",
      "    Line #2601 (got 1 columns instead of 5)\n",
      "    Line #2606 (got 1 columns instead of 5)\n",
      "    Line #2611 (got 1 columns instead of 5)\n",
      "    Line #2616 (got 1 columns instead of 5)\n",
      "    Line #2621 (got 1 columns instead of 5)\n",
      "    Line #2626 (got 1 columns instead of 5)\n",
      "    Line #2631 (got 1 columns instead of 5)\n",
      "    Line #2636 (got 1 columns instead of 5)\n",
      "    Line #2641 (got 1 columns instead of 5)\n",
      "    Line #2646 (got 1 columns instead of 5)\n",
      "    Line #2651 (got 1 columns instead of 5)\n",
      "    Line #2656 (got 1 columns instead of 5)\n",
      "    Line #2661 (got 1 columns instead of 5)\n",
      "    Line #2666 (got 1 columns instead of 5)\n",
      "    Line #2671 (got 1 columns instead of 5)\n",
      "    Line #2676 (got 1 columns instead of 5)\n",
      "    Line #2681 (got 1 columns instead of 5)\n",
      "    Line #2686 (got 1 columns instead of 5)\n",
      "    Line #2691 (got 1 columns instead of 5)\n",
      "    Line #2696 (got 1 columns instead of 5)\n",
      "    Line #2701 (got 1 columns instead of 5)\n",
      "    Line #2706 (got 1 columns instead of 5)\n",
      "    Line #2711 (got 1 columns instead of 5)\n",
      "    Line #2716 (got 1 columns instead of 5)\n",
      "    Line #2721 (got 1 columns instead of 5)\n",
      "    Line #2726 (got 1 columns instead of 5)\n",
      "    Line #2731 (got 1 columns instead of 5)\n",
      "    Line #2736 (got 1 columns instead of 5)\n",
      "    Line #2741 (got 1 columns instead of 5)\n",
      "    Line #2746 (got 1 columns instead of 5)\n",
      "    Line #2751 (got 1 columns instead of 5)\n",
      "    Line #2756 (got 1 columns instead of 5)\n",
      "    Line #2761 (got 1 columns instead of 5)\n",
      "    Line #2766 (got 1 columns instead of 5)\n",
      "    Line #2771 (got 1 columns instead of 5)\n",
      "    Line #2776 (got 1 columns instead of 5)\n",
      "    Line #2781 (got 1 columns instead of 5)\n",
      "    Line #2786 (got 1 columns instead of 5)\n",
      "    Line #2791 (got 1 columns instead of 5)\n",
      "    Line #2796 (got 1 columns instead of 5)\n",
      "    Line #2801 (got 1 columns instead of 5)\n",
      "    Line #2806 (got 1 columns instead of 5)\n",
      "    Line #2811 (got 1 columns instead of 5)\n",
      "    Line #2816 (got 1 columns instead of 5)\n",
      "    Line #2821 (got 1 columns instead of 5)\n",
      "    Line #2826 (got 1 columns instead of 5)\n",
      "    Line #2831 (got 1 columns instead of 5)\n",
      "    Line #2836 (got 1 columns instead of 5)\n",
      "    Line #2841 (got 1 columns instead of 5)\n",
      "    Line #2846 (got 1 columns instead of 5)\n",
      "    Line #2851 (got 1 columns instead of 5)\n",
      "    Line #2856 (got 1 columns instead of 5)\n",
      "    Line #2861 (got 1 columns instead of 5)\n",
      "    Line #2866 (got 1 columns instead of 5)\n",
      "    Line #2871 (got 1 columns instead of 5)\n",
      "    Line #2876 (got 1 columns instead of 5)\n",
      "    Line #2881 (got 1 columns instead of 5)\n",
      "    Line #2886 (got 1 columns instead of 5)\n",
      "    Line #2891 (got 1 columns instead of 5)\n",
      "    Line #2896 (got 1 columns instead of 5)\n",
      "    Line #2901 (got 1 columns instead of 5)\n",
      "    Line #2906 (got 1 columns instead of 5)\n",
      "    Line #2911 (got 1 columns instead of 5)\n",
      "    Line #2916 (got 1 columns instead of 5)\n",
      "    Line #2921 (got 1 columns instead of 5)\n",
      "    Line #2926 (got 1 columns instead of 5)\n",
      "    Line #2931 (got 1 columns instead of 5)\n",
      "    Line #2936 (got 1 columns instead of 5)\n",
      "    Line #2941 (got 1 columns instead of 5)\n",
      "    Line #2946 (got 1 columns instead of 5)\n",
      "    Line #2951 (got 1 columns instead of 5)\n",
      "    Line #2956 (got 1 columns instead of 5)\n",
      "    Line #2961 (got 1 columns instead of 5)\n",
      "    Line #2966 (got 1 columns instead of 5)\n",
      "    Line #2971 (got 1 columns instead of 5)\n",
      "    Line #2976 (got 1 columns instead of 5)\n",
      "    Line #2981 (got 1 columns instead of 5)\n",
      "    Line #2986 (got 1 columns instead of 5)\n",
      "    Line #2991 (got 1 columns instead of 5)\n",
      "    Line #2996 (got 1 columns instead of 5)\n",
      "    Line #3001 (got 1 columns instead of 5)\n",
      "    Line #3006 (got 1 columns instead of 5)\n",
      "    Line #3011 (got 1 columns instead of 5)\n",
      "    Line #3016 (got 1 columns instead of 5)\n",
      "    Line #3021 (got 1 columns instead of 5)\n",
      "    Line #3026 (got 1 columns instead of 5)\n",
      "    Line #3031 (got 1 columns instead of 5)\n",
      "    Line #3036 (got 1 columns instead of 5)\n",
      "    Line #3041 (got 1 columns instead of 5)\n",
      "    Line #3046 (got 1 columns instead of 5)\n",
      "    Line #3051 (got 1 columns instead of 5)\n",
      "    Line #3056 (got 1 columns instead of 5)\n",
      "    Line #3061 (got 1 columns instead of 5)\n",
      "    Line #3066 (got 1 columns instead of 5)\n",
      "    Line #3071 (got 1 columns instead of 5)\n",
      "    Line #3076 (got 1 columns instead of 5)\n",
      "    Line #3081 (got 1 columns instead of 5)\n",
      "    Line #3086 (got 1 columns instead of 5)\n",
      "    Line #3091 (got 1 columns instead of 5)\n",
      "    Line #3096 (got 1 columns instead of 5)\n",
      "    Line #3101 (got 1 columns instead of 5)\n",
      "    Line #3106 (got 1 columns instead of 5)\n",
      "    Line #3111 (got 1 columns instead of 5)\n",
      "    Line #3116 (got 1 columns instead of 5)\n",
      "    Line #3121 (got 1 columns instead of 5)\n",
      "    Line #3126 (got 1 columns instead of 5)\n",
      "    Line #3131 (got 1 columns instead of 5)\n",
      "    Line #3136 (got 1 columns instead of 5)\n",
      "    Line #3141 (got 1 columns instead of 5)\n",
      "    Line #3146 (got 1 columns instead of 5)\n",
      "    Line #3151 (got 1 columns instead of 5)\n",
      "    Line #3156 (got 1 columns instead of 5)\n",
      "    Line #3161 (got 1 columns instead of 5)\n",
      "    Line #3166 (got 1 columns instead of 5)\n",
      "    Line #3171 (got 1 columns instead of 5)\n",
      "    Line #3176 (got 1 columns instead of 5)\n",
      "    Line #3181 (got 1 columns instead of 5)\n",
      "    Line #3186 (got 1 columns instead of 5)\n",
      "    Line #3191 (got 1 columns instead of 5)\n",
      "    Line #3196 (got 1 columns instead of 5)\n",
      "    Line #3201 (got 1 columns instead of 5)\n",
      "    Line #3206 (got 1 columns instead of 5)\n",
      "    Line #3211 (got 1 columns instead of 5)\n",
      "    Line #3216 (got 1 columns instead of 5)\n",
      "    Line #3221 (got 1 columns instead of 5)\n",
      "    Line #3226 (got 1 columns instead of 5)\n",
      "    Line #3231 (got 1 columns instead of 5)\n",
      "    Line #3236 (got 1 columns instead of 5)\n",
      "    Line #3241 (got 1 columns instead of 5)\n",
      "    Line #3246 (got 1 columns instead of 5)\n",
      "    Line #3251 (got 1 columns instead of 5)\n",
      "    Line #3256 (got 1 columns instead of 5)\n",
      "    Line #3261 (got 1 columns instead of 5)\n",
      "    Line #3266 (got 1 columns instead of 5)\n",
      "    Line #3271 (got 1 columns instead of 5)\n",
      "    Line #3276 (got 1 columns instead of 5)\n",
      "    Line #3281 (got 1 columns instead of 5)\n",
      "    Line #3286 (got 1 columns instead of 5)\n",
      "    Line #3291 (got 1 columns instead of 5)\n",
      "    Line #3296 (got 1 columns instead of 5)\n",
      "    Line #3301 (got 1 columns instead of 5)\n",
      "    Line #3306 (got 1 columns instead of 5)\n",
      "    Line #3311 (got 1 columns instead of 5)\n",
      "    Line #3316 (got 1 columns instead of 5)\n",
      "    Line #3321 (got 1 columns instead of 5)\n",
      "    Line #3326 (got 1 columns instead of 5)\n",
      "    Line #3331 (got 1 columns instead of 5)\n",
      "    Line #3336 (got 1 columns instead of 5)\n",
      "    Line #3341 (got 1 columns instead of 5)\n",
      "    Line #3346 (got 1 columns instead of 5)\n",
      "    Line #3351 (got 1 columns instead of 5)\n",
      "    Line #3356 (got 1 columns instead of 5)\n",
      "    Line #3361 (got 1 columns instead of 5)\n",
      "    Line #3366 (got 1 columns instead of 5)\n",
      "    Line #3371 (got 1 columns instead of 5)\n",
      "    Line #3376 (got 1 columns instead of 5)\n",
      "    Line #3381 (got 1 columns instead of 5)\n",
      "    Line #3386 (got 1 columns instead of 5)\n",
      "    Line #3391 (got 1 columns instead of 5)\n",
      "    Line #3396 (got 1 columns instead of 5)\n",
      "    Line #3401 (got 1 columns instead of 5)\n",
      "    Line #3406 (got 1 columns instead of 5)\n",
      "    Line #3411 (got 1 columns instead of 5)\n",
      "    Line #3416 (got 1 columns instead of 5)\n",
      "    Line #3421 (got 1 columns instead of 5)\n",
      "    Line #3426 (got 1 columns instead of 5)\n",
      "    Line #3431 (got 1 columns instead of 5)\n",
      "    Line #3436 (got 1 columns instead of 5)\n",
      "    Line #3441 (got 1 columns instead of 5)\n",
      "    Line #3446 (got 1 columns instead of 5)\n",
      "    Line #3451 (got 1 columns instead of 5)\n",
      "    Line #3456 (got 1 columns instead of 5)\n",
      "    Line #3461 (got 1 columns instead of 5)\n",
      "    Line #3466 (got 1 columns instead of 5)\n",
      "    Line #3471 (got 1 columns instead of 5)\n",
      "    Line #3476 (got 1 columns instead of 5)\n",
      "    Line #3481 (got 1 columns instead of 5)\n",
      "    Line #3486 (got 1 columns instead of 5)\n",
      "    Line #3491 (got 1 columns instead of 5)\n",
      "    Line #3496 (got 1 columns instead of 5)\n",
      "    Line #3501 (got 1 columns instead of 5)\n",
      "    Line #3506 (got 1 columns instead of 5)\n",
      "    Line #3511 (got 1 columns instead of 5)\n",
      "    Line #3516 (got 1 columns instead of 5)\n",
      "    Line #3521 (got 1 columns instead of 5)\n",
      "    Line #3526 (got 1 columns instead of 5)\n",
      "    Line #3531 (got 1 columns instead of 5)\n",
      "    Line #3536 (got 1 columns instead of 5)\n",
      "    Line #3541 (got 1 columns instead of 5)\n",
      "    Line #3546 (got 1 columns instead of 5)\n",
      "    Line #3551 (got 1 columns instead of 5)\n",
      "    Line #3556 (got 1 columns instead of 5)\n",
      "    Line #3561 (got 1 columns instead of 5)\n",
      "    Line #3566 (got 1 columns instead of 5)\n",
      "    Line #3571 (got 1 columns instead of 5)\n",
      "    Line #3576 (got 1 columns instead of 5)\n",
      "    Line #3581 (got 1 columns instead of 5)\n",
      "    Line #3586 (got 1 columns instead of 5)\n",
      "    Line #3591 (got 1 columns instead of 5)\n",
      "    Line #3596 (got 1 columns instead of 5)\n",
      "    Line #3601 (got 1 columns instead of 5)\n",
      "    Line #3606 (got 1 columns instead of 5)\n",
      "    Line #3611 (got 1 columns instead of 5)\n",
      "    Line #3616 (got 1 columns instead of 5)\n",
      "    Line #3621 (got 1 columns instead of 5)\n",
      "    Line #3626 (got 1 columns instead of 5)\n",
      "    Line #3631 (got 1 columns instead of 5)\n",
      "    Line #3636 (got 1 columns instead of 5)\n",
      "    Line #3641 (got 1 columns instead of 5)\n",
      "    Line #3646 (got 1 columns instead of 5)\n",
      "    Line #3651 (got 1 columns instead of 5)\n",
      "    Line #3656 (got 1 columns instead of 5)\n",
      "    Line #3661 (got 1 columns instead of 5)\n",
      "    Line #3666 (got 1 columns instead of 5)\n",
      "    Line #3671 (got 1 columns instead of 5)\n",
      "    Line #3676 (got 1 columns instead of 5)\n",
      "    Line #3681 (got 1 columns instead of 5)\n",
      "    Line #3686 (got 1 columns instead of 5)\n",
      "    Line #3691 (got 1 columns instead of 5)\n",
      "    Line #3696 (got 1 columns instead of 5)\n",
      "    Line #3701 (got 1 columns instead of 5)\n",
      "    Line #3706 (got 1 columns instead of 5)\n",
      "    Line #3711 (got 1 columns instead of 5)\n",
      "    Line #3716 (got 1 columns instead of 5)\n",
      "    Line #3721 (got 1 columns instead of 5)\n",
      "    Line #3726 (got 1 columns instead of 5)\n",
      "    Line #3731 (got 1 columns instead of 5)\n",
      "    Line #3736 (got 1 columns instead of 5)\n",
      "    Line #3741 (got 1 columns instead of 5)\n",
      "    Line #3746 (got 1 columns instead of 5)\n",
      "    Line #3751 (got 1 columns instead of 5)\n",
      "    Line #3756 (got 1 columns instead of 5)\n",
      "    Line #3761 (got 1 columns instead of 5)\n",
      "    Line #3766 (got 1 columns instead of 5)\n",
      "    Line #3771 (got 1 columns instead of 5)\n",
      "    Line #3776 (got 1 columns instead of 5)\n",
      "    Line #3781 (got 1 columns instead of 5)\n",
      "    Line #3786 (got 1 columns instead of 5)\n",
      "    Line #3791 (got 1 columns instead of 5)\n",
      "    Line #3796 (got 1 columns instead of 5)\n",
      "    Line #3801 (got 1 columns instead of 5)\n",
      "    Line #3806 (got 1 columns instead of 5)\n",
      "    Line #3811 (got 1 columns instead of 5)\n",
      "    Line #3816 (got 1 columns instead of 5)\n",
      "    Line #3821 (got 1 columns instead of 5)\n",
      "    Line #3826 (got 1 columns instead of 5)\n",
      "    Line #3831 (got 1 columns instead of 5)\n",
      "    Line #3836 (got 1 columns instead of 5)\n",
      "    Line #3841 (got 1 columns instead of 5)\n",
      "    Line #3846 (got 1 columns instead of 5)\n",
      "    Line #3851 (got 1 columns instead of 5)\n",
      "    Line #3856 (got 1 columns instead of 5)\n",
      "    Line #3861 (got 1 columns instead of 5)\n",
      "    Line #3866 (got 1 columns instead of 5)\n",
      "    Line #3871 (got 1 columns instead of 5)\n",
      "    Line #3876 (got 1 columns instead of 5)\n",
      "    Line #3881 (got 1 columns instead of 5)\n",
      "    Line #3886 (got 1 columns instead of 5)\n",
      "    Line #3891 (got 1 columns instead of 5)\n",
      "    Line #3896 (got 1 columns instead of 5)\n",
      "    Line #3901 (got 1 columns instead of 5)\n",
      "    Line #3906 (got 1 columns instead of 5)\n",
      "    Line #3911 (got 1 columns instead of 5)\n",
      "    Line #3916 (got 1 columns instead of 5)\n",
      "    Line #3921 (got 1 columns instead of 5)\n",
      "    Line #3926 (got 1 columns instead of 5)\n",
      "    Line #3931 (got 1 columns instead of 5)\n",
      "    Line #3936 (got 1 columns instead of 5)\n",
      "    Line #3941 (got 1 columns instead of 5)\n",
      "    Line #3946 (got 1 columns instead of 5)\n",
      "    Line #3951 (got 1 columns instead of 5)\n",
      "    Line #3956 (got 1 columns instead of 5)\n",
      "    Line #3961 (got 1 columns instead of 5)\n",
      "    Line #3966 (got 1 columns instead of 5)\n",
      "    Line #3971 (got 1 columns instead of 5)\n",
      "    Line #3976 (got 1 columns instead of 5)\n",
      "    Line #3981 (got 1 columns instead of 5)\n",
      "    Line #3986 (got 1 columns instead of 5)\n",
      "    Line #3991 (got 1 columns instead of 5)\n",
      "    Line #3996 (got 1 columns instead of 5)\n",
      "    Line #4001 (got 1 columns instead of 5)\n",
      "    Line #4006 (got 1 columns instead of 5)\n",
      "    Line #4011 (got 1 columns instead of 5)\n",
      "    Line #4016 (got 1 columns instead of 5)\n",
      "    Line #4021 (got 1 columns instead of 5)\n",
      "    Line #4026 (got 1 columns instead of 5)\n",
      "    Line #4031 (got 1 columns instead of 5)\n",
      "    Line #4036 (got 1 columns instead of 5)\n",
      "    Line #4041 (got 1 columns instead of 5)\n",
      "    Line #4046 (got 1 columns instead of 5)\n",
      "    Line #4051 (got 1 columns instead of 5)\n",
      "    Line #4056 (got 1 columns instead of 5)\n",
      "    Line #4061 (got 1 columns instead of 5)\n",
      "    Line #4066 (got 1 columns instead of 5)\n",
      "    Line #4071 (got 1 columns instead of 5)\n",
      "    Line #4076 (got 1 columns instead of 5)\n",
      "    Line #4081 (got 1 columns instead of 5)\n",
      "    Line #4086 (got 1 columns instead of 5)\n",
      "    Line #4091 (got 1 columns instead of 5)\n",
      "    Line #4096 (got 1 columns instead of 5)\n",
      "    Line #4101 (got 1 columns instead of 5)\n",
      "    Line #4106 (got 1 columns instead of 5)\n",
      "    Line #4111 (got 1 columns instead of 5)\n",
      "    Line #4116 (got 1 columns instead of 5)\n",
      "    Line #4121 (got 1 columns instead of 5)\n",
      "    Line #4126 (got 1 columns instead of 5)\n",
      "    Line #4131 (got 1 columns instead of 5)\n",
      "    Line #4136 (got 1 columns instead of 5)\n",
      "    Line #4141 (got 1 columns instead of 5)\n",
      "    Line #4146 (got 1 columns instead of 5)\n",
      "    Line #4151 (got 1 columns instead of 5)\n",
      "    Line #4156 (got 1 columns instead of 5)\n",
      "    Line #4161 (got 1 columns instead of 5)\n",
      "    Line #4166 (got 1 columns instead of 5)\n",
      "    Line #4171 (got 1 columns instead of 5)\n",
      "    Line #4176 (got 1 columns instead of 5)\n",
      "    Line #4181 (got 1 columns instead of 5)\n",
      "    Line #4186 (got 1 columns instead of 5)\n",
      "    Line #4191 (got 1 columns instead of 5)\n",
      "    Line #4196 (got 1 columns instead of 5)\n",
      "    Line #4201 (got 1 columns instead of 5)\n",
      "    Line #4206 (got 1 columns instead of 5)\n",
      "    Line #4211 (got 1 columns instead of 5)\n",
      "    Line #4216 (got 1 columns instead of 5)\n",
      "    Line #4221 (got 1 columns instead of 5)\n",
      "    Line #4226 (got 1 columns instead of 5)\n",
      "    Line #4231 (got 1 columns instead of 5)\n",
      "    Line #4236 (got 1 columns instead of 5)\n",
      "    Line #4241 (got 1 columns instead of 5)\n",
      "    Line #4246 (got 1 columns instead of 5)\n",
      "    Line #4251 (got 1 columns instead of 5)\n",
      "    Line #4256 (got 1 columns instead of 5)\n",
      "    Line #4261 (got 1 columns instead of 5)\n",
      "    Line #4266 (got 1 columns instead of 5)\n",
      "    Line #4271 (got 1 columns instead of 5)\n",
      "    Line #4276 (got 1 columns instead of 5)\n",
      "    Line #4281 (got 1 columns instead of 5)\n",
      "    Line #4286 (got 1 columns instead of 5)\n",
      "    Line #4291 (got 1 columns instead of 5)\n",
      "    Line #4296 (got 1 columns instead of 5)\n",
      "    Line #4301 (got 1 columns instead of 5)\n",
      "    Line #4306 (got 1 columns instead of 5)\n",
      "    Line #4311 (got 1 columns instead of 5)\n",
      "    Line #4316 (got 1 columns instead of 5)\n",
      "    Line #4321 (got 1 columns instead of 5)\n",
      "    Line #4326 (got 1 columns instead of 5)\n",
      "    Line #4331 (got 1 columns instead of 5)\n",
      "    Line #4336 (got 1 columns instead of 5)\n",
      "    Line #4341 (got 1 columns instead of 5)\n",
      "    Line #4346 (got 1 columns instead of 5)\n",
      "    Line #4351 (got 1 columns instead of 5)\n",
      "    Line #4356 (got 1 columns instead of 5)\n",
      "    Line #4361 (got 1 columns instead of 5)\n",
      "    Line #4366 (got 1 columns instead of 5)\n",
      "    Line #4371 (got 1 columns instead of 5)\n",
      "    Line #4376 (got 1 columns instead of 5)\n",
      "    Line #4381 (got 1 columns instead of 5)\n",
      "    Line #4386 (got 1 columns instead of 5)\n",
      "    Line #4391 (got 1 columns instead of 5)\n",
      "    Line #4396 (got 1 columns instead of 5)\n",
      "    Line #4401 (got 1 columns instead of 5)\n",
      "    Line #4406 (got 1 columns instead of 5)\n",
      "    Line #4411 (got 1 columns instead of 5)\n",
      "    Line #4416 (got 1 columns instead of 5)\n",
      "    Line #4421 (got 1 columns instead of 5)\n",
      "    Line #4426 (got 1 columns instead of 5)\n",
      "    Line #4431 (got 1 columns instead of 5)\n",
      "    Line #4436 (got 1 columns instead of 5)\n",
      "    Line #4441 (got 1 columns instead of 5)\n",
      "    Line #4446 (got 1 columns instead of 5)\n",
      "    Line #4451 (got 1 columns instead of 5)\n",
      "    Line #4456 (got 1 columns instead of 5)\n",
      "    Line #4461 (got 1 columns instead of 5)\n",
      "    Line #4466 (got 1 columns instead of 5)\n",
      "    Line #4471 (got 1 columns instead of 5)\n",
      "    Line #4476 (got 1 columns instead of 5)\n",
      "    Line #4481 (got 1 columns instead of 5)\n",
      "    Line #4486 (got 1 columns instead of 5)\n",
      "    Line #4491 (got 1 columns instead of 5)\n",
      "    Line #4496 (got 1 columns instead of 5)\n",
      "    Line #4501 (got 1 columns instead of 5)\n",
      "    Line #4506 (got 1 columns instead of 5)\n",
      "    Line #4511 (got 1 columns instead of 5)\n",
      "    Line #4516 (got 1 columns instead of 5)\n",
      "    Line #4521 (got 1 columns instead of 5)\n",
      "    Line #4526 (got 1 columns instead of 5)\n",
      "    Line #4531 (got 1 columns instead of 5)\n",
      "    Line #4536 (got 1 columns instead of 5)\n",
      "    Line #4541 (got 1 columns instead of 5)\n",
      "    Line #4546 (got 1 columns instead of 5)\n",
      "    Line #4551 (got 1 columns instead of 5)\n",
      "    Line #4556 (got 1 columns instead of 5)\n",
      "    Line #4561 (got 1 columns instead of 5)\n",
      "    Line #4566 (got 1 columns instead of 5)\n",
      "    Line #4571 (got 1 columns instead of 5)\n",
      "    Line #4576 (got 1 columns instead of 5)\n",
      "    Line #4581 (got 1 columns instead of 5)\n",
      "    Line #4586 (got 1 columns instead of 5)\n",
      "    Line #4591 (got 1 columns instead of 5)\n",
      "    Line #4596 (got 1 columns instead of 5)\n",
      "    Line #4601 (got 1 columns instead of 5)\n",
      "    Line #4606 (got 1 columns instead of 5)\n",
      "    Line #4611 (got 1 columns instead of 5)\n",
      "    Line #4616 (got 1 columns instead of 5)\n",
      "    Line #4621 (got 1 columns instead of 5)\n",
      "    Line #4626 (got 1 columns instead of 5)\n",
      "    Line #4631 (got 1 columns instead of 5)\n",
      "    Line #4636 (got 1 columns instead of 5)\n",
      "    Line #4641 (got 1 columns instead of 5)\n",
      "    Line #4646 (got 1 columns instead of 5)\n",
      "    Line #4651 (got 1 columns instead of 5)\n",
      "    Line #4656 (got 1 columns instead of 5)\n",
      "    Line #4661 (got 1 columns instead of 5)\n",
      "    Line #4666 (got 1 columns instead of 5)\n",
      "    Line #4671 (got 1 columns instead of 5)\n",
      "    Line #4676 (got 1 columns instead of 5)\n",
      "    Line #4681 (got 1 columns instead of 5)\n",
      "    Line #4686 (got 1 columns instead of 5)\n",
      "    Line #4691 (got 1 columns instead of 5)\n",
      "    Line #4696 (got 1 columns instead of 5)\n",
      "    Line #4701 (got 1 columns instead of 5)\n",
      "    Line #4706 (got 1 columns instead of 5)\n",
      "    Line #4711 (got 1 columns instead of 5)\n",
      "    Line #4716 (got 1 columns instead of 5)\n",
      "    Line #4721 (got 1 columns instead of 5)\n",
      "    Line #4726 (got 1 columns instead of 5)\n",
      "    Line #4731 (got 1 columns instead of 5)\n",
      "    Line #4736 (got 1 columns instead of 5)\n",
      "    Line #4741 (got 1 columns instead of 5)\n",
      "    Line #4746 (got 1 columns instead of 5)\n",
      "    Line #4751 (got 1 columns instead of 5)\n",
      "    Line #4756 (got 1 columns instead of 5)\n",
      "    Line #4761 (got 1 columns instead of 5)\n",
      "    Line #4766 (got 1 columns instead of 5)\n",
      "    Line #4771 (got 1 columns instead of 5)\n",
      "    Line #4776 (got 1 columns instead of 5)\n",
      "    Line #4781 (got 1 columns instead of 5)\n",
      "    Line #4786 (got 1 columns instead of 5)\n",
      "    Line #4791 (got 1 columns instead of 5)\n",
      "    Line #4796 (got 1 columns instead of 5)\n",
      "    Line #4801 (got 1 columns instead of 5)\n",
      "    Line #4806 (got 1 columns instead of 5)\n",
      "    Line #4811 (got 1 columns instead of 5)\n",
      "    Line #4816 (got 1 columns instead of 5)\n",
      "    Line #4821 (got 1 columns instead of 5)\n",
      "    Line #4826 (got 1 columns instead of 5)\n",
      "    Line #4831 (got 1 columns instead of 5)\n",
      "    Line #4836 (got 1 columns instead of 5)\n",
      "    Line #4841 (got 1 columns instead of 5)\n",
      "    Line #4846 (got 1 columns instead of 5)\n",
      "    Line #4851 (got 1 columns instead of 5)\n",
      "    Line #4856 (got 1 columns instead of 5)\n",
      "    Line #4861 (got 1 columns instead of 5)\n",
      "    Line #4866 (got 1 columns instead of 5)\n",
      "    Line #4871 (got 1 columns instead of 5)\n",
      "    Line #4876 (got 1 columns instead of 5)\n",
      "    Line #4881 (got 1 columns instead of 5)\n",
      "    Line #4886 (got 1 columns instead of 5)\n",
      "    Line #4891 (got 1 columns instead of 5)\n",
      "    Line #4896 (got 1 columns instead of 5)\n",
      "    Line #4901 (got 1 columns instead of 5)\n",
      "    Line #4906 (got 1 columns instead of 5)\n",
      "    Line #4911 (got 1 columns instead of 5)\n",
      "    Line #4916 (got 1 columns instead of 5)\n",
      "    Line #4921 (got 1 columns instead of 5)\n",
      "    Line #4926 (got 1 columns instead of 5)\n",
      "    Line #4931 (got 1 columns instead of 5)\n",
      "    Line #4936 (got 1 columns instead of 5)\n",
      "    Line #4941 (got 1 columns instead of 5)\n",
      "    Line #4946 (got 1 columns instead of 5)\n",
      "    Line #4951 (got 1 columns instead of 5)\n",
      "    Line #4956 (got 1 columns instead of 5)\n",
      "    Line #4961 (got 1 columns instead of 5)\n",
      "    Line #4966 (got 1 columns instead of 5)\n",
      "    Line #4971 (got 1 columns instead of 5)\n",
      "    Line #4976 (got 1 columns instead of 5)\n",
      "    Line #4981 (got 1 columns instead of 5)\n",
      "    Line #4986 (got 1 columns instead of 5)\n",
      "    Line #4991 (got 1 columns instead of 5)\n",
      "    Line #4996 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_water = (xyz_file[:,1:-1])\n",
    "coordinates_water = coordinates_water.astype(np.float)\n",
    "atomic_numbers_water = (xyz_file[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1]\n",
      " [1]\n",
      " [8]\n",
      " ...\n",
      " [1]\n",
      " [1]\n",
      " [8]]\n",
      "(3000, 1)\n"
     ]
    }
   ],
   "source": [
    "atomic_numbers_water = atomic_numbers_water.astype(int)\n",
    "atomic_numbers_water = np.reshape(atomic_numbers_water,(len(coordinates_water),1))\n",
    "#atomic_numbers_water = torch.from_numpy(atomic_numbers_water)\n",
    "print(type(atomic_numbers_water))\n",
    "print(atomic_numbers_water)\n",
    "print(np.shape(atomic_numbers_water))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.769767    0.55746937]\n",
      " [ 0.         -0.71017975  0.50340914]\n",
      " [ 0.         -0.0037242  -0.06630491]\n",
      " ...\n",
      " [ 0.          0.81441381  0.59863567]\n",
      " [ 0.         -0.76145415  0.54978922]\n",
      " [ 0.         -0.00330998 -0.07177656]]\n",
      "(3000, 3)\n"
     ]
    }
   ],
   "source": [
    "print(coordinates_water)\n",
    "print(np.shape(coordinates_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.48093379 0.99367147]\n",
      " [1.48093379 0.         0.9075536 ]\n",
      " [0.99367147 0.9075536  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                     # i-th molecule\n",
    "N = 3                                     # N atoms per molecule\n",
    "coord = coordinates_water[N*i:N*(i+1),:]  # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                          # pairwise distances between all of its 3 atom\n",
    "    \n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "        for j in range(i+1,N):\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65865495 1.67618952 1.79537284]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = 3\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.19473151 5.24520024 5.28298916]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     \n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Rotation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix():\n",
    "    theta = np.arccos(2*np.random.uniform(low = 0,high = 1)-1)\n",
    "    phi = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    u = np.array([np.sin(theta)*np.cos(phi),np.sin(theta)*np.sin(phi),np.cos(theta)])\n",
    "    theta = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    A = np.zeros((3,3))\n",
    "    A[0][0] = np.cos(theta) + (u[0]**2)*(1-np.cos(theta))\n",
    "    A[0][1] = u[0]*u[1]*(1-np.cos(theta)) - u[2]*np.sin(theta)\n",
    "    A[0][2] = u[0]*u[2]*(1-np.cos(theta)) + u[1]*np.sin(theta)\n",
    "    A[1][0] = u[1]*u[0]*(1-np.cos(theta)) + u[2]*np.sin(theta)\n",
    "    A[1][1] = np.cos(theta) + (u[1]**2)*(1-np.cos(theta))\n",
    "    A[1][2] = u[1]*u[2]*(1-np.cos(theta)) - u[0]*np.sin(theta)\n",
    "    A[2][0] = u[2]*u[0]*(1-np.cos(theta)) - u[1]*np.sin(theta)\n",
    "    A[2][1] = u[2]*u[1]*(1-np.cos(theta)) + u[0]*np.sin(theta)\n",
    "    A[2][2] = np.cos(theta) + (u[2]**2)*(1-np.cos(theta))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly rotate molecules in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(A,data):\n",
    "    data = np.array(data)\n",
    "    m = np.shape(data)[1]\n",
    "    for i in range(m):\n",
    "        data[:,i] = np.matmul(A,data[:,i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-de07486920b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_size' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data_set(coordinates,N,number_of_features,data_size,training_set_size, \\\n",
    "                    use_atom_num_as_feat,atomic_numbers,heta,Rs,lambdaa,zeta):\n",
    "\n",
    "    #Randomly rotate each molecule in the data\n",
    "    rotated_molec_coord = np.zeros((np.shape(coordinates)))\n",
    "    for i in range(data_size):\n",
    "        coord = coordinates_water[N*i:N*(i+1),:]\n",
    "        coord = coord - coord[2,:]                   # move origin to oxygen for each molecule before rotating\n",
    "        coord = np.transpose(coord)\n",
    "        A = random_rotation_matrix()\n",
    "        rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "        \n",
    "    \n",
    "    G_rot = np.zeros((len(rotated_molec_coord), number_of_features))\n",
    "    print(np.shape(G_rot))\n",
    "    data_size = int(len(rotated_molec_coord)/N)\n",
    "    for i in range(data_size):\n",
    "        coord = rotated_molec_coord[N*i:N*(i+1),:]\n",
    "        Dp    = pairwise_distances(coord)\n",
    "        for j in range(0,number_of_features,2):\n",
    "            if j < number_of_features - 1: \n",
    "                G_rot[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j]) \n",
    "                G_rot[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "            else:\n",
    "                G_rot[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j]) \n",
    "                if number_of_features%2 == 0:\n",
    "                    G_rot[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "    # Computing variance and mean on the training data only!\n",
    "    G_rot_train = G_rot[:training_set_size,:]\n",
    "    var  = np.var(G_rot_train,axis=0)\n",
    "    mean = np.mean(G_rot_train,axis=0)\n",
    "\n",
    "    G_rot_norm = np.zeros((len(coordinates), number_of_features))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(G_rot)[0]):\n",
    "        for j in range(np.shape(G_rot)[1]):\n",
    "            G_rot_norm[i,j] = (G_rot[i,j]-mean[j])/var[j]   \n",
    "    \n",
    "    \n",
    "    if use_atom_num_as_feat == 1:\n",
    "        G_rot_norm = np.append(G_rot_norm, atomic_numbers, axis=1)           # Adding atomic number as a feature\n",
    "    \n",
    "\n",
    "\n",
    "    data_set_rot = np.vsplit(G_rot_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "    #data_set = np.random.permutation(training_set)\n",
    "    data_set_rot = torch.FloatTensor(data_set_rot)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "    \n",
    "    \n",
    "    # Splitting the dataset into training and test set\n",
    "    training_set_rot         = data_set_rot[:training_set_size]\n",
    "    test_set_rot             = data_set_rot[training_set_size:]\n",
    "    \n",
    "    return test_set_rot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_H2O                    = 3       # number of atoms per molecule\n",
    "# number_of_features_H2O   = 3       # number of features (symmetry functions) for each atom (we create one radial)\n",
    "#                                    # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "# use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "# data_size_H2O            = np.shape(energies_water)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(N,number_of_features,coordinates,energies,atomic_numbers, use_atom_num_as_feat,batches):\n",
    "    \n",
    "\n",
    "    # Randomly picking the parameters from within a range\n",
    "    heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "    random.shuffle(heta)\n",
    "\n",
    "    Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "    random.shuffle(Rs)\n",
    "\n",
    "    lambdaa = np.ones(number_of_features)\n",
    "    random.shuffle(lambdaa)\n",
    "\n",
    "    zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "    random.shuffle(zeta)\n",
    "\n",
    "    # A good set of parameters if we have 6 features    \n",
    "    if number_of_features == 6:\n",
    "        heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "        zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "        Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "        lambdaa = [1., 1., 1., 1., 1., 1.]\n",
    "        \n",
    "        \n",
    "    #Used to compare convergence for different numbers of features (we set them all to have the same values for a fair comparison)        \n",
    "    heta    = np.ones(20)\n",
    "    zeta    = np.ones(20)\n",
    "    Rs      = np.zeros(20)\n",
    "    lambdaa = np.ones(20)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "    test_set_size        = 100\n",
    "    training_set_size    = data_size - test_set_size\n",
    "\n",
    "\n",
    "    G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "    for i in range(data_size):\n",
    "        coord = coordinates[N*i:N*(i+1),:]\n",
    "        Dp    = pairwise_distances(coord)\n",
    "        for j in range(0,number_of_features,2):\n",
    "            if j < number_of_features - 1:      # for j = number_of_features compute either 1 or 2 symmetry functions depending on whether number_of_features is odd or even\n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "            else:                  \n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                if number_of_features % 2 == 0:    #i.e. if number of features is even\n",
    "                    G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "\n",
    "    \n",
    "    # Computing variance and mean on the training data only!\n",
    "    G_train = G[:training_set_size,:]\n",
    "    var  = np.var(G_train,axis=0)\n",
    "    mean = np.mean(G_train,axis=0)\n",
    "    print(mean)\n",
    "    G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "    print(np.shape(G))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(G)[0]):\n",
    "        for j in range(np.shape(G)[1]):\n",
    "            G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "        \n",
    "    print(type(G_norm))  \n",
    "    \n",
    "    if use_atom_num_as_feat == 1:\n",
    "        G_norm = np.append(G_norm, atomic_numbers, axis=1)    # Adding atomic number as a feature\n",
    "       \n",
    "        \n",
    "    data_set = np.vsplit(G_norm,data_size)                    # Going from a (3000,6) np.array to a (1000,3,6) list\n",
    "    #data_set = np.random.permutation(training_set)\n",
    "    data_set = torch.FloatTensor(data_set)                    # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "   \n",
    "    # print(data_set[0])\n",
    "    # print(data_set[0][1][1])\n",
    "\n",
    "    labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "    \n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "#     shuffler = np.random.permutation(len(labels))\n",
    "\n",
    "#     data_set = data_set[shuffler]\n",
    "\n",
    "#     labels = labels[shuffler]\n",
    "\n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Computing variance and mean on the training data only!\n",
    "    lab_train = labels[:training_set_size]\n",
    "    var_lab  = np.var(lab_train,axis=0)\n",
    "    mean_lab = np.mean(lab_train,axis=0)\n",
    "\n",
    "    labels_norm = np.zeros((np.shape(labels)))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(labels)[0]):\n",
    "        labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "    labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "    # Splitting the dataset into training and test set\n",
    "    training_set         = data_set[:training_set_size]\n",
    "    test_set             = data_set[training_set_size:]\n",
    "    test_set_rot         = rotate_data_set(coordinates,N,number_of_features,data_size,training_set_size, \\\n",
    "                                          use_atom_num_as_feat,atomic_numbers,heta,Rs,lambdaa,zeta)\n",
    "\n",
    "    train_labels         = labels_norm[:training_set_size]\n",
    "    train_labels         = torch.FloatTensor(train_labels)\n",
    "    test_labels          = labels_norm[training_set_size:]\n",
    "    test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = TensorDataset(training_set, train_labels)\n",
    "    #print(dataset[0])\n",
    "\n",
    "    # Creating the batches\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batches, #25, #300,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "    print(np.shape(training_set))\n",
    "    \n",
    "    return [training_set, test_set, train_labels, test_labels, dataloader,var_lab,mean_lab, test_set_rot,labels_norm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "#                       atomic_numbers_water,use_atom_num_as_feat, 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_set = data_water[0]\n",
    "# print('Training set:')\n",
    "# print(type(training_set))\n",
    "# print(np.shape(training_set))\n",
    "# test_set     = data_water[1]\n",
    "# print('\\n')\n",
    "# print('Test set:')\n",
    "# print(type(test_set))\n",
    "# print(np.shape(test_set))\n",
    "# train_labels = data_water[2]\n",
    "# print('\\n')\n",
    "# print('Training labels:')\n",
    "# print(type(train_labels))\n",
    "# print(np.shape(train_labels))\n",
    "# test_labels  = data_water[3]\n",
    "# print('\\n')\n",
    "# print('Test labels:')\n",
    "# print(type(test_labels))\n",
    "# print(np.shape(test_labels))\n",
    "# dataloader   = data_water[4]\n",
    "# print('\\n')\n",
    "# print('data_waterloader:')\n",
    "# print(type(dataloader))\n",
    "# print(np.shape(dataloader))\n",
    "\n",
    "# var_lab = data_water[5]\n",
    "# print('\\n')\n",
    "# print('Variance of labels:')\n",
    "# print(type(var_lab))\n",
    "# print(var_lab)\n",
    "\n",
    "# mean_lab = data_water[6]\n",
    "# print('\\n')\n",
    "# print('Mean value of labels:')\n",
    "# print(type(mean_lab))\n",
    "# print(mean_lab)\n",
    "\n",
    "# test_set_rot = data_water[7]\n",
    "# print('\\n')\n",
    "# print('Rotated test set:')\n",
    "# print(type(test_set_rot))\n",
    "# print(np.shape(test_set_rot))\n",
    "\n",
    "# labels_norm = data_water[8]\n",
    "# print('\\n')\n",
    "# print('Normalised labels:')\n",
    "# print(type(labels_norm))\n",
    "# print(np.shape(labels_norm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets_H2O, self).__init__()\n",
    "        num_hid_feat = math.ceil(number_of_features/2)\n",
    "        self.fc1 = nn.Linear(number_of_features, num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat, num_hid_feat)\n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN_H2O, self).__init__()\n",
    "        self.network1 = Subnets_H2O(number_of_features)\n",
    "        self.network2 = Subnets_H2O(number_of_features)\n",
    "        self.network3 = Subnets_H2O(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "# model = BPNN_H2O(3) #+1)                # +1 because he have added the atomic number as a feature\n",
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O)# +1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network1.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc3.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network2.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc3.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network3.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O+1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(learning_rate, nepochs,net,dataloader):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), learning_rate) \n",
    "    #torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    train_loss = np.zeros(nepochs)\n",
    "    test_loss = np.zeros(nepochs)\n",
    "\n",
    "    train_acc = np.zeros(nepochs)\n",
    "    test_acc = np.zeros(nepochs)\n",
    "\n",
    "    #===========================================================================\n",
    "    for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "    #===========================================================================\n",
    "    \n",
    "        running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "        epoch_train_loss = 0.0             \n",
    "        epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "        counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                                  # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                                  # be 36 elements in each batch.\n",
    "        #---------------------------------------\n",
    "        for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "        #---------------------------------------\n",
    "            inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                                  # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                                  # after each .step() call. \n",
    "\n",
    "            # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "            outputs = torch.zeros(np.shape(inputs)[0])\n",
    "            for j in range(np.shape(inputs)[0]):\n",
    "                outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "                # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "                # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "                # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "            loss.backward()                   # performs one back-propagation step \n",
    "            optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                               # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "            epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                               # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                               # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "            net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "            with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                              # the requires_grad flags to false, i.e. makes all the \n",
    "                                                              # operations in the block have no gradients\n",
    "                for k in range(np.shape(test_set)[0]):\n",
    "                       net_test_set[k] = net(test_set[k][0],test_set[k][1],test_set[k][2])\n",
    "                epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch           \n",
    "            if i % 10 == 2:    # print average loss every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "            counter += 1\n",
    "            #------------------------------------       \n",
    "        # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "        train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                       # to obtain average training loss for each epoch\n",
    "        test_loss[epoch] = epoch_test_loss/counter\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_test_loss = 0.0\n",
    "\n",
    "    #=================================================================================\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57507889 0.45271239 0.57507889 0.45271239 0.57507889 0.45271239]\n",
      "(3000, 6)\n",
      "<class 'numpy.ndarray'>\n",
      "(3000, 6)\n",
      "torch.Size([900, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Training for 3 Behler and Parinello features, batch size = 300 \n",
    "\n",
    "N_H2O                    = 3       # number of atoms per molecule\n",
    "number_of_features_H2O   = 6 #3       # number of features (radial and angular symmetry functions) to be used\n",
    "use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "data_size_H2O            = np.shape(energies_water)[0]\n",
    "batch_size               = 25 #300 \n",
    "test_set_size = 100\n",
    "\n",
    "\n",
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "training_set = data_water[0];     test_set     = data_water[1];\n",
    "train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "labels_norm = data_water[8];\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and initialise the model\n",
    "net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     3] loss: 0.08185\n",
      "[1,    13] loss: 0.25329\n",
      "[1,    23] loss: 0.25480\n",
      "[1,    33] loss: 0.22142\n",
      "[2,     3] loss: 0.07305\n",
      "[2,    13] loss: 0.21628\n",
      "[2,    23] loss: 0.20118\n",
      "[2,    33] loss: 0.19860\n",
      "[3,     3] loss: 0.05907\n",
      "[3,    13] loss: 0.18465\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-31eae424b15b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mtrain_loss_G_3_feat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtest_loss_G_3_feat\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-d09cb38f3e2b>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(learning_rate, nepochs, net, dataloader)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The net is designed to take a (3 x num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;31m# tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0;31m# output vector with as many elements as the batch size. If our net was designed to take one row as input we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-d13962d519c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, x3, train)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-d13962d519c5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Apply a tanh activation on fully connected layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# Using a linear function (identity function) for the subnet output layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train for nepochs using learning_rate = 0.0001\n",
    "nepochs = 20 #1500\n",
    "learning_rate = 0.0001 #0.005\n",
    "\n",
    "\n",
    "losses = training(learning_rate , nepochs, net, dataloader)\n",
    "train_loss_G_3_feat = losses[0]\n",
    "test_loss_G_3_feat  = losses[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss functions\n",
    "def plot_train_test_loss(train_loss, test_loss, nepochs, num_feat_G):\n",
    "    x = np.arange(1,nepochs+1)\n",
    "\n",
    "    plt.plot(x,train_loss,'blue',label = 'Training loss')\n",
    "    plt.plot(x,test_loss,'red',label = 'Test loss')\n",
    "\n",
    "    #plt.ylim([0,0.075])\n",
    "\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.xlabel('Epoch',fontsize=14)\n",
    "    plt.ylabel('Loss function',fontsize=14)\n",
    "    plt.title('Average loss function per epoch for $H_2O$ training and test set',fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('loss_graph_H2O_G_{0}'.format(num_feat_G),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_train_test_loss(train_loss_G_3_feat,test_loss_G_3_feat,nepochs,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.46727288 -0.45966285 -0.08840507 -0.25618482 -0.36480796  0.04455274\n",
      "  0.15641296  0.06448114  0.13132173  0.11771655 -0.50033689 -0.3126415\n",
      " -0.08109719 -0.45301342 -0.39152193 -0.09833252 -0.11328721  0.08724988\n",
      " -0.17165327 -0.61469907  0.40145975  0.1720075  -0.07618159  0.08093882\n",
      " -0.48187989  0.09771156 -0.5329845   0.08378696 -0.22846842  0.09708881\n",
      "  0.2875042   0.20740533  0.52269053 -0.31283808 -0.02923548 -0.29756975\n",
      " -0.32358575 -0.16255563  0.23877105 -0.42299706 -0.15753675 -0.48038602\n",
      "  0.54949307 -0.09815669 -0.26158005  0.34449604 -0.32665896  0.06545138\n",
      "  0.01785213  0.11552042  0.05106241 -0.14311135 -0.14755046 -0.1575439\n",
      " -0.02029514  0.12049377 -0.41778868  0.24319392 -0.1644783  -0.54436499\n",
      "  0.06445098 -0.19770992  0.20177218 -0.56904763  0.0371924   0.13542807\n",
      " -0.18139982  0.30230325  0.70709324 -0.31025326 -0.11287522 -0.00447863\n",
      "  0.27288353  0.56428981  0.44275641  0.16900864 -0.02722347 -0.14426637\n",
      " -0.49623823 -0.39662731  0.04576743 -0.20238292  0.02211976  0.03153932\n",
      " -0.211456   -0.21255672 -0.08254653 -0.32685119  0.51225686 -0.44696802\n",
      "  0.26831758 -0.23288965 -0.56189787 -0.40955848 -0.43158472  0.61445403\n",
      " -0.06989193 -0.09721553 -0.00071645 -0.10118848]\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "    \n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900])\n",
      "-2.6490954e-09\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n"
     ]
    }
   ],
   "source": [
    "print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_prediction(prediction,test_labels,var_lab,mean_lab, number_of_features_H2O):\n",
    "    prediction = torch.tensor(prediction)\n",
    "\n",
    "    x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "    print(min(torch.cat((test_labels,prediction),0)))\n",
    "    y = x\n",
    "    plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "    plt.plot(x,y, color='red',label = 'y=x')\n",
    "    plt.grid()\n",
    "    #plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "    #plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "    #plt.ylim([-13822,-13800])\n",
    "    #plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    #plt.tick_params(axis='both',labelsize=14)\n",
    "    plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "    plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "    plt.title('Actual vs Predicted energy value for $H_2O$ molecules',fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig('predicted_energies_H2O_G_{0}'.format(number_of_features_H2O),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6147, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0225, -0.0837,  0.4186, -0.1038, -0.2901, -0.0561],\n",
      "        [ 0.6530, -0.0961,  0.6532, -0.0044,  0.3303, -0.3511],\n",
      "        [ 0.2893, -0.1791, -0.1163, -0.1706,  0.0865, -0.0165]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(net.network1.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training for 6 Behler and Parinello features, batch size = 300 \n",
    "\n",
    "number_of_features_H2O   = 6       # number of features (radial and angular symmetry functions) to be used\n",
    "batch_size               = 300 \n",
    "\n",
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "training_set = data_water[0];     test_set     = data_water[1];\n",
    "train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "labels_norm = data_water[8];\n",
    "\n",
    "# Create and initialise the model\n",
    "net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature\n",
    "\n",
    "# # Train for  nepochs using learning_rate = 0.0001\n",
    "# nepochs = 2000\n",
    "# learning_rate=0.0001\n",
    "\n",
    "losses = training(learning_rate , nepochs, net, dataloader)\n",
    "train_loss_G_6_feat = losses[0]\n",
    "test_loss_G_6_feat  = losses[1]\n",
    "\n",
    "\n",
    "plot_train_test_loss(train_loss_G_6_feat,test_loss_G_6_feat,nepochs,number_of_features_H2O)\n",
    "\n",
    "\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n",
    "print(mean_lab)\n",
    "\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training for 9 Behler and Parinello features, batch size = 300 \n",
    "\n",
    "number_of_features_H2O   = 9       # number of features (radial and angular symmetry functions) to be used\n",
    "batch_size               = 300 \n",
    "\n",
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "training_set = data_water[0];     test_set     = data_water[1];\n",
    "train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "labels_norm = data_water[8];\n",
    "\n",
    "# Create and initialise the model\n",
    "net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature\n",
    "\n",
    "# Train for  nepochs using learning_rate = 0.0001\n",
    "# nepochs = 20000\n",
    "# learning_rate=0.0001\n",
    "\n",
    "losses = training(learning_rate , nepochs, net9, dataloader)\n",
    "train_loss_G_9_feat = losses[0]\n",
    "test_loss_G_9_feat  = losses[1]\n",
    "\n",
    "\n",
    "plot_train_test_loss(train_loss_G_9_feat,test_loss_G_9_feat,nepochs,number_of_features_H2O)\n",
    "prediction = np.zeros(test_set_size)\n",
    "\n",
    "\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n",
    "print(mean_lab)\n",
    "\n",
    "\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 12 Behler and Parinello features, batch size = 300 \n",
    "\n",
    "number_of_features_H2O   = 12       # number of features (radial and angular symmetry functions) to be used\n",
    "batch_size               = 300 \n",
    "\n",
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "training_set = data_water[0];     test_set     = data_water[1];\n",
    "train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "labels_norm = data_water[8];\n",
    "\n",
    "# Create and initialise the model\n",
    "net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature\n",
    "\n",
    "# Train for  nepochs using learning_rate = 0.0001\n",
    "# nepochs = 20000\n",
    "# learning_rate=0.0001\n",
    "\n",
    "losses = training(learning_rate , nepochs, net12, dataloader)\n",
    "train_loss_G_12_feat = losses[0]\n",
    "test_loss_G_12_feat  = losses[1]\n",
    "\n",
    "\n",
    "plot_train_test_loss(train_loss_G_12_feat,test_loss_G_12_feat,nepochs,number_of_features_H2O)\n",
    "prediction = np.zeros(test_set_size)\n",
    "\n",
    "\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n",
    "print(mean_lab)\n",
    "\n",
    "\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing convergence plots for different numbers of features\n",
    "x = np.arange(1,nepochs+1)\n",
    "\n",
    "plt.plot(x[:2000],train_loss_G_3_feat[:2000],'blue',label = '3 features')\n",
    "plt.plot(x[:2000],train_loss_G_6_feat[:2000],'red',label = '6 features')\n",
    "plt.plot(x[:2000],train_loss_G_9_feat[:2000],'green',label = '9 features')\n",
    "plt.plot(x[:2000],train_loss_G_12_feat[:2000],'orange',label = '12 features')\n",
    "\n",
    "#plt.ylim([0,0.075])\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Training loss function',fontsize=14)\n",
    "plt.title('Convergence plot for different numbers of features',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('loss_H2O_compar_diff_num_feat_BP_symm',bbox_inches='tight')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotating test set molecules and checking performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.6147, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_rotated = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_rot[i]\n",
    "    prediction_rotated[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "\n",
    "prediction_rotated = torch.tensor(prediction_rotated)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_rotated),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set',markersize=8)\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_rotated*var_lab+mean_lab, '*', color='yellow', label = 'Rotated test set')\n",
    "\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for rotated $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('rotated_predicted_energies_H2O_G_feat',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "N                        = 3         # number of atoms per molecule\n",
    "number_of_features_xyz   = 3         # number of features for each atom\n",
    "                                  \n",
    "training_set_size    = data_size_H2O - test_set_size\n",
    "\n",
    "#################################################################################\n",
    "for i in range(np.shape(energies_water)[0]):  # Moving all oxygens to the origin!!!!!!\n",
    "    coord = coordinates_water[N*i:N*(i+1),:]\n",
    "    coord = coord - coord[2,:]                # moving the origin to the oxygen\n",
    "    coordinates_water[N*i:N*(i+1),:] = coord\n",
    "#################################################################################\n",
    "    \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates_water[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(coordinates_water), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates_water)[0]):\n",
    "    for j in range(1,np.shape(coordinates_water)[1]):  # omit first column since for our dataset x=0 always\n",
    "        coordinates_water_norm[i,j] = (coordinates_water[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "data_set_xyz = np.vsplit(coordinates_water_norm,data_size_H2O)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "#################################################################################\n",
    "#Randomly rotating each molecule in the dataset around its oxygen\n",
    "rotated_training_set_xyz= np.zeros((np.shape(training_set_xyz)))\n",
    "for i in range(training_set_size):\n",
    "    coord = training_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_training_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_training_set_xyz))\n",
    "rotated_training_set_xyz = torch.FloatTensor(rotated_training_set_xyz)\n",
    "training_set_xyz = rotated_training_set_xyz\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=300, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training new neural net on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xyz = BPNN_H2O(3)    # 3 features (x,y and z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.2476308107\n",
      "[2,     1] loss: 0.2077746630\n",
      "[3,     1] loss: 0.1727546811\n",
      "[4,     1] loss: 0.1423677444\n",
      "[5,     1] loss: 0.1163555026\n",
      "[6,     1] loss: 0.0943756282\n",
      "[7,     1] loss: 0.0760682583\n",
      "[8,     1] loss: 0.0610760093\n",
      "[9,     1] loss: 0.0490364134\n",
      "[10,     1] loss: 0.0395802826\n",
      "[11,     1] loss: 0.0323382020\n",
      "[12,     1] loss: 0.0269487113\n",
      "[13,     1] loss: 0.0230655417\n",
      "[14,     1] loss: 0.0203642860\n",
      "[15,     1] loss: 0.0185493276\n",
      "[16,     1] loss: 0.0173612043\n",
      "[17,     1] loss: 0.0165836990\n",
      "[18,     1] loss: 0.0160489574\n",
      "[19,     1] loss: 0.0156386137\n",
      "[20,     1] loss: 0.0152796030\n",
      "[21,     1] loss: 0.0149354309\n",
      "[22,     1] loss: 0.0145949095\n",
      "[23,     1] loss: 0.0142611817\n",
      "[24,     1] loss: 0.0139429137\n",
      "[25,     1] loss: 0.0136485711\n",
      "[26,     1] loss: 0.0133835569\n",
      "[27,     1] loss: 0.0131495774\n",
      "[28,     1] loss: 0.0129452333\n",
      "[29,     1] loss: 0.0127671987\n",
      "[30,     1] loss: 0.0126113594\n",
      "[31,     1] loss: 0.0124736801\n",
      "[32,     1] loss: 0.0123507261\n",
      "[33,     1] loss: 0.0122398660\n",
      "[34,     1] loss: 0.0121392228\n",
      "[35,     1] loss: 0.0120475285\n",
      "[36,     1] loss: 0.0119639285\n",
      "[37,     1] loss: 0.0118877910\n",
      "[38,     1] loss: 0.0118185803\n",
      "[39,     1] loss: 0.0117557831\n",
      "[40,     1] loss: 0.0116988756\n",
      "[41,     1] loss: 0.0116473287\n",
      "[42,     1] loss: 0.0116006307\n",
      "[43,     1] loss: 0.0115583003\n",
      "[44,     1] loss: 0.0115199111\n",
      "[45,     1] loss: 0.0114850998\n",
      "[46,     1] loss: 0.0114535443\n",
      "[47,     1] loss: 0.0114249788\n",
      "[48,     1] loss: 0.0113991648\n",
      "[49,     1] loss: 0.0113758840\n",
      "[50,     1] loss: 0.0113549374\n",
      "[51,     1] loss: 0.0113361292\n",
      "[52,     1] loss: 0.0113192737\n",
      "[53,     1] loss: 0.0113041945\n",
      "[54,     1] loss: 0.0112907179\n",
      "[55,     1] loss: 0.0112786837\n",
      "[56,     1] loss: 0.0112679467\n",
      "[57,     1] loss: 0.0112583660\n",
      "[58,     1] loss: 0.0112498157\n",
      "[59,     1] loss: 0.0112421811\n",
      "[60,     1] loss: 0.0112353608\n",
      "[61,     1] loss: 0.0112292647\n",
      "[62,     1] loss: 0.0112238079\n",
      "[63,     1] loss: 0.0112189189\n",
      "[64,     1] loss: 0.0112145402\n",
      "[65,     1] loss: 0.0112106107\n",
      "[66,     1] loss: 0.0112070858\n",
      "[67,     1] loss: 0.0112039246\n",
      "[68,     1] loss: 0.0112010881\n",
      "[69,     1] loss: 0.0111985490\n",
      "[70,     1] loss: 0.0111962736\n",
      "[71,     1] loss: 0.0111942418\n",
      "[72,     1] loss: 0.0111924276\n",
      "[73,     1] loss: 0.0111908108\n",
      "[74,     1] loss: 0.0111893743\n",
      "[75,     1] loss: 0.0111881003\n",
      "[76,     1] loss: 0.0111869745\n",
      "[77,     1] loss: 0.0111859828\n",
      "[78,     1] loss: 0.0111851096\n",
      "[79,     1] loss: 0.0111843452\n",
      "[80,     1] loss: 0.0111836776\n",
      "[81,     1] loss: 0.0111830994\n",
      "[82,     1] loss: 0.0111826010\n",
      "[83,     1] loss: 0.0111821763\n",
      "[84,     1] loss: 0.0111818150\n",
      "[85,     1] loss: 0.0111815169\n",
      "[86,     1] loss: 0.0111812733\n",
      "[87,     1] loss: 0.0111810796\n",
      "[88,     1] loss: 0.0111809336\n",
      "[89,     1] loss: 0.0111808293\n",
      "[90,     1] loss: 0.0111807644\n",
      "[91,     1] loss: 0.0111807406\n",
      "[92,     1] loss: 0.0111807480\n",
      "[93,     1] loss: 0.0111807875\n",
      "[94,     1] loss: 0.0111808583\n",
      "[95,     1] loss: 0.0111809582\n",
      "[96,     1] loss: 0.0111810811\n",
      "[97,     1] loss: 0.0111812301\n",
      "[98,     1] loss: 0.0111814015\n",
      "[99,     1] loss: 0.0111815952\n",
      "[100,     1] loss: 0.0111818075\n",
      "[101,     1] loss: 0.0111820400\n",
      "[102,     1] loss: 0.0111822903\n",
      "[103,     1] loss: 0.0111825563\n",
      "[104,     1] loss: 0.0111828357\n",
      "[105,     1] loss: 0.0111831307\n",
      "[106,     1] loss: 0.0111834414\n",
      "[107,     1] loss: 0.0111837618\n",
      "[108,     1] loss: 0.0111840919\n",
      "[109,     1] loss: 0.0111844368\n",
      "[110,     1] loss: 0.0111847878\n",
      "[111,     1] loss: 0.0111851491\n",
      "[112,     1] loss: 0.0111855179\n",
      "[113,     1] loss: 0.0111858927\n",
      "[114,     1] loss: 0.0111862741\n",
      "[115,     1] loss: 0.0111866623\n",
      "[116,     1] loss: 0.0111870527\n",
      "[117,     1] loss: 0.0111874491\n",
      "[118,     1] loss: 0.0111878484\n",
      "[119,     1] loss: 0.0111882493\n",
      "[120,     1] loss: 0.0111886524\n",
      "[121,     1] loss: 0.0111890562\n",
      "[122,     1] loss: 0.0111894608\n",
      "[123,     1] loss: 0.0111898638\n",
      "[124,     1] loss: 0.0111902669\n",
      "[125,     1] loss: 0.0111906685\n",
      "[126,     1] loss: 0.0111910678\n",
      "[127,     1] loss: 0.0111914657\n",
      "[128,     1] loss: 0.0111918591\n",
      "[129,     1] loss: 0.0111922495\n",
      "[130,     1] loss: 0.0111926369\n",
      "[131,     1] loss: 0.0111930162\n",
      "[132,     1] loss: 0.0111933939\n",
      "[133,     1] loss: 0.0111937627\n",
      "[134,     1] loss: 0.0111941271\n",
      "[135,     1] loss: 0.0111944862\n",
      "[136,     1] loss: 0.0111948371\n",
      "[137,     1] loss: 0.0111951806\n",
      "[138,     1] loss: 0.0111955173\n",
      "[139,     1] loss: 0.0111958452\n",
      "[140,     1] loss: 0.0111961655\n",
      "[141,     1] loss: 0.0111964762\n",
      "[142,     1] loss: 0.0111967802\n",
      "[143,     1] loss: 0.0111970738\n",
      "[144,     1] loss: 0.0111973584\n",
      "[145,     1] loss: 0.0111976318\n",
      "[146,     1] loss: 0.0111978978\n",
      "[147,     1] loss: 0.0111981533\n",
      "[148,     1] loss: 0.0111983962\n",
      "[149,     1] loss: 0.0111986309\n",
      "[150,     1] loss: 0.0111988567\n",
      "[151,     1] loss: 0.0111990713\n",
      "[152,     1] loss: 0.0111992747\n",
      "[153,     1] loss: 0.0111994676\n",
      "[154,     1] loss: 0.0111996509\n",
      "[155,     1] loss: 0.0111998238\n",
      "[156,     1] loss: 0.0111999869\n",
      "[157,     1] loss: 0.0112001382\n",
      "[158,     1] loss: 0.0112002805\n",
      "[159,     1] loss: 0.0112004116\n",
      "[160,     1] loss: 0.0112005346\n",
      "[161,     1] loss: 0.0112006471\n",
      "[162,     1] loss: 0.0112007484\n",
      "[163,     1] loss: 0.0112008423\n",
      "[164,     1] loss: 0.0112009265\n",
      "[165,     1] loss: 0.0112010017\n",
      "[166,     1] loss: 0.0112010688\n",
      "[167,     1] loss: 0.0112011284\n",
      "[168,     1] loss: 0.0112011798\n",
      "[169,     1] loss: 0.0112012237\n",
      "[170,     1] loss: 0.0112012625\n",
      "[171,     1] loss: 0.0112012915\n",
      "[172,     1] loss: 0.0112013154\n",
      "[173,     1] loss: 0.0112013347\n",
      "[174,     1] loss: 0.0112013474\n",
      "[175,     1] loss: 0.0112013549\n",
      "[176,     1] loss: 0.0112013601\n",
      "[177,     1] loss: 0.0112013601\n",
      "[178,     1] loss: 0.0112013549\n",
      "[179,     1] loss: 0.0112013489\n",
      "[180,     1] loss: 0.0112013400\n",
      "[181,     1] loss: 0.0112013280\n",
      "[182,     1] loss: 0.0112013154\n",
      "[183,     1] loss: 0.0112013012\n",
      "[184,     1] loss: 0.0112012848\n",
      "[185,     1] loss: 0.0112012684\n",
      "[186,     1] loss: 0.0112012543\n",
      "[187,     1] loss: 0.0112012379\n",
      "[188,     1] loss: 0.0112012252\n",
      "[189,     1] loss: 0.0112012103\n",
      "[190,     1] loss: 0.0112011984\n",
      "[191,     1] loss: 0.0112011887\n",
      "[192,     1] loss: 0.0112011805\n",
      "[193,     1] loss: 0.0112011746\n",
      "[194,     1] loss: 0.0112011716\n",
      "[195,     1] loss: 0.0112011716\n",
      "[196,     1] loss: 0.0112011746\n",
      "[197,     1] loss: 0.0112011783\n",
      "[198,     1] loss: 0.0112011872\n",
      "[199,     1] loss: 0.0112011984\n",
      "[200,     1] loss: 0.0112012148\n",
      "[201,     1] loss: 0.0112012342\n",
      "[202,     1] loss: 0.0112012558\n",
      "[203,     1] loss: 0.0112012826\n",
      "[204,     1] loss: 0.0112013116\n",
      "[205,     1] loss: 0.0112013459\n",
      "[206,     1] loss: 0.0112013832\n",
      "[207,     1] loss: 0.0112014249\n",
      "[208,     1] loss: 0.0112014681\n",
      "[209,     1] loss: 0.0112015165\n",
      "[210,     1] loss: 0.0112015687\n",
      "[211,     1] loss: 0.0112016231\n",
      "[212,     1] loss: 0.0112016805\n",
      "[213,     1] loss: 0.0112017430\n",
      "[214,     1] loss: 0.0112018064\n",
      "[215,     1] loss: 0.0112018727\n",
      "[216,     1] loss: 0.0112019427\n",
      "[217,     1] loss: 0.0112020165\n",
      "[218,     1] loss: 0.0112020910\n",
      "[219,     1] loss: 0.0112021685\n",
      "[220,     1] loss: 0.0112022489\n",
      "[221,     1] loss: 0.0112023301\n",
      "[222,     1] loss: 0.0112024166\n",
      "[223,     1] loss: 0.0112025023\n",
      "[224,     1] loss: 0.0112025909\n",
      "[225,     1] loss: 0.0112026811\n",
      "[226,     1] loss: 0.0112027727\n",
      "[227,     1] loss: 0.0112028658\n",
      "[228,     1] loss: 0.0112029620\n",
      "[229,     1] loss: 0.0112030573\n",
      "[230,     1] loss: 0.0112031557\n",
      "[231,     1] loss: 0.0112032533\n",
      "[232,     1] loss: 0.0112033539\n",
      "[233,     1] loss: 0.0112034544\n",
      "[234,     1] loss: 0.0112035558\n",
      "[235,     1] loss: 0.0112036593\n",
      "[236,     1] loss: 0.0112037636\n",
      "[237,     1] loss: 0.0112038679\n",
      "[238,     1] loss: 0.0112039715\n",
      "[239,     1] loss: 0.0112040773\n",
      "[240,     1] loss: 0.0112041838\n",
      "[241,     1] loss: 0.0112042911\n",
      "[242,     1] loss: 0.0112043977\n",
      "[243,     1] loss: 0.0112045050\n",
      "[244,     1] loss: 0.0112046115\n",
      "[245,     1] loss: 0.0112047195\n",
      "[246,     1] loss: 0.0112048276\n",
      "[247,     1] loss: 0.0112049356\n",
      "[248,     1] loss: 0.0112050436\n",
      "[249,     1] loss: 0.0112051517\n",
      "[250,     1] loss: 0.0112052597\n",
      "[251,     1] loss: 0.0112053677\n",
      "[252,     1] loss: 0.0112054758\n",
      "[253,     1] loss: 0.0112055846\n",
      "[254,     1] loss: 0.0112056911\n",
      "[255,     1] loss: 0.0112057976\n",
      "[256,     1] loss: 0.0112059049\n",
      "[257,     1] loss: 0.0112060115\n",
      "[258,     1] loss: 0.0112061180\n",
      "[259,     1] loss: 0.0112062223\n",
      "[260,     1] loss: 0.0112063281\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[261,     1] loss: 0.0112064339\n",
      "[262,     1] loss: 0.0112065367\n",
      "[263,     1] loss: 0.0112066410\n",
      "[264,     1] loss: 0.0112067439\n",
      "[265,     1] loss: 0.0112068459\n",
      "[266,     1] loss: 0.0112069473\n",
      "[267,     1] loss: 0.0112070464\n",
      "[268,     1] loss: 0.0112071469\n",
      "[269,     1] loss: 0.0112072445\n",
      "[270,     1] loss: 0.0112073414\n",
      "[271,     1] loss: 0.0112074398\n",
      "[272,     1] loss: 0.0112075351\n",
      "[273,     1] loss: 0.0112076305\n",
      "[274,     1] loss: 0.0112077229\n",
      "[275,     1] loss: 0.0112078145\n",
      "[276,     1] loss: 0.0112079076\n",
      "[277,     1] loss: 0.0112079963\n",
      "[278,     1] loss: 0.0112080865\n",
      "[279,     1] loss: 0.0112081729\n",
      "[280,     1] loss: 0.0112082593\n",
      "[281,     1] loss: 0.0112083457\n",
      "[282,     1] loss: 0.0112084277\n",
      "[283,     1] loss: 0.0112085104\n",
      "[284,     1] loss: 0.0112085916\n",
      "[285,     1] loss: 0.0112086706\n",
      "[286,     1] loss: 0.0112087481\n",
      "[287,     1] loss: 0.0112088226\n",
      "[288,     1] loss: 0.0112088978\n",
      "[289,     1] loss: 0.0112089701\n",
      "[290,     1] loss: 0.0112090416\n",
      "[291,     1] loss: 0.0112091117\n",
      "[292,     1] loss: 0.0112091802\n",
      "[293,     1] loss: 0.0112092450\n",
      "[294,     1] loss: 0.0112093084\n",
      "[295,     1] loss: 0.0112093709\n",
      "[296,     1] loss: 0.0112094320\n",
      "[297,     1] loss: 0.0112094916\n",
      "[298,     1] loss: 0.0112095490\n",
      "[299,     1] loss: 0.0112096027\n",
      "[300,     1] loss: 0.0112096556\n",
      "[301,     1] loss: 0.0112097055\n",
      "[302,     1] loss: 0.0112097546\n",
      "[303,     1] loss: 0.0112098031\n",
      "[304,     1] loss: 0.0112098478\n",
      "[305,     1] loss: 0.0112098910\n",
      "[306,     1] loss: 0.0112099305\n",
      "[307,     1] loss: 0.0112099700\n",
      "[308,     1] loss: 0.0112100065\n",
      "[309,     1] loss: 0.0112100400\n",
      "[310,     1] loss: 0.0112100713\n",
      "[311,     1] loss: 0.0112101018\n",
      "[312,     1] loss: 0.0112101302\n",
      "[313,     1] loss: 0.0112101540\n",
      "[314,     1] loss: 0.0112101756\n",
      "[315,     1] loss: 0.0112101972\n",
      "[316,     1] loss: 0.0112102151\n",
      "[317,     1] loss: 0.0112102292\n",
      "[318,     1] loss: 0.0112102434\n",
      "[319,     1] loss: 0.0112102523\n",
      "[320,     1] loss: 0.0112102613\n",
      "[321,     1] loss: 0.0112102658\n",
      "[322,     1] loss: 0.0112102687\n",
      "[323,     1] loss: 0.0112102687\n",
      "[324,     1] loss: 0.0112102650\n",
      "[325,     1] loss: 0.0112102613\n",
      "[326,     1] loss: 0.0112102523\n",
      "[327,     1] loss: 0.0112102404\n",
      "[328,     1] loss: 0.0112102270\n",
      "[329,     1] loss: 0.0112102091\n",
      "[330,     1] loss: 0.0112101912\n",
      "[331,     1] loss: 0.0112101667\n",
      "[332,     1] loss: 0.0112101413\n",
      "[333,     1] loss: 0.0112101123\n",
      "[334,     1] loss: 0.0112100795\n",
      "[335,     1] loss: 0.0112100437\n",
      "[336,     1] loss: 0.0112100065\n",
      "[337,     1] loss: 0.0112099633\n",
      "[338,     1] loss: 0.0112099193\n",
      "[339,     1] loss: 0.0112098694\n",
      "[340,     1] loss: 0.0112098172\n",
      "[341,     1] loss: 0.0112097628\n",
      "[342,     1] loss: 0.0112097017\n",
      "[343,     1] loss: 0.0112096392\n",
      "[344,     1] loss: 0.0112095721\n",
      "[345,     1] loss: 0.0112095006\n",
      "[346,     1] loss: 0.0112094253\n",
      "[347,     1] loss: 0.0112093478\n",
      "[348,     1] loss: 0.0112092644\n",
      "[349,     1] loss: 0.0112091780\n",
      "[350,     1] loss: 0.0112090871\n",
      "[351,     1] loss: 0.0112089895\n",
      "[352,     1] loss: 0.0112088904\n",
      "[353,     1] loss: 0.0112087861\n",
      "[354,     1] loss: 0.0112086765\n",
      "[355,     1] loss: 0.0112085626\n",
      "[356,     1] loss: 0.0112084441\n",
      "[357,     1] loss: 0.0112083197\n",
      "[358,     1] loss: 0.0112081908\n",
      "[359,     1] loss: 0.0112080589\n",
      "[360,     1] loss: 0.0112079173\n",
      "[361,     1] loss: 0.0112077735\n",
      "[362,     1] loss: 0.0112076223\n",
      "[363,     1] loss: 0.0112074673\n",
      "[364,     1] loss: 0.0112073056\n",
      "[365,     1] loss: 0.0112071380\n",
      "[366,     1] loss: 0.0112069651\n",
      "[367,     1] loss: 0.0112067834\n",
      "[368,     1] loss: 0.0112065978\n",
      "[369,     1] loss: 0.0112064034\n",
      "[370,     1] loss: 0.0112062044\n",
      "[371,     1] loss: 0.0112059973\n",
      "[372,     1] loss: 0.0112057827\n",
      "[373,     1] loss: 0.0112055629\n",
      "[374,     1] loss: 0.0112053335\n",
      "[375,     1] loss: 0.0112050973\n",
      "[376,     1] loss: 0.0112048529\n",
      "[377,     1] loss: 0.0112046003\n",
      "[378,     1] loss: 0.0112043396\n",
      "[379,     1] loss: 0.0112040713\n",
      "[380,     1] loss: 0.0112037942\n",
      "[381,     1] loss: 0.0112035066\n",
      "[382,     1] loss: 0.0112032101\n",
      "[383,     1] loss: 0.0112029038\n",
      "[384,     1] loss: 0.0112025894\n",
      "[385,     1] loss: 0.0112022653\n",
      "[386,     1] loss: 0.0112019300\n",
      "[387,     1] loss: 0.0112015836\n",
      "[388,     1] loss: 0.0112012252\n",
      "[389,     1] loss: 0.0112008594\n",
      "[390,     1] loss: 0.0112004802\n",
      "[391,     1] loss: 0.0112000883\n",
      "[392,     1] loss: 0.0111996867\n",
      "[393,     1] loss: 0.0111992724\n",
      "[394,     1] loss: 0.0111988425\n",
      "[395,     1] loss: 0.0111984022\n",
      "[396,     1] loss: 0.0111979499\n",
      "[397,     1] loss: 0.0111974806\n",
      "[398,     1] loss: 0.0111969985\n",
      "[399,     1] loss: 0.0111965016\n",
      "[400,     1] loss: 0.0111959912\n",
      "[401,     1] loss: 0.0111954652\n",
      "[402,     1] loss: 0.0111949235\n",
      "[403,     1] loss: 0.0111943655\n",
      "[404,     1] loss: 0.0111937903\n",
      "[405,     1] loss: 0.0111931995\n",
      "[406,     1] loss: 0.0111925900\n",
      "[407,     1] loss: 0.0111919641\n",
      "[408,     1] loss: 0.0111913197\n",
      "[409,     1] loss: 0.0111906573\n",
      "[410,     1] loss: 0.0111899741\n",
      "[411,     1] loss: 0.0111892723\n",
      "[412,     1] loss: 0.0111885518\n",
      "[413,     1] loss: 0.0111878090\n",
      "[414,     1] loss: 0.0111870460\n",
      "[415,     1] loss: 0.0111862615\n",
      "[416,     1] loss: 0.0111854568\n",
      "[417,     1] loss: 0.0111846276\n",
      "[418,     1] loss: 0.0111837745\n",
      "[419,     1] loss: 0.0111828998\n",
      "[420,     1] loss: 0.0111820005\n",
      "[421,     1] loss: 0.0111810789\n",
      "[422,     1] loss: 0.0111801289\n",
      "[423,     1] loss: 0.0111791544\n",
      "[424,     1] loss: 0.0111781567\n",
      "[425,     1] loss: 0.0111771300\n",
      "[426,     1] loss: 0.0111760788\n",
      "[427,     1] loss: 0.0111749977\n",
      "[428,     1] loss: 0.0111738920\n",
      "[429,     1] loss: 0.0111727551\n",
      "[430,     1] loss: 0.0111715913\n",
      "[431,     1] loss: 0.0111703984\n",
      "[432,     1] loss: 0.0111691765\n",
      "[433,     1] loss: 0.0111679241\n",
      "[434,     1] loss: 0.0111666426\n",
      "[435,     1] loss: 0.0111653306\n",
      "[436,     1] loss: 0.0111639872\n",
      "[437,     1] loss: 0.0111626141\n",
      "[438,     1] loss: 0.0111612082\n",
      "[439,     1] loss: 0.0111597724\n",
      "[440,     1] loss: 0.0111583047\n",
      "[441,     1] loss: 0.0111568056\n",
      "[442,     1] loss: 0.0111552738\n",
      "[443,     1] loss: 0.0111537121\n",
      "[444,     1] loss: 0.0111521177\n",
      "[445,     1] loss: 0.0111504897\n",
      "[446,     1] loss: 0.0111488320\n",
      "[447,     1] loss: 0.0111471407\n",
      "[448,     1] loss: 0.0111454174\n",
      "[449,     1] loss: 0.0111436643\n",
      "[450,     1] loss: 0.0111418776\n",
      "[451,     1] loss: 0.0111400589\n",
      "[452,     1] loss: 0.0111382090\n",
      "[453,     1] loss: 0.0111363269\n",
      "[454,     1] loss: 0.0111344144\n",
      "[455,     1] loss: 0.0111324690\n",
      "[456,     1] loss: 0.0111304931\n",
      "[457,     1] loss: 0.0111284852\n",
      "[458,     1] loss: 0.0111264460\n",
      "[459,     1] loss: 0.0111243702\n",
      "[460,     1] loss: 0.0111222662\n",
      "[461,     1] loss: 0.0111201271\n",
      "[462,     1] loss: 0.0111179546\n",
      "[463,     1] loss: 0.0111157455\n",
      "[464,     1] loss: 0.0111135028\n",
      "[465,     1] loss: 0.0111112215\n",
      "[466,     1] loss: 0.0111089021\n",
      "[467,     1] loss: 0.0111065432\n",
      "[468,     1] loss: 0.0111041412\n",
      "[469,     1] loss: 0.0111016959\n",
      "[470,     1] loss: 0.0110992052\n",
      "[471,     1] loss: 0.0110966645\n",
      "[472,     1] loss: 0.0110940732\n",
      "[473,     1] loss: 0.0110914297\n",
      "[474,     1] loss: 0.0110887259\n",
      "[475,     1] loss: 0.0110859640\n",
      "[476,     1] loss: 0.0110831402\n",
      "[477,     1] loss: 0.0110802487\n",
      "[478,     1] loss: 0.0110772885\n",
      "[479,     1] loss: 0.0110742554\n",
      "[480,     1] loss: 0.0110711493\n",
      "[481,     1] loss: 0.0110679649\n",
      "[482,     1] loss: 0.0110647038\n",
      "[483,     1] loss: 0.0110613622\n",
      "[484,     1] loss: 0.0110579401\n",
      "[485,     1] loss: 0.0110544369\n",
      "[486,     1] loss: 0.0110508539\n",
      "[487,     1] loss: 0.0110471927\n",
      "[488,     1] loss: 0.0110434547\n",
      "[489,     1] loss: 0.0110396422\n",
      "[490,     1] loss: 0.0110357590\n",
      "[491,     1] loss: 0.0110318117\n",
      "[492,     1] loss: 0.0110278003\n",
      "[493,     1] loss: 0.0110237338\n",
      "[494,     1] loss: 0.0110196181\n",
      "[495,     1] loss: 0.0110154547\n",
      "[496,     1] loss: 0.0110112518\n",
      "[497,     1] loss: 0.0110070154\n",
      "[498,     1] loss: 0.0110027507\n",
      "[499,     1] loss: 0.0109984614\n",
      "[500,     1] loss: 0.0109941535\n",
      "[501,     1] loss: 0.0109898329\n",
      "[502,     1] loss: 0.0109855019\n",
      "[503,     1] loss: 0.0109811641\n",
      "[504,     1] loss: 0.0109768257\n",
      "[505,     1] loss: 0.0109724849\n",
      "[506,     1] loss: 0.0109681472\n",
      "[507,     1] loss: 0.0109638162\n",
      "[508,     1] loss: 0.0109594904\n",
      "[509,     1] loss: 0.0109551735\n",
      "[510,     1] loss: 0.0109508663\n",
      "[511,     1] loss: 0.0109465711\n",
      "[512,     1] loss: 0.0109422877\n",
      "[513,     1] loss: 0.0109380178\n",
      "[514,     1] loss: 0.0109337628\n",
      "[515,     1] loss: 0.0109295219\n",
      "[516,     1] loss: 0.0109252997\n",
      "[517,     1] loss: 0.0109210953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[518,     1] loss: 0.0109169081\n",
      "[519,     1] loss: 0.0109127425\n",
      "[520,     1] loss: 0.0109085962\n",
      "[521,     1] loss: 0.0109044723\n",
      "[522,     1] loss: 0.0109003700\n",
      "[523,     1] loss: 0.0108962908\n",
      "[524,     1] loss: 0.0108922370\n",
      "[525,     1] loss: 0.0108882062\n",
      "[526,     1] loss: 0.0108842023\n",
      "[527,     1] loss: 0.0108802222\n",
      "[528,     1] loss: 0.0108762704\n",
      "[529,     1] loss: 0.0108723409\n",
      "[530,     1] loss: 0.0108684413\n",
      "[531,     1] loss: 0.0108645678\n",
      "[532,     1] loss: 0.0108607203\n",
      "[533,     1] loss: 0.0108569033\n",
      "[534,     1] loss: 0.0108531140\n",
      "[535,     1] loss: 0.0108493537\n",
      "[536,     1] loss: 0.0108456217\n",
      "[537,     1] loss: 0.0108419217\n",
      "[538,     1] loss: 0.0108382516\n",
      "[539,     1] loss: 0.0108346164\n",
      "[540,     1] loss: 0.0108310141\n",
      "[541,     1] loss: 0.0108274460\n",
      "[542,     1] loss: 0.0108239137\n",
      "[543,     1] loss: 0.0108204193\n",
      "[544,     1] loss: 0.0108169630\n",
      "[545,     1] loss: 0.0108135477\n",
      "[546,     1] loss: 0.0108101733\n",
      "[547,     1] loss: 0.0108068399\n",
      "[548,     1] loss: 0.0108035535\n",
      "[549,     1] loss: 0.0108003110\n",
      "[550,     1] loss: 0.0107971139\n",
      "[551,     1] loss: 0.0107939653\n",
      "[552,     1] loss: 0.0107908659\n",
      "[553,     1] loss: 0.0107878178\n",
      "[554,     1] loss: 0.0107848190\n",
      "[555,     1] loss: 0.0107818730\n",
      "[556,     1] loss: 0.0107789792\n",
      "[557,     1] loss: 0.0107761405\n",
      "[558,     1] loss: 0.0107733548\n",
      "[559,     1] loss: 0.0107706234\n",
      "[560,     1] loss: 0.0107679471\n",
      "[561,     1] loss: 0.0107653260\n",
      "[562,     1] loss: 0.0107627600\n",
      "[563,     1] loss: 0.0107602499\n",
      "[564,     1] loss: 0.0107577935\n",
      "[565,     1] loss: 0.0107553929\n",
      "[566,     1] loss: 0.0107530467\n",
      "[567,     1] loss: 0.0107507527\n",
      "[568,     1] loss: 0.0107485123\n",
      "[569,     1] loss: 0.0107463248\n",
      "[570,     1] loss: 0.0107441902\n",
      "[571,     1] loss: 0.0107421048\n",
      "[572,     1] loss: 0.0107400700\n",
      "[573,     1] loss: 0.0107380852\n",
      "[574,     1] loss: 0.0107361473\n",
      "[575,     1] loss: 0.0107342578\n",
      "[576,     1] loss: 0.0107324153\n",
      "[577,     1] loss: 0.0107306160\n",
      "[578,     1] loss: 0.0107288644\n",
      "[579,     1] loss: 0.0107271522\n",
      "[580,     1] loss: 0.0107254855\n",
      "[581,     1] loss: 0.0107238591\n",
      "[582,     1] loss: 0.0107222736\n",
      "[583,     1] loss: 0.0107207276\n",
      "[584,     1] loss: 0.0107192181\n",
      "[585,     1] loss: 0.0107177466\n",
      "[586,     1] loss: 0.0107163139\n",
      "[587,     1] loss: 0.0107149139\n",
      "[588,     1] loss: 0.0107135490\n",
      "[589,     1] loss: 0.0107122183\n",
      "[590,     1] loss: 0.0107109182\n",
      "[591,     1] loss: 0.0107096523\n",
      "[592,     1] loss: 0.0107084163\n",
      "[593,     1] loss: 0.0107072093\n",
      "[594,     1] loss: 0.0107060328\n",
      "[595,     1] loss: 0.0107048847\n",
      "[596,     1] loss: 0.0107037634\n",
      "[597,     1] loss: 0.0107026674\n",
      "[598,     1] loss: 0.0107015990\n",
      "[599,     1] loss: 0.0107005551\n",
      "[600,     1] loss: 0.0106995367\n",
      "[601,     1] loss: 0.0106985398\n",
      "[602,     1] loss: 0.0106975660\n",
      "[603,     1] loss: 0.0106966160\n",
      "[604,     1] loss: 0.0106956847\n",
      "[605,     1] loss: 0.0106947772\n",
      "[606,     1] loss: 0.0106938884\n",
      "[607,     1] loss: 0.0106930174\n",
      "[608,     1] loss: 0.0106921665\n",
      "[609,     1] loss: 0.0106913336\n",
      "[610,     1] loss: 0.0106905185\n",
      "[611,     1] loss: 0.0106897205\n",
      "[612,     1] loss: 0.0106889397\n",
      "[613,     1] loss: 0.0106881715\n",
      "[614,     1] loss: 0.0106874198\n",
      "[615,     1] loss: 0.0106866851\n",
      "[616,     1] loss: 0.0106859617\n",
      "[617,     1] loss: 0.0106852517\n",
      "[618,     1] loss: 0.0106845565\n",
      "[619,     1] loss: 0.0106838733\n",
      "[620,     1] loss: 0.0106832020\n",
      "[621,     1] loss: 0.0106825419\n",
      "[622,     1] loss: 0.0106818937\n",
      "[623,     1] loss: 0.0106812567\n",
      "[624,     1] loss: 0.0106806301\n",
      "[625,     1] loss: 0.0106800117\n",
      "[626,     1] loss: 0.0106794037\n",
      "[627,     1] loss: 0.0106788047\n",
      "[628,     1] loss: 0.0106782153\n",
      "[629,     1] loss: 0.0106776349\n",
      "[630,     1] loss: 0.0106770605\n",
      "[631,     1] loss: 0.0106764957\n",
      "[632,     1] loss: 0.0106759362\n",
      "[633,     1] loss: 0.0106753856\n",
      "[634,     1] loss: 0.0106748402\n",
      "[635,     1] loss: 0.0106743023\n",
      "[636,     1] loss: 0.0106737711\n",
      "[637,     1] loss: 0.0106732443\n",
      "[638,     1] loss: 0.0106727235\n",
      "[639,     1] loss: 0.0106722079\n",
      "[640,     1] loss: 0.0106716983\n",
      "[641,     1] loss: 0.0106711924\n",
      "[642,     1] loss: 0.0106706925\n",
      "[643,     1] loss: 0.0106701955\n",
      "[644,     1] loss: 0.0106697045\n",
      "[645,     1] loss: 0.0106692150\n",
      "[646,     1] loss: 0.0106687315\n",
      "[647,     1] loss: 0.0106682494\n",
      "[648,     1] loss: 0.0106677726\n",
      "[649,     1] loss: 0.0106673002\n",
      "[650,     1] loss: 0.0106668279\n",
      "[651,     1] loss: 0.0106663592\n",
      "[652,     1] loss: 0.0106658936\n",
      "[653,     1] loss: 0.0106654316\n",
      "[654,     1] loss: 0.0106649704\n",
      "[655,     1] loss: 0.0106645130\n",
      "[656,     1] loss: 0.0106640555\n",
      "[657,     1] loss: 0.0106636010\n",
      "[658,     1] loss: 0.0106631503\n",
      "[659,     1] loss: 0.0106626987\n",
      "[660,     1] loss: 0.0106622510\n",
      "[661,     1] loss: 0.0106618039\n",
      "[662,     1] loss: 0.0106613569\n",
      "[663,     1] loss: 0.0106609136\n",
      "[664,     1] loss: 0.0106604695\n",
      "[665,     1] loss: 0.0106600285\n",
      "[666,     1] loss: 0.0106595881\n",
      "[667,     1] loss: 0.0106591493\n",
      "[668,     1] loss: 0.0106587090\n",
      "[669,     1] loss: 0.0106582724\n",
      "[670,     1] loss: 0.0106578343\n",
      "[671,     1] loss: 0.0106573984\n",
      "[672,     1] loss: 0.0106569618\n",
      "[673,     1] loss: 0.0106565282\n",
      "[674,     1] loss: 0.0106560938\n",
      "[675,     1] loss: 0.0106556579\n",
      "[676,     1] loss: 0.0106552251\n",
      "[677,     1] loss: 0.0106547922\n",
      "[678,     1] loss: 0.0106543593\n",
      "[679,     1] loss: 0.0106539264\n",
      "[680,     1] loss: 0.0106534936\n",
      "[681,     1] loss: 0.0106530607\n",
      "[682,     1] loss: 0.0106526293\n",
      "[683,     1] loss: 0.0106521972\n",
      "[684,     1] loss: 0.0106517658\n",
      "[685,     1] loss: 0.0106513336\n",
      "[686,     1] loss: 0.0106509015\n",
      "[687,     1] loss: 0.0106504694\n",
      "[688,     1] loss: 0.0106500387\n",
      "[689,     1] loss: 0.0106496051\n",
      "[690,     1] loss: 0.0106491737\n",
      "[691,     1] loss: 0.0106487408\n",
      "[692,     1] loss: 0.0106483087\n",
      "[693,     1] loss: 0.0106478758\n",
      "[694,     1] loss: 0.0106474429\n",
      "[695,     1] loss: 0.0106470101\n",
      "[696,     1] loss: 0.0106465757\n",
      "[697,     1] loss: 0.0106461413\n",
      "[698,     1] loss: 0.0106457070\n",
      "[699,     1] loss: 0.0106452741\n",
      "[700,     1] loss: 0.0106448375\n",
      "[701,     1] loss: 0.0106444031\n",
      "[702,     1] loss: 0.0106439665\n",
      "[703,     1] loss: 0.0106435291\n",
      "[704,     1] loss: 0.0106430940\n",
      "[705,     1] loss: 0.0106426559\n",
      "[706,     1] loss: 0.0106422186\n",
      "[707,     1] loss: 0.0106417805\n",
      "[708,     1] loss: 0.0106413424\n",
      "[709,     1] loss: 0.0106409028\n",
      "[710,     1] loss: 0.0106404632\n",
      "[711,     1] loss: 0.0106400229\n",
      "[712,     1] loss: 0.0106395818\n",
      "[713,     1] loss: 0.0106391400\n",
      "[714,     1] loss: 0.0106386982\n",
      "[715,     1] loss: 0.0106382556\n",
      "[716,     1] loss: 0.0106378108\n",
      "[717,     1] loss: 0.0106373683\n",
      "[718,     1] loss: 0.0106369220\n",
      "[719,     1] loss: 0.0106364787\n",
      "[720,     1] loss: 0.0106360316\n",
      "[721,     1] loss: 0.0106355846\n",
      "[722,     1] loss: 0.0106351368\n",
      "[723,     1] loss: 0.0106346890\n",
      "[724,     1] loss: 0.0106342405\n",
      "[725,     1] loss: 0.0106337912\n",
      "[726,     1] loss: 0.0106333397\n",
      "[727,     1] loss: 0.0106328897\n",
      "[728,     1] loss: 0.0106324367\n",
      "[729,     1] loss: 0.0106319845\n",
      "[730,     1] loss: 0.0106315307\n",
      "[731,     1] loss: 0.0106310770\n",
      "[732,     1] loss: 0.0106306203\n",
      "[733,     1] loss: 0.0106301643\n",
      "[734,     1] loss: 0.0106297083\n",
      "[735,     1] loss: 0.0106292494\n",
      "[736,     1] loss: 0.0106287919\n",
      "[737,     1] loss: 0.0106283322\n",
      "[738,     1] loss: 0.0106278725\n",
      "[739,     1] loss: 0.0106274098\n",
      "[740,     1] loss: 0.0106269479\n",
      "[741,     1] loss: 0.0106264845\n",
      "[742,     1] loss: 0.0106260195\n",
      "[743,     1] loss: 0.0106255554\n",
      "[744,     1] loss: 0.0106250890\n",
      "[745,     1] loss: 0.0106246218\n",
      "[746,     1] loss: 0.0106241547\n",
      "[747,     1] loss: 0.0106236853\n",
      "[748,     1] loss: 0.0106232144\n",
      "[749,     1] loss: 0.0106227428\n",
      "[750,     1] loss: 0.0106222704\n",
      "[751,     1] loss: 0.0106217973\n",
      "[752,     1] loss: 0.0106213227\n",
      "[753,     1] loss: 0.0106208473\n",
      "[754,     1] loss: 0.0106203705\n",
      "[755,     1] loss: 0.0106198929\n",
      "[756,     1] loss: 0.0106194131\n",
      "[757,     1] loss: 0.0106189348\n",
      "[758,     1] loss: 0.0106184527\n",
      "[759,     1] loss: 0.0106179707\n",
      "[760,     1] loss: 0.0106174879\n",
      "[761,     1] loss: 0.0106170028\n",
      "[762,     1] loss: 0.0106165171\n",
      "[763,     1] loss: 0.0106160305\n",
      "[764,     1] loss: 0.0106155410\n",
      "[765,     1] loss: 0.0106150515\n",
      "[766,     1] loss: 0.0106145620\n",
      "[767,     1] loss: 0.0106140696\n",
      "[768,     1] loss: 0.0106135763\n",
      "[769,     1] loss: 0.0106130823\n",
      "[770,     1] loss: 0.0106125861\n",
      "[771,     1] loss: 0.0106120899\n",
      "[772,     1] loss: 0.0106115915\n",
      "[773,     1] loss: 0.0106110923\n",
      "[774,     1] loss: 0.0106105901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[775,     1] loss: 0.0106100887\n",
      "[776,     1] loss: 0.0106095836\n",
      "[777,     1] loss: 0.0106090799\n",
      "[778,     1] loss: 0.0106085740\n",
      "[779,     1] loss: 0.0106080651\n",
      "[780,     1] loss: 0.0106075563\n",
      "[781,     1] loss: 0.0106070451\n",
      "[782,     1] loss: 0.0106065333\n",
      "[783,     1] loss: 0.0106060199\n",
      "[784,     1] loss: 0.0106055051\n",
      "[785,     1] loss: 0.0106049895\n",
      "[786,     1] loss: 0.0106044710\n",
      "[787,     1] loss: 0.0106039517\n",
      "[788,     1] loss: 0.0106034301\n",
      "[789,     1] loss: 0.0106029093\n",
      "[790,     1] loss: 0.0106023856\n",
      "[791,     1] loss: 0.0106018595\n",
      "[792,     1] loss: 0.0106013335\n",
      "[793,     1] loss: 0.0106008053\n",
      "[794,     1] loss: 0.0106002748\n",
      "[795,     1] loss: 0.0105997443\n",
      "[796,     1] loss: 0.0105992109\n",
      "[797,     1] loss: 0.0105986774\n",
      "[798,     1] loss: 0.0105981410\n",
      "[799,     1] loss: 0.0105976038\n",
      "[800,     1] loss: 0.0105970643\n",
      "[801,     1] loss: 0.0105965234\n",
      "[802,     1] loss: 0.0105959810\n",
      "[803,     1] loss: 0.0105954371\n",
      "[804,     1] loss: 0.0105948918\n",
      "[805,     1] loss: 0.0105943441\n",
      "[806,     1] loss: 0.0105937950\n",
      "[807,     1] loss: 0.0105932452\n",
      "[808,     1] loss: 0.0105926931\n",
      "[809,     1] loss: 0.0105921395\n",
      "[810,     1] loss: 0.0105915844\n",
      "[811,     1] loss: 0.0105910279\n",
      "[812,     1] loss: 0.0105904691\n",
      "[813,     1] loss: 0.0105899088\n",
      "[814,     1] loss: 0.0105893463\n",
      "[815,     1] loss: 0.0105887830\n",
      "[816,     1] loss: 0.0105882190\n",
      "[817,     1] loss: 0.0105876513\n",
      "[818,     1] loss: 0.0105870835\n",
      "[819,     1] loss: 0.0105865121\n",
      "[820,     1] loss: 0.0105859414\n",
      "[821,     1] loss: 0.0105853677\n",
      "[822,     1] loss: 0.0105847917\n",
      "[823,     1] loss: 0.0105842158\n",
      "[824,     1] loss: 0.0105836377\n",
      "[825,     1] loss: 0.0105830565\n",
      "[826,     1] loss: 0.0105824754\n",
      "[827,     1] loss: 0.0105818927\n",
      "[828,     1] loss: 0.0105813064\n",
      "[829,     1] loss: 0.0105807200\n",
      "[830,     1] loss: 0.0105801322\n",
      "[831,     1] loss: 0.0105795421\n",
      "[832,     1] loss: 0.0105789512\n",
      "[833,     1] loss: 0.0105783574\n",
      "[834,     1] loss: 0.0105777629\n",
      "[835,     1] loss: 0.0105771676\n",
      "[836,     1] loss: 0.0105765693\n",
      "[837,     1] loss: 0.0105759695\n",
      "[838,     1] loss: 0.0105753697\n",
      "[839,     1] loss: 0.0105747662\n",
      "[840,     1] loss: 0.0105741613\n",
      "[841,     1] loss: 0.0105735563\n",
      "[842,     1] loss: 0.0105729498\n",
      "[843,     1] loss: 0.0105723403\n",
      "[844,     1] loss: 0.0105717301\n",
      "[845,     1] loss: 0.0105711184\n",
      "[846,     1] loss: 0.0105705053\n",
      "[847,     1] loss: 0.0105698898\n",
      "[848,     1] loss: 0.0105692744\n",
      "[849,     1] loss: 0.0105686568\n",
      "[850,     1] loss: 0.0105680376\n",
      "[851,     1] loss: 0.0105674155\n",
      "[852,     1] loss: 0.0105667941\n",
      "[853,     1] loss: 0.0105661720\n",
      "[854,     1] loss: 0.0105655462\n",
      "[855,     1] loss: 0.0105649203\n",
      "[856,     1] loss: 0.0105642922\n",
      "[857,     1] loss: 0.0105636649\n",
      "[858,     1] loss: 0.0105630338\n",
      "[859,     1] loss: 0.0105624020\n",
      "[860,     1] loss: 0.0105617702\n",
      "[861,     1] loss: 0.0105611369\n",
      "[862,     1] loss: 0.0105605014\n",
      "[863,     1] loss: 0.0105598643\n",
      "[864,     1] loss: 0.0105592273\n",
      "[865,     1] loss: 0.0105585881\n",
      "[866,     1] loss: 0.0105579488\n",
      "[867,     1] loss: 0.0105573080\n",
      "[868,     1] loss: 0.0105566651\n",
      "[869,     1] loss: 0.0105560221\n",
      "[870,     1] loss: 0.0105553776\n",
      "[871,     1] loss: 0.0105547316\n",
      "[872,     1] loss: 0.0105540849\n",
      "[873,     1] loss: 0.0105534382\n",
      "[874,     1] loss: 0.0105527900\n",
      "[875,     1] loss: 0.0105521396\n",
      "[876,     1] loss: 0.0105514891\n",
      "[877,     1] loss: 0.0105508395\n",
      "[878,     1] loss: 0.0105501875\n",
      "[879,     1] loss: 0.0105495334\n",
      "[880,     1] loss: 0.0105488792\n",
      "[881,     1] loss: 0.0105482236\n",
      "[882,     1] loss: 0.0105475672\n",
      "[883,     1] loss: 0.0105469115\n",
      "[884,     1] loss: 0.0105462544\n",
      "[885,     1] loss: 0.0105455957\n",
      "[886,     1] loss: 0.0105449371\n",
      "[887,     1] loss: 0.0105442770\n",
      "[888,     1] loss: 0.0105436176\n",
      "[889,     1] loss: 0.0105429560\n",
      "[890,     1] loss: 0.0105422951\n",
      "[891,     1] loss: 0.0105416343\n",
      "[892,     1] loss: 0.0105409704\n",
      "[893,     1] loss: 0.0105403058\n",
      "[894,     1] loss: 0.0105396420\n",
      "[895,     1] loss: 0.0105389781\n",
      "[896,     1] loss: 0.0105383135\n",
      "[897,     1] loss: 0.0105376482\n",
      "[898,     1] loss: 0.0105369806\n",
      "[899,     1] loss: 0.0105363145\n",
      "[900,     1] loss: 0.0105356485\n",
      "[901,     1] loss: 0.0105349816\n",
      "[902,     1] loss: 0.0105343133\n",
      "[903,     1] loss: 0.0105336443\n",
      "[904,     1] loss: 0.0105329759\n",
      "[905,     1] loss: 0.0105323069\n",
      "[906,     1] loss: 0.0105316378\n",
      "[907,     1] loss: 0.0105309680\n",
      "[908,     1] loss: 0.0105302967\n",
      "[909,     1] loss: 0.0105296254\n",
      "[910,     1] loss: 0.0105289556\n",
      "[911,     1] loss: 0.0105282851\n",
      "[912,     1] loss: 0.0105276138\n",
      "[913,     1] loss: 0.0105269417\n",
      "[914,     1] loss: 0.0105262704\n",
      "[915,     1] loss: 0.0105255991\n",
      "[916,     1] loss: 0.0105249256\n",
      "[917,     1] loss: 0.0105242535\n",
      "[918,     1] loss: 0.0105235808\n",
      "[919,     1] loss: 0.0105229072\n",
      "[920,     1] loss: 0.0105222344\n",
      "[921,     1] loss: 0.0105215617\n",
      "[922,     1] loss: 0.0105208881\n",
      "[923,     1] loss: 0.0105202146\n",
      "[924,     1] loss: 0.0105195411\n",
      "[925,     1] loss: 0.0105188668\n",
      "[926,     1] loss: 0.0105181932\n",
      "[927,     1] loss: 0.0105175182\n",
      "[928,     1] loss: 0.0105168447\n",
      "[929,     1] loss: 0.0105161704\n",
      "[930,     1] loss: 0.0105154954\n",
      "[931,     1] loss: 0.0105148219\n",
      "[932,     1] loss: 0.0105141468\n",
      "[933,     1] loss: 0.0105134718\n",
      "[934,     1] loss: 0.0105127968\n",
      "[935,     1] loss: 0.0105121225\n",
      "[936,     1] loss: 0.0105114482\n",
      "[937,     1] loss: 0.0105107732\n",
      "[938,     1] loss: 0.0105100982\n",
      "[939,     1] loss: 0.0105094224\n",
      "[940,     1] loss: 0.0105087481\n",
      "[941,     1] loss: 0.0105080731\n",
      "[942,     1] loss: 0.0105073981\n",
      "[943,     1] loss: 0.0105067231\n",
      "[944,     1] loss: 0.0105060473\n",
      "[945,     1] loss: 0.0105053730\n",
      "[946,     1] loss: 0.0105046965\n",
      "[947,     1] loss: 0.0105040222\n",
      "[948,     1] loss: 0.0105033472\n",
      "[949,     1] loss: 0.0105026715\n",
      "[950,     1] loss: 0.0105019964\n",
      "[951,     1] loss: 0.0105013214\n",
      "[952,     1] loss: 0.0105006456\n",
      "[953,     1] loss: 0.0104999706\n",
      "[954,     1] loss: 0.0104992956\n",
      "[955,     1] loss: 0.0104986206\n",
      "[956,     1] loss: 0.0104979448\n",
      "[957,     1] loss: 0.0104972698\n",
      "[958,     1] loss: 0.0104965962\n",
      "[959,     1] loss: 0.0104959205\n",
      "[960,     1] loss: 0.0104952455\n",
      "[961,     1] loss: 0.0104945712\n",
      "[962,     1] loss: 0.0104938962\n",
      "[963,     1] loss: 0.0104932204\n",
      "[964,     1] loss: 0.0104925461\n",
      "[965,     1] loss: 0.0104918718\n",
      "[966,     1] loss: 0.0104911976\n",
      "[967,     1] loss: 0.0104905240\n",
      "[968,     1] loss: 0.0104898490\n",
      "[969,     1] loss: 0.0104891755\n",
      "[970,     1] loss: 0.0104885019\n",
      "[971,     1] loss: 0.0104878277\n",
      "[972,     1] loss: 0.0104871556\n",
      "[973,     1] loss: 0.0104864813\n",
      "[974,     1] loss: 0.0104858086\n",
      "[975,     1] loss: 0.0104851358\n",
      "[976,     1] loss: 0.0104844637\n",
      "[977,     1] loss: 0.0104837917\n",
      "[978,     1] loss: 0.0104831196\n",
      "[979,     1] loss: 0.0104824476\n",
      "[980,     1] loss: 0.0104817770\n",
      "[981,     1] loss: 0.0104811065\n",
      "[982,     1] loss: 0.0104804359\n",
      "[983,     1] loss: 0.0104797654\n",
      "[984,     1] loss: 0.0104790963\n",
      "[985,     1] loss: 0.0104784280\n",
      "[986,     1] loss: 0.0104777582\n",
      "[987,     1] loss: 0.0104770899\n",
      "[988,     1] loss: 0.0104764223\n",
      "[989,     1] loss: 0.0104757562\n",
      "[990,     1] loss: 0.0104750909\n",
      "[991,     1] loss: 0.0104744241\n",
      "[992,     1] loss: 0.0104737587\n",
      "[993,     1] loss: 0.0104730949\n",
      "[994,     1] loss: 0.0104724325\n",
      "[995,     1] loss: 0.0104717687\n",
      "[996,     1] loss: 0.0104711078\n",
      "[997,     1] loss: 0.0104704469\n",
      "[998,     1] loss: 0.0104697868\n",
      "[999,     1] loss: 0.0104691274\n",
      "[1000,     1] loss: 0.0104684711\n",
      "[1001,     1] loss: 0.0104678132\n",
      "[1002,     1] loss: 0.0104671590\n",
      "[1003,     1] loss: 0.0104665048\n",
      "[1004,     1] loss: 0.0104658522\n",
      "[1005,     1] loss: 0.0104652017\n",
      "[1006,     1] loss: 0.0104645528\n",
      "[1007,     1] loss: 0.0104639031\n",
      "[1008,     1] loss: 0.0104632571\n",
      "[1009,     1] loss: 0.0104626127\n",
      "[1010,     1] loss: 0.0104619697\n",
      "[1011,     1] loss: 0.0104613282\n",
      "[1012,     1] loss: 0.0104606889\n",
      "[1013,     1] loss: 0.0104600526\n",
      "[1014,     1] loss: 0.0104594178\n",
      "[1015,     1] loss: 0.0104587846\n",
      "[1016,     1] loss: 0.0104581550\n",
      "[1017,     1] loss: 0.0104575261\n",
      "[1018,     1] loss: 0.0104569025\n",
      "[1019,     1] loss: 0.0104562789\n",
      "[1020,     1] loss: 0.0104556590\n",
      "[1021,     1] loss: 0.0104550429\n",
      "[1022,     1] loss: 0.0104544275\n",
      "[1023,     1] loss: 0.0104538180\n",
      "[1024,     1] loss: 0.0104532100\n",
      "[1025,     1] loss: 0.0104526065\n",
      "[1026,     1] loss: 0.0104520060\n",
      "[1027,     1] loss: 0.0104514092\n",
      "[1028,     1] loss: 0.0104508169\n",
      "[1029,     1] loss: 0.0104502283\n",
      "[1030,     1] loss: 0.0104496427\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1031,     1] loss: 0.0104490615\n",
      "[1032,     1] loss: 0.0104484849\n",
      "[1033,     1] loss: 0.0104479134\n",
      "[1034,     1] loss: 0.0104473457\n",
      "[1035,     1] loss: 0.0104467839\n",
      "[1036,     1] loss: 0.0104462259\n",
      "[1037,     1] loss: 0.0104456738\n",
      "[1038,     1] loss: 0.0104451254\n",
      "[1039,     1] loss: 0.0104445852\n",
      "[1040,     1] loss: 0.0104440473\n",
      "[1041,     1] loss: 0.0104435168\n",
      "[1042,     1] loss: 0.0104429916\n",
      "[1043,     1] loss: 0.0104424715\n",
      "[1044,     1] loss: 0.0104419582\n",
      "[1045,     1] loss: 0.0104414515\n",
      "[1046,     1] loss: 0.0104409501\n",
      "[1047,     1] loss: 0.0104404546\n",
      "[1048,     1] loss: 0.0104399659\n",
      "[1049,     1] loss: 0.0104394831\n",
      "[1050,     1] loss: 0.0104390070\n",
      "[1051,     1] loss: 0.0104385361\n",
      "[1052,     1] loss: 0.0104380719\n",
      "[1053,     1] loss: 0.0104376152\n",
      "[1054,     1] loss: 0.0104371645\n",
      "[1055,     1] loss: 0.0104367197\n",
      "[1056,     1] loss: 0.0104362823\n",
      "[1057,     1] loss: 0.0104358509\n",
      "[1058,     1] loss: 0.0104354262\n",
      "[1059,     1] loss: 0.0104350068\n",
      "[1060,     1] loss: 0.0104345940\n",
      "[1061,     1] loss: 0.0104341872\n",
      "[1062,     1] loss: 0.0104337871\n",
      "[1063,     1] loss: 0.0104333915\n",
      "[1064,     1] loss: 0.0104330033\n",
      "[1065,     1] loss: 0.0104326196\n",
      "[1066,     1] loss: 0.0104322419\n",
      "[1067,     1] loss: 0.0104318701\n",
      "[1068,     1] loss: 0.0104315028\n",
      "[1069,     1] loss: 0.0104311407\n",
      "[1070,     1] loss: 0.0104307838\n",
      "[1071,     1] loss: 0.0104304314\n",
      "[1072,     1] loss: 0.0104300842\n",
      "[1073,     1] loss: 0.0104297407\n",
      "[1074,     1] loss: 0.0104294017\n",
      "[1075,     1] loss: 0.0104290679\n",
      "[1076,     1] loss: 0.0104287364\n",
      "[1077,     1] loss: 0.0104284093\n",
      "[1078,     1] loss: 0.0104280859\n",
      "[1079,     1] loss: 0.0104277670\n",
      "[1080,     1] loss: 0.0104274511\n",
      "[1081,     1] loss: 0.0104271367\n",
      "[1082,     1] loss: 0.0104268275\n",
      "[1083,     1] loss: 0.0104265191\n",
      "[1084,     1] loss: 0.0104262158\n",
      "[1085,     1] loss: 0.0104259156\n",
      "[1086,     1] loss: 0.0104256161\n",
      "[1087,     1] loss: 0.0104253195\n",
      "[1088,     1] loss: 0.0104250267\n",
      "[1089,     1] loss: 0.0104247354\n",
      "[1090,     1] loss: 0.0104244456\n",
      "[1091,     1] loss: 0.0104241595\n",
      "[1092,     1] loss: 0.0104238734\n",
      "[1093,     1] loss: 0.0104235902\n",
      "[1094,     1] loss: 0.0104233116\n",
      "[1095,     1] loss: 0.0104230322\n",
      "[1096,     1] loss: 0.0104227565\n",
      "[1097,     1] loss: 0.0104224809\n",
      "[1098,     1] loss: 0.0104222089\n",
      "[1099,     1] loss: 0.0104219370\n",
      "[1100,     1] loss: 0.0104216687\n",
      "[1101,     1] loss: 0.0104214028\n",
      "[1102,     1] loss: 0.0104211375\n",
      "[1103,     1] loss: 0.0104208745\n",
      "[1104,     1] loss: 0.0104206130\n",
      "[1105,     1] loss: 0.0104203537\n",
      "[1106,     1] loss: 0.0104200967\n",
      "[1107,     1] loss: 0.0104198419\n",
      "[1108,     1] loss: 0.0104195885\n",
      "[1109,     1] loss: 0.0104193367\n",
      "[1110,     1] loss: 0.0104190879\n",
      "[1111,     1] loss: 0.0104188405\n",
      "[1112,     1] loss: 0.0104185954\n",
      "[1113,     1] loss: 0.0104183517\n",
      "[1114,     1] loss: 0.0104181118\n",
      "[1115,     1] loss: 0.0104178719\n",
      "[1116,     1] loss: 0.0104176372\n",
      "[1117,     1] loss: 0.0104174018\n",
      "[1118,     1] loss: 0.0104171701\n",
      "[1119,     1] loss: 0.0104169413\n",
      "[1120,     1] loss: 0.0104167134\n",
      "[1121,     1] loss: 0.0104164883\n",
      "[1122,     1] loss: 0.0104162663\n",
      "[1123,     1] loss: 0.0104160465\n",
      "[1124,     1] loss: 0.0104158290\n",
      "[1125,     1] loss: 0.0104156137\n",
      "[1126,     1] loss: 0.0104154006\n",
      "[1127,     1] loss: 0.0104151905\n",
      "[1128,     1] loss: 0.0104149833\n",
      "[1129,     1] loss: 0.0104147770\n",
      "[1130,     1] loss: 0.0104145765\n",
      "[1131,     1] loss: 0.0104143776\n",
      "[1132,     1] loss: 0.0104141794\n",
      "[1133,     1] loss: 0.0104139864\n",
      "[1134,     1] loss: 0.0104137957\n",
      "[1135,     1] loss: 0.0104136057\n",
      "[1136,     1] loss: 0.0104134195\n",
      "[1137,     1] loss: 0.0104132369\n",
      "[1138,     1] loss: 0.0104130574\n",
      "[1139,     1] loss: 0.0104128793\n",
      "[1140,     1] loss: 0.0104127057\n",
      "[1141,     1] loss: 0.0104125336\n",
      "[1142,     1] loss: 0.0104123652\n",
      "[1143,     1] loss: 0.0104121983\n",
      "[1144,     1] loss: 0.0104120366\n",
      "[1145,     1] loss: 0.0104118757\n",
      "[1146,     1] loss: 0.0104117185\n",
      "[1147,     1] loss: 0.0104115635\n",
      "[1148,     1] loss: 0.0104114123\n",
      "[1149,     1] loss: 0.0104112625\n",
      "[1150,     1] loss: 0.0104111172\n",
      "[1151,     1] loss: 0.0104109749\n",
      "[1152,     1] loss: 0.0104108356\n",
      "[1153,     1] loss: 0.0104106978\n",
      "[1154,     1] loss: 0.0104105629\n",
      "[1155,     1] loss: 0.0104104325\n",
      "[1156,     1] loss: 0.0104103029\n",
      "[1157,     1] loss: 0.0104101777\n",
      "[1158,     1] loss: 0.0104100555\n",
      "[1159,     1] loss: 0.0104099348\n",
      "[1160,     1] loss: 0.0104098186\n",
      "[1161,     1] loss: 0.0104097039\n",
      "[1162,     1] loss: 0.0104095921\n",
      "[1163,     1] loss: 0.0104094826\n",
      "[1164,     1] loss: 0.0104093783\n",
      "[1165,     1] loss: 0.0104092747\n",
      "[1166,     1] loss: 0.0104091756\n",
      "[1167,     1] loss: 0.0104090758\n",
      "[1168,     1] loss: 0.0104089811\n",
      "[1169,     1] loss: 0.0104088902\n",
      "[1170,     1] loss: 0.0104088001\n",
      "[1171,     1] loss: 0.0104087137\n",
      "[1172,     1] loss: 0.0104086287\n",
      "[1173,     1] loss: 0.0104085483\n",
      "[1174,     1] loss: 0.0104084693\n",
      "[1175,     1] loss: 0.0104083911\n",
      "[1176,     1] loss: 0.0104083188\n",
      "[1177,     1] loss: 0.0104082473\n",
      "[1178,     1] loss: 0.0104081787\n",
      "[1179,     1] loss: 0.0104081132\n",
      "[1180,     1] loss: 0.0104080491\n",
      "[1181,     1] loss: 0.0104079880\n",
      "[1182,     1] loss: 0.0104079299\n",
      "[1183,     1] loss: 0.0104078725\n",
      "[1184,     1] loss: 0.0104078181\n",
      "[1185,     1] loss: 0.0104077667\n",
      "[1186,     1] loss: 0.0104077190\n",
      "[1187,     1] loss: 0.0104076713\n",
      "[1188,     1] loss: 0.0104076259\n",
      "[1189,     1] loss: 0.0104075842\n",
      "[1190,     1] loss: 0.0104075439\n",
      "[1191,     1] loss: 0.0104075067\n",
      "[1192,     1] loss: 0.0104074702\n",
      "[1193,     1] loss: 0.0104074366\n",
      "[1194,     1] loss: 0.0104074046\n",
      "[1195,     1] loss: 0.0104073763\n",
      "[1196,     1] loss: 0.0104073487\n",
      "[1197,     1] loss: 0.0104073241\n",
      "[1198,     1] loss: 0.0104073010\n",
      "[1199,     1] loss: 0.0104072787\n",
      "[1200,     1] loss: 0.0104072593\n",
      "[1201,     1] loss: 0.0104072422\n",
      "[1202,     1] loss: 0.0104072280\n",
      "[1203,     1] loss: 0.0104072161\n",
      "[1204,     1] loss: 0.0104072034\n",
      "[1205,     1] loss: 0.0104071938\n",
      "[1206,     1] loss: 0.0104071856\n",
      "[1207,     1] loss: 0.0104071796\n",
      "[1208,     1] loss: 0.0104071759\n",
      "[1209,     1] loss: 0.0104071729\n",
      "[1210,     1] loss: 0.0104071721\n",
      "[1211,     1] loss: 0.0104071729\n",
      "[1212,     1] loss: 0.0104071759\n",
      "[1213,     1] loss: 0.0104071796\n",
      "[1214,     1] loss: 0.0104071856\n",
      "[1215,     1] loss: 0.0104071938\n",
      "[1216,     1] loss: 0.0104072027\n",
      "[1217,     1] loss: 0.0104072139\n",
      "[1218,     1] loss: 0.0104072258\n",
      "[1219,     1] loss: 0.0104072392\n",
      "[1220,     1] loss: 0.0104072548\n",
      "[1221,     1] loss: 0.0104072712\n",
      "[1222,     1] loss: 0.0104072891\n",
      "[1223,     1] loss: 0.0104073085\n",
      "[1224,     1] loss: 0.0104073286\n",
      "[1225,     1] loss: 0.0104073510\n",
      "[1226,     1] loss: 0.0104073741\n",
      "[1227,     1] loss: 0.0104073994\n",
      "[1228,     1] loss: 0.0104074240\n",
      "[1229,     1] loss: 0.0104074515\n",
      "[1230,     1] loss: 0.0104074784\n",
      "[1231,     1] loss: 0.0104075074\n",
      "[1232,     1] loss: 0.0104075395\n",
      "[1233,     1] loss: 0.0104075715\n",
      "[1234,     1] loss: 0.0104076043\n",
      "[1235,     1] loss: 0.0104076378\n",
      "[1236,     1] loss: 0.0104076721\n",
      "[1237,     1] loss: 0.0104077086\n",
      "[1238,     1] loss: 0.0104077451\n",
      "[1239,     1] loss: 0.0104077838\n",
      "[1240,     1] loss: 0.0104078226\n",
      "[1241,     1] loss: 0.0104078621\n",
      "[1242,     1] loss: 0.0104079023\n",
      "[1243,     1] loss: 0.0104079440\n",
      "[1244,     1] loss: 0.0104079872\n",
      "[1245,     1] loss: 0.0104080305\n",
      "[1246,     1] loss: 0.0104080744\n",
      "[1247,     1] loss: 0.0104081191\n",
      "[1248,     1] loss: 0.0104081646\n",
      "[1249,     1] loss: 0.0104082115\n",
      "[1250,     1] loss: 0.0104082577\n",
      "[1251,     1] loss: 0.0104083054\n",
      "[1252,     1] loss: 0.0104083538\n",
      "[1253,     1] loss: 0.0104084022\n",
      "[1254,     1] loss: 0.0104084522\n",
      "[1255,     1] loss: 0.0104085036\n",
      "[1256,     1] loss: 0.0104085535\n",
      "[1257,     1] loss: 0.0104086056\n",
      "[1258,     1] loss: 0.0104086578\n",
      "[1259,     1] loss: 0.0104087099\n",
      "[1260,     1] loss: 0.0104087636\n",
      "[1261,     1] loss: 0.0104088157\n",
      "[1262,     1] loss: 0.0104088701\n",
      "[1263,     1] loss: 0.0104089245\n",
      "[1264,     1] loss: 0.0104089804\n",
      "[1265,     1] loss: 0.0104090355\n",
      "[1266,     1] loss: 0.0104090907\n",
      "[1267,     1] loss: 0.0104091465\n",
      "[1268,     1] loss: 0.0104092024\n",
      "[1269,     1] loss: 0.0104092583\n",
      "[1270,     1] loss: 0.0104093157\n",
      "[1271,     1] loss: 0.0104093738\n",
      "[1272,     1] loss: 0.0104094304\n",
      "[1273,     1] loss: 0.0104094885\n",
      "[1274,     1] loss: 0.0104095459\n",
      "[1275,     1] loss: 0.0104096040\n",
      "[1276,     1] loss: 0.0104096636\n",
      "[1277,     1] loss: 0.0104097217\n",
      "[1278,     1] loss: 0.0104097813\n",
      "[1279,     1] loss: 0.0104098387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1280,     1] loss: 0.0104098968\n",
      "[1281,     1] loss: 0.0104099572\n",
      "[1282,     1] loss: 0.0104100160\n",
      "[1283,     1] loss: 0.0104100749\n",
      "[1284,     1] loss: 0.0104101345\n",
      "[1285,     1] loss: 0.0104101934\n",
      "[1286,     1] loss: 0.0104102530\n",
      "[1287,     1] loss: 0.0104103118\n",
      "[1288,     1] loss: 0.0104103714\n",
      "[1289,     1] loss: 0.0104104310\n",
      "[1290,     1] loss: 0.0104104899\n",
      "[1291,     1] loss: 0.0104105495\n",
      "[1292,     1] loss: 0.0104106084\n",
      "[1293,     1] loss: 0.0104106672\n",
      "[1294,     1] loss: 0.0104107268\n",
      "[1295,     1] loss: 0.0104107857\n",
      "[1296,     1] loss: 0.0104108438\n",
      "[1297,     1] loss: 0.0104109034\n",
      "[1298,     1] loss: 0.0104109615\n",
      "[1299,     1] loss: 0.0104110196\n",
      "[1300,     1] loss: 0.0104110777\n",
      "[1301,     1] loss: 0.0104111351\n",
      "[1302,     1] loss: 0.0104111940\n",
      "[1303,     1] loss: 0.0104112513\n",
      "[1304,     1] loss: 0.0104113080\n",
      "[1305,     1] loss: 0.0104113661\n",
      "[1306,     1] loss: 0.0104114227\n",
      "[1307,     1] loss: 0.0104114786\n",
      "[1308,     1] loss: 0.0104115367\n",
      "[1309,     1] loss: 0.0104115918\n",
      "[1310,     1] loss: 0.0104116470\n",
      "[1311,     1] loss: 0.0104117028\n",
      "[1312,     1] loss: 0.0104117587\n",
      "[1313,     1] loss: 0.0104118131\n",
      "[1314,     1] loss: 0.0104118675\n",
      "[1315,     1] loss: 0.0104119219\n",
      "[1316,     1] loss: 0.0104119748\n",
      "[1317,     1] loss: 0.0104120277\n",
      "[1318,     1] loss: 0.0104120798\n",
      "[1319,     1] loss: 0.0104121335\n",
      "[1320,     1] loss: 0.0104121864\n",
      "[1321,     1] loss: 0.0104122370\n",
      "[1322,     1] loss: 0.0104122885\n",
      "[1323,     1] loss: 0.0104123399\n",
      "[1324,     1] loss: 0.0104123913\n",
      "[1325,     1] loss: 0.0104124404\n",
      "[1326,     1] loss: 0.0104124911\n",
      "[1327,     1] loss: 0.0104125403\n",
      "[1328,     1] loss: 0.0104125887\n",
      "[1329,     1] loss: 0.0104126364\n",
      "[1330,     1] loss: 0.0104126848\n",
      "[1331,     1] loss: 0.0104127318\n",
      "[1332,     1] loss: 0.0104127795\n",
      "[1333,     1] loss: 0.0104128242\n",
      "[1334,     1] loss: 0.0104128703\n",
      "[1335,     1] loss: 0.0104129165\n",
      "[1336,     1] loss: 0.0104129598\n",
      "[1337,     1] loss: 0.0104130045\n",
      "[1338,     1] loss: 0.0104130484\n",
      "[1339,     1] loss: 0.0104130909\n",
      "[1340,     1] loss: 0.0104131341\n",
      "[1341,     1] loss: 0.0104131766\n",
      "[1342,     1] loss: 0.0104132168\n",
      "[1343,     1] loss: 0.0104132585\n",
      "[1344,     1] loss: 0.0104132980\n",
      "[1345,     1] loss: 0.0104133375\n",
      "[1346,     1] loss: 0.0104133755\n",
      "[1347,     1] loss: 0.0104134150\n",
      "[1348,     1] loss: 0.0104134537\n",
      "[1349,     1] loss: 0.0104134887\n",
      "[1350,     1] loss: 0.0104135260\n",
      "[1351,     1] loss: 0.0104135625\n",
      "[1352,     1] loss: 0.0104135983\n",
      "[1353,     1] loss: 0.0104136318\n",
      "[1354,     1] loss: 0.0104136661\n",
      "[1355,     1] loss: 0.0104137003\n",
      "[1356,     1] loss: 0.0104137316\n",
      "[1357,     1] loss: 0.0104137629\n",
      "[1358,     1] loss: 0.0104137950\n",
      "[1359,     1] loss: 0.0104138248\n",
      "[1360,     1] loss: 0.0104138553\n",
      "[1361,     1] loss: 0.0104138844\n",
      "[1362,     1] loss: 0.0104139134\n",
      "[1363,     1] loss: 0.0104139403\n",
      "[1364,     1] loss: 0.0104139671\n",
      "[1365,     1] loss: 0.0104139931\n",
      "[1366,     1] loss: 0.0104140185\n",
      "[1367,     1] loss: 0.0104140446\n",
      "[1368,     1] loss: 0.0104140684\n",
      "[1369,     1] loss: 0.0104140908\n",
      "[1370,     1] loss: 0.0104141138\n",
      "[1371,     1] loss: 0.0104141355\n",
      "[1372,     1] loss: 0.0104141563\n",
      "[1373,     1] loss: 0.0104141772\n",
      "[1374,     1] loss: 0.0104141966\n",
      "[1375,     1] loss: 0.0104142152\n",
      "[1376,     1] loss: 0.0104142338\n",
      "[1377,     1] loss: 0.0104142517\n",
      "[1378,     1] loss: 0.0104142681\n",
      "[1379,     1] loss: 0.0104142830\n",
      "[1380,     1] loss: 0.0104142994\n",
      "[1381,     1] loss: 0.0104143120\n",
      "[1382,     1] loss: 0.0104143262\n",
      "[1383,     1] loss: 0.0104143389\n",
      "[1384,     1] loss: 0.0104143508\n",
      "[1385,     1] loss: 0.0104143605\n",
      "[1386,     1] loss: 0.0104143701\n",
      "[1387,     1] loss: 0.0104143791\n",
      "[1388,     1] loss: 0.0104143873\n",
      "[1389,     1] loss: 0.0104143955\n",
      "[1390,     1] loss: 0.0104144029\n",
      "[1391,     1] loss: 0.0104144074\n",
      "[1392,     1] loss: 0.0104144134\n",
      "[1393,     1] loss: 0.0104144171\n",
      "[1394,     1] loss: 0.0104144208\n",
      "[1395,     1] loss: 0.0104144238\n",
      "[1396,     1] loss: 0.0104144245\n",
      "[1397,     1] loss: 0.0104144245\n",
      "[1398,     1] loss: 0.0104144245\n",
      "[1399,     1] loss: 0.0104144230\n",
      "[1400,     1] loss: 0.0104144223\n",
      "[1401,     1] loss: 0.0104144186\n",
      "[1402,     1] loss: 0.0104144156\n",
      "[1403,     1] loss: 0.0104144104\n",
      "[1404,     1] loss: 0.0104144044\n",
      "[1405,     1] loss: 0.0104143985\n",
      "[1406,     1] loss: 0.0104143903\n",
      "[1407,     1] loss: 0.0104143813\n",
      "[1408,     1] loss: 0.0104143731\n",
      "[1409,     1] loss: 0.0104143627\n",
      "[1410,     1] loss: 0.0104143508\n",
      "[1411,     1] loss: 0.0104143381\n",
      "[1412,     1] loss: 0.0104143262\n",
      "[1413,     1] loss: 0.0104143120\n",
      "[1414,     1] loss: 0.0104142971\n",
      "[1415,     1] loss: 0.0104142800\n",
      "[1416,     1] loss: 0.0104142644\n",
      "[1417,     1] loss: 0.0104142457\n",
      "[1418,     1] loss: 0.0104142264\n",
      "[1419,     1] loss: 0.0104142070\n",
      "[1420,     1] loss: 0.0104141861\n",
      "[1421,     1] loss: 0.0104141638\n",
      "[1422,     1] loss: 0.0104141414\n",
      "[1423,     1] loss: 0.0104141168\n",
      "[1424,     1] loss: 0.0104140922\n",
      "[1425,     1] loss: 0.0104140654\n",
      "[1426,     1] loss: 0.0104140393\n",
      "[1427,     1] loss: 0.0104140118\n",
      "[1428,     1] loss: 0.0104139827\n",
      "[1429,     1] loss: 0.0104139522\n",
      "[1430,     1] loss: 0.0104139209\n",
      "[1431,     1] loss: 0.0104138881\n",
      "[1432,     1] loss: 0.0104138553\n",
      "[1433,     1] loss: 0.0104138203\n",
      "[1434,     1] loss: 0.0104137860\n",
      "[1435,     1] loss: 0.0104137488\n",
      "[1436,     1] loss: 0.0104137093\n",
      "[1437,     1] loss: 0.0104136720\n",
      "[1438,     1] loss: 0.0104136340\n",
      "[1439,     1] loss: 0.0104135923\n",
      "[1440,     1] loss: 0.0104135506\n",
      "[1441,     1] loss: 0.0104135081\n",
      "[1442,     1] loss: 0.0104134634\n",
      "[1443,     1] loss: 0.0104134180\n",
      "[1444,     1] loss: 0.0104133710\n",
      "[1445,     1] loss: 0.0104133241\n",
      "[1446,     1] loss: 0.0104132757\n",
      "[1447,     1] loss: 0.0104132250\n",
      "[1448,     1] loss: 0.0104131751\n",
      "[1449,     1] loss: 0.0104131229\n",
      "[1450,     1] loss: 0.0104130678\n",
      "[1451,     1] loss: 0.0104130149\n",
      "[1452,     1] loss: 0.0104129583\n",
      "[1453,     1] loss: 0.0104129024\n",
      "[1454,     1] loss: 0.0104128428\n",
      "[1455,     1] loss: 0.0104127847\n",
      "[1456,     1] loss: 0.0104127243\n",
      "[1457,     1] loss: 0.0104126625\n",
      "[1458,     1] loss: 0.0104125991\n",
      "[1459,     1] loss: 0.0104125358\n",
      "[1460,     1] loss: 0.0104124695\n",
      "[1461,     1] loss: 0.0104124032\n",
      "[1462,     1] loss: 0.0104123354\n",
      "[1463,     1] loss: 0.0104122661\n",
      "[1464,     1] loss: 0.0104121953\n",
      "[1465,     1] loss: 0.0104121238\n",
      "[1466,     1] loss: 0.0104120515\n",
      "[1467,     1] loss: 0.0104119785\n",
      "[1468,     1] loss: 0.0104119018\n",
      "[1469,     1] loss: 0.0104118243\n",
      "[1470,     1] loss: 0.0104117468\n",
      "[1471,     1] loss: 0.0104116671\n",
      "[1472,     1] loss: 0.0104115881\n",
      "[1473,     1] loss: 0.0104115061\n",
      "[1474,     1] loss: 0.0104114212\n",
      "[1475,     1] loss: 0.0104113370\n",
      "[1476,     1] loss: 0.0104112513\n",
      "[1477,     1] loss: 0.0104111649\n",
      "[1478,     1] loss: 0.0104110740\n",
      "[1479,     1] loss: 0.0104109846\n",
      "[1480,     1] loss: 0.0104108937\n",
      "[1481,     1] loss: 0.0104108006\n",
      "[1482,     1] loss: 0.0104107052\n",
      "[1483,     1] loss: 0.0104106106\n",
      "[1484,     1] loss: 0.0104105137\n",
      "[1485,     1] loss: 0.0104104146\n",
      "[1486,     1] loss: 0.0104103148\n",
      "[1487,     1] loss: 0.0104102135\n",
      "[1488,     1] loss: 0.0104101107\n",
      "[1489,     1] loss: 0.0104100063\n",
      "[1490,     1] loss: 0.0104099005\n",
      "[1491,     1] loss: 0.0104097933\n",
      "[1492,     1] loss: 0.0104096845\n",
      "[1493,     1] loss: 0.0104095742\n",
      "[1494,     1] loss: 0.0104094632\n",
      "[1495,     1] loss: 0.0104093492\n",
      "[1496,     1] loss: 0.0104092360\n",
      "[1497,     1] loss: 0.0104091190\n",
      "[1498,     1] loss: 0.0104090013\n",
      "[1499,     1] loss: 0.0104088821\n",
      "[1500,     1] loss: 0.0104087599\n",
      "[1501,     1] loss: 0.0104086377\n",
      "[1502,     1] loss: 0.0104085140\n",
      "[1503,     1] loss: 0.0104083896\n",
      "[1504,     1] loss: 0.0104082614\n",
      "[1505,     1] loss: 0.0104081333\n",
      "[1506,     1] loss: 0.0104080029\n",
      "[1507,     1] loss: 0.0104078703\n",
      "[1508,     1] loss: 0.0104077376\n",
      "[1509,     1] loss: 0.0104076020\n",
      "[1510,     1] loss: 0.0104074657\n",
      "[1511,     1] loss: 0.0104073256\n",
      "[1512,     1] loss: 0.0104071863\n",
      "[1513,     1] loss: 0.0104070455\n",
      "[1514,     1] loss: 0.0104069009\n",
      "[1515,     1] loss: 0.0104067557\n",
      "[1516,     1] loss: 0.0104066089\n",
      "[1517,     1] loss: 0.0104064614\n",
      "[1518,     1] loss: 0.0104063101\n",
      "[1519,     1] loss: 0.0104061581\n",
      "[1520,     1] loss: 0.0104060046\n",
      "[1521,     1] loss: 0.0104058482\n",
      "[1522,     1] loss: 0.0104056902\n",
      "[1523,     1] loss: 0.0104055315\n",
      "[1524,     1] loss: 0.0104053713\n",
      "[1525,     1] loss: 0.0104052089\n",
      "[1526,     1] loss: 0.0104050443\n",
      "[1527,     1] loss: 0.0104048774\n",
      "[1528,     1] loss: 0.0104047097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1529,     1] loss: 0.0104045391\n",
      "[1530,     1] loss: 0.0104043677\n",
      "[1531,     1] loss: 0.0104041941\n",
      "[1532,     1] loss: 0.0104040168\n",
      "[1533,     1] loss: 0.0104038402\n",
      "[1534,     1] loss: 0.0104036614\n",
      "[1535,     1] loss: 0.0104034796\n",
      "[1536,     1] loss: 0.0104032971\n",
      "[1537,     1] loss: 0.0104031108\n",
      "[1538,     1] loss: 0.0104029246\n",
      "[1539,     1] loss: 0.0104027346\n",
      "[1540,     1] loss: 0.0104025431\n",
      "[1541,     1] loss: 0.0104023516\n",
      "[1542,     1] loss: 0.0104021549\n",
      "[1543,     1] loss: 0.0104019575\n",
      "[1544,     1] loss: 0.0104017593\n",
      "[1545,     1] loss: 0.0104015589\n",
      "[1546,     1] loss: 0.0104013547\n",
      "[1547,     1] loss: 0.0104011491\n",
      "[1548,     1] loss: 0.0104009435\n",
      "[1549,     1] loss: 0.0104007334\n",
      "[1550,     1] loss: 0.0104005203\n",
      "[1551,     1] loss: 0.0104003064\n",
      "[1552,     1] loss: 0.0104000911\n",
      "[1553,     1] loss: 0.0103998728\n",
      "[1554,     1] loss: 0.0103996523\n",
      "[1555,     1] loss: 0.0103994302\n",
      "[1556,     1] loss: 0.0103992060\n",
      "[1557,     1] loss: 0.0103989787\n",
      "[1558,     1] loss: 0.0103987500\n",
      "[1559,     1] loss: 0.0103985183\n",
      "[1560,     1] loss: 0.0103982843\n",
      "[1561,     1] loss: 0.0103980474\n",
      "[1562,     1] loss: 0.0103978112\n",
      "[1563,     1] loss: 0.0103975706\n",
      "[1564,     1] loss: 0.0103973277\n",
      "[1565,     1] loss: 0.0103970818\n",
      "[1566,     1] loss: 0.0103968352\n",
      "[1567,     1] loss: 0.0103965849\n",
      "[1568,     1] loss: 0.0103963338\n",
      "[1569,     1] loss: 0.0103960782\n",
      "[1570,     1] loss: 0.0103958212\n",
      "[1571,     1] loss: 0.0103955612\n",
      "[1572,     1] loss: 0.0103953004\n",
      "[1573,     1] loss: 0.0103950351\n",
      "[1574,     1] loss: 0.0103947677\n",
      "[1575,     1] loss: 0.0103944987\n",
      "[1576,     1] loss: 0.0103942260\n",
      "[1577,     1] loss: 0.0103939518\n",
      "[1578,     1] loss: 0.0103936739\n",
      "[1579,     1] loss: 0.0103933945\n",
      "[1580,     1] loss: 0.0103931122\n",
      "[1581,     1] loss: 0.0103928275\n",
      "[1582,     1] loss: 0.0103925385\n",
      "[1583,     1] loss: 0.0103922501\n",
      "[1584,     1] loss: 0.0103919566\n",
      "[1585,     1] loss: 0.0103916608\n",
      "[1586,     1] loss: 0.0103913620\n",
      "[1587,     1] loss: 0.0103910595\n",
      "[1588,     1] loss: 0.0103907563\n",
      "[1589,     1] loss: 0.0103904501\n",
      "[1590,     1] loss: 0.0103901401\n",
      "[1591,     1] loss: 0.0103898272\n",
      "[1592,     1] loss: 0.0103895128\n",
      "[1593,     1] loss: 0.0103891946\n",
      "[1594,     1] loss: 0.0103888728\n",
      "[1595,     1] loss: 0.0103885502\n",
      "[1596,     1] loss: 0.0103882231\n",
      "[1597,     1] loss: 0.0103878923\n",
      "[1598,     1] loss: 0.0103875600\n",
      "[1599,     1] loss: 0.0103872232\n",
      "[1600,     1] loss: 0.0103868842\n",
      "[1601,     1] loss: 0.0103865430\n",
      "[1602,     1] loss: 0.0103861973\n",
      "[1603,     1] loss: 0.0103858493\n",
      "[1604,     1] loss: 0.0103854984\n",
      "[1605,     1] loss: 0.0103851430\n",
      "[1606,     1] loss: 0.0103847854\n",
      "[1607,     1] loss: 0.0103844248\n",
      "[1608,     1] loss: 0.0103840612\n",
      "[1609,     1] loss: 0.0103836931\n",
      "[1610,     1] loss: 0.0103833221\n",
      "[1611,     1] loss: 0.0103829496\n",
      "[1612,     1] loss: 0.0103825718\n",
      "[1613,     1] loss: 0.0103821903\n",
      "[1614,     1] loss: 0.0103818066\n",
      "[1615,     1] loss: 0.0103814192\n",
      "[1616,     1] loss: 0.0103810281\n",
      "[1617,     1] loss: 0.0103806332\n",
      "[1618,     1] loss: 0.0103802353\n",
      "[1619,     1] loss: 0.0103798337\n",
      "[1620,     1] loss: 0.0103794284\n",
      "[1621,     1] loss: 0.0103790216\n",
      "[1622,     1] loss: 0.0103786089\n",
      "[1623,     1] loss: 0.0103781916\n",
      "[1624,     1] loss: 0.0103777736\n",
      "[1625,     1] loss: 0.0103773504\n",
      "[1626,     1] loss: 0.0103769228\n",
      "[1627,     1] loss: 0.0103764929\n",
      "[1628,     1] loss: 0.0103760578\n",
      "[1629,     1] loss: 0.0103756204\n",
      "[1630,     1] loss: 0.0103751779\n",
      "[1631,     1] loss: 0.0103747323\n",
      "[1632,     1] loss: 0.0103742830\n",
      "[1633,     1] loss: 0.0103738286\n",
      "[1634,     1] loss: 0.0103733711\n",
      "[1635,     1] loss: 0.0103729106\n",
      "[1636,     1] loss: 0.0103724442\n",
      "[1637,     1] loss: 0.0103719749\n",
      "[1638,     1] loss: 0.0103715010\n",
      "[1639,     1] loss: 0.0103710234\n",
      "[1640,     1] loss: 0.0103705414\n",
      "[1641,     1] loss: 0.0103700556\n",
      "[1642,     1] loss: 0.0103695653\n",
      "[1643,     1] loss: 0.0103690714\n",
      "[1644,     1] loss: 0.0103685722\n",
      "[1645,     1] loss: 0.0103680708\n",
      "[1646,     1] loss: 0.0103675626\n",
      "[1647,     1] loss: 0.0103670515\n",
      "[1648,     1] loss: 0.0103665359\n",
      "[1649,     1] loss: 0.0103660144\n",
      "[1650,     1] loss: 0.0103654914\n",
      "[1651,     1] loss: 0.0103649609\n",
      "[1652,     1] loss: 0.0103644274\n",
      "[1653,     1] loss: 0.0103638902\n",
      "[1654,     1] loss: 0.0103633471\n",
      "[1655,     1] loss: 0.0103628010\n",
      "[1656,     1] loss: 0.0103622481\n",
      "[1657,     1] loss: 0.0103616923\n",
      "[1658,     1] loss: 0.0103611320\n",
      "[1659,     1] loss: 0.0103605658\n",
      "[1660,     1] loss: 0.0103599958\n",
      "[1661,     1] loss: 0.0103594206\n",
      "[1662,     1] loss: 0.0103588425\n",
      "[1663,     1] loss: 0.0103582568\n",
      "[1664,     1] loss: 0.0103576668\n",
      "[1665,     1] loss: 0.0103570737\n",
      "[1666,     1] loss: 0.0103564739\n",
      "[1667,     1] loss: 0.0103558689\n",
      "[1668,     1] loss: 0.0103552617\n",
      "[1669,     1] loss: 0.0103546470\n",
      "[1670,     1] loss: 0.0103540286\n",
      "[1671,     1] loss: 0.0103534043\n",
      "[1672,     1] loss: 0.0103527755\n",
      "[1673,     1] loss: 0.0103521414\n",
      "[1674,     1] loss: 0.0103515036\n",
      "[1675,     1] loss: 0.0103508592\n",
      "[1676,     1] loss: 0.0103502095\n",
      "[1677,     1] loss: 0.0103495568\n",
      "[1678,     1] loss: 0.0103488959\n",
      "[1679,     1] loss: 0.0103482313\n",
      "[1680,     1] loss: 0.0103475630\n",
      "[1681,     1] loss: 0.0103468865\n",
      "[1682,     1] loss: 0.0103462063\n",
      "[1683,     1] loss: 0.0103455208\n",
      "[1684,     1] loss: 0.0103448294\n",
      "[1685,     1] loss: 0.0103441350\n",
      "[1686,     1] loss: 0.0103434324\n",
      "[1687,     1] loss: 0.0103427254\n",
      "[1688,     1] loss: 0.0103420138\n",
      "[1689,     1] loss: 0.0103412956\n",
      "[1690,     1] loss: 0.0103405729\n",
      "[1691,     1] loss: 0.0103398450\n",
      "[1692,     1] loss: 0.0103391103\n",
      "[1693,     1] loss: 0.0103383720\n",
      "[1694,     1] loss: 0.0103376277\n",
      "[1695,     1] loss: 0.0103368774\n",
      "[1696,     1] loss: 0.0103361204\n",
      "[1697,     1] loss: 0.0103353605\n",
      "[1698,     1] loss: 0.0103345938\n",
      "[1699,     1] loss: 0.0103338212\n",
      "[1700,     1] loss: 0.0103330441\n",
      "[1701,     1] loss: 0.0103322610\n",
      "[1702,     1] loss: 0.0103314720\n",
      "[1703,     1] loss: 0.0103306785\n",
      "[1704,     1] loss: 0.0103298791\n",
      "[1705,     1] loss: 0.0103290737\n",
      "[1706,     1] loss: 0.0103282630\n",
      "[1707,     1] loss: 0.0103274480\n",
      "[1708,     1] loss: 0.0103266262\n",
      "[1709,     1] loss: 0.0103257991\n",
      "[1710,     1] loss: 0.0103249662\n",
      "[1711,     1] loss: 0.0103241272\n",
      "[1712,     1] loss: 0.0103232838\n",
      "[1713,     1] loss: 0.0103224352\n",
      "[1714,     1] loss: 0.0103215806\n",
      "[1715,     1] loss: 0.0103207216\n",
      "[1716,     1] loss: 0.0103198543\n",
      "[1717,     1] loss: 0.0103189841\n",
      "[1718,     1] loss: 0.0103181079\n",
      "[1719,     1] loss: 0.0103172265\n",
      "[1720,     1] loss: 0.0103163391\n",
      "[1721,     1] loss: 0.0103154466\n",
      "[1722,     1] loss: 0.0103145495\n",
      "[1723,     1] loss: 0.0103136465\n",
      "[1724,     1] loss: 0.0103127375\n",
      "[1725,     1] loss: 0.0103118241\n",
      "[1726,     1] loss: 0.0103109047\n",
      "[1727,     1] loss: 0.0103099808\n",
      "[1728,     1] loss: 0.0103090517\n",
      "[1729,     1] loss: 0.0103081174\n",
      "[1730,     1] loss: 0.0103071786\n",
      "[1731,     1] loss: 0.0103062339\n",
      "[1732,     1] loss: 0.0103052847\n",
      "[1733,     1] loss: 0.0103043303\n",
      "[1734,     1] loss: 0.0103033707\n",
      "[1735,     1] loss: 0.0103024065\n",
      "[1736,     1] loss: 0.0103014372\n",
      "[1737,     1] loss: 0.0103004642\n",
      "[1738,     1] loss: 0.0102994852\n",
      "[1739,     1] loss: 0.0102985024\n",
      "[1740,     1] loss: 0.0102975130\n",
      "[1741,     1] loss: 0.0102965213\n",
      "[1742,     1] loss: 0.0102955252\n",
      "[1743,     1] loss: 0.0102945238\n",
      "[1744,     1] loss: 0.0102935173\n",
      "[1745,     1] loss: 0.0102925070\n",
      "[1746,     1] loss: 0.0102914944\n",
      "[1747,     1] loss: 0.0102904744\n",
      "[1748,     1] loss: 0.0102894522\n",
      "[1749,     1] loss: 0.0102884255\n",
      "[1750,     1] loss: 0.0102873951\n",
      "[1751,     1] loss: 0.0102863610\n",
      "[1752,     1] loss: 0.0102853224\n",
      "[1753,     1] loss: 0.0102842800\n",
      "[1754,     1] loss: 0.0102832340\n",
      "[1755,     1] loss: 0.0102821857\n",
      "[1756,     1] loss: 0.0102811329\n",
      "[1757,     1] loss: 0.0102800779\n",
      "[1758,     1] loss: 0.0102790184\n",
      "[1759,     1] loss: 0.0102779560\n",
      "[1760,     1] loss: 0.0102768905\n",
      "[1761,     1] loss: 0.0102758214\n",
      "[1762,     1] loss: 0.0102747500\n",
      "[1763,     1] loss: 0.0102736756\n",
      "[1764,     1] loss: 0.0102725990\n",
      "[1765,     1] loss: 0.0102715209\n",
      "[1766,     1] loss: 0.0102704398\n",
      "[1767,     1] loss: 0.0102693550\n",
      "[1768,     1] loss: 0.0102682672\n",
      "[1769,     1] loss: 0.0102671795\n",
      "[1770,     1] loss: 0.0102660894\n",
      "[1771,     1] loss: 0.0102649972\n",
      "[1772,     1] loss: 0.0102639034\n",
      "[1773,     1] loss: 0.0102628075\n",
      "[1774,     1] loss: 0.0102617100\n",
      "[1775,     1] loss: 0.0102606110\n",
      "[1776,     1] loss: 0.0102595106\n",
      "[1777,     1] loss: 0.0102584086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1778,     1] loss: 0.0102573067\n",
      "[1779,     1] loss: 0.0102562040\n",
      "[1780,     1] loss: 0.0102550983\n",
      "[1781,     1] loss: 0.0102539949\n",
      "[1782,     1] loss: 0.0102528900\n",
      "[1783,     1] loss: 0.0102517836\n",
      "[1784,     1] loss: 0.0102506772\n",
      "[1785,     1] loss: 0.0102495715\n",
      "[1786,     1] loss: 0.0102484651\n",
      "[1787,     1] loss: 0.0102473587\n",
      "[1788,     1] loss: 0.0102462530\n",
      "[1789,     1] loss: 0.0102451466\n",
      "[1790,     1] loss: 0.0102440409\n",
      "[1791,     1] loss: 0.0102429368\n",
      "[1792,     1] loss: 0.0102418326\n",
      "[1793,     1] loss: 0.0102407292\n",
      "[1794,     1] loss: 0.0102396272\n",
      "[1795,     1] loss: 0.0102385253\n",
      "[1796,     1] loss: 0.0102374263\n",
      "[1797,     1] loss: 0.0102363266\n",
      "[1798,     1] loss: 0.0102352299\n",
      "[1799,     1] loss: 0.0102341339\n",
      "[1800,     1] loss: 0.0102330402\n",
      "[1801,     1] loss: 0.0102319479\n",
      "[1802,     1] loss: 0.0102308579\n",
      "[1803,     1] loss: 0.0102297686\n",
      "[1804,     1] loss: 0.0102286823\n",
      "[1805,     1] loss: 0.0102275990\n",
      "[1806,     1] loss: 0.0102265172\n",
      "[1807,     1] loss: 0.0102254376\n",
      "[1808,     1] loss: 0.0102243617\n",
      "[1809,     1] loss: 0.0102232873\n",
      "[1810,     1] loss: 0.0102222160\n",
      "[1811,     1] loss: 0.0102211483\n",
      "[1812,     1] loss: 0.0102200821\n",
      "[1813,     1] loss: 0.0102190204\n",
      "[1814,     1] loss: 0.0102179617\n",
      "[1815,     1] loss: 0.0102169052\n",
      "[1816,     1] loss: 0.0102158517\n",
      "[1817,     1] loss: 0.0102148034\n",
      "[1818,     1] loss: 0.0102137573\n",
      "[1819,     1] loss: 0.0102127142\n",
      "[1820,     1] loss: 0.0102116749\n",
      "[1821,     1] loss: 0.0102106392\n",
      "[1822,     1] loss: 0.0102096073\n",
      "[1823,     1] loss: 0.0102085792\n",
      "[1824,     1] loss: 0.0102075554\n",
      "[1825,     1] loss: 0.0102065347\n",
      "[1826,     1] loss: 0.0102055185\n",
      "[1827,     1] loss: 0.0102045059\n",
      "[1828,     1] loss: 0.0102034971\n",
      "[1829,     1] loss: 0.0102024928\n",
      "[1830,     1] loss: 0.0102014907\n",
      "[1831,     1] loss: 0.0102004953\n",
      "[1832,     1] loss: 0.0101995029\n",
      "[1833,     1] loss: 0.0101985142\n",
      "[1834,     1] loss: 0.0101975307\n",
      "[1835,     1] loss: 0.0101965502\n",
      "[1836,     1] loss: 0.0101955742\n",
      "[1837,     1] loss: 0.0101946019\n",
      "[1838,     1] loss: 0.0101936333\n",
      "[1839,     1] loss: 0.0101926707\n",
      "[1840,     1] loss: 0.0101917110\n",
      "[1841,     1] loss: 0.0101907544\n",
      "[1842,     1] loss: 0.0101898044\n",
      "[1843,     1] loss: 0.0101888567\n",
      "[1844,     1] loss: 0.0101879142\n",
      "[1845,     1] loss: 0.0101869754\n",
      "[1846,     1] loss: 0.0101860389\n",
      "[1847,     1] loss: 0.0101851076\n",
      "[1848,     1] loss: 0.0101841800\n",
      "[1849,     1] loss: 0.0101832576\n",
      "[1850,     1] loss: 0.0101823390\n",
      "[1851,     1] loss: 0.0101814218\n",
      "[1852,     1] loss: 0.0101805113\n",
      "[1853,     1] loss: 0.0101796038\n",
      "[1854,     1] loss: 0.0101786979\n",
      "[1855,     1] loss: 0.0101777978\n",
      "[1856,     1] loss: 0.0101769023\n",
      "[1857,     1] loss: 0.0101760074\n",
      "[1858,     1] loss: 0.0101751179\n",
      "[1859,     1] loss: 0.0101742320\n",
      "[1860,     1] loss: 0.0101733491\n",
      "[1861,     1] loss: 0.0101724692\n",
      "[1862,     1] loss: 0.0101715930\n",
      "[1863,     1] loss: 0.0101707190\n",
      "[1864,     1] loss: 0.0101698495\n",
      "[1865,     1] loss: 0.0101689823\n",
      "[1866,     1] loss: 0.0101681180\n",
      "[1867,     1] loss: 0.0101672575\n",
      "[1868,     1] loss: 0.0101663999\n",
      "[1869,     1] loss: 0.0101655446\n",
      "[1870,     1] loss: 0.0101646908\n",
      "[1871,     1] loss: 0.0101638429\n",
      "[1872,     1] loss: 0.0101629943\n",
      "[1873,     1] loss: 0.0101621494\n",
      "[1874,     1] loss: 0.0101613075\n",
      "[1875,     1] loss: 0.0101604678\n",
      "[1876,     1] loss: 0.0101596296\n",
      "[1877,     1] loss: 0.0101587944\n",
      "[1878,     1] loss: 0.0101579607\n",
      "[1879,     1] loss: 0.0101571299\n",
      "[1880,     1] loss: 0.0101562999\n",
      "[1881,     1] loss: 0.0101554736\n",
      "[1882,     1] loss: 0.0101546481\n",
      "[1883,     1] loss: 0.0101538248\n",
      "[1884,     1] loss: 0.0101530023\n",
      "[1885,     1] loss: 0.0101521827\n",
      "[1886,     1] loss: 0.0101513632\n",
      "[1887,     1] loss: 0.0101505458\n",
      "[1888,     1] loss: 0.0101497307\n",
      "[1889,     1] loss: 0.0101489171\n",
      "[1890,     1] loss: 0.0101481028\n",
      "[1891,     1] loss: 0.0101472899\n",
      "[1892,     1] loss: 0.0101464801\n",
      "[1893,     1] loss: 0.0101456702\n",
      "[1894,     1] loss: 0.0101448610\n",
      "[1895,     1] loss: 0.0101440527\n",
      "[1896,     1] loss: 0.0101432458\n",
      "[1897,     1] loss: 0.0101424396\n",
      "[1898,     1] loss: 0.0101416335\n",
      "[1899,     1] loss: 0.0101408288\n",
      "[1900,     1] loss: 0.0101400234\n",
      "[1901,     1] loss: 0.0101392195\n",
      "[1902,     1] loss: 0.0101384155\n",
      "[1903,     1] loss: 0.0101376124\n",
      "[1904,     1] loss: 0.0101368092\n",
      "[1905,     1] loss: 0.0101360053\n",
      "[1906,     1] loss: 0.0101352021\n",
      "[1907,     1] loss: 0.0101343997\n",
      "[1908,     1] loss: 0.0101335965\n",
      "[1909,     1] loss: 0.0101327933\n",
      "[1910,     1] loss: 0.0101319902\n",
      "[1911,     1] loss: 0.0101311862\n",
      "[1912,     1] loss: 0.0101303831\n",
      "[1913,     1] loss: 0.0101295777\n",
      "[1914,     1] loss: 0.0101287730\n",
      "[1915,     1] loss: 0.0101279691\n",
      "[1916,     1] loss: 0.0101271622\n",
      "[1917,     1] loss: 0.0101263545\n",
      "[1918,     1] loss: 0.0101255491\n",
      "[1919,     1] loss: 0.0101247407\n",
      "[1920,     1] loss: 0.0101239316\n",
      "[1921,     1] loss: 0.0101231217\n",
      "[1922,     1] loss: 0.0101223111\n",
      "[1923,     1] loss: 0.0101214983\n",
      "[1924,     1] loss: 0.0101206847\n",
      "[1925,     1] loss: 0.0101198711\n",
      "[1926,     1] loss: 0.0101190552\n",
      "[1927,     1] loss: 0.0101182394\n",
      "[1928,     1] loss: 0.0101174206\n",
      "[1929,     1] loss: 0.0101166002\n",
      "[1930,     1] loss: 0.0101157784\n",
      "[1931,     1] loss: 0.0101149544\n",
      "[1932,     1] loss: 0.0101141296\n",
      "[1933,     1] loss: 0.0101133041\n",
      "[1934,     1] loss: 0.0101124749\n",
      "[1935,     1] loss: 0.0101116441\n",
      "[1936,     1] loss: 0.0101108111\n",
      "[1937,     1] loss: 0.0101099767\n",
      "[1938,     1] loss: 0.0101091385\n",
      "[1939,     1] loss: 0.0101082988\n",
      "[1940,     1] loss: 0.0101074569\n",
      "[1941,     1] loss: 0.0101066135\n",
      "[1942,     1] loss: 0.0101057671\n",
      "[1943,     1] loss: 0.0101049170\n",
      "[1944,     1] loss: 0.0101040646\n",
      "[1945,     1] loss: 0.0101032093\n",
      "[1946,     1] loss: 0.0101023518\n",
      "[1947,     1] loss: 0.0101014890\n",
      "[1948,     1] loss: 0.0101006262\n",
      "[1949,     1] loss: 0.0100997582\n",
      "[1950,     1] loss: 0.0100988865\n",
      "[1951,     1] loss: 0.0100980125\n",
      "[1952,     1] loss: 0.0100971334\n",
      "[1953,     1] loss: 0.0100962520\n",
      "[1954,     1] loss: 0.0100953661\n",
      "[1955,     1] loss: 0.0100944757\n",
      "[1956,     1] loss: 0.0100935824\n",
      "[1957,     1] loss: 0.0100926846\n",
      "[1958,     1] loss: 0.0100917816\n",
      "[1959,     1] loss: 0.0100908749\n",
      "[1960,     1] loss: 0.0100899629\n",
      "[1961,     1] loss: 0.0100890458\n",
      "[1962,     1] loss: 0.0100881249\n",
      "[1963,     1] loss: 0.0100871973\n",
      "[1964,     1] loss: 0.0100862660\n",
      "[1965,     1] loss: 0.0100853294\n",
      "[1966,     1] loss: 0.0100843862\n",
      "[1967,     1] loss: 0.0100834385\n",
      "[1968,     1] loss: 0.0100824840\n",
      "[1969,     1] loss: 0.0100815229\n",
      "[1970,     1] loss: 0.0100805566\n",
      "[1971,     1] loss: 0.0100795835\n",
      "[1972,     1] loss: 0.0100786045\n",
      "[1973,     1] loss: 0.0100776181\n",
      "[1974,     1] loss: 0.0100766249\n",
      "[1975,     1] loss: 0.0100756250\n",
      "[1976,     1] loss: 0.0100746177\n",
      "[1977,     1] loss: 0.0100736044\n",
      "[1978,     1] loss: 0.0100725830\n",
      "[1979,     1] loss: 0.0100715548\n",
      "[1980,     1] loss: 0.0100705184\n",
      "[1981,     1] loss: 0.0100694753\n",
      "[1982,     1] loss: 0.0100684248\n",
      "[1983,     1] loss: 0.0100673661\n",
      "[1984,     1] loss: 0.0100663014\n",
      "[1985,     1] loss: 0.0100652285\n",
      "[1986,     1] loss: 0.0100641489\n",
      "[1987,     1] loss: 0.0100630619\n",
      "[1988,     1] loss: 0.0100619681\n",
      "[1989,     1] loss: 0.0100608662\n",
      "[1990,     1] loss: 0.0100597613\n",
      "[1991,     1] loss: 0.0100586481\n",
      "[1992,     1] loss: 0.0100575306\n",
      "[1993,     1] loss: 0.0100564092\n",
      "[1994,     1] loss: 0.0100552827\n",
      "[1995,     1] loss: 0.0100541525\n",
      "[1996,     1] loss: 0.0100530207\n",
      "[1997,     1] loss: 0.0100518860\n",
      "[1998,     1] loss: 0.0100507528\n",
      "[1999,     1] loss: 0.0100496173\n",
      "[2000,     1] loss: 0.0100484833\n",
      "[2001,     1] loss: 0.0100473538\n",
      "[2002,     1] loss: 0.0100462288\n",
      "[2003,     1] loss: 0.0100451082\n",
      "[2004,     1] loss: 0.0100439951\n",
      "[2005,     1] loss: 0.0100428909\n",
      "[2006,     1] loss: 0.0100417979\n",
      "[2007,     1] loss: 0.0100407176\n",
      "[2008,     1] loss: 0.0100396499\n",
      "[2009,     1] loss: 0.0100385986\n",
      "[2010,     1] loss: 0.0100375660\n",
      "[2011,     1] loss: 0.0100365527\n",
      "[2012,     1] loss: 0.0100355603\n",
      "[2013,     1] loss: 0.0100345932\n",
      "[2014,     1] loss: 0.0100336492\n",
      "[2015,     1] loss: 0.0100327298\n",
      "[2016,     1] loss: 0.0100318402\n",
      "[2017,     1] loss: 0.0100309782\n",
      "[2018,     1] loss: 0.0100301445\n",
      "[2019,     1] loss: 0.0100293420\n",
      "[2020,     1] loss: 0.0100285679\n",
      "[2021,     1] loss: 0.0100278266\n",
      "[2022,     1] loss: 0.0100271158\n",
      "[2023,     1] loss: 0.0100264341\n",
      "[2024,     1] loss: 0.0100257806\n",
      "[2025,     1] loss: 0.0100251578\n",
      "[2026,     1] loss: 0.0100245625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2027,     1] loss: 0.0100239933\n",
      "[2028,     1] loss: 0.0100234501\n",
      "[2029,     1] loss: 0.0100229308\n",
      "[2030,     1] loss: 0.0100224338\n",
      "[2031,     1] loss: 0.0100219585\n",
      "[2032,     1] loss: 0.0100215040\n",
      "[2033,     1] loss: 0.0100210682\n",
      "[2034,     1] loss: 0.0100206487\n",
      "[2035,     1] loss: 0.0100202464\n",
      "[2036,     1] loss: 0.0100198582\n",
      "[2037,     1] loss: 0.0100194819\n",
      "[2038,     1] loss: 0.0100191198\n",
      "[2039,     1] loss: 0.0100187674\n",
      "[2040,     1] loss: 0.0100184269\n",
      "[2041,     1] loss: 0.0100180946\n",
      "[2042,     1] loss: 0.0100177713\n",
      "[2043,     1] loss: 0.0100174539\n",
      "[2044,     1] loss: 0.0100171439\n",
      "[2045,     1] loss: 0.0100168392\n",
      "[2046,     1] loss: 0.0100165412\n",
      "[2047,     1] loss: 0.0100162469\n",
      "[2048,     1] loss: 0.0100159578\n",
      "[2049,     1] loss: 0.0100156724\n",
      "[2050,     1] loss: 0.0100153901\n",
      "[2051,     1] loss: 0.0100151114\n",
      "[2052,     1] loss: 0.0100148350\n",
      "[2053,     1] loss: 0.0100145601\n",
      "[2054,     1] loss: 0.0100142874\n",
      "[2055,     1] loss: 0.0100140162\n",
      "[2056,     1] loss: 0.0100137472\n",
      "[2057,     1] loss: 0.0100134790\n",
      "[2058,     1] loss: 0.0100132123\n",
      "[2059,     1] loss: 0.0100129455\n",
      "[2060,     1] loss: 0.0100126810\n",
      "[2061,     1] loss: 0.0100124165\n",
      "[2062,     1] loss: 0.0100121535\n",
      "[2063,     1] loss: 0.0100118898\n",
      "[2064,     1] loss: 0.0100116268\n",
      "[2065,     1] loss: 0.0100113630\n",
      "[2066,     1] loss: 0.0100111008\n",
      "[2067,     1] loss: 0.0100108370\n",
      "[2068,     1] loss: 0.0100105740\n",
      "[2069,     1] loss: 0.0100103125\n",
      "[2070,     1] loss: 0.0100100495\n",
      "[2071,     1] loss: 0.0100097872\n",
      "[2072,     1] loss: 0.0100095242\n",
      "[2073,     1] loss: 0.0100092605\n",
      "[2074,     1] loss: 0.0100089975\n",
      "[2075,     1] loss: 0.0100087345\n",
      "[2076,     1] loss: 0.0100084715\n",
      "[2077,     1] loss: 0.0100082077\n",
      "[2078,     1] loss: 0.0100079454\n",
      "[2079,     1] loss: 0.0100076810\n",
      "[2080,     1] loss: 0.0100074172\n",
      "[2081,     1] loss: 0.0100071542\n",
      "[2082,     1] loss: 0.0100068897\n",
      "[2083,     1] loss: 0.0100066267\n",
      "[2084,     1] loss: 0.0100063637\n",
      "[2085,     1] loss: 0.0100060992\n",
      "[2086,     1] loss: 0.0100058369\n",
      "[2087,     1] loss: 0.0100055747\n",
      "[2088,     1] loss: 0.0100053109\n",
      "[2089,     1] loss: 0.0100050487\n",
      "[2090,     1] loss: 0.0100047849\n",
      "[2091,     1] loss: 0.0100045227\n",
      "[2092,     1] loss: 0.0100042604\n",
      "[2093,     1] loss: 0.0100040004\n",
      "[2094,     1] loss: 0.0100037381\n",
      "[2095,     1] loss: 0.0100034758\n",
      "[2096,     1] loss: 0.0100032158\n",
      "[2097,     1] loss: 0.0100029543\n",
      "[2098,     1] loss: 0.0100026958\n",
      "[2099,     1] loss: 0.0100024365\n",
      "[2100,     1] loss: 0.0100021772\n",
      "[2101,     1] loss: 0.0100019179\n",
      "[2102,     1] loss: 0.0100016601\n",
      "[2103,     1] loss: 0.0100014023\n",
      "[2104,     1] loss: 0.0100011453\n",
      "[2105,     1] loss: 0.0100008890\n",
      "[2106,     1] loss: 0.0100006334\n",
      "[2107,     1] loss: 0.0100003779\n",
      "[2108,     1] loss: 0.0100001223\n",
      "[2109,     1] loss: 0.0099998683\n",
      "[2110,     1] loss: 0.0099996150\n",
      "[2111,     1] loss: 0.0099993616\n",
      "[2112,     1] loss: 0.0099991098\n",
      "[2113,     1] loss: 0.0099988580\n",
      "[2114,     1] loss: 0.0099986054\n",
      "[2115,     1] loss: 0.0099983551\n",
      "[2116,     1] loss: 0.0099981047\n",
      "[2117,     1] loss: 0.0099978559\n",
      "[2118,     1] loss: 0.0099976070\n",
      "[2119,     1] loss: 0.0099973597\n",
      "[2120,     1] loss: 0.0099971130\n",
      "[2121,     1] loss: 0.0099968664\n",
      "[2122,     1] loss: 0.0099966198\n",
      "[2123,     1] loss: 0.0099963740\n",
      "[2124,     1] loss: 0.0099961288\n",
      "[2125,     1] loss: 0.0099958852\n",
      "[2126,     1] loss: 0.0099956423\n",
      "[2127,     1] loss: 0.0099953994\n",
      "[2128,     1] loss: 0.0099951558\n",
      "[2129,     1] loss: 0.0099949144\n",
      "[2130,     1] loss: 0.0099946745\n",
      "[2131,     1] loss: 0.0099944331\n",
      "[2132,     1] loss: 0.0099941939\n",
      "[2133,     1] loss: 0.0099939547\n",
      "[2134,     1] loss: 0.0099937163\n",
      "[2135,     1] loss: 0.0099934779\n",
      "[2136,     1] loss: 0.0099932402\n",
      "[2137,     1] loss: 0.0099930048\n",
      "[2138,     1] loss: 0.0099927671\n",
      "[2139,     1] loss: 0.0099925324\n",
      "[2140,     1] loss: 0.0099922977\n",
      "[2141,     1] loss: 0.0099920630\n",
      "[2142,     1] loss: 0.0099918284\n",
      "[2143,     1] loss: 0.0099915959\n",
      "[2144,     1] loss: 0.0099913634\n",
      "[2145,     1] loss: 0.0099911295\n",
      "[2146,     1] loss: 0.0099908978\n",
      "[2147,     1] loss: 0.0099906668\n",
      "[2148,     1] loss: 0.0099904351\n",
      "[2149,     1] loss: 0.0099902049\n",
      "[2150,     1] loss: 0.0099899754\n",
      "[2151,     1] loss: 0.0099897452\n",
      "[2152,     1] loss: 0.0099895157\n",
      "[2153,     1] loss: 0.0099892877\n",
      "[2154,     1] loss: 0.0099890582\n",
      "[2155,     1] loss: 0.0099888302\n",
      "[2156,     1] loss: 0.0099886030\n",
      "[2157,     1] loss: 0.0099883765\n",
      "[2158,     1] loss: 0.0099881500\n",
      "[2159,     1] loss: 0.0099879235\n",
      "[2160,     1] loss: 0.0099876963\n",
      "[2161,     1] loss: 0.0099874705\n",
      "[2162,     1] loss: 0.0099872455\n",
      "[2163,     1] loss: 0.0099870205\n",
      "[2164,     1] loss: 0.0099867947\n",
      "[2165,     1] loss: 0.0099865697\n",
      "[2166,     1] loss: 0.0099863455\n",
      "[2167,     1] loss: 0.0099861212\n",
      "[2168,     1] loss: 0.0099858969\n",
      "[2169,     1] loss: 0.0099856734\n",
      "[2170,     1] loss: 0.0099854492\n",
      "[2171,     1] loss: 0.0099852264\n",
      "[2172,     1] loss: 0.0099850029\n",
      "[2173,     1] loss: 0.0099847801\n",
      "[2174,     1] loss: 0.0099845558\n",
      "[2175,     1] loss: 0.0099843346\n",
      "[2176,     1] loss: 0.0099841133\n",
      "[2177,     1] loss: 0.0099838883\n",
      "[2178,     1] loss: 0.0099836662\n",
      "[2179,     1] loss: 0.0099834427\n",
      "[2180,     1] loss: 0.0099832207\n",
      "[2181,     1] loss: 0.0099829987\n",
      "[2182,     1] loss: 0.0099827752\n",
      "[2183,     1] loss: 0.0099825539\n",
      "[2184,     1] loss: 0.0099823311\n",
      "[2185,     1] loss: 0.0099821076\n",
      "[2186,     1] loss: 0.0099818863\n",
      "[2187,     1] loss: 0.0099816628\n",
      "[2188,     1] loss: 0.0099814408\n",
      "[2189,     1] loss: 0.0099812180\n",
      "[2190,     1] loss: 0.0099809960\n",
      "[2191,     1] loss: 0.0099807732\n",
      "[2192,     1] loss: 0.0099805504\n",
      "[2193,     1] loss: 0.0099803261\n",
      "[2194,     1] loss: 0.0099801026\n",
      "[2195,     1] loss: 0.0099798799\n",
      "[2196,     1] loss: 0.0099796571\n",
      "[2197,     1] loss: 0.0099794328\n",
      "[2198,     1] loss: 0.0099792093\n",
      "[2199,     1] loss: 0.0099789850\n",
      "[2200,     1] loss: 0.0099787593\n",
      "[2201,     1] loss: 0.0099785358\n",
      "[2202,     1] loss: 0.0099783093\n",
      "[2203,     1] loss: 0.0099780843\n",
      "[2204,     1] loss: 0.0099778593\n",
      "[2205,     1] loss: 0.0099776343\n",
      "[2206,     1] loss: 0.0099774078\n",
      "[2207,     1] loss: 0.0099771805\n",
      "[2208,     1] loss: 0.0099769540\n",
      "[2209,     1] loss: 0.0099767268\n",
      "[2210,     1] loss: 0.0099764988\n",
      "[2211,     1] loss: 0.0099762708\n",
      "[2212,     1] loss: 0.0099760413\n",
      "[2213,     1] loss: 0.0099758133\n",
      "[2214,     1] loss: 0.0099755839\n",
      "[2215,     1] loss: 0.0099753544\n",
      "[2216,     1] loss: 0.0099751242\n",
      "[2217,     1] loss: 0.0099748939\n",
      "[2218,     1] loss: 0.0099746622\n",
      "[2219,     1] loss: 0.0099744305\n",
      "[2220,     1] loss: 0.0099741988\n",
      "[2221,     1] loss: 0.0099739663\n",
      "[2222,     1] loss: 0.0099737339\n",
      "[2223,     1] loss: 0.0099734977\n",
      "[2224,     1] loss: 0.0099732645\n",
      "[2225,     1] loss: 0.0099730298\n",
      "[2226,     1] loss: 0.0099727944\n",
      "[2227,     1] loss: 0.0099725574\n",
      "[2228,     1] loss: 0.0099723205\n",
      "[2229,     1] loss: 0.0099720843\n",
      "[2230,     1] loss: 0.0099718459\n",
      "[2231,     1] loss: 0.0099716075\n",
      "[2232,     1] loss: 0.0099713676\n",
      "[2233,     1] loss: 0.0099711269\n",
      "[2234,     1] loss: 0.0099708863\n",
      "[2235,     1] loss: 0.0099706449\n",
      "[2236,     1] loss: 0.0099704035\n",
      "[2237,     1] loss: 0.0099701606\n",
      "[2238,     1] loss: 0.0099699169\n",
      "[2239,     1] loss: 0.0099696718\n",
      "[2240,     1] loss: 0.0099694267\n",
      "[2241,     1] loss: 0.0099691808\n",
      "[2242,     1] loss: 0.0099689342\n",
      "[2243,     1] loss: 0.0099686876\n",
      "[2244,     1] loss: 0.0099684387\n",
      "[2245,     1] loss: 0.0099681914\n",
      "[2246,     1] loss: 0.0099679403\n",
      "[2247,     1] loss: 0.0099676892\n",
      "[2248,     1] loss: 0.0099674374\n",
      "[2249,     1] loss: 0.0099671856\n",
      "[2250,     1] loss: 0.0099669330\n",
      "[2251,     1] loss: 0.0099666789\n",
      "[2252,     1] loss: 0.0099664234\n",
      "[2253,     1] loss: 0.0099661686\n",
      "[2254,     1] loss: 0.0099659123\n",
      "[2255,     1] loss: 0.0099656537\n",
      "[2256,     1] loss: 0.0099653959\n",
      "[2257,     1] loss: 0.0099651359\n",
      "[2258,     1] loss: 0.0099648766\n",
      "[2259,     1] loss: 0.0099646166\n",
      "[2260,     1] loss: 0.0099643551\n",
      "[2261,     1] loss: 0.0099640913\n",
      "[2262,     1] loss: 0.0099638276\n",
      "[2263,     1] loss: 0.0099635631\n",
      "[2264,     1] loss: 0.0099632978\n",
      "[2265,     1] loss: 0.0099630326\n",
      "[2266,     1] loss: 0.0099627644\n",
      "[2267,     1] loss: 0.0099624962\n",
      "[2268,     1] loss: 0.0099622272\n",
      "[2269,     1] loss: 0.0099619575\n",
      "[2270,     1] loss: 0.0099616870\n",
      "[2271,     1] loss: 0.0099614143\n",
      "[2272,     1] loss: 0.0099611416\n",
      "[2273,     1] loss: 0.0099608675\n",
      "[2274,     1] loss: 0.0099605925\n",
      "[2275,     1] loss: 0.0099603191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2276,     1] loss: 0.0099600419\n",
      "[2277,     1] loss: 0.0099597640\n",
      "[2278,     1] loss: 0.0099594854\n",
      "[2279,     1] loss: 0.0099592067\n",
      "[2280,     1] loss: 0.0099589273\n",
      "[2281,     1] loss: 0.0099586464\n",
      "[2282,     1] loss: 0.0099583648\n",
      "[2283,     1] loss: 0.0099580824\n",
      "[2284,     1] loss: 0.0099577971\n",
      "[2285,     1] loss: 0.0099575132\n",
      "[2286,     1] loss: 0.0099572286\n",
      "[2287,     1] loss: 0.0099569432\n",
      "[2288,     1] loss: 0.0099566557\n",
      "[2289,     1] loss: 0.0099563688\n",
      "[2290,     1] loss: 0.0099560797\n",
      "[2291,     1] loss: 0.0099557899\n",
      "[2292,     1] loss: 0.0099554993\n",
      "[2293,     1] loss: 0.0099552095\n",
      "[2294,     1] loss: 0.0099549174\n",
      "[2295,     1] loss: 0.0099546246\n",
      "[2296,     1] loss: 0.0099543318\n",
      "[2297,     1] loss: 0.0099540368\n",
      "[2298,     1] loss: 0.0099537432\n",
      "[2299,     1] loss: 0.0099534467\n",
      "[2300,     1] loss: 0.0099531509\n",
      "[2301,     1] loss: 0.0099528529\n",
      "[2302,     1] loss: 0.0099525556\n",
      "[2303,     1] loss: 0.0099522568\n",
      "[2304,     1] loss: 0.0099519573\n",
      "[2305,     1] loss: 0.0099516585\n",
      "[2306,     1] loss: 0.0099513575\n",
      "[2307,     1] loss: 0.0099510573\n",
      "[2308,     1] loss: 0.0099507548\n",
      "[2309,     1] loss: 0.0099504530\n",
      "[2310,     1] loss: 0.0099501505\n",
      "[2311,     1] loss: 0.0099498466\n",
      "[2312,     1] loss: 0.0099495411\n",
      "[2313,     1] loss: 0.0099492371\n",
      "[2314,     1] loss: 0.0099489331\n",
      "[2315,     1] loss: 0.0099486262\n",
      "[2316,     1] loss: 0.0099483207\n",
      "[2317,     1] loss: 0.0099480145\n",
      "[2318,     1] loss: 0.0099477082\n",
      "[2319,     1] loss: 0.0099474005\n",
      "[2320,     1] loss: 0.0099470921\n",
      "[2321,     1] loss: 0.0099467844\n",
      "[2322,     1] loss: 0.0099464744\n",
      "[2323,     1] loss: 0.0099461660\n",
      "[2324,     1] loss: 0.0099458568\n",
      "[2325,     1] loss: 0.0099455468\n",
      "[2326,     1] loss: 0.0099452369\n",
      "[2327,     1] loss: 0.0099449269\n",
      "[2328,     1] loss: 0.0099446155\n",
      "[2329,     1] loss: 0.0099443041\n",
      "[2330,     1] loss: 0.0099439949\n",
      "[2331,     1] loss: 0.0099436834\n",
      "[2332,     1] loss: 0.0099433705\n",
      "[2333,     1] loss: 0.0099430598\n",
      "[2334,     1] loss: 0.0099427477\n",
      "[2335,     1] loss: 0.0099424377\n",
      "[2336,     1] loss: 0.0099421240\n",
      "[2337,     1] loss: 0.0099418126\n",
      "[2338,     1] loss: 0.0099415004\n",
      "[2339,     1] loss: 0.0099411882\n",
      "[2340,     1] loss: 0.0099408746\n",
      "[2341,     1] loss: 0.0099405631\n",
      "[2342,     1] loss: 0.0099402502\n",
      "[2343,     1] loss: 0.0099399388\n",
      "[2344,     1] loss: 0.0099396259\n",
      "[2345,     1] loss: 0.0099393144\n",
      "[2346,     1] loss: 0.0099390015\n",
      "[2347,     1] loss: 0.0099386901\n",
      "[2348,     1] loss: 0.0099383786\n",
      "[2349,     1] loss: 0.0099380657\n",
      "[2350,     1] loss: 0.0099377550\n",
      "[2351,     1] loss: 0.0099374443\n",
      "[2352,     1] loss: 0.0099371321\n",
      "[2353,     1] loss: 0.0099368215\n",
      "[2354,     1] loss: 0.0099365093\n",
      "[2355,     1] loss: 0.0099362001\n",
      "[2356,     1] loss: 0.0099358901\n",
      "[2357,     1] loss: 0.0099355809\n",
      "[2358,     1] loss: 0.0099352710\n",
      "[2359,     1] loss: 0.0099349611\n",
      "[2360,     1] loss: 0.0099346519\n",
      "[2361,     1] loss: 0.0099343449\n",
      "[2362,     1] loss: 0.0099340357\n",
      "[2363,     1] loss: 0.0099337280\n",
      "[2364,     1] loss: 0.0099334203\n",
      "[2365,     1] loss: 0.0099331126\n",
      "[2366,     1] loss: 0.0099328063\n",
      "[2367,     1] loss: 0.0099325016\n",
      "[2368,     1] loss: 0.0099321954\n",
      "[2369,     1] loss: 0.0099318892\n",
      "[2370,     1] loss: 0.0099315852\n",
      "[2371,     1] loss: 0.0099312820\n",
      "[2372,     1] loss: 0.0099309780\n",
      "[2373,     1] loss: 0.0099306755\n",
      "[2374,     1] loss: 0.0099303722\n",
      "[2375,     1] loss: 0.0099300712\n",
      "[2376,     1] loss: 0.0099297702\n",
      "[2377,     1] loss: 0.0099294700\n",
      "[2378,     1] loss: 0.0099291690\n",
      "[2379,     1] loss: 0.0099288695\n",
      "[2380,     1] loss: 0.0099285714\n",
      "[2381,     1] loss: 0.0099282727\n",
      "[2382,     1] loss: 0.0099279769\n",
      "[2383,     1] loss: 0.0099276796\n",
      "[2384,     1] loss: 0.0099273831\n",
      "[2385,     1] loss: 0.0099270888\n",
      "[2386,     1] loss: 0.0099267945\n",
      "[2387,     1] loss: 0.0099264994\n",
      "[2388,     1] loss: 0.0099262074\n",
      "[2389,     1] loss: 0.0099259153\n",
      "[2390,     1] loss: 0.0099256240\n",
      "[2391,     1] loss: 0.0099253342\n",
      "[2392,     1] loss: 0.0099250451\n",
      "[2393,     1] loss: 0.0099247545\n",
      "[2394,     1] loss: 0.0099244662\n",
      "[2395,     1] loss: 0.0099241793\n",
      "[2396,     1] loss: 0.0099238940\n",
      "[2397,     1] loss: 0.0099236079\n",
      "[2398,     1] loss: 0.0099233225\n",
      "[2399,     1] loss: 0.0099230386\n",
      "[2400,     1] loss: 0.0099227548\n",
      "[2401,     1] loss: 0.0099224731\n",
      "[2402,     1] loss: 0.0099221915\n",
      "[2403,     1] loss: 0.0099219121\n",
      "[2404,     1] loss: 0.0099216320\n",
      "[2405,     1] loss: 0.0099213533\n",
      "[2406,     1] loss: 0.0099210761\n",
      "[2407,     1] loss: 0.0099207990\n",
      "[2408,     1] loss: 0.0099205241\n",
      "[2409,     1] loss: 0.0099202484\n",
      "[2410,     1] loss: 0.0099199764\n",
      "[2411,     1] loss: 0.0099197023\n",
      "[2412,     1] loss: 0.0099194296\n",
      "[2413,     1] loss: 0.0099191591\n",
      "[2414,     1] loss: 0.0099188894\n",
      "[2415,     1] loss: 0.0099186212\n",
      "[2416,     1] loss: 0.0099183522\n",
      "[2417,     1] loss: 0.0099180855\n",
      "[2418,     1] loss: 0.0099178188\n",
      "[2419,     1] loss: 0.0099175543\n",
      "[2420,     1] loss: 0.0099172913\n",
      "[2421,     1] loss: 0.0099170275\n",
      "[2422,     1] loss: 0.0099167660\n",
      "[2423,     1] loss: 0.0099165052\n",
      "[2424,     1] loss: 0.0099162444\n",
      "[2425,     1] loss: 0.0099159867\n",
      "[2426,     1] loss: 0.0099157289\n",
      "[2427,     1] loss: 0.0099154733\n",
      "[2428,     1] loss: 0.0099152163\n",
      "[2429,     1] loss: 0.0099149615\n",
      "[2430,     1] loss: 0.0099147074\n",
      "[2431,     1] loss: 0.0099144541\n",
      "[2432,     1] loss: 0.0099142030\n",
      "[2433,     1] loss: 0.0099139526\n",
      "[2434,     1] loss: 0.0099137045\n",
      "[2435,     1] loss: 0.0099134557\n",
      "[2436,     1] loss: 0.0099132083\n",
      "[2437,     1] loss: 0.0099129625\n",
      "[2438,     1] loss: 0.0099127181\n",
      "[2439,     1] loss: 0.0099124730\n",
      "[2440,     1] loss: 0.0099122308\n",
      "[2441,     1] loss: 0.0099119887\n",
      "[2442,     1] loss: 0.0099117480\n",
      "[2443,     1] loss: 0.0099115089\n",
      "[2444,     1] loss: 0.0099112704\n",
      "[2445,     1] loss: 0.0099110328\n",
      "[2446,     1] loss: 0.0099107951\n",
      "[2447,     1] loss: 0.0099105597\n",
      "[2448,     1] loss: 0.0099103257\n",
      "[2449,     1] loss: 0.0099100940\n",
      "[2450,     1] loss: 0.0099098608\n",
      "[2451,     1] loss: 0.0099096321\n",
      "[2452,     1] loss: 0.0099094018\n",
      "[2453,     1] loss: 0.0099091724\n",
      "[2454,     1] loss: 0.0099089451\n",
      "[2455,     1] loss: 0.0099087194\n",
      "[2456,     1] loss: 0.0099084944\n",
      "[2457,     1] loss: 0.0099082701\n",
      "[2458,     1] loss: 0.0099080466\n",
      "[2459,     1] loss: 0.0099078253\n",
      "[2460,     1] loss: 0.0099076033\n",
      "[2461,     1] loss: 0.0099073842\n",
      "[2462,     1] loss: 0.0099071652\n",
      "[2463,     1] loss: 0.0099069476\n",
      "[2464,     1] loss: 0.0099067315\n",
      "[2465,     1] loss: 0.0099065170\n",
      "[2466,     1] loss: 0.0099063024\n",
      "[2467,     1] loss: 0.0099060901\n",
      "[2468,     1] loss: 0.0099058777\n",
      "[2469,     1] loss: 0.0099056654\n",
      "[2470,     1] loss: 0.0099054575\n",
      "[2471,     1] loss: 0.0099052481\n",
      "[2472,     1] loss: 0.0099050410\n",
      "[2473,     1] loss: 0.0099048361\n",
      "[2474,     1] loss: 0.0099046312\n",
      "[2475,     1] loss: 0.0099044256\n",
      "[2476,     1] loss: 0.0099042237\n",
      "[2477,     1] loss: 0.0099040218\n",
      "[2478,     1] loss: 0.0099038228\n",
      "[2479,     1] loss: 0.0099036209\n",
      "[2480,     1] loss: 0.0099034227\n",
      "[2481,     1] loss: 0.0099032268\n",
      "[2482,     1] loss: 0.0099030316\n",
      "[2483,     1] loss: 0.0099028364\n",
      "[2484,     1] loss: 0.0099026427\n",
      "[2485,     1] loss: 0.0099024497\n",
      "[2486,     1] loss: 0.0099022582\n",
      "[2487,     1] loss: 0.0099020682\n",
      "[2488,     1] loss: 0.0099018805\n",
      "[2489,     1] loss: 0.0099016912\n",
      "[2490,     1] loss: 0.0099015042\n",
      "[2491,     1] loss: 0.0099013187\n",
      "[2492,     1] loss: 0.0099011354\n",
      "[2493,     1] loss: 0.0099009499\n",
      "[2494,     1] loss: 0.0099007688\n",
      "[2495,     1] loss: 0.0099005878\n",
      "[2496,     1] loss: 0.0099004082\n",
      "[2497,     1] loss: 0.0099002294\n",
      "[2498,     1] loss: 0.0099000514\n",
      "[2499,     1] loss: 0.0098998755\n",
      "[2500,     1] loss: 0.0098997004\n",
      "[2501,     1] loss: 0.0098995261\n",
      "[2502,     1] loss: 0.0098993525\n",
      "[2503,     1] loss: 0.0098991811\n",
      "[2504,     1] loss: 0.0098990113\n",
      "[2505,     1] loss: 0.0098988406\n",
      "[2506,     1] loss: 0.0098986730\n",
      "[2507,     1] loss: 0.0098985046\n",
      "[2508,     1] loss: 0.0098983385\n",
      "[2509,     1] loss: 0.0098981738\n",
      "[2510,     1] loss: 0.0098980106\n",
      "[2511,     1] loss: 0.0098978490\n",
      "[2512,     1] loss: 0.0098976873\n",
      "[2513,     1] loss: 0.0098975264\n",
      "[2514,     1] loss: 0.0098973684\n",
      "[2515,     1] loss: 0.0098972104\n",
      "[2516,     1] loss: 0.0098970525\n",
      "[2517,     1] loss: 0.0098968983\n",
      "[2518,     1] loss: 0.0098967440\n",
      "[2519,     1] loss: 0.0098965891\n",
      "[2520,     1] loss: 0.0098964378\n",
      "[2521,     1] loss: 0.0098962873\n",
      "[2522,     1] loss: 0.0098961383\n",
      "[2523,     1] loss: 0.0098959893\n",
      "[2524,     1] loss: 0.0098958418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2525,     1] loss: 0.0098956957\n",
      "[2526,     1] loss: 0.0098955512\n",
      "[2527,     1] loss: 0.0098954082\n",
      "[2528,     1] loss: 0.0098952658\n",
      "[2529,     1] loss: 0.0098951235\n",
      "[2530,     1] loss: 0.0098949827\n",
      "[2531,     1] loss: 0.0098948434\n",
      "[2532,     1] loss: 0.0098947071\n",
      "[2533,     1] loss: 0.0098945692\n",
      "[2534,     1] loss: 0.0098944336\n",
      "[2535,     1] loss: 0.0098943010\n",
      "[2536,     1] loss: 0.0098941669\n",
      "[2537,     1] loss: 0.0098940350\n",
      "[2538,     1] loss: 0.0098939054\n",
      "[2539,     1] loss: 0.0098937750\n",
      "[2540,     1] loss: 0.0098936468\n",
      "[2541,     1] loss: 0.0098935194\n",
      "[2542,     1] loss: 0.0098933935\n",
      "[2543,     1] loss: 0.0098932691\n",
      "[2544,     1] loss: 0.0098931454\n",
      "[2545,     1] loss: 0.0098930232\n",
      "[2546,     1] loss: 0.0098929018\n",
      "[2547,     1] loss: 0.0098927826\n",
      "[2548,     1] loss: 0.0098926641\n",
      "[2549,     1] loss: 0.0098925456\n",
      "[2550,     1] loss: 0.0098924287\n",
      "[2551,     1] loss: 0.0098923139\n",
      "[2552,     1] loss: 0.0098921999\n",
      "[2553,     1] loss: 0.0098920874\n",
      "[2554,     1] loss: 0.0098919764\n",
      "[2555,     1] loss: 0.0098918647\n",
      "[2556,     1] loss: 0.0098917551\n",
      "[2557,     1] loss: 0.0098916478\n",
      "[2558,     1] loss: 0.0098915428\n",
      "[2559,     1] loss: 0.0098914362\n",
      "[2560,     1] loss: 0.0098913319\n",
      "[2561,     1] loss: 0.0098912276\n",
      "[2562,     1] loss: 0.0098911263\n",
      "[2563,     1] loss: 0.0098910265\n",
      "[2564,     1] loss: 0.0098909266\n",
      "[2565,     1] loss: 0.0098908275\n",
      "[2566,     1] loss: 0.0098907307\n",
      "[2567,     1] loss: 0.0098906346\n",
      "[2568,     1] loss: 0.0098905399\n",
      "[2569,     1] loss: 0.0098904461\n",
      "[2570,     1] loss: 0.0098903537\n",
      "[2571,     1] loss: 0.0098902628\n",
      "[2572,     1] loss: 0.0098901726\n",
      "[2573,     1] loss: 0.0098900832\n",
      "[2574,     1] loss: 0.0098899953\n",
      "[2575,     1] loss: 0.0098899096\n",
      "[2576,     1] loss: 0.0098898254\n",
      "[2577,     1] loss: 0.0098897398\n",
      "[2578,     1] loss: 0.0098896571\n",
      "[2579,     1] loss: 0.0098895766\n",
      "[2580,     1] loss: 0.0098894954\n",
      "[2581,     1] loss: 0.0098894157\n",
      "[2582,     1] loss: 0.0098893382\n",
      "[2583,     1] loss: 0.0098892607\n",
      "[2584,     1] loss: 0.0098891847\n",
      "[2585,     1] loss: 0.0098891102\n",
      "[2586,     1] loss: 0.0098890372\n",
      "[2587,     1] loss: 0.0098889641\n",
      "[2588,     1] loss: 0.0098888934\n",
      "[2589,     1] loss: 0.0098888226\n",
      "[2590,     1] loss: 0.0098887548\n",
      "[2591,     1] loss: 0.0098886870\n",
      "[2592,     1] loss: 0.0098886207\n",
      "[2593,     1] loss: 0.0098885559\n",
      "[2594,     1] loss: 0.0098884903\n",
      "[2595,     1] loss: 0.0098884277\n",
      "[2596,     1] loss: 0.0098883651\n",
      "[2597,     1] loss: 0.0098883048\n",
      "[2598,     1] loss: 0.0098882452\n",
      "[2599,     1] loss: 0.0098881863\n",
      "[2600,     1] loss: 0.0098881289\n",
      "[2601,     1] loss: 0.0098880723\n",
      "[2602,     1] loss: 0.0098880172\n",
      "[2603,     1] loss: 0.0098879620\n",
      "[2604,     1] loss: 0.0098879091\n",
      "[2605,     1] loss: 0.0098878577\n",
      "[2606,     1] loss: 0.0098878071\n",
      "[2607,     1] loss: 0.0098877579\n",
      "[2608,     1] loss: 0.0098877080\n",
      "[2609,     1] loss: 0.0098876595\n",
      "[2610,     1] loss: 0.0098876141\n",
      "[2611,     1] loss: 0.0098875694\n",
      "[2612,     1] loss: 0.0098875232\n",
      "[2613,     1] loss: 0.0098874800\n",
      "[2614,     1] loss: 0.0098874375\n",
      "[2615,     1] loss: 0.0098873965\n",
      "[2616,     1] loss: 0.0098873556\n",
      "[2617,     1] loss: 0.0098873161\n",
      "[2618,     1] loss: 0.0098872773\n",
      "[2619,     1] loss: 0.0098872393\n",
      "[2620,     1] loss: 0.0098872036\n",
      "[2621,     1] loss: 0.0098871663\n",
      "[2622,     1] loss: 0.0098871320\n",
      "[2623,     1] loss: 0.0098870978\n",
      "[2624,     1] loss: 0.0098870642\n",
      "[2625,     1] loss: 0.0098870330\n",
      "[2626,     1] loss: 0.0098870024\n",
      "[2627,     1] loss: 0.0098869719\n",
      "[2628,     1] loss: 0.0098869435\n",
      "[2629,     1] loss: 0.0098869137\n",
      "[2630,     1] loss: 0.0098868869\n",
      "[2631,     1] loss: 0.0098868608\n",
      "[2632,     1] loss: 0.0098868348\n",
      "[2633,     1] loss: 0.0098868094\n",
      "[2634,     1] loss: 0.0098867856\n",
      "[2635,     1] loss: 0.0098867610\n",
      "[2636,     1] loss: 0.0098867387\n",
      "[2637,     1] loss: 0.0098867171\n",
      "[2638,     1] loss: 0.0098866962\n",
      "[2639,     1] loss: 0.0098866753\n",
      "[2640,     1] loss: 0.0098866567\n",
      "[2641,     1] loss: 0.0098866373\n",
      "[2642,     1] loss: 0.0098866187\n",
      "[2643,     1] loss: 0.0098866008\n",
      "[2644,     1] loss: 0.0098865837\n",
      "[2645,     1] loss: 0.0098865688\n",
      "[2646,     1] loss: 0.0098865531\n",
      "[2647,     1] loss: 0.0098865382\n",
      "[2648,     1] loss: 0.0098865233\n",
      "[2649,     1] loss: 0.0098865114\n",
      "[2650,     1] loss: 0.0098864973\n",
      "[2651,     1] loss: 0.0098864846\n",
      "[2652,     1] loss: 0.0098864734\n",
      "[2653,     1] loss: 0.0098864615\n",
      "[2654,     1] loss: 0.0098864511\n",
      "[2655,     1] loss: 0.0098864406\n",
      "[2656,     1] loss: 0.0098864317\n",
      "[2657,     1] loss: 0.0098864213\n",
      "[2658,     1] loss: 0.0098864123\n",
      "[2659,     1] loss: 0.0098864049\n",
      "[2660,     1] loss: 0.0098863967\n",
      "[2661,     1] loss: 0.0098863892\n",
      "[2662,     1] loss: 0.0098863818\n",
      "[2663,     1] loss: 0.0098863758\n",
      "[2664,     1] loss: 0.0098863699\n",
      "[2665,     1] loss: 0.0098863624\n",
      "[2666,     1] loss: 0.0098863579\n",
      "[2667,     1] loss: 0.0098863520\n",
      "[2668,     1] loss: 0.0098863475\n",
      "[2669,     1] loss: 0.0098863423\n",
      "[2670,     1] loss: 0.0098863371\n",
      "[2671,     1] loss: 0.0098863333\n",
      "[2672,     1] loss: 0.0098863304\n",
      "[2673,     1] loss: 0.0098863252\n",
      "[2674,     1] loss: 0.0098863207\n",
      "[2675,     1] loss: 0.0098863192\n",
      "[2676,     1] loss: 0.0098863155\n",
      "[2677,     1] loss: 0.0098863117\n",
      "[2678,     1] loss: 0.0098863102\n",
      "[2679,     1] loss: 0.0098863065\n",
      "[2680,     1] loss: 0.0098863058\n",
      "[2681,     1] loss: 0.0098863013\n",
      "[2682,     1] loss: 0.0098862991\n",
      "[2683,     1] loss: 0.0098862968\n",
      "[2684,     1] loss: 0.0098862939\n",
      "[2685,     1] loss: 0.0098862916\n",
      "[2686,     1] loss: 0.0098862894\n",
      "[2687,     1] loss: 0.0098862879\n",
      "[2688,     1] loss: 0.0098862857\n",
      "[2689,     1] loss: 0.0098862834\n",
      "[2690,     1] loss: 0.0098862797\n",
      "[2691,     1] loss: 0.0098862790\n",
      "[2692,     1] loss: 0.0098862760\n",
      "[2693,     1] loss: 0.0098862723\n",
      "[2694,     1] loss: 0.0098862715\n",
      "[2695,     1] loss: 0.0098862685\n",
      "[2696,     1] loss: 0.0098862641\n",
      "[2697,     1] loss: 0.0098862611\n",
      "[2698,     1] loss: 0.0098862581\n",
      "[2699,     1] loss: 0.0098862551\n",
      "[2700,     1] loss: 0.0098862514\n",
      "[2701,     1] loss: 0.0098862477\n",
      "[2702,     1] loss: 0.0098862454\n",
      "[2703,     1] loss: 0.0098862410\n",
      "[2704,     1] loss: 0.0098862365\n",
      "[2705,     1] loss: 0.0098862328\n",
      "[2706,     1] loss: 0.0098862283\n",
      "[2707,     1] loss: 0.0098862231\n",
      "[2708,     1] loss: 0.0098862179\n",
      "[2709,     1] loss: 0.0098862119\n",
      "[2710,     1] loss: 0.0098862074\n",
      "[2711,     1] loss: 0.0098862007\n",
      "[2712,     1] loss: 0.0098861948\n",
      "[2713,     1] loss: 0.0098861888\n",
      "[2714,     1] loss: 0.0098861806\n",
      "[2715,     1] loss: 0.0098861746\n",
      "[2716,     1] loss: 0.0098861672\n",
      "[2717,     1] loss: 0.0098861590\n",
      "[2718,     1] loss: 0.0098861516\n",
      "[2719,     1] loss: 0.0098861434\n",
      "[2720,     1] loss: 0.0098861359\n",
      "[2721,     1] loss: 0.0098861255\n",
      "[2722,     1] loss: 0.0098861158\n",
      "[2723,     1] loss: 0.0098861068\n",
      "[2724,     1] loss: 0.0098860972\n",
      "[2725,     1] loss: 0.0098860860\n",
      "[2726,     1] loss: 0.0098860763\n",
      "[2727,     1] loss: 0.0098860651\n",
      "[2728,     1] loss: 0.0098860539\n",
      "[2729,     1] loss: 0.0098860413\n",
      "[2730,     1] loss: 0.0098860286\n",
      "[2731,     1] loss: 0.0098860160\n",
      "[2732,     1] loss: 0.0098860025\n",
      "[2733,     1] loss: 0.0098859899\n",
      "[2734,     1] loss: 0.0098859765\n",
      "[2735,     1] loss: 0.0098859623\n",
      "[2736,     1] loss: 0.0098859474\n",
      "[2737,     1] loss: 0.0098859333\n",
      "[2738,     1] loss: 0.0098859169\n",
      "[2739,     1] loss: 0.0098859012\n",
      "[2740,     1] loss: 0.0098858848\n",
      "[2741,     1] loss: 0.0098858684\n",
      "[2742,     1] loss: 0.0098858505\n",
      "[2743,     1] loss: 0.0098858334\n",
      "[2744,     1] loss: 0.0098858155\n",
      "[2745,     1] loss: 0.0098857984\n",
      "[2746,     1] loss: 0.0098857768\n",
      "[2747,     1] loss: 0.0098857597\n",
      "[2748,     1] loss: 0.0098857395\n",
      "[2749,     1] loss: 0.0098857202\n",
      "[2750,     1] loss: 0.0098856993\n",
      "[2751,     1] loss: 0.0098856792\n",
      "[2752,     1] loss: 0.0098856568\n",
      "[2753,     1] loss: 0.0098856352\n",
      "[2754,     1] loss: 0.0098856144\n",
      "[2755,     1] loss: 0.0098855905\n",
      "[2756,     1] loss: 0.0098855689\n",
      "[2757,     1] loss: 0.0098855458\n",
      "[2758,     1] loss: 0.0098855212\n",
      "[2759,     1] loss: 0.0098854974\n",
      "[2760,     1] loss: 0.0098854735\n",
      "[2761,     1] loss: 0.0098854475\n",
      "[2762,     1] loss: 0.0098854214\n",
      "[2763,     1] loss: 0.0098853961\n",
      "[2764,     1] loss: 0.0098853700\n",
      "[2765,     1] loss: 0.0098853439\n",
      "[2766,     1] loss: 0.0098853156\n",
      "[2767,     1] loss: 0.0098852895\n",
      "[2768,     1] loss: 0.0098852612\n",
      "[2769,     1] loss: 0.0098852329\n",
      "[2770,     1] loss: 0.0098852046\n",
      "[2771,     1] loss: 0.0098851748\n",
      "[2772,     1] loss: 0.0098851472\n",
      "[2773,     1] loss: 0.0098851182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2774,     1] loss: 0.0098850869\n",
      "[2775,     1] loss: 0.0098850563\n",
      "[2776,     1] loss: 0.0098850258\n",
      "[2777,     1] loss: 0.0098849952\n",
      "[2778,     1] loss: 0.0098849624\n",
      "[2779,     1] loss: 0.0098849311\n",
      "[2780,     1] loss: 0.0098848976\n",
      "[2781,     1] loss: 0.0098848671\n",
      "[2782,     1] loss: 0.0098848328\n",
      "[2783,     1] loss: 0.0098848000\n",
      "[2784,     1] loss: 0.0098847657\n",
      "[2785,     1] loss: 0.0098847315\n",
      "[2786,     1] loss: 0.0098846965\n",
      "[2787,     1] loss: 0.0098846622\n",
      "[2788,     1] loss: 0.0098846272\n",
      "[2789,     1] loss: 0.0098845907\n",
      "[2790,     1] loss: 0.0098845556\n",
      "[2791,     1] loss: 0.0098845191\n",
      "[2792,     1] loss: 0.0098844834\n",
      "[2793,     1] loss: 0.0098844454\n",
      "[2794,     1] loss: 0.0098844096\n",
      "[2795,     1] loss: 0.0098843724\n",
      "[2796,     1] loss: 0.0098843336\n",
      "[2797,     1] loss: 0.0098842964\n",
      "[2798,     1] loss: 0.0098842569\n",
      "[2799,     1] loss: 0.0098842174\n",
      "[2800,     1] loss: 0.0098841801\n",
      "[2801,     1] loss: 0.0098841406\n",
      "[2802,     1] loss: 0.0098841004\n",
      "[2803,     1] loss: 0.0098840602\n",
      "[2804,     1] loss: 0.0098840199\n",
      "[2805,     1] loss: 0.0098839797\n",
      "[2806,     1] loss: 0.0098839380\n",
      "[2807,     1] loss: 0.0098838970\n",
      "[2808,     1] loss: 0.0098838560\n",
      "[2809,     1] loss: 0.0098838136\n",
      "[2810,     1] loss: 0.0098837726\n",
      "[2811,     1] loss: 0.0098837309\n",
      "[2812,     1] loss: 0.0098836869\n",
      "[2813,     1] loss: 0.0098836444\n",
      "[2814,     1] loss: 0.0098836020\n",
      "[2815,     1] loss: 0.0098835580\n",
      "[2816,     1] loss: 0.0098835140\n",
      "[2817,     1] loss: 0.0098834708\n",
      "[2818,     1] loss: 0.0098834269\n",
      "[2819,     1] loss: 0.0098833822\n",
      "[2820,     1] loss: 0.0098833375\n",
      "[2821,     1] loss: 0.0098832935\n",
      "[2822,     1] loss: 0.0098832481\n",
      "[2823,     1] loss: 0.0098832019\n",
      "[2824,     1] loss: 0.0098831579\n",
      "[2825,     1] loss: 0.0098831110\n",
      "[2826,     1] loss: 0.0098830640\n",
      "[2827,     1] loss: 0.0098830193\n",
      "[2828,     1] loss: 0.0098829739\n",
      "[2829,     1] loss: 0.0098829262\n",
      "[2830,     1] loss: 0.0098828793\n",
      "[2831,     1] loss: 0.0098828316\n",
      "[2832,     1] loss: 0.0098827839\n",
      "[2833,     1] loss: 0.0098827370\n",
      "[2834,     1] loss: 0.0098826885\n",
      "[2835,     1] loss: 0.0098826401\n",
      "[2836,     1] loss: 0.0098825932\n",
      "[2837,     1] loss: 0.0098825440\n",
      "[2838,     1] loss: 0.0098824956\n",
      "[2839,     1] loss: 0.0098824479\n",
      "[2840,     1] loss: 0.0098823979\n",
      "[2841,     1] loss: 0.0098823495\n",
      "[2842,     1] loss: 0.0098823003\n",
      "[2843,     1] loss: 0.0098822504\n",
      "[2844,     1] loss: 0.0098822005\n",
      "[2845,     1] loss: 0.0098821506\n",
      "[2846,     1] loss: 0.0098821014\n",
      "[2847,     1] loss: 0.0098820515\n",
      "[2848,     1] loss: 0.0098820008\n",
      "[2849,     1] loss: 0.0098819494\n",
      "[2850,     1] loss: 0.0098819003\n",
      "[2851,     1] loss: 0.0098818488\n",
      "[2852,     1] loss: 0.0098817974\n",
      "[2853,     1] loss: 0.0098817468\n",
      "[2854,     1] loss: 0.0098816954\n",
      "[2855,     1] loss: 0.0098816440\n",
      "[2856,     1] loss: 0.0098815933\n",
      "[2857,     1] loss: 0.0098815404\n",
      "[2858,     1] loss: 0.0098814890\n",
      "[2859,     1] loss: 0.0098814376\n",
      "[2860,     1] loss: 0.0098813862\n",
      "[2861,     1] loss: 0.0098813333\n",
      "[2862,     1] loss: 0.0098812804\n",
      "[2863,     1] loss: 0.0098812282\n",
      "[2864,     1] loss: 0.0098811761\n",
      "[2865,     1] loss: 0.0098811224\n",
      "[2866,     1] loss: 0.0098810695\n",
      "[2867,     1] loss: 0.0098810174\n",
      "[2868,     1] loss: 0.0098809645\n",
      "[2869,     1] loss: 0.0098809108\n",
      "[2870,     1] loss: 0.0098808572\n",
      "[2871,     1] loss: 0.0098808035\n",
      "[2872,     1] loss: 0.0098807499\n",
      "[2873,     1] loss: 0.0098806970\n",
      "[2874,     1] loss: 0.0098806433\n",
      "[2875,     1] loss: 0.0098805889\n",
      "[2876,     1] loss: 0.0098805353\n",
      "[2877,     1] loss: 0.0098804809\n",
      "[2878,     1] loss: 0.0098804265\n",
      "[2879,     1] loss: 0.0098803721\n",
      "[2880,     1] loss: 0.0098803192\n",
      "[2881,     1] loss: 0.0098802656\n",
      "[2882,     1] loss: 0.0098802090\n",
      "[2883,     1] loss: 0.0098801553\n",
      "[2884,     1] loss: 0.0098801017\n",
      "[2885,     1] loss: 0.0098800465\n",
      "[2886,     1] loss: 0.0098799907\n",
      "[2887,     1] loss: 0.0098799370\n",
      "[2888,     1] loss: 0.0098798819\n",
      "[2889,     1] loss: 0.0098798260\n",
      "[2890,     1] loss: 0.0098797716\n",
      "[2891,     1] loss: 0.0098797150\n",
      "[2892,     1] loss: 0.0098796606\n",
      "[2893,     1] loss: 0.0098796055\n",
      "[2894,     1] loss: 0.0098795496\n",
      "[2895,     1] loss: 0.0098794945\n",
      "[2896,     1] loss: 0.0098794393\n",
      "[2897,     1] loss: 0.0098793834\n",
      "[2898,     1] loss: 0.0098793276\n",
      "[2899,     1] loss: 0.0098792717\n",
      "[2900,     1] loss: 0.0098792151\n",
      "[2901,     1] loss: 0.0098791599\n",
      "[2902,     1] loss: 0.0098791040\n",
      "[2903,     1] loss: 0.0098790497\n",
      "[2904,     1] loss: 0.0098789923\n",
      "[2905,     1] loss: 0.0098789379\n",
      "[2906,     1] loss: 0.0098788813\n",
      "[2907,     1] loss: 0.0098788239\n",
      "[2908,     1] loss: 0.0098787688\n",
      "[2909,     1] loss: 0.0098787144\n",
      "[2910,     1] loss: 0.0098786585\n",
      "[2911,     1] loss: 0.0098785996\n",
      "[2912,     1] loss: 0.0098785438\n",
      "[2913,     1] loss: 0.0098784886\n",
      "[2914,     1] loss: 0.0098784320\n",
      "[2915,     1] loss: 0.0098783754\n",
      "[2916,     1] loss: 0.0098783188\n",
      "[2917,     1] loss: 0.0098782621\n",
      "[2918,     1] loss: 0.0098782070\n",
      "[2919,     1] loss: 0.0098781504\n",
      "[2920,     1] loss: 0.0098780923\n",
      "[2921,     1] loss: 0.0098780364\n",
      "[2922,     1] loss: 0.0098779798\n",
      "[2923,     1] loss: 0.0098779239\n",
      "[2924,     1] loss: 0.0098778673\n",
      "[2925,     1] loss: 0.0098778106\n",
      "[2926,     1] loss: 0.0098777540\n",
      "[2927,     1] loss: 0.0098776981\n",
      "[2928,     1] loss: 0.0098776415\n",
      "[2929,     1] loss: 0.0098775841\n",
      "[2930,     1] loss: 0.0098775275\n",
      "[2931,     1] loss: 0.0098774716\n",
      "[2932,     1] loss: 0.0098774150\n",
      "[2933,     1] loss: 0.0098773576\n",
      "[2934,     1] loss: 0.0098773025\n",
      "[2935,     1] loss: 0.0098772451\n",
      "[2936,     1] loss: 0.0098771885\n",
      "[2937,     1] loss: 0.0098771319\n",
      "[2938,     1] loss: 0.0098770753\n",
      "[2939,     1] loss: 0.0098770186\n",
      "[2940,     1] loss: 0.0098769620\n",
      "[2941,     1] loss: 0.0098769054\n",
      "[2942,     1] loss: 0.0098768488\n",
      "[2943,     1] loss: 0.0098767914\n",
      "[2944,     1] loss: 0.0098767355\n",
      "[2945,     1] loss: 0.0098766781\n",
      "[2946,     1] loss: 0.0098766223\n",
      "[2947,     1] loss: 0.0098765656\n",
      "[2948,     1] loss: 0.0098765090\n",
      "[2949,     1] loss: 0.0098764531\n",
      "[2950,     1] loss: 0.0098763965\n",
      "[2951,     1] loss: 0.0098763399\n",
      "[2952,     1] loss: 0.0098762840\n",
      "[2953,     1] loss: 0.0098762259\n",
      "[2954,     1] loss: 0.0098761693\n",
      "[2955,     1] loss: 0.0098761134\n",
      "[2956,     1] loss: 0.0098760583\n",
      "[2957,     1] loss: 0.0098760001\n",
      "[2958,     1] loss: 0.0098759435\n",
      "[2959,     1] loss: 0.0098758869\n",
      "[2960,     1] loss: 0.0098758310\n",
      "[2961,     1] loss: 0.0098757744\n",
      "[2962,     1] loss: 0.0098757170\n",
      "[2963,     1] loss: 0.0098756604\n",
      "[2964,     1] loss: 0.0098756045\n",
      "[2965,     1] loss: 0.0098755479\n",
      "[2966,     1] loss: 0.0098754913\n",
      "[2967,     1] loss: 0.0098754361\n",
      "[2968,     1] loss: 0.0098753795\n",
      "[2969,     1] loss: 0.0098753229\n",
      "[2970,     1] loss: 0.0098752663\n",
      "[2971,     1] loss: 0.0098752096\n",
      "[2972,     1] loss: 0.0098751538\n",
      "[2973,     1] loss: 0.0098750971\n",
      "[2974,     1] loss: 0.0098750412\n",
      "[2975,     1] loss: 0.0098749854\n",
      "[2976,     1] loss: 0.0098749287\n",
      "[2977,     1] loss: 0.0098748729\n",
      "[2978,     1] loss: 0.0098748177\n",
      "[2979,     1] loss: 0.0098747611\n",
      "[2980,     1] loss: 0.0098747037\n",
      "[2981,     1] loss: 0.0098746486\n",
      "[2982,     1] loss: 0.0098745920\n",
      "[2983,     1] loss: 0.0098745354\n",
      "[2984,     1] loss: 0.0098744802\n",
      "[2985,     1] loss: 0.0098744243\n",
      "[2986,     1] loss: 0.0098743685\n",
      "[2987,     1] loss: 0.0098743126\n",
      "[2988,     1] loss: 0.0098742560\n",
      "[2989,     1] loss: 0.0098742016\n",
      "[2990,     1] loss: 0.0098741457\n",
      "[2991,     1] loss: 0.0098740883\n",
      "[2992,     1] loss: 0.0098740332\n",
      "[2993,     1] loss: 0.0098739773\n",
      "[2994,     1] loss: 0.0098739214\n",
      "[2995,     1] loss: 0.0098738655\n",
      "[2996,     1] loss: 0.0098738097\n",
      "[2997,     1] loss: 0.0098737545\n",
      "[2998,     1] loss: 0.0098737001\n",
      "[2999,     1] loss: 0.0098736435\n",
      "[3000,     1] loss: 0.0098735861\n",
      "[3001,     1] loss: 0.0098735318\n",
      "[3002,     1] loss: 0.0098734766\n",
      "[3003,     1] loss: 0.0098734207\n",
      "[3004,     1] loss: 0.0098733664\n",
      "[3005,     1] loss: 0.0098733090\n",
      "[3006,     1] loss: 0.0098732539\n",
      "[3007,     1] loss: 0.0098731995\n",
      "[3008,     1] loss: 0.0098731428\n",
      "[3009,     1] loss: 0.0098730884\n",
      "[3010,     1] loss: 0.0098730318\n",
      "[3011,     1] loss: 0.0098729759\n",
      "[3012,     1] loss: 0.0098729223\n",
      "[3013,     1] loss: 0.0098728657\n",
      "[3014,     1] loss: 0.0098728113\n",
      "[3015,     1] loss: 0.0098727562\n",
      "[3016,     1] loss: 0.0098727010\n",
      "[3017,     1] loss: 0.0098726451\n",
      "[3018,     1] loss: 0.0098725908\n",
      "[3019,     1] loss: 0.0098725341\n",
      "[3020,     1] loss: 0.0098724797\n",
      "[3021,     1] loss: 0.0098724253\n",
      "[3022,     1] loss: 0.0098723702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3023,     1] loss: 0.0098723143\n",
      "[3024,     1] loss: 0.0098722599\n",
      "[3025,     1] loss: 0.0098722056\n",
      "[3026,     1] loss: 0.0098721489\n",
      "[3027,     1] loss: 0.0098720953\n",
      "[3028,     1] loss: 0.0098720402\n",
      "[3029,     1] loss: 0.0098719850\n",
      "[3030,     1] loss: 0.0098719314\n",
      "[3031,     1] loss: 0.0098718747\n",
      "[3032,     1] loss: 0.0098718196\n",
      "[3033,     1] loss: 0.0098717660\n",
      "[3034,     1] loss: 0.0098717101\n",
      "[3035,     1] loss: 0.0098716550\n",
      "[3036,     1] loss: 0.0098716021\n",
      "[3037,     1] loss: 0.0098715454\n",
      "[3038,     1] loss: 0.0098714918\n",
      "[3039,     1] loss: 0.0098714359\n",
      "[3040,     1] loss: 0.0098713808\n",
      "[3041,     1] loss: 0.0098713264\n",
      "[3042,     1] loss: 0.0098712720\n",
      "[3043,     1] loss: 0.0098712161\n",
      "[3044,     1] loss: 0.0098711625\n",
      "[3045,     1] loss: 0.0098711066\n",
      "[3046,     1] loss: 0.0098710515\n",
      "[3047,     1] loss: 0.0098709978\n",
      "[3048,     1] loss: 0.0098709434\n",
      "[3049,     1] loss: 0.0098708883\n",
      "[3050,     1] loss: 0.0098708346\n",
      "[3051,     1] loss: 0.0098707795\n",
      "[3052,     1] loss: 0.0098707244\n",
      "[3053,     1] loss: 0.0098706692\n",
      "[3054,     1] loss: 0.0098706149\n",
      "[3055,     1] loss: 0.0098705605\n",
      "[3056,     1] loss: 0.0098705053\n",
      "[3057,     1] loss: 0.0098704517\n",
      "[3058,     1] loss: 0.0098703966\n",
      "[3059,     1] loss: 0.0098703414\n",
      "[3060,     1] loss: 0.0098702870\n",
      "[3061,     1] loss: 0.0098702319\n",
      "[3062,     1] loss: 0.0098701783\n",
      "[3063,     1] loss: 0.0098701239\n",
      "[3064,     1] loss: 0.0098700687\n",
      "[3065,     1] loss: 0.0098700151\n",
      "[3066,     1] loss: 0.0098699592\n",
      "[3067,     1] loss: 0.0098699056\n",
      "[3068,     1] loss: 0.0098698504\n",
      "[3069,     1] loss: 0.0098697968\n",
      "[3070,     1] loss: 0.0098697416\n",
      "[3071,     1] loss: 0.0098696873\n",
      "[3072,     1] loss: 0.0098696336\n",
      "[3073,     1] loss: 0.0098695777\n",
      "[3074,     1] loss: 0.0098695226\n",
      "[3075,     1] loss: 0.0098694690\n",
      "[3076,     1] loss: 0.0098694138\n",
      "[3077,     1] loss: 0.0098693594\n",
      "[3078,     1] loss: 0.0098693058\n",
      "[3079,     1] loss: 0.0098692507\n",
      "[3080,     1] loss: 0.0098691963\n",
      "[3081,     1] loss: 0.0098691404\n",
      "[3082,     1] loss: 0.0098690867\n",
      "[3083,     1] loss: 0.0098690324\n",
      "[3084,     1] loss: 0.0098689772\n",
      "[3085,     1] loss: 0.0098689228\n",
      "[3086,     1] loss: 0.0098688692\n",
      "[3087,     1] loss: 0.0098688141\n",
      "[3088,     1] loss: 0.0098687597\n",
      "[3089,     1] loss: 0.0098687053\n",
      "[3090,     1] loss: 0.0098686501\n",
      "[3091,     1] loss: 0.0098685957\n",
      "[3092,     1] loss: 0.0098685406\n",
      "[3093,     1] loss: 0.0098684862\n",
      "[3094,     1] loss: 0.0098684326\n",
      "[3095,     1] loss: 0.0098683774\n",
      "[3096,     1] loss: 0.0098683231\n",
      "[3097,     1] loss: 0.0098682679\n",
      "[3098,     1] loss: 0.0098682135\n",
      "[3099,     1] loss: 0.0098681584\n",
      "[3100,     1] loss: 0.0098681048\n",
      "[3101,     1] loss: 0.0098680496\n",
      "[3102,     1] loss: 0.0098679945\n",
      "[3103,     1] loss: 0.0098679401\n",
      "[3104,     1] loss: 0.0098678850\n",
      "[3105,     1] loss: 0.0098678298\n",
      "[3106,     1] loss: 0.0098677769\n",
      "[3107,     1] loss: 0.0098677211\n",
      "[3108,     1] loss: 0.0098676667\n",
      "[3109,     1] loss: 0.0098676123\n",
      "[3110,     1] loss: 0.0098675571\n",
      "[3111,     1] loss: 0.0098675027\n",
      "[3112,     1] loss: 0.0098674484\n",
      "[3113,     1] loss: 0.0098673932\n",
      "[3114,     1] loss: 0.0098673381\n",
      "[3115,     1] loss: 0.0098672837\n",
      "[3116,     1] loss: 0.0098672301\n",
      "[3117,     1] loss: 0.0098671749\n",
      "[3118,     1] loss: 0.0098671198\n",
      "[3119,     1] loss: 0.0098670661\n",
      "[3120,     1] loss: 0.0098670110\n",
      "[3121,     1] loss: 0.0098669559\n",
      "[3122,     1] loss: 0.0098669030\n",
      "[3123,     1] loss: 0.0098668471\n",
      "[3124,     1] loss: 0.0098667920\n",
      "[3125,     1] loss: 0.0098667383\n",
      "[3126,     1] loss: 0.0098666832\n",
      "[3127,     1] loss: 0.0098666281\n",
      "[3128,     1] loss: 0.0098665737\n",
      "[3129,     1] loss: 0.0098665193\n",
      "[3130,     1] loss: 0.0098664641\n",
      "[3131,     1] loss: 0.0098664105\n",
      "[3132,     1] loss: 0.0098663568\n",
      "[3133,     1] loss: 0.0098663010\n",
      "[3134,     1] loss: 0.0098662481\n",
      "[3135,     1] loss: 0.0098661922\n",
      "[3136,     1] loss: 0.0098661378\n",
      "[3137,     1] loss: 0.0098660842\n",
      "[3138,     1] loss: 0.0098660290\n",
      "[3139,     1] loss: 0.0098659746\n",
      "[3140,     1] loss: 0.0098659202\n",
      "[3141,     1] loss: 0.0098658666\n",
      "[3142,     1] loss: 0.0098658115\n",
      "[3143,     1] loss: 0.0098657578\n",
      "[3144,     1] loss: 0.0098657034\n",
      "[3145,     1] loss: 0.0098656490\n",
      "[3146,     1] loss: 0.0098655954\n",
      "[3147,     1] loss: 0.0098655418\n",
      "[3148,     1] loss: 0.0098654859\n",
      "[3149,     1] loss: 0.0098654330\n",
      "[3150,     1] loss: 0.0098653801\n",
      "[3151,     1] loss: 0.0098653249\n",
      "[3152,     1] loss: 0.0098652713\n",
      "[3153,     1] loss: 0.0098652177\n",
      "[3154,     1] loss: 0.0098651648\n",
      "[3155,     1] loss: 0.0098651104\n",
      "[3156,     1] loss: 0.0098650575\n",
      "[3157,     1] loss: 0.0098650031\n",
      "[3158,     1] loss: 0.0098649502\n",
      "[3159,     1] loss: 0.0098648965\n",
      "[3160,     1] loss: 0.0098648429\n",
      "[3161,     1] loss: 0.0098647892\n",
      "[3162,     1] loss: 0.0098647386\n",
      "[3163,     1] loss: 0.0098646842\n",
      "[3164,     1] loss: 0.0098646313\n",
      "[3165,     1] loss: 0.0098645791\n",
      "[3166,     1] loss: 0.0098645270\n",
      "[3167,     1] loss: 0.0098644741\n",
      "[3168,     1] loss: 0.0098644219\n",
      "[3169,     1] loss: 0.0098643698\n",
      "[3170,     1] loss: 0.0098643176\n",
      "[3171,     1] loss: 0.0098642655\n",
      "[3172,     1] loss: 0.0098642133\n",
      "[3173,     1] loss: 0.0098641619\n",
      "[3174,     1] loss: 0.0098641098\n",
      "[3175,     1] loss: 0.0098640583\n",
      "[3176,     1] loss: 0.0098640077\n",
      "[3177,     1] loss: 0.0098639563\n",
      "[3178,     1] loss: 0.0098639041\n",
      "[3179,     1] loss: 0.0098638542\n",
      "[3180,     1] loss: 0.0098638043\n",
      "[3181,     1] loss: 0.0098637529\n",
      "[3182,     1] loss: 0.0098637030\n",
      "[3183,     1] loss: 0.0098636530\n",
      "[3184,     1] loss: 0.0098636031\n",
      "[3185,     1] loss: 0.0098635525\n",
      "[3186,     1] loss: 0.0098635048\n",
      "[3187,     1] loss: 0.0098634563\n",
      "[3188,     1] loss: 0.0098634057\n",
      "[3189,     1] loss: 0.0098633572\n",
      "[3190,     1] loss: 0.0098633088\n",
      "[3191,     1] loss: 0.0098632611\n",
      "[3192,     1] loss: 0.0098632127\n",
      "[3193,     1] loss: 0.0098631635\n",
      "[3194,     1] loss: 0.0098631173\n",
      "[3195,     1] loss: 0.0098630689\n",
      "[3196,     1] loss: 0.0098630235\n",
      "[3197,     1] loss: 0.0098629758\n",
      "[3198,     1] loss: 0.0098629296\n",
      "[3199,     1] loss: 0.0098628834\n",
      "[3200,     1] loss: 0.0098628372\n",
      "[3201,     1] loss: 0.0098627932\n",
      "[3202,     1] loss: 0.0098627456\n",
      "[3203,     1] loss: 0.0098627023\n",
      "[3204,     1] loss: 0.0098626569\n",
      "[3205,     1] loss: 0.0098626137\n",
      "[3206,     1] loss: 0.0098625690\n",
      "[3207,     1] loss: 0.0098625258\n",
      "[3208,     1] loss: 0.0098624833\n",
      "[3209,     1] loss: 0.0098624408\n",
      "[3210,     1] loss: 0.0098623976\n",
      "[3211,     1] loss: 0.0098623559\n",
      "[3212,     1] loss: 0.0098623149\n",
      "[3213,     1] loss: 0.0098622732\n",
      "[3214,     1] loss: 0.0098622322\n",
      "[3215,     1] loss: 0.0098621920\n",
      "[3216,     1] loss: 0.0098621517\n",
      "[3217,     1] loss: 0.0098621130\n",
      "[3218,     1] loss: 0.0098620743\n",
      "[3219,     1] loss: 0.0098620355\n",
      "[3220,     1] loss: 0.0098619983\n",
      "[3221,     1] loss: 0.0098619610\n",
      "[3222,     1] loss: 0.0098619238\n",
      "[3223,     1] loss: 0.0098618872\n",
      "[3224,     1] loss: 0.0098618515\n",
      "[3225,     1] loss: 0.0098618165\n",
      "[3226,     1] loss: 0.0098617814\n",
      "[3227,     1] loss: 0.0098617457\n",
      "[3228,     1] loss: 0.0098617122\n",
      "[3229,     1] loss: 0.0098616794\n",
      "[3230,     1] loss: 0.0098616473\n",
      "[3231,     1] loss: 0.0098616146\n",
      "[3232,     1] loss: 0.0098615825\n",
      "[3233,     1] loss: 0.0098615512\n",
      "[3234,     1] loss: 0.0098615214\n",
      "[3235,     1] loss: 0.0098614924\n",
      "[3236,     1] loss: 0.0098614626\n",
      "[3237,     1] loss: 0.0098614350\n",
      "[3238,     1] loss: 0.0098614059\n",
      "[3239,     1] loss: 0.0098613799\n",
      "[3240,     1] loss: 0.0098613530\n",
      "[3241,     1] loss: 0.0098613270\n",
      "[3242,     1] loss: 0.0098613016\n",
      "[3243,     1] loss: 0.0098612770\n",
      "[3244,     1] loss: 0.0098612525\n",
      "[3245,     1] loss: 0.0098612309\n",
      "[3246,     1] loss: 0.0098612070\n",
      "[3247,     1] loss: 0.0098611861\n",
      "[3248,     1] loss: 0.0098611645\n",
      "[3249,     1] loss: 0.0098611444\n",
      "[3250,     1] loss: 0.0098611258\n",
      "[3251,     1] loss: 0.0098611072\n",
      "[3252,     1] loss: 0.0098610878\n",
      "[3253,     1] loss: 0.0098610707\n",
      "[3254,     1] loss: 0.0098610550\n",
      "[3255,     1] loss: 0.0098610379\n",
      "[3256,     1] loss: 0.0098610230\n",
      "[3257,     1] loss: 0.0098610103\n",
      "[3258,     1] loss: 0.0098609954\n",
      "[3259,     1] loss: 0.0098609835\n",
      "[3260,     1] loss: 0.0098609723\n",
      "[3261,     1] loss: 0.0098609611\n",
      "[3262,     1] loss: 0.0098609492\n",
      "[3263,     1] loss: 0.0098609418\n",
      "[3264,     1] loss: 0.0098609328\n",
      "[3265,     1] loss: 0.0098609254\n",
      "[3266,     1] loss: 0.0098609172\n",
      "[3267,     1] loss: 0.0098609120\n",
      "[3268,     1] loss: 0.0098609082\n",
      "[3269,     1] loss: 0.0098609030\n",
      "[3270,     1] loss: 0.0098609000\n",
      "[3271,     1] loss: 0.0098608963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3272,     1] loss: 0.0098608956\n",
      "[3273,     1] loss: 0.0098608956\n",
      "[3274,     1] loss: 0.0098608948\n",
      "[3275,     1] loss: 0.0098608948\n",
      "[3276,     1] loss: 0.0098608971\n",
      "[3277,     1] loss: 0.0098608993\n",
      "[3278,     1] loss: 0.0098609038\n",
      "[3279,     1] loss: 0.0098609082\n",
      "[3280,     1] loss: 0.0098609127\n",
      "[3281,     1] loss: 0.0098609194\n",
      "[3282,     1] loss: 0.0098609261\n",
      "[3283,     1] loss: 0.0098609343\n",
      "[3284,     1] loss: 0.0098609425\n",
      "[3285,     1] loss: 0.0098609537\n",
      "[3286,     1] loss: 0.0098609641\n",
      "[3287,     1] loss: 0.0098609746\n",
      "[3288,     1] loss: 0.0098609872\n",
      "[3289,     1] loss: 0.0098609999\n",
      "[3290,     1] loss: 0.0098610140\n",
      "[3291,     1] loss: 0.0098610289\n",
      "[3292,     1] loss: 0.0098610446\n",
      "[3293,     1] loss: 0.0098610617\n",
      "[3294,     1] loss: 0.0098610789\n",
      "[3295,     1] loss: 0.0098610975\n",
      "[3296,     1] loss: 0.0098611169\n",
      "[3297,     1] loss: 0.0098611362\n",
      "[3298,     1] loss: 0.0098611578\n",
      "[3299,     1] loss: 0.0098611794\n",
      "[3300,     1] loss: 0.0098612025\n",
      "[3301,     1] loss: 0.0098612249\n",
      "[3302,     1] loss: 0.0098612495\n",
      "[3303,     1] loss: 0.0098612748\n",
      "[3304,     1] loss: 0.0098613001\n",
      "[3305,     1] loss: 0.0098613277\n",
      "[3306,     1] loss: 0.0098613553\n",
      "[3307,     1] loss: 0.0098613821\n",
      "[3308,     1] loss: 0.0098614119\n",
      "[3309,     1] loss: 0.0098614424\n",
      "[3310,     1] loss: 0.0098614730\n",
      "[3311,     1] loss: 0.0098615043\n",
      "[3312,     1] loss: 0.0098615363\n",
      "[3313,     1] loss: 0.0098615691\n",
      "[3314,     1] loss: 0.0098616019\n",
      "[3315,     1] loss: 0.0098616369\n",
      "[3316,     1] loss: 0.0098616719\n",
      "[3317,     1] loss: 0.0098617077\n",
      "[3318,     1] loss: 0.0098617442\n",
      "[3319,     1] loss: 0.0098617822\n",
      "[3320,     1] loss: 0.0098618187\n",
      "[3321,     1] loss: 0.0098618574\n",
      "[3322,     1] loss: 0.0098618977\n",
      "[3323,     1] loss: 0.0098619379\n",
      "[3324,     1] loss: 0.0098619774\n",
      "[3325,     1] loss: 0.0098620176\n",
      "[3326,     1] loss: 0.0098620608\n",
      "[3327,     1] loss: 0.0098621033\n",
      "[3328,     1] loss: 0.0098621465\n",
      "[3329,     1] loss: 0.0098621905\n",
      "[3330,     1] loss: 0.0098622344\n",
      "[3331,     1] loss: 0.0098622799\n",
      "[3332,     1] loss: 0.0098623246\n",
      "[3333,     1] loss: 0.0098623708\n",
      "[3334,     1] loss: 0.0098624170\n",
      "[3335,     1] loss: 0.0098624639\n",
      "[3336,     1] loss: 0.0098625109\n",
      "[3337,     1] loss: 0.0098625593\n",
      "[3338,     1] loss: 0.0098626077\n",
      "[3339,     1] loss: 0.0098626569\n",
      "[3340,     1] loss: 0.0098627068\n",
      "[3341,     1] loss: 0.0098627552\n",
      "[3342,     1] loss: 0.0098628066\n",
      "[3343,     1] loss: 0.0098628566\n",
      "[3344,     1] loss: 0.0098629087\n",
      "[3345,     1] loss: 0.0098629586\n",
      "[3346,     1] loss: 0.0098630100\n",
      "[3347,     1] loss: 0.0098630652\n",
      "[3348,     1] loss: 0.0098631166\n",
      "[3349,     1] loss: 0.0098631702\n",
      "[3350,     1] loss: 0.0098632239\n",
      "[3351,     1] loss: 0.0098632775\n",
      "[3352,     1] loss: 0.0098633312\n",
      "[3353,     1] loss: 0.0098633870\n",
      "[3354,     1] loss: 0.0098634399\n",
      "[3355,     1] loss: 0.0098634958\n",
      "[3356,     1] loss: 0.0098635510\n",
      "[3357,     1] loss: 0.0098636061\n",
      "[3358,     1] loss: 0.0098636620\n",
      "[3359,     1] loss: 0.0098637179\n",
      "[3360,     1] loss: 0.0098637752\n",
      "[3361,     1] loss: 0.0098638311\n",
      "[3362,     1] loss: 0.0098638877\n",
      "[3363,     1] loss: 0.0098639444\n",
      "[3364,     1] loss: 0.0098640025\n",
      "[3365,     1] loss: 0.0098640591\n",
      "[3366,     1] loss: 0.0098641172\n",
      "[3367,     1] loss: 0.0098641738\n",
      "[3368,     1] loss: 0.0098642327\n",
      "[3369,     1] loss: 0.0098642908\n",
      "[3370,     1] loss: 0.0098643489\n",
      "[3371,     1] loss: 0.0098644063\n",
      "[3372,     1] loss: 0.0098644651\n",
      "[3373,     1] loss: 0.0098645233\n",
      "[3374,     1] loss: 0.0098645821\n",
      "[3375,     1] loss: 0.0098646402\n",
      "[3376,     1] loss: 0.0098646998\n",
      "[3377,     1] loss: 0.0098647580\n",
      "[3378,     1] loss: 0.0098648176\n",
      "[3379,     1] loss: 0.0098648764\n",
      "[3380,     1] loss: 0.0098649345\n",
      "[3381,     1] loss: 0.0098649926\n",
      "[3382,     1] loss: 0.0098650523\n",
      "[3383,     1] loss: 0.0098651104\n",
      "[3384,     1] loss: 0.0098651692\n",
      "[3385,     1] loss: 0.0098652288\n",
      "[3386,     1] loss: 0.0098652884\n",
      "[3387,     1] loss: 0.0098653458\n",
      "[3388,     1] loss: 0.0098654054\n",
      "[3389,     1] loss: 0.0098654635\n",
      "[3390,     1] loss: 0.0098655216\n",
      "[3391,     1] loss: 0.0098655805\n",
      "[3392,     1] loss: 0.0098656386\n",
      "[3393,     1] loss: 0.0098656975\n",
      "[3394,     1] loss: 0.0098657571\n",
      "[3395,     1] loss: 0.0098658152\n",
      "[3396,     1] loss: 0.0098658711\n",
      "[3397,     1] loss: 0.0098659322\n",
      "[3398,     1] loss: 0.0098659880\n",
      "[3399,     1] loss: 0.0098660477\n",
      "[3400,     1] loss: 0.0098661043\n",
      "[3401,     1] loss: 0.0098661624\n",
      "[3402,     1] loss: 0.0098662190\n",
      "[3403,     1] loss: 0.0098662771\n",
      "[3404,     1] loss: 0.0098663352\n",
      "[3405,     1] loss: 0.0098663911\n",
      "[3406,     1] loss: 0.0098664485\n",
      "[3407,     1] loss: 0.0098665044\n",
      "[3408,     1] loss: 0.0098665603\n",
      "[3409,     1] loss: 0.0098666169\n",
      "[3410,     1] loss: 0.0098666735\n",
      "[3411,     1] loss: 0.0098667286\n",
      "[3412,     1] loss: 0.0098667845\n",
      "[3413,     1] loss: 0.0098668396\n",
      "[3414,     1] loss: 0.0098668963\n",
      "[3415,     1] loss: 0.0098669514\n",
      "[3416,     1] loss: 0.0098670065\n",
      "[3417,     1] loss: 0.0098670617\n",
      "[3418,     1] loss: 0.0098671153\n",
      "[3419,     1] loss: 0.0098671690\n",
      "[3420,     1] loss: 0.0098672241\n",
      "[3421,     1] loss: 0.0098672763\n",
      "[3422,     1] loss: 0.0098673306\n",
      "[3423,     1] loss: 0.0098673850\n",
      "[3424,     1] loss: 0.0098674387\n",
      "[3425,     1] loss: 0.0098674908\n",
      "[3426,     1] loss: 0.0098675430\n",
      "[3427,     1] loss: 0.0098675944\n",
      "[3428,     1] loss: 0.0098676458\n",
      "[3429,     1] loss: 0.0098676987\n",
      "[3430,     1] loss: 0.0098677509\n",
      "[3431,     1] loss: 0.0098678015\n",
      "[3432,     1] loss: 0.0098678529\n",
      "[3433,     1] loss: 0.0098679036\n",
      "[3434,     1] loss: 0.0098679528\n",
      "[3435,     1] loss: 0.0098680027\n",
      "[3436,     1] loss: 0.0098680526\n",
      "[3437,     1] loss: 0.0098681025\n",
      "[3438,     1] loss: 0.0098681517\n",
      "[3439,     1] loss: 0.0098682009\n",
      "[3440,     1] loss: 0.0098682493\n",
      "[3441,     1] loss: 0.0098682970\n",
      "[3442,     1] loss: 0.0098683462\n",
      "[3443,     1] loss: 0.0098683931\n",
      "[3444,     1] loss: 0.0098684408\n",
      "[3445,     1] loss: 0.0098684870\n",
      "[3446,     1] loss: 0.0098685339\n",
      "[3447,     1] loss: 0.0098685808\n",
      "[3448,     1] loss: 0.0098686263\n",
      "[3449,     1] loss: 0.0098686725\n",
      "[3450,     1] loss: 0.0098687172\n",
      "[3451,     1] loss: 0.0098687619\n",
      "[3452,     1] loss: 0.0098688081\n",
      "[3453,     1] loss: 0.0098688528\n",
      "[3454,     1] loss: 0.0098688960\n",
      "[3455,     1] loss: 0.0098689407\n",
      "[3456,     1] loss: 0.0098689839\n",
      "[3457,     1] loss: 0.0098690271\n",
      "[3458,     1] loss: 0.0098690696\n",
      "[3459,     1] loss: 0.0098691128\n",
      "[3460,     1] loss: 0.0098691538\n",
      "[3461,     1] loss: 0.0098691963\n",
      "[3462,     1] loss: 0.0098692387\n",
      "[3463,     1] loss: 0.0098692790\n",
      "[3464,     1] loss: 0.0098693207\n",
      "[3465,     1] loss: 0.0098693609\n",
      "[3466,     1] loss: 0.0098694019\n",
      "[3467,     1] loss: 0.0098694406\n",
      "[3468,     1] loss: 0.0098694801\n",
      "[3469,     1] loss: 0.0098695196\n",
      "[3470,     1] loss: 0.0098695591\n",
      "[3471,     1] loss: 0.0098695971\n",
      "[3472,     1] loss: 0.0098696351\n",
      "[3473,     1] loss: 0.0098696731\n",
      "[3474,     1] loss: 0.0098697104\n",
      "[3475,     1] loss: 0.0098697476\n",
      "[3476,     1] loss: 0.0098697849\n",
      "[3477,     1] loss: 0.0098698206\n",
      "[3478,     1] loss: 0.0098698564\n",
      "[3479,     1] loss: 0.0098698914\n",
      "[3480,     1] loss: 0.0098699272\n",
      "[3481,     1] loss: 0.0098699622\n",
      "[3482,     1] loss: 0.0098699972\n",
      "[3483,     1] loss: 0.0098700322\n",
      "[3484,     1] loss: 0.0098700650\n",
      "[3485,     1] loss: 0.0098700985\n",
      "[3486,     1] loss: 0.0098701328\n",
      "[3487,     1] loss: 0.0098701656\n",
      "[3488,     1] loss: 0.0098701976\n",
      "[3489,     1] loss: 0.0098702289\n",
      "[3490,     1] loss: 0.0098702610\n",
      "[3491,     1] loss: 0.0098702930\n",
      "[3492,     1] loss: 0.0098703243\n",
      "[3493,     1] loss: 0.0098703548\n",
      "[3494,     1] loss: 0.0098703854\n",
      "[3495,     1] loss: 0.0098704159\n",
      "[3496,     1] loss: 0.0098704450\n",
      "[3497,     1] loss: 0.0098704748\n",
      "[3498,     1] loss: 0.0098705038\n",
      "[3499,     1] loss: 0.0098705322\n",
      "[3500,     1] loss: 0.0098705620\n",
      "[3501,     1] loss: 0.0098705880\n",
      "[3502,     1] loss: 0.0098706171\n",
      "[3503,     1] loss: 0.0098706439\n",
      "[3504,     1] loss: 0.0098706700\n",
      "[3505,     1] loss: 0.0098706976\n",
      "[3506,     1] loss: 0.0098707236\n",
      "[3507,     1] loss: 0.0098707505\n",
      "[3508,     1] loss: 0.0098707765\n",
      "[3509,     1] loss: 0.0098708011\n",
      "[3510,     1] loss: 0.0098708265\n",
      "[3511,     1] loss: 0.0098708510\n",
      "[3512,     1] loss: 0.0098708749\n",
      "[3513,     1] loss: 0.0098708995\n",
      "[3514,     1] loss: 0.0098709248\n",
      "[3515,     1] loss: 0.0098709464\n",
      "[3516,     1] loss: 0.0098709695\n",
      "[3517,     1] loss: 0.0098709919\n",
      "[3518,     1] loss: 0.0098710150\n",
      "[3519,     1] loss: 0.0098710373\n",
      "[3520,     1] loss: 0.0098710582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3521,     1] loss: 0.0098710798\n",
      "[3522,     1] loss: 0.0098711006\n",
      "[3523,     1] loss: 0.0098711208\n",
      "[3524,     1] loss: 0.0098711409\n",
      "[3525,     1] loss: 0.0098711602\n",
      "[3526,     1] loss: 0.0098711811\n",
      "[3527,     1] loss: 0.0098712005\n",
      "[3528,     1] loss: 0.0098712191\n",
      "[3529,     1] loss: 0.0098712377\n",
      "[3530,     1] loss: 0.0098712556\n",
      "[3531,     1] loss: 0.0098712742\n",
      "[3532,     1] loss: 0.0098712921\n",
      "[3533,     1] loss: 0.0098713085\n",
      "[3534,     1] loss: 0.0098713271\n",
      "[3535,     1] loss: 0.0098713428\n",
      "[3536,     1] loss: 0.0098713599\n",
      "[3537,     1] loss: 0.0098713756\n",
      "[3538,     1] loss: 0.0098713905\n",
      "[3539,     1] loss: 0.0098714061\n",
      "[3540,     1] loss: 0.0098714218\n",
      "[3541,     1] loss: 0.0098714367\n",
      "[3542,     1] loss: 0.0098714516\n",
      "[3543,     1] loss: 0.0098714642\n",
      "[3544,     1] loss: 0.0098714791\n",
      "[3545,     1] loss: 0.0098714933\n",
      "[3546,     1] loss: 0.0098715052\n",
      "[3547,     1] loss: 0.0098715194\n",
      "[3548,     1] loss: 0.0098715313\n",
      "[3549,     1] loss: 0.0098715439\n",
      "[3550,     1] loss: 0.0098715566\n",
      "[3551,     1] loss: 0.0098715670\n",
      "[3552,     1] loss: 0.0098715782\n",
      "[3553,     1] loss: 0.0098715894\n",
      "[3554,     1] loss: 0.0098716006\n",
      "[3555,     1] loss: 0.0098716110\n",
      "[3556,     1] loss: 0.0098716207\n",
      "[3557,     1] loss: 0.0098716319\n",
      "[3558,     1] loss: 0.0098716415\n",
      "[3559,     1] loss: 0.0098716497\n",
      "[3560,     1] loss: 0.0098716587\n",
      "[3561,     1] loss: 0.0098716676\n",
      "[3562,     1] loss: 0.0098716751\n",
      "[3563,     1] loss: 0.0098716848\n",
      "[3564,     1] loss: 0.0098716922\n",
      "[3565,     1] loss: 0.0098716989\n",
      "[3566,     1] loss: 0.0098717064\n",
      "[3567,     1] loss: 0.0098717138\n",
      "[3568,     1] loss: 0.0098717205\n",
      "[3569,     1] loss: 0.0098717265\n",
      "[3570,     1] loss: 0.0098717324\n",
      "[3571,     1] loss: 0.0098717384\n",
      "[3572,     1] loss: 0.0098717451\n",
      "[3573,     1] loss: 0.0098717496\n",
      "[3574,     1] loss: 0.0098717548\n",
      "[3575,     1] loss: 0.0098717585\n",
      "[3576,     1] loss: 0.0098717622\n",
      "[3577,     1] loss: 0.0098717667\n",
      "[3578,     1] loss: 0.0098717712\n",
      "[3579,     1] loss: 0.0098717749\n",
      "[3580,     1] loss: 0.0098717786\n",
      "[3581,     1] loss: 0.0098717809\n",
      "[3582,     1] loss: 0.0098717846\n",
      "[3583,     1] loss: 0.0098717861\n",
      "[3584,     1] loss: 0.0098717883\n",
      "[3585,     1] loss: 0.0098717906\n",
      "[3586,     1] loss: 0.0098717928\n",
      "[3587,     1] loss: 0.0098717928\n",
      "[3588,     1] loss: 0.0098717943\n",
      "[3589,     1] loss: 0.0098717958\n",
      "[3590,     1] loss: 0.0098717958\n",
      "[3591,     1] loss: 0.0098717965\n",
      "[3592,     1] loss: 0.0098717958\n",
      "[3593,     1] loss: 0.0098717973\n",
      "[3594,     1] loss: 0.0098717958\n",
      "[3595,     1] loss: 0.0098717958\n",
      "[3596,     1] loss: 0.0098717935\n",
      "[3597,     1] loss: 0.0098717928\n",
      "[3598,     1] loss: 0.0098717920\n",
      "[3599,     1] loss: 0.0098717891\n",
      "[3600,     1] loss: 0.0098717876\n",
      "[3601,     1] loss: 0.0098717861\n",
      "[3602,     1] loss: 0.0098717831\n",
      "[3603,     1] loss: 0.0098717801\n",
      "[3604,     1] loss: 0.0098717771\n",
      "[3605,     1] loss: 0.0098717742\n",
      "[3606,     1] loss: 0.0098717697\n",
      "[3607,     1] loss: 0.0098717675\n",
      "[3608,     1] loss: 0.0098717622\n",
      "[3609,     1] loss: 0.0098717585\n",
      "[3610,     1] loss: 0.0098717541\n",
      "[3611,     1] loss: 0.0098717496\n",
      "[3612,     1] loss: 0.0098717444\n",
      "[3613,     1] loss: 0.0098717399\n",
      "[3614,     1] loss: 0.0098717332\n",
      "[3615,     1] loss: 0.0098717280\n",
      "[3616,     1] loss: 0.0098717228\n",
      "[3617,     1] loss: 0.0098717153\n",
      "[3618,     1] loss: 0.0098717093\n",
      "[3619,     1] loss: 0.0098717019\n",
      "[3620,     1] loss: 0.0098716959\n",
      "[3621,     1] loss: 0.0098716885\n",
      "[3622,     1] loss: 0.0098716818\n",
      "[3623,     1] loss: 0.0098716743\n",
      "[3624,     1] loss: 0.0098716661\n",
      "[3625,     1] loss: 0.0098716579\n",
      "[3626,     1] loss: 0.0098716497\n",
      "[3627,     1] loss: 0.0098716415\n",
      "[3628,     1] loss: 0.0098716326\n",
      "[3629,     1] loss: 0.0098716237\n",
      "[3630,     1] loss: 0.0098716147\n",
      "[3631,     1] loss: 0.0098716058\n",
      "[3632,     1] loss: 0.0098715946\n",
      "[3633,     1] loss: 0.0098715849\n",
      "[3634,     1] loss: 0.0098715760\n",
      "[3635,     1] loss: 0.0098715670\n",
      "[3636,     1] loss: 0.0098715551\n",
      "[3637,     1] loss: 0.0098715447\n",
      "[3638,     1] loss: 0.0098715328\n",
      "[3639,     1] loss: 0.0098715223\n",
      "[3640,     1] loss: 0.0098715112\n",
      "[3641,     1] loss: 0.0098715000\n",
      "[3642,     1] loss: 0.0098714873\n",
      "[3643,     1] loss: 0.0098714761\n",
      "[3644,     1] loss: 0.0098714635\n",
      "[3645,     1] loss: 0.0098714508\n",
      "[3646,     1] loss: 0.0098714381\n",
      "[3647,     1] loss: 0.0098714262\n",
      "[3648,     1] loss: 0.0098714113\n",
      "[3649,     1] loss: 0.0098714001\n",
      "[3650,     1] loss: 0.0098713875\n",
      "[3651,     1] loss: 0.0098713733\n",
      "[3652,     1] loss: 0.0098713592\n",
      "[3653,     1] loss: 0.0098713450\n",
      "[3654,     1] loss: 0.0098713309\n",
      "[3655,     1] loss: 0.0098713167\n",
      "[3656,     1] loss: 0.0098713018\n",
      "[3657,     1] loss: 0.0098712869\n",
      "[3658,     1] loss: 0.0098712727\n",
      "[3659,     1] loss: 0.0098712571\n",
      "[3660,     1] loss: 0.0098712415\n",
      "[3661,     1] loss: 0.0098712251\n",
      "[3662,     1] loss: 0.0098712094\n",
      "[3663,     1] loss: 0.0098711945\n",
      "[3664,     1] loss: 0.0098711781\n",
      "[3665,     1] loss: 0.0098711602\n",
      "[3666,     1] loss: 0.0098711446\n",
      "[3667,     1] loss: 0.0098711275\n",
      "[3668,     1] loss: 0.0098711118\n",
      "[3669,     1] loss: 0.0098710939\n",
      "[3670,     1] loss: 0.0098710775\n",
      "[3671,     1] loss: 0.0098710597\n",
      "[3672,     1] loss: 0.0098710418\n",
      "[3673,     1] loss: 0.0098710239\n",
      "[3674,     1] loss: 0.0098710045\n",
      "[3675,     1] loss: 0.0098709874\n",
      "[3676,     1] loss: 0.0098709688\n",
      "[3677,     1] loss: 0.0098709516\n",
      "[3678,     1] loss: 0.0098709308\n",
      "[3679,     1] loss: 0.0098709121\n",
      "[3680,     1] loss: 0.0098708920\n",
      "[3681,     1] loss: 0.0098708741\n",
      "[3682,     1] loss: 0.0098708555\n",
      "[3683,     1] loss: 0.0098708346\n",
      "[3684,     1] loss: 0.0098708153\n",
      "[3685,     1] loss: 0.0098707952\n",
      "[3686,     1] loss: 0.0098707743\n",
      "[3687,     1] loss: 0.0098707549\n",
      "[3688,     1] loss: 0.0098707341\n",
      "[3689,     1] loss: 0.0098707125\n",
      "[3690,     1] loss: 0.0098706916\n",
      "[3691,     1] loss: 0.0098706722\n",
      "[3692,     1] loss: 0.0098706499\n",
      "[3693,     1] loss: 0.0098706283\n",
      "[3694,     1] loss: 0.0098706067\n",
      "[3695,     1] loss: 0.0098705858\n",
      "[3696,     1] loss: 0.0098705634\n",
      "[3697,     1] loss: 0.0098705404\n",
      "[3698,     1] loss: 0.0098705180\n",
      "[3699,     1] loss: 0.0098704971\n",
      "[3700,     1] loss: 0.0098704726\n",
      "[3701,     1] loss: 0.0098704517\n",
      "[3702,     1] loss: 0.0098704271\n",
      "[3703,     1] loss: 0.0098704055\n",
      "[3704,     1] loss: 0.0098703824\n",
      "[3705,     1] loss: 0.0098703586\n",
      "[3706,     1] loss: 0.0098703355\n",
      "[3707,     1] loss: 0.0098703116\n",
      "[3708,     1] loss: 0.0098702885\n",
      "[3709,     1] loss: 0.0098702647\n",
      "[3710,     1] loss: 0.0098702386\n",
      "[3711,     1] loss: 0.0098702148\n",
      "[3712,     1] loss: 0.0098701909\n",
      "[3713,     1] loss: 0.0098701656\n",
      "[3714,     1] loss: 0.0098701410\n",
      "[3715,     1] loss: 0.0098701172\n",
      "[3716,     1] loss: 0.0098700911\n",
      "[3717,     1] loss: 0.0098700665\n",
      "[3718,     1] loss: 0.0098700412\n",
      "[3719,     1] loss: 0.0098700158\n",
      "[3720,     1] loss: 0.0098699898\n",
      "[3721,     1] loss: 0.0098699637\n",
      "[3722,     1] loss: 0.0098699376\n",
      "[3723,     1] loss: 0.0098699115\n",
      "[3724,     1] loss: 0.0098698854\n",
      "[3725,     1] loss: 0.0098698594\n",
      "[3726,     1] loss: 0.0098698318\n",
      "[3727,     1] loss: 0.0098698065\n",
      "[3728,     1] loss: 0.0098697782\n",
      "[3729,     1] loss: 0.0098697521\n",
      "[3730,     1] loss: 0.0098697253\n",
      "[3731,     1] loss: 0.0098696977\n",
      "[3732,     1] loss: 0.0098696709\n",
      "[3733,     1] loss: 0.0098696426\n",
      "[3734,     1] loss: 0.0098696150\n",
      "[3735,     1] loss: 0.0098695867\n",
      "[3736,     1] loss: 0.0098695584\n",
      "[3737,     1] loss: 0.0098695315\n",
      "[3738,     1] loss: 0.0098695017\n",
      "[3739,     1] loss: 0.0098694734\n",
      "[3740,     1] loss: 0.0098694459\n",
      "[3741,     1] loss: 0.0098694168\n",
      "[3742,     1] loss: 0.0098693870\n",
      "[3743,     1] loss: 0.0098693587\n",
      "[3744,     1] loss: 0.0098693304\n",
      "[3745,     1] loss: 0.0098692998\n",
      "[3746,     1] loss: 0.0098692715\n",
      "[3747,     1] loss: 0.0098692410\n",
      "[3748,     1] loss: 0.0098692127\n",
      "[3749,     1] loss: 0.0098691821\n",
      "[3750,     1] loss: 0.0098691531\n",
      "[3751,     1] loss: 0.0098691233\n",
      "[3752,     1] loss: 0.0098690927\n",
      "[3753,     1] loss: 0.0098690629\n",
      "[3754,     1] loss: 0.0098690316\n",
      "[3755,     1] loss: 0.0098690011\n",
      "[3756,     1] loss: 0.0098689698\n",
      "[3757,     1] loss: 0.0098689407\n",
      "[3758,     1] loss: 0.0098689079\n",
      "[3759,     1] loss: 0.0098688781\n",
      "[3760,     1] loss: 0.0098688476\n",
      "[3761,     1] loss: 0.0098688155\n",
      "[3762,     1] loss: 0.0098687835\n",
      "[3763,     1] loss: 0.0098687530\n",
      "[3764,     1] loss: 0.0098687209\n",
      "[3765,     1] loss: 0.0098686881\n",
      "[3766,     1] loss: 0.0098686583\n",
      "[3767,     1] loss: 0.0098686248\n",
      "[3768,     1] loss: 0.0098685935\n",
      "[3769,     1] loss: 0.0098685622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3770,     1] loss: 0.0098685287\n",
      "[3771,     1] loss: 0.0098684967\n",
      "[3772,     1] loss: 0.0098684639\n",
      "[3773,     1] loss: 0.0098684311\n",
      "[3774,     1] loss: 0.0098683983\n",
      "[3775,     1] loss: 0.0098683640\n",
      "[3776,     1] loss: 0.0098683313\n",
      "[3777,     1] loss: 0.0098682985\n",
      "[3778,     1] loss: 0.0098682657\n",
      "[3779,     1] loss: 0.0098682314\n",
      "[3780,     1] loss: 0.0098681979\n",
      "[3781,     1] loss: 0.0098681644\n",
      "[3782,     1] loss: 0.0098681301\n",
      "[3783,     1] loss: 0.0098680966\n",
      "[3784,     1] loss: 0.0098680638\n",
      "[3785,     1] loss: 0.0098680288\n",
      "[3786,     1] loss: 0.0098679930\n",
      "[3787,     1] loss: 0.0098679602\n",
      "[3788,     1] loss: 0.0098679252\n",
      "[3789,     1] loss: 0.0098678902\n",
      "[3790,     1] loss: 0.0098678559\n",
      "[3791,     1] loss: 0.0098678209\n",
      "[3792,     1] loss: 0.0098677859\n",
      "[3793,     1] loss: 0.0098677523\n",
      "[3794,     1] loss: 0.0098677166\n",
      "[3795,     1] loss: 0.0098676808\n",
      "[3796,     1] loss: 0.0098676458\n",
      "[3797,     1] loss: 0.0098676093\n",
      "[3798,     1] loss: 0.0098675750\n",
      "[3799,     1] loss: 0.0098675393\n",
      "[3800,     1] loss: 0.0098675027\n",
      "[3801,     1] loss: 0.0098674685\n",
      "[3802,     1] loss: 0.0098674320\n",
      "[3803,     1] loss: 0.0098673962\n",
      "[3804,     1] loss: 0.0098673604\n",
      "[3805,     1] loss: 0.0098673224\n",
      "[3806,     1] loss: 0.0098672874\n",
      "[3807,     1] loss: 0.0098672502\n",
      "[3808,     1] loss: 0.0098672137\n",
      "[3809,     1] loss: 0.0098671786\n",
      "[3810,     1] loss: 0.0098671399\n",
      "[3811,     1] loss: 0.0098671027\n",
      "[3812,     1] loss: 0.0098670654\n",
      "[3813,     1] loss: 0.0098670289\n",
      "[3814,     1] loss: 0.0098669916\n",
      "[3815,     1] loss: 0.0098669536\n",
      "[3816,     1] loss: 0.0098669164\n",
      "[3817,     1] loss: 0.0098668791\n",
      "[3818,     1] loss: 0.0098668419\n",
      "[3819,     1] loss: 0.0098668039\n",
      "[3820,     1] loss: 0.0098667666\n",
      "[3821,     1] loss: 0.0098667271\n",
      "[3822,     1] loss: 0.0098666899\n",
      "[3823,     1] loss: 0.0098666511\n",
      "[3824,     1] loss: 0.0098666131\n",
      "[3825,     1] loss: 0.0098665752\n",
      "[3826,     1] loss: 0.0098665372\n",
      "[3827,     1] loss: 0.0098664984\n",
      "[3828,     1] loss: 0.0098664612\n",
      "[3829,     1] loss: 0.0098664209\n",
      "[3830,     1] loss: 0.0098663822\n",
      "[3831,     1] loss: 0.0098663434\n",
      "[3832,     1] loss: 0.0098663047\n",
      "[3833,     1] loss: 0.0098662660\n",
      "[3834,     1] loss: 0.0098662265\n",
      "[3835,     1] loss: 0.0098661870\n",
      "[3836,     1] loss: 0.0098661482\n",
      "[3837,     1] loss: 0.0098661087\n",
      "[3838,     1] loss: 0.0098660693\n",
      "[3839,     1] loss: 0.0098660290\n",
      "[3840,     1] loss: 0.0098659895\n",
      "[3841,     1] loss: 0.0098659500\n",
      "[3842,     1] loss: 0.0098659083\n",
      "[3843,     1] loss: 0.0098658696\n",
      "[3844,     1] loss: 0.0098658279\n",
      "[3845,     1] loss: 0.0098657891\n",
      "[3846,     1] loss: 0.0098657496\n",
      "[3847,     1] loss: 0.0098657094\n",
      "[3848,     1] loss: 0.0098656677\n",
      "[3849,     1] loss: 0.0098656274\n",
      "[3850,     1] loss: 0.0098655865\n",
      "[3851,     1] loss: 0.0098655470\n",
      "[3852,     1] loss: 0.0098655075\n",
      "[3853,     1] loss: 0.0098654658\n",
      "[3854,     1] loss: 0.0098654248\n",
      "[3855,     1] loss: 0.0098653823\n",
      "[3856,     1] loss: 0.0098653428\n",
      "[3857,     1] loss: 0.0098653018\n",
      "[3858,     1] loss: 0.0098652594\n",
      "[3859,     1] loss: 0.0098652177\n",
      "[3860,     1] loss: 0.0098651759\n",
      "[3861,     1] loss: 0.0098651350\n",
      "[3862,     1] loss: 0.0098650940\n",
      "[3863,     1] loss: 0.0098650523\n",
      "[3864,     1] loss: 0.0098650098\n",
      "[3865,     1] loss: 0.0098649681\n",
      "[3866,     1] loss: 0.0098649263\n",
      "[3867,     1] loss: 0.0098648831\n",
      "[3868,     1] loss: 0.0098648407\n",
      "[3869,     1] loss: 0.0098648004\n",
      "[3870,     1] loss: 0.0098647572\n",
      "[3871,     1] loss: 0.0098647147\n",
      "[3872,     1] loss: 0.0098646708\n",
      "[3873,     1] loss: 0.0098646298\n",
      "[3874,     1] loss: 0.0098645881\n",
      "[3875,     1] loss: 0.0098645449\n",
      "[3876,     1] loss: 0.0098645024\n",
      "[3877,     1] loss: 0.0098644599\n",
      "[3878,     1] loss: 0.0098644167\n",
      "[3879,     1] loss: 0.0098643735\n",
      "[3880,     1] loss: 0.0098643303\n",
      "[3881,     1] loss: 0.0098642856\n",
      "[3882,     1] loss: 0.0098642439\n",
      "[3883,     1] loss: 0.0098642007\n",
      "[3884,     1] loss: 0.0098641567\n",
      "[3885,     1] loss: 0.0098641142\n",
      "[3886,     1] loss: 0.0098640695\n",
      "[3887,     1] loss: 0.0098640256\n",
      "[3888,     1] loss: 0.0098639823\n",
      "[3889,     1] loss: 0.0098639376\n",
      "[3890,     1] loss: 0.0098638944\n",
      "[3891,     1] loss: 0.0098638512\n",
      "[3892,     1] loss: 0.0098638073\n",
      "[3893,     1] loss: 0.0098637626\n",
      "[3894,     1] loss: 0.0098637186\n",
      "[3895,     1] loss: 0.0098636739\n",
      "[3896,     1] loss: 0.0098636307\n",
      "[3897,     1] loss: 0.0098635867\n",
      "[3898,     1] loss: 0.0098635405\n",
      "[3899,     1] loss: 0.0098634973\n",
      "[3900,     1] loss: 0.0098634526\n",
      "[3901,     1] loss: 0.0098634079\n",
      "[3902,     1] loss: 0.0098633632\n",
      "[3903,     1] loss: 0.0098633178\n",
      "[3904,     1] loss: 0.0098632731\n",
      "[3905,     1] loss: 0.0098632284\n",
      "[3906,     1] loss: 0.0098631829\n",
      "[3907,     1] loss: 0.0098631382\n",
      "[3908,     1] loss: 0.0098630935\n",
      "[3909,     1] loss: 0.0098630488\n",
      "[3910,     1] loss: 0.0098630019\n",
      "[3911,     1] loss: 0.0098629557\n",
      "[3912,     1] loss: 0.0098629110\n",
      "[3913,     1] loss: 0.0098628655\n",
      "[3914,     1] loss: 0.0098628193\n",
      "[3915,     1] loss: 0.0098627746\n",
      "[3916,     1] loss: 0.0098627292\n",
      "[3917,     1] loss: 0.0098626822\n",
      "[3918,     1] loss: 0.0098626375\n",
      "[3919,     1] loss: 0.0098625913\n",
      "[3920,     1] loss: 0.0098625451\n",
      "[3921,     1] loss: 0.0098625004\n",
      "[3922,     1] loss: 0.0098624520\n",
      "[3923,     1] loss: 0.0098624066\n",
      "[3924,     1] loss: 0.0098623604\n",
      "[3925,     1] loss: 0.0098623127\n",
      "[3926,     1] loss: 0.0098622687\n",
      "[3927,     1] loss: 0.0098622210\n",
      "[3928,     1] loss: 0.0098621733\n",
      "[3929,     1] loss: 0.0098621279\n",
      "[3930,     1] loss: 0.0098620802\n",
      "[3931,     1] loss: 0.0098620340\n",
      "[3932,     1] loss: 0.0098619871\n",
      "[3933,     1] loss: 0.0098619409\n",
      "[3934,     1] loss: 0.0098618932\n",
      "[3935,     1] loss: 0.0098618470\n",
      "[3936,     1] loss: 0.0098617993\n",
      "[3937,     1] loss: 0.0098617524\n",
      "[3938,     1] loss: 0.0098617055\n",
      "[3939,     1] loss: 0.0098616563\n",
      "[3940,     1] loss: 0.0098616101\n",
      "[3941,     1] loss: 0.0098615624\n",
      "[3942,     1] loss: 0.0098615155\n",
      "[3943,     1] loss: 0.0098614685\n",
      "[3944,     1] loss: 0.0098614193\n",
      "[3945,     1] loss: 0.0098613717\n",
      "[3946,     1] loss: 0.0098613240\n",
      "[3947,     1] loss: 0.0098612763\n",
      "[3948,     1] loss: 0.0098612286\n",
      "[3949,     1] loss: 0.0098611809\n",
      "[3950,     1] loss: 0.0098611325\n",
      "[3951,     1] loss: 0.0098610856\n",
      "[3952,     1] loss: 0.0098610342\n",
      "[3953,     1] loss: 0.0098609887\n",
      "[3954,     1] loss: 0.0098609395\n",
      "[3955,     1] loss: 0.0098608918\n",
      "[3956,     1] loss: 0.0098608442\n",
      "[3957,     1] loss: 0.0098607950\n",
      "[3958,     1] loss: 0.0098607473\n",
      "[3959,     1] loss: 0.0098606989\n",
      "[3960,     1] loss: 0.0098606490\n",
      "[3961,     1] loss: 0.0098605998\n",
      "[3962,     1] loss: 0.0098605514\n",
      "[3963,     1] loss: 0.0098605029\n",
      "[3964,     1] loss: 0.0098604530\n",
      "[3965,     1] loss: 0.0098604053\n",
      "[3966,     1] loss: 0.0098603554\n",
      "[3967,     1] loss: 0.0098603055\n",
      "[3968,     1] loss: 0.0098602578\n",
      "[3969,     1] loss: 0.0098602086\n",
      "[3970,     1] loss: 0.0098601595\n",
      "[3971,     1] loss: 0.0098601103\n",
      "[3972,     1] loss: 0.0098600604\n",
      "[3973,     1] loss: 0.0098600119\n",
      "[3974,     1] loss: 0.0098599620\n",
      "[3975,     1] loss: 0.0098599114\n",
      "[3976,     1] loss: 0.0098598614\n",
      "[3977,     1] loss: 0.0098598130\n",
      "[3978,     1] loss: 0.0098597631\n",
      "[3979,     1] loss: 0.0098597132\n",
      "[3980,     1] loss: 0.0098596640\n",
      "[3981,     1] loss: 0.0098596141\n",
      "[3982,     1] loss: 0.0098595634\n",
      "[3983,     1] loss: 0.0098595135\n",
      "[3984,     1] loss: 0.0098594643\n",
      "[3985,     1] loss: 0.0098594129\n",
      "[3986,     1] loss: 0.0098593630\n",
      "[3987,     1] loss: 0.0098593123\n",
      "[3988,     1] loss: 0.0098592609\n",
      "[3989,     1] loss: 0.0098592125\n",
      "[3990,     1] loss: 0.0098591611\n",
      "[3991,     1] loss: 0.0098591112\n",
      "[3992,     1] loss: 0.0098590612\n",
      "[3993,     1] loss: 0.0098590091\n",
      "[3994,     1] loss: 0.0098589607\n",
      "[3995,     1] loss: 0.0098589085\n",
      "[3996,     1] loss: 0.0098588578\n",
      "[3997,     1] loss: 0.0098588064\n",
      "[3998,     1] loss: 0.0098587573\n",
      "[3999,     1] loss: 0.0098587058\n",
      "[4000,     1] loss: 0.0098586559\n",
      "[4001,     1] loss: 0.0098586030\n",
      "[4002,     1] loss: 0.0098585531\n",
      "[4003,     1] loss: 0.0098585017\n",
      "[4004,     1] loss: 0.0098584503\n",
      "[4005,     1] loss: 0.0098583996\n",
      "[4006,     1] loss: 0.0098583475\n",
      "[4007,     1] loss: 0.0098582961\n",
      "[4008,     1] loss: 0.0098582454\n",
      "[4009,     1] loss: 0.0098581940\n",
      "[4010,     1] loss: 0.0098581418\n",
      "[4011,     1] loss: 0.0098580904\n",
      "[4012,     1] loss: 0.0098580383\n",
      "[4013,     1] loss: 0.0098579869\n",
      "[4014,     1] loss: 0.0098579369\n",
      "[4015,     1] loss: 0.0098578848\n",
      "[4016,     1] loss: 0.0098578319\n",
      "[4017,     1] loss: 0.0098577820\n",
      "[4018,     1] loss: 0.0098577283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4019,     1] loss: 0.0098576769\n",
      "[4020,     1] loss: 0.0098576248\n",
      "[4021,     1] loss: 0.0098575734\n",
      "[4022,     1] loss: 0.0098575212\n",
      "[4023,     1] loss: 0.0098574676\n",
      "[4024,     1] loss: 0.0098574162\n",
      "[4025,     1] loss: 0.0098573647\n",
      "[4026,     1] loss: 0.0098573118\n",
      "[4027,     1] loss: 0.0098572597\n",
      "[4028,     1] loss: 0.0098572068\n",
      "[4029,     1] loss: 0.0098571546\n",
      "[4030,     1] loss: 0.0098571017\n",
      "[4031,     1] loss: 0.0098570481\n",
      "[4032,     1] loss: 0.0098569967\n",
      "[4033,     1] loss: 0.0098569445\n",
      "[4034,     1] loss: 0.0098568916\n",
      "[4035,     1] loss: 0.0098568380\n",
      "[4036,     1] loss: 0.0098567866\n",
      "[4037,     1] loss: 0.0098567329\n",
      "[4038,     1] loss: 0.0098566800\n",
      "[4039,     1] loss: 0.0098566271\n",
      "[4040,     1] loss: 0.0098565742\n",
      "[4041,     1] loss: 0.0098565213\n",
      "[4042,     1] loss: 0.0098564684\n",
      "[4043,     1] loss: 0.0098564140\n",
      "[4044,     1] loss: 0.0098563626\n",
      "[4045,     1] loss: 0.0098563090\n",
      "[4046,     1] loss: 0.0098562554\n",
      "[4047,     1] loss: 0.0098562010\n",
      "[4048,     1] loss: 0.0098561488\n",
      "[4049,     1] loss: 0.0098560959\n",
      "[4050,     1] loss: 0.0098560415\n",
      "[4051,     1] loss: 0.0098559879\n",
      "[4052,     1] loss: 0.0098559350\n",
      "[4053,     1] loss: 0.0098558806\n",
      "[4054,     1] loss: 0.0098558269\n",
      "[4055,     1] loss: 0.0098557733\n",
      "[4056,     1] loss: 0.0098557189\n",
      "[4057,     1] loss: 0.0098556660\n",
      "[4058,     1] loss: 0.0098556124\n",
      "[4059,     1] loss: 0.0098555587\n",
      "[4060,     1] loss: 0.0098555051\n",
      "[4061,     1] loss: 0.0098554499\n",
      "[4062,     1] loss: 0.0098553970\n",
      "[4063,     1] loss: 0.0098553419\n",
      "[4064,     1] loss: 0.0098552890\n",
      "[4065,     1] loss: 0.0098552339\n",
      "[4066,     1] loss: 0.0098551795\n",
      "[4067,     1] loss: 0.0098551251\n",
      "[4068,     1] loss: 0.0098550707\n",
      "[4069,     1] loss: 0.0098550171\n",
      "[4070,     1] loss: 0.0098549627\n",
      "[4071,     1] loss: 0.0098549083\n",
      "[4072,     1] loss: 0.0098548532\n",
      "[4073,     1] loss: 0.0098548003\n",
      "[4074,     1] loss: 0.0098547451\n",
      "[4075,     1] loss: 0.0098546900\n",
      "[4076,     1] loss: 0.0098546349\n",
      "[4077,     1] loss: 0.0098545790\n",
      "[4078,     1] loss: 0.0098545246\n",
      "[4079,     1] loss: 0.0098544709\n",
      "[4080,     1] loss: 0.0098544173\n",
      "[4081,     1] loss: 0.0098543592\n",
      "[4082,     1] loss: 0.0098543055\n",
      "[4083,     1] loss: 0.0098542511\n",
      "[4084,     1] loss: 0.0098541953\n",
      "[4085,     1] loss: 0.0098541409\n",
      "[4086,     1] loss: 0.0098540865\n",
      "[4087,     1] loss: 0.0098540321\n",
      "[4088,     1] loss: 0.0098539762\n",
      "[4089,     1] loss: 0.0098539203\n",
      "[4090,     1] loss: 0.0098538652\n",
      "[4091,     1] loss: 0.0098538101\n",
      "[4092,     1] loss: 0.0098537542\n",
      "[4093,     1] loss: 0.0098536991\n",
      "[4094,     1] loss: 0.0098536432\n",
      "[4095,     1] loss: 0.0098535880\n",
      "[4096,     1] loss: 0.0098535329\n",
      "[4097,     1] loss: 0.0098534778\n",
      "[4098,     1] loss: 0.0098534219\n",
      "[4099,     1] loss: 0.0098533660\n",
      "[4100,     1] loss: 0.0098533116\n",
      "[4101,     1] loss: 0.0098532557\n",
      "[4102,     1] loss: 0.0098531999\n",
      "[4103,     1] loss: 0.0098531432\n",
      "[4104,     1] loss: 0.0098530881\n",
      "[4105,     1] loss: 0.0098530322\n",
      "[4106,     1] loss: 0.0098529756\n",
      "[4107,     1] loss: 0.0098529205\n",
      "[4108,     1] loss: 0.0098528631\n",
      "[4109,     1] loss: 0.0098528080\n",
      "[4110,     1] loss: 0.0098527521\n",
      "[4111,     1] loss: 0.0098526955\n",
      "[4112,     1] loss: 0.0098526396\n",
      "[4113,     1] loss: 0.0098525837\n",
      "[4114,     1] loss: 0.0098525278\n",
      "[4115,     1] loss: 0.0098524705\n",
      "[4116,     1] loss: 0.0098524146\n",
      "[4117,     1] loss: 0.0098523572\n",
      "[4118,     1] loss: 0.0098523013\n",
      "[4119,     1] loss: 0.0098522462\n",
      "[4120,     1] loss: 0.0098521896\n",
      "[4121,     1] loss: 0.0098521322\n",
      "[4122,     1] loss: 0.0098520771\n",
      "[4123,     1] loss: 0.0098520197\n",
      "[4124,     1] loss: 0.0098519638\n",
      "[4125,     1] loss: 0.0098519064\n",
      "[4126,     1] loss: 0.0098518498\n",
      "[4127,     1] loss: 0.0098517925\n",
      "[4128,     1] loss: 0.0098517358\n",
      "[4129,     1] loss: 0.0098516792\n",
      "[4130,     1] loss: 0.0098516226\n",
      "[4131,     1] loss: 0.0098515660\n",
      "[4132,     1] loss: 0.0098515093\n",
      "[4133,     1] loss: 0.0098514512\n",
      "[4134,     1] loss: 0.0098513961\n",
      "[4135,     1] loss: 0.0098513387\n",
      "[4136,     1] loss: 0.0098512806\n",
      "[4137,     1] loss: 0.0098512240\n",
      "[4138,     1] loss: 0.0098511674\n",
      "[4139,     1] loss: 0.0098511092\n",
      "[4140,     1] loss: 0.0098510526\n",
      "[4141,     1] loss: 0.0098509945\n",
      "[4142,     1] loss: 0.0098509386\n",
      "[4143,     1] loss: 0.0098508812\n",
      "[4144,     1] loss: 0.0098508231\n",
      "[4145,     1] loss: 0.0098507658\n",
      "[4146,     1] loss: 0.0098507091\n",
      "[4147,     1] loss: 0.0098506518\n",
      "[4148,     1] loss: 0.0098505937\n",
      "[4149,     1] loss: 0.0098505363\n",
      "[4150,     1] loss: 0.0098504797\n",
      "[4151,     1] loss: 0.0098504215\n",
      "[4152,     1] loss: 0.0098503634\n",
      "[4153,     1] loss: 0.0098503053\n",
      "[4154,     1] loss: 0.0098502487\n",
      "[4155,     1] loss: 0.0098501921\n",
      "[4156,     1] loss: 0.0098501332\n",
      "[4157,     1] loss: 0.0098500751\n",
      "[4158,     1] loss: 0.0098500177\n",
      "[4159,     1] loss: 0.0098499596\n",
      "[4160,     1] loss: 0.0098499022\n",
      "[4161,     1] loss: 0.0098498449\n",
      "[4162,     1] loss: 0.0098497868\n",
      "[4163,     1] loss: 0.0098497294\n",
      "[4164,     1] loss: 0.0098496705\n",
      "[4165,     1] loss: 0.0098496124\n",
      "[4166,     1] loss: 0.0098495550\n",
      "[4167,     1] loss: 0.0098494954\n",
      "[4168,     1] loss: 0.0098494381\n",
      "[4169,     1] loss: 0.0098493800\n",
      "[4170,     1] loss: 0.0098493218\n",
      "[4171,     1] loss: 0.0098492622\n",
      "[4172,     1] loss: 0.0098492049\n",
      "[4173,     1] loss: 0.0098491468\n",
      "[4174,     1] loss: 0.0098490901\n",
      "[4175,     1] loss: 0.0098490305\n",
      "[4176,     1] loss: 0.0098489717\n",
      "[4177,     1] loss: 0.0098489136\n",
      "[4178,     1] loss: 0.0098488554\n",
      "[4179,     1] loss: 0.0098487966\n",
      "[4180,     1] loss: 0.0098487385\n",
      "[4181,     1] loss: 0.0098486796\n",
      "[4182,     1] loss: 0.0098486215\n",
      "[4183,     1] loss: 0.0098485626\n",
      "[4184,     1] loss: 0.0098485045\n",
      "[4185,     1] loss: 0.0098484457\n",
      "[4186,     1] loss: 0.0098483868\n",
      "[4187,     1] loss: 0.0098483279\n",
      "[4188,     1] loss: 0.0098482698\n",
      "[4189,     1] loss: 0.0098482110\n",
      "[4190,     1] loss: 0.0098481521\n",
      "[4191,     1] loss: 0.0098480932\n",
      "[4192,     1] loss: 0.0098480336\n",
      "[4193,     1] loss: 0.0098479755\n",
      "[4194,     1] loss: 0.0098479167\n",
      "[4195,     1] loss: 0.0098478585\n",
      "[4196,     1] loss: 0.0098477989\n",
      "[4197,     1] loss: 0.0098477393\n",
      "[4198,     1] loss: 0.0098476820\n",
      "[4199,     1] loss: 0.0098476216\n",
      "[4200,     1] loss: 0.0098475643\n",
      "[4201,     1] loss: 0.0098475039\n",
      "[4202,     1] loss: 0.0098474450\n",
      "[4203,     1] loss: 0.0098473862\n",
      "[4204,     1] loss: 0.0098473266\n",
      "[4205,     1] loss: 0.0098472662\n",
      "[4206,     1] loss: 0.0098472096\n",
      "[4207,     1] loss: 0.0098471485\n",
      "[4208,     1] loss: 0.0098470889\n",
      "[4209,     1] loss: 0.0098470315\n",
      "[4210,     1] loss: 0.0098469727\n",
      "[4211,     1] loss: 0.0098469123\n",
      "[4212,     1] loss: 0.0098468535\n",
      "[4213,     1] loss: 0.0098467946\n",
      "[4214,     1] loss: 0.0098467335\n",
      "[4215,     1] loss: 0.0098466747\n",
      "[4216,     1] loss: 0.0098466150\n",
      "[4217,     1] loss: 0.0098465554\n",
      "[4218,     1] loss: 0.0098464951\n",
      "[4219,     1] loss: 0.0098464362\n",
      "[4220,     1] loss: 0.0098463774\n",
      "[4221,     1] loss: 0.0098463178\n",
      "[4222,     1] loss: 0.0098462567\n",
      "[4223,     1] loss: 0.0098461978\n",
      "[4224,     1] loss: 0.0098461382\n",
      "[4225,     1] loss: 0.0098460793\n",
      "[4226,     1] loss: 0.0098460205\n",
      "[4227,     1] loss: 0.0098459601\n",
      "[4228,     1] loss: 0.0098459005\n",
      "[4229,     1] loss: 0.0098458409\n",
      "[4230,     1] loss: 0.0098457813\n",
      "[4231,     1] loss: 0.0098457210\n",
      "[4232,     1] loss: 0.0098456614\n",
      "[4233,     1] loss: 0.0098456018\n",
      "[4234,     1] loss: 0.0098455414\n",
      "[4235,     1] loss: 0.0098454818\n",
      "[4236,     1] loss: 0.0098454215\n",
      "[4237,     1] loss: 0.0098453626\n",
      "[4238,     1] loss: 0.0098453023\n",
      "[4239,     1] loss: 0.0098452426\n",
      "[4240,     1] loss: 0.0098451823\n",
      "[4241,     1] loss: 0.0098451219\n",
      "[4242,     1] loss: 0.0098450623\n",
      "[4243,     1] loss: 0.0098450027\n",
      "[4244,     1] loss: 0.0098449424\n",
      "[4245,     1] loss: 0.0098448813\n",
      "[4246,     1] loss: 0.0098448217\n",
      "[4247,     1] loss: 0.0098447613\n",
      "[4248,     1] loss: 0.0098447010\n",
      "[4249,     1] loss: 0.0098446406\n",
      "[4250,     1] loss: 0.0098445803\n",
      "[4251,     1] loss: 0.0098445199\n",
      "[4252,     1] loss: 0.0098444603\n",
      "[4253,     1] loss: 0.0098443992\n",
      "[4254,     1] loss: 0.0098443396\n",
      "[4255,     1] loss: 0.0098442793\n",
      "[4256,     1] loss: 0.0098442189\n",
      "[4257,     1] loss: 0.0098441578\n",
      "[4258,     1] loss: 0.0098440982\n",
      "[4259,     1] loss: 0.0098440386\n",
      "[4260,     1] loss: 0.0098439790\n",
      "[4261,     1] loss: 0.0098439172\n",
      "[4262,     1] loss: 0.0098438568\n",
      "[4263,     1] loss: 0.0098437972\n",
      "[4264,     1] loss: 0.0098437369\n",
      "[4265,     1] loss: 0.0098436765\n",
      "[4266,     1] loss: 0.0098436162\n",
      "[4267,     1] loss: 0.0098435558\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4268,     1] loss: 0.0098434947\n",
      "[4269,     1] loss: 0.0098434336\n",
      "[4270,     1] loss: 0.0098433733\n",
      "[4271,     1] loss: 0.0098433137\n",
      "[4272,     1] loss: 0.0098432519\n",
      "[4273,     1] loss: 0.0098431915\n",
      "[4274,     1] loss: 0.0098431304\n",
      "[4275,     1] loss: 0.0098430701\n",
      "[4276,     1] loss: 0.0098430090\n",
      "[4277,     1] loss: 0.0098429486\n",
      "[4278,     1] loss: 0.0098428868\n",
      "[4279,     1] loss: 0.0098428264\n",
      "[4280,     1] loss: 0.0098427661\n",
      "[4281,     1] loss: 0.0098427050\n",
      "[4282,     1] loss: 0.0098426454\n",
      "[4283,     1] loss: 0.0098425843\n",
      "[4284,     1] loss: 0.0098425232\n",
      "[4285,     1] loss: 0.0098424628\n",
      "[4286,     1] loss: 0.0098424025\n",
      "[4287,     1] loss: 0.0098423414\n",
      "[4288,     1] loss: 0.0098422803\n",
      "[4289,     1] loss: 0.0098422192\n",
      "[4290,     1] loss: 0.0098421581\n",
      "[4291,     1] loss: 0.0098420978\n",
      "[4292,     1] loss: 0.0098420367\n",
      "[4293,     1] loss: 0.0098419756\n",
      "[4294,     1] loss: 0.0098419152\n",
      "[4295,     1] loss: 0.0098418541\n",
      "[4296,     1] loss: 0.0098417930\n",
      "[4297,     1] loss: 0.0098417327\n",
      "[4298,     1] loss: 0.0098416708\n",
      "[4299,     1] loss: 0.0098416097\n",
      "[4300,     1] loss: 0.0098415494\n",
      "[4301,     1] loss: 0.0098414876\n",
      "[4302,     1] loss: 0.0098414272\n",
      "[4303,     1] loss: 0.0098413661\n",
      "[4304,     1] loss: 0.0098413050\n",
      "[4305,     1] loss: 0.0098412447\n",
      "[4306,     1] loss: 0.0098411828\n",
      "[4307,     1] loss: 0.0098411225\n",
      "[4308,     1] loss: 0.0098410606\n",
      "[4309,     1] loss: 0.0098409995\n",
      "[4310,     1] loss: 0.0098409392\n",
      "[4311,     1] loss: 0.0098408774\n",
      "[4312,     1] loss: 0.0098408163\n",
      "[4313,     1] loss: 0.0098407544\n",
      "[4314,     1] loss: 0.0098406933\n",
      "[4315,     1] loss: 0.0098406322\n",
      "[4316,     1] loss: 0.0098405711\n",
      "[4317,     1] loss: 0.0098405100\n",
      "[4318,     1] loss: 0.0098404482\n",
      "[4319,     1] loss: 0.0098403871\n",
      "[4320,     1] loss: 0.0098403268\n",
      "[4321,     1] loss: 0.0098402657\n",
      "[4322,     1] loss: 0.0098402038\n",
      "[4323,     1] loss: 0.0098401420\n",
      "[4324,     1] loss: 0.0098400809\n",
      "[4325,     1] loss: 0.0098400190\n",
      "[4326,     1] loss: 0.0098399572\n",
      "[4327,     1] loss: 0.0098398969\n",
      "[4328,     1] loss: 0.0098398358\n",
      "[4329,     1] loss: 0.0098397747\n",
      "[4330,     1] loss: 0.0098397128\n",
      "[4331,     1] loss: 0.0098396510\n",
      "[4332,     1] loss: 0.0098395899\n",
      "[4333,     1] loss: 0.0098395281\n",
      "[4334,     1] loss: 0.0098394670\n",
      "[4335,     1] loss: 0.0098394059\n",
      "[4336,     1] loss: 0.0098393440\n",
      "[4337,     1] loss: 0.0098392844\n",
      "[4338,     1] loss: 0.0098392218\n",
      "[4339,     1] loss: 0.0098391607\n",
      "[4340,     1] loss: 0.0098390989\n",
      "[4341,     1] loss: 0.0098390371\n",
      "[4342,     1] loss: 0.0098389760\n",
      "[4343,     1] loss: 0.0098389141\n",
      "[4344,     1] loss: 0.0098388523\n",
      "[4345,     1] loss: 0.0098387904\n",
      "[4346,     1] loss: 0.0098387301\n",
      "[4347,     1] loss: 0.0098386683\n",
      "[4348,     1] loss: 0.0098386049\n",
      "[4349,     1] loss: 0.0098385446\n",
      "[4350,     1] loss: 0.0098384835\n",
      "[4351,     1] loss: 0.0098384209\n",
      "[4352,     1] loss: 0.0098383591\n",
      "[4353,     1] loss: 0.0098382972\n",
      "[4354,     1] loss: 0.0098382376\n",
      "[4355,     1] loss: 0.0098381758\n",
      "[4356,     1] loss: 0.0098381139\n",
      "[4357,     1] loss: 0.0098380528\n",
      "[4358,     1] loss: 0.0098379895\n",
      "[4359,     1] loss: 0.0098379292\n",
      "[4360,     1] loss: 0.0098378666\n",
      "[4361,     1] loss: 0.0098378055\n",
      "[4362,     1] loss: 0.0098377429\n",
      "[4363,     1] loss: 0.0098376818\n",
      "[4364,     1] loss: 0.0098376207\n",
      "[4365,     1] loss: 0.0098375589\n",
      "[4366,     1] loss: 0.0098374978\n",
      "[4367,     1] loss: 0.0098374352\n",
      "[4368,     1] loss: 0.0098373741\n",
      "[4369,     1] loss: 0.0098373123\n",
      "[4370,     1] loss: 0.0098372497\n",
      "[4371,     1] loss: 0.0098371886\n",
      "[4372,     1] loss: 0.0098371275\n",
      "[4373,     1] loss: 0.0098370664\n",
      "[4374,     1] loss: 0.0098370045\n",
      "[4375,     1] loss: 0.0098369434\n",
      "[4376,     1] loss: 0.0098368801\n",
      "[4377,     1] loss: 0.0098368190\n",
      "[4378,     1] loss: 0.0098367564\n",
      "[4379,     1] loss: 0.0098366953\n",
      "[4380,     1] loss: 0.0098366328\n",
      "[4381,     1] loss: 0.0098365717\n",
      "[4382,     1] loss: 0.0098365106\n",
      "[4383,     1] loss: 0.0098364472\n",
      "[4384,     1] loss: 0.0098363869\n",
      "[4385,     1] loss: 0.0098363250\n",
      "[4386,     1] loss: 0.0098362625\n",
      "[4387,     1] loss: 0.0098362006\n",
      "[4388,     1] loss: 0.0098361388\n",
      "[4389,     1] loss: 0.0098360777\n",
      "[4390,     1] loss: 0.0098360159\n",
      "[4391,     1] loss: 0.0098359525\n",
      "[4392,     1] loss: 0.0098358922\n",
      "[4393,     1] loss: 0.0098358296\n",
      "[4394,     1] loss: 0.0098357685\n",
      "[4395,     1] loss: 0.0098357067\n",
      "[4396,     1] loss: 0.0098356441\n",
      "[4397,     1] loss: 0.0098355822\n",
      "[4398,     1] loss: 0.0098355219\n",
      "[4399,     1] loss: 0.0098354593\n",
      "[4400,     1] loss: 0.0098353975\n",
      "[4401,     1] loss: 0.0098353364\n",
      "[4402,     1] loss: 0.0098352745\n",
      "[4403,     1] loss: 0.0098352127\n",
      "[4404,     1] loss: 0.0098351501\n",
      "[4405,     1] loss: 0.0098350890\n",
      "[4406,     1] loss: 0.0098350257\n",
      "[4407,     1] loss: 0.0098349638\n",
      "[4408,     1] loss: 0.0098349027\n",
      "[4409,     1] loss: 0.0098348409\n",
      "[4410,     1] loss: 0.0098347805\n",
      "[4411,     1] loss: 0.0098347172\n",
      "[4412,     1] loss: 0.0098346561\n",
      "[4413,     1] loss: 0.0098345935\n",
      "[4414,     1] loss: 0.0098345324\n",
      "[4415,     1] loss: 0.0098344691\n",
      "[4416,     1] loss: 0.0098344073\n",
      "[4417,     1] loss: 0.0098343454\n",
      "[4418,     1] loss: 0.0098342851\n",
      "[4419,     1] loss: 0.0098342225\n",
      "[4420,     1] loss: 0.0098341614\n",
      "[4421,     1] loss: 0.0098340996\n",
      "[4422,     1] loss: 0.0098340377\n",
      "[4423,     1] loss: 0.0098339751\n",
      "[4424,     1] loss: 0.0098339148\n",
      "[4425,     1] loss: 0.0098338522\n",
      "[4426,     1] loss: 0.0098337904\n",
      "[4427,     1] loss: 0.0098337285\n",
      "[4428,     1] loss: 0.0098336659\n",
      "[4429,     1] loss: 0.0098336034\n",
      "[4430,     1] loss: 0.0098335415\n",
      "[4431,     1] loss: 0.0098334797\n",
      "[4432,     1] loss: 0.0098334178\n",
      "[4433,     1] loss: 0.0098333560\n",
      "[4434,     1] loss: 0.0098332942\n",
      "[4435,     1] loss: 0.0098332331\n",
      "[4436,     1] loss: 0.0098331705\n",
      "[4437,     1] loss: 0.0098331086\n",
      "[4438,     1] loss: 0.0098330460\n",
      "[4439,     1] loss: 0.0098329842\n",
      "[4440,     1] loss: 0.0098329224\n",
      "[4441,     1] loss: 0.0098328613\n",
      "[4442,     1] loss: 0.0098327994\n",
      "[4443,     1] loss: 0.0098327376\n",
      "[4444,     1] loss: 0.0098326743\n",
      "[4445,     1] loss: 0.0098326132\n",
      "[4446,     1] loss: 0.0098325513\n",
      "[4447,     1] loss: 0.0098324887\n",
      "[4448,     1] loss: 0.0098324277\n",
      "[4449,     1] loss: 0.0098323658\n",
      "[4450,     1] loss: 0.0098323032\n",
      "[4451,     1] loss: 0.0098322414\n",
      "[4452,     1] loss: 0.0098321803\n",
      "[4453,     1] loss: 0.0098321177\n",
      "[4454,     1] loss: 0.0098320551\n",
      "[4455,     1] loss: 0.0098319940\n",
      "[4456,     1] loss: 0.0098319322\n",
      "[4457,     1] loss: 0.0098318711\n",
      "[4458,     1] loss: 0.0098318085\n",
      "[4459,     1] loss: 0.0098317467\n",
      "[4460,     1] loss: 0.0098316848\n",
      "[4461,     1] loss: 0.0098316237\n",
      "[4462,     1] loss: 0.0098315604\n",
      "[4463,     1] loss: 0.0098314986\n",
      "[4464,     1] loss: 0.0098314367\n",
      "[4465,     1] loss: 0.0098313749\n",
      "[4466,     1] loss: 0.0098313130\n",
      "[4467,     1] loss: 0.0098312519\n",
      "[4468,     1] loss: 0.0098311901\n",
      "[4469,     1] loss: 0.0098311275\n",
      "[4470,     1] loss: 0.0098310664\n",
      "[4471,     1] loss: 0.0098310031\n",
      "[4472,     1] loss: 0.0098309435\n",
      "[4473,     1] loss: 0.0098308794\n",
      "[4474,     1] loss: 0.0098308183\n",
      "[4475,     1] loss: 0.0098307565\n",
      "[4476,     1] loss: 0.0098306932\n",
      "[4477,     1] loss: 0.0098306328\n",
      "[4478,     1] loss: 0.0098305710\n",
      "[4479,     1] loss: 0.0098305076\n",
      "[4480,     1] loss: 0.0098304465\n",
      "[4481,     1] loss: 0.0098303847\n",
      "[4482,     1] loss: 0.0098303221\n",
      "[4483,     1] loss: 0.0098302610\n",
      "[4484,     1] loss: 0.0098301984\n",
      "[4485,     1] loss: 0.0098301381\n",
      "[4486,     1] loss: 0.0098300755\n",
      "[4487,     1] loss: 0.0098300129\n",
      "[4488,     1] loss: 0.0098299526\n",
      "[4489,     1] loss: 0.0098298900\n",
      "[4490,     1] loss: 0.0098298281\n",
      "[4491,     1] loss: 0.0098297656\n",
      "[4492,     1] loss: 0.0098297037\n",
      "[4493,     1] loss: 0.0098296411\n",
      "[4494,     1] loss: 0.0098295800\n",
      "[4495,     1] loss: 0.0098295189\n",
      "[4496,     1] loss: 0.0098294564\n",
      "[4497,     1] loss: 0.0098293945\n",
      "[4498,     1] loss: 0.0098293327\n",
      "[4499,     1] loss: 0.0098292716\n",
      "[4500,     1] loss: 0.0098292090\n",
      "[4501,     1] loss: 0.0098291464\n",
      "[4502,     1] loss: 0.0098290861\n",
      "[4503,     1] loss: 0.0098290235\n",
      "[4504,     1] loss: 0.0098289616\n",
      "[4505,     1] loss: 0.0098288991\n",
      "[4506,     1] loss: 0.0098288380\n",
      "[4507,     1] loss: 0.0098287746\n",
      "[4508,     1] loss: 0.0098287128\n",
      "[4509,     1] loss: 0.0098286517\n",
      "[4510,     1] loss: 0.0098285899\n",
      "[4511,     1] loss: 0.0098285273\n",
      "[4512,     1] loss: 0.0098284662\n",
      "[4513,     1] loss: 0.0098284043\n",
      "[4514,     1] loss: 0.0098283418\n",
      "[4515,     1] loss: 0.0098282807\n",
      "[4516,     1] loss: 0.0098282188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4517,     1] loss: 0.0098281577\n",
      "[4518,     1] loss: 0.0098280959\n",
      "[4519,     1] loss: 0.0098280348\n",
      "[4520,     1] loss: 0.0098279729\n",
      "[4521,     1] loss: 0.0098279104\n",
      "[4522,     1] loss: 0.0098278493\n",
      "[4523,     1] loss: 0.0098277859\n",
      "[4524,     1] loss: 0.0098277241\n",
      "[4525,     1] loss: 0.0098276630\n",
      "[4526,     1] loss: 0.0098276012\n",
      "[4527,     1] loss: 0.0098275386\n",
      "[4528,     1] loss: 0.0098274775\n",
      "[4529,     1] loss: 0.0098274164\n",
      "[4530,     1] loss: 0.0098273538\n",
      "[4531,     1] loss: 0.0098272920\n",
      "[4532,     1] loss: 0.0098272301\n",
      "[4533,     1] loss: 0.0098271683\n",
      "[4534,     1] loss: 0.0098271072\n",
      "[4535,     1] loss: 0.0098270454\n",
      "[4536,     1] loss: 0.0098269828\n",
      "[4537,     1] loss: 0.0098269217\n",
      "[4538,     1] loss: 0.0098268598\n",
      "[4539,     1] loss: 0.0098267987\n",
      "[4540,     1] loss: 0.0098267362\n",
      "[4541,     1] loss: 0.0098266743\n",
      "[4542,     1] loss: 0.0098266125\n",
      "[4543,     1] loss: 0.0098265499\n",
      "[4544,     1] loss: 0.0098264888\n",
      "[4545,     1] loss: 0.0098264277\n",
      "[4546,     1] loss: 0.0098263651\n",
      "[4547,     1] loss: 0.0098263040\n",
      "[4548,     1] loss: 0.0098262422\n",
      "[4549,     1] loss: 0.0098261788\n",
      "[4550,     1] loss: 0.0098261185\n",
      "[4551,     1] loss: 0.0098260574\n",
      "[4552,     1] loss: 0.0098259941\n",
      "[4553,     1] loss: 0.0098259322\n",
      "[4554,     1] loss: 0.0098258719\n",
      "[4555,     1] loss: 0.0098258100\n",
      "[4556,     1] loss: 0.0098257467\n",
      "[4557,     1] loss: 0.0098256849\n",
      "[4558,     1] loss: 0.0098256245\n",
      "[4559,     1] loss: 0.0098255612\n",
      "[4560,     1] loss: 0.0098255001\n",
      "[4561,     1] loss: 0.0098254398\n",
      "[4562,     1] loss: 0.0098253764\n",
      "[4563,     1] loss: 0.0098253146\n",
      "[4564,     1] loss: 0.0098252535\n",
      "[4565,     1] loss: 0.0098251916\n",
      "[4566,     1] loss: 0.0098251306\n",
      "[4567,     1] loss: 0.0098250680\n",
      "[4568,     1] loss: 0.0098250069\n",
      "[4569,     1] loss: 0.0098249450\n",
      "[4570,     1] loss: 0.0098248839\n",
      "[4571,     1] loss: 0.0098248214\n",
      "[4572,     1] loss: 0.0098247595\n",
      "[4573,     1] loss: 0.0098246984\n",
      "[4574,     1] loss: 0.0098246358\n",
      "[4575,     1] loss: 0.0098245747\n",
      "[4576,     1] loss: 0.0098245129\n",
      "[4577,     1] loss: 0.0098244525\n",
      "[4578,     1] loss: 0.0098243907\n",
      "[4579,     1] loss: 0.0098243289\n",
      "[4580,     1] loss: 0.0098242670\n",
      "[4581,     1] loss: 0.0098242052\n",
      "[4582,     1] loss: 0.0098241426\n",
      "[4583,     1] loss: 0.0098240823\n",
      "[4584,     1] loss: 0.0098240204\n",
      "[4585,     1] loss: 0.0098239586\n",
      "[4586,     1] loss: 0.0098238967\n",
      "[4587,     1] loss: 0.0098238356\n",
      "[4588,     1] loss: 0.0098237731\n",
      "[4589,     1] loss: 0.0098237120\n",
      "[4590,     1] loss: 0.0098236516\n",
      "[4591,     1] loss: 0.0098235890\n",
      "[4592,     1] loss: 0.0098235279\n",
      "[4593,     1] loss: 0.0098234676\n",
      "[4594,     1] loss: 0.0098234043\n",
      "[4595,     1] loss: 0.0098233424\n",
      "[4596,     1] loss: 0.0098232813\n",
      "[4597,     1] loss: 0.0098232195\n",
      "[4598,     1] loss: 0.0098231584\n",
      "[4599,     1] loss: 0.0098230973\n",
      "[4600,     1] loss: 0.0098230340\n",
      "[4601,     1] loss: 0.0098229729\n",
      "[4602,     1] loss: 0.0098229110\n",
      "[4603,     1] loss: 0.0098228499\n",
      "[4604,     1] loss: 0.0098227873\n",
      "[4605,     1] loss: 0.0098227262\n",
      "[4606,     1] loss: 0.0098226652\n",
      "[4607,     1] loss: 0.0098226026\n",
      "[4608,     1] loss: 0.0098225415\n",
      "[4609,     1] loss: 0.0098224804\n",
      "[4610,     1] loss: 0.0098224185\n",
      "[4611,     1] loss: 0.0098223560\n",
      "[4612,     1] loss: 0.0098222956\n",
      "[4613,     1] loss: 0.0098222338\n",
      "[4614,     1] loss: 0.0098221712\n",
      "[4615,     1] loss: 0.0098221108\n",
      "[4616,     1] loss: 0.0098220497\n",
      "[4617,     1] loss: 0.0098219879\n",
      "[4618,     1] loss: 0.0098219261\n",
      "[4619,     1] loss: 0.0098218642\n",
      "[4620,     1] loss: 0.0098218031\n",
      "[4621,     1] loss: 0.0098217413\n",
      "[4622,     1] loss: 0.0098216787\n",
      "[4623,     1] loss: 0.0098216183\n",
      "[4624,     1] loss: 0.0098215558\n",
      "[4625,     1] loss: 0.0098214939\n",
      "[4626,     1] loss: 0.0098214328\n",
      "[4627,     1] loss: 0.0098213710\n",
      "[4628,     1] loss: 0.0098213091\n",
      "[4629,     1] loss: 0.0098212481\n",
      "[4630,     1] loss: 0.0098211855\n",
      "[4631,     1] loss: 0.0098211251\n",
      "[4632,     1] loss: 0.0098210648\n",
      "[4633,     1] loss: 0.0098210014\n",
      "[4634,     1] loss: 0.0098209418\n",
      "[4635,     1] loss: 0.0098208793\n",
      "[4636,     1] loss: 0.0098208182\n",
      "[4637,     1] loss: 0.0098207563\n",
      "[4638,     1] loss: 0.0098206952\n",
      "[4639,     1] loss: 0.0098206334\n",
      "[4640,     1] loss: 0.0098205715\n",
      "[4641,     1] loss: 0.0098205097\n",
      "[4642,     1] loss: 0.0098204479\n",
      "[4643,     1] loss: 0.0098203860\n",
      "[4644,     1] loss: 0.0098203249\n",
      "[4645,     1] loss: 0.0098202646\n",
      "[4646,     1] loss: 0.0098202020\n",
      "[4647,     1] loss: 0.0098201424\n",
      "[4648,     1] loss: 0.0098200798\n",
      "[4649,     1] loss: 0.0098200195\n",
      "[4650,     1] loss: 0.0098199576\n",
      "[4651,     1] loss: 0.0098198958\n",
      "[4652,     1] loss: 0.0098198339\n",
      "[4653,     1] loss: 0.0098197721\n",
      "[4654,     1] loss: 0.0098197110\n",
      "[4655,     1] loss: 0.0098196499\n",
      "[4656,     1] loss: 0.0098195881\n",
      "[4657,     1] loss: 0.0098195270\n",
      "[4658,     1] loss: 0.0098194651\n",
      "[4659,     1] loss: 0.0098194033\n",
      "[4660,     1] loss: 0.0098193415\n",
      "[4661,     1] loss: 0.0098192804\n",
      "[4662,     1] loss: 0.0098192200\n",
      "[4663,     1] loss: 0.0098191582\n",
      "[4664,     1] loss: 0.0098190971\n",
      "[4665,     1] loss: 0.0098190345\n",
      "[4666,     1] loss: 0.0098189749\n",
      "[4667,     1] loss: 0.0098189123\n",
      "[4668,     1] loss: 0.0098188512\n",
      "[4669,     1] loss: 0.0098187901\n",
      "[4670,     1] loss: 0.0098187290\n",
      "[4671,     1] loss: 0.0098186672\n",
      "[4672,     1] loss: 0.0098186053\n",
      "[4673,     1] loss: 0.0098185442\n",
      "[4674,     1] loss: 0.0098184831\n",
      "[4675,     1] loss: 0.0098184220\n",
      "[4676,     1] loss: 0.0098183610\n",
      "[4677,     1] loss: 0.0098182999\n",
      "[4678,     1] loss: 0.0098182373\n",
      "[4679,     1] loss: 0.0098181762\n",
      "[4680,     1] loss: 0.0098181143\n",
      "[4681,     1] loss: 0.0098180532\n",
      "[4682,     1] loss: 0.0098179922\n",
      "[4683,     1] loss: 0.0098179311\n",
      "[4684,     1] loss: 0.0098178685\n",
      "[4685,     1] loss: 0.0098178074\n",
      "[4686,     1] loss: 0.0098177455\n",
      "[4687,     1] loss: 0.0098176852\n",
      "[4688,     1] loss: 0.0098176233\n",
      "[4689,     1] loss: 0.0098175630\n",
      "[4690,     1] loss: 0.0098175012\n",
      "[4691,     1] loss: 0.0098174408\n",
      "[4692,     1] loss: 0.0098173790\n",
      "[4693,     1] loss: 0.0098173164\n",
      "[4694,     1] loss: 0.0098172568\n",
      "[4695,     1] loss: 0.0098171949\n",
      "[4696,     1] loss: 0.0098171331\n",
      "[4697,     1] loss: 0.0098170713\n",
      "[4698,     1] loss: 0.0098170087\n",
      "[4699,     1] loss: 0.0098169476\n",
      "[4700,     1] loss: 0.0098168872\n",
      "[4701,     1] loss: 0.0098168261\n",
      "[4702,     1] loss: 0.0098167650\n",
      "[4703,     1] loss: 0.0098167025\n",
      "[4704,     1] loss: 0.0098166421\n",
      "[4705,     1] loss: 0.0098165818\n",
      "[4706,     1] loss: 0.0098165184\n",
      "[4707,     1] loss: 0.0098164581\n",
      "[4708,     1] loss: 0.0098163962\n",
      "[4709,     1] loss: 0.0098163359\n",
      "[4710,     1] loss: 0.0098162740\n",
      "[4711,     1] loss: 0.0098162137\n",
      "[4712,     1] loss: 0.0098161504\n",
      "[4713,     1] loss: 0.0098160900\n",
      "[4714,     1] loss: 0.0098160274\n",
      "[4715,     1] loss: 0.0098159678\n",
      "[4716,     1] loss: 0.0098159067\n",
      "[4717,     1] loss: 0.0098158434\n",
      "[4718,     1] loss: 0.0098157823\n",
      "[4719,     1] loss: 0.0098157212\n",
      "[4720,     1] loss: 0.0098156601\n",
      "[4721,     1] loss: 0.0098155998\n",
      "[4722,     1] loss: 0.0098155387\n",
      "[4723,     1] loss: 0.0098154768\n",
      "[4724,     1] loss: 0.0098154157\n",
      "[4725,     1] loss: 0.0098153539\n",
      "[4726,     1] loss: 0.0098152928\n",
      "[4727,     1] loss: 0.0098152317\n",
      "[4728,     1] loss: 0.0098151699\n",
      "[4729,     1] loss: 0.0098151095\n",
      "[4730,     1] loss: 0.0098150469\n",
      "[4731,     1] loss: 0.0098149858\n",
      "[4732,     1] loss: 0.0098149247\n",
      "[4733,     1] loss: 0.0098148629\n",
      "[4734,     1] loss: 0.0098148018\n",
      "[4735,     1] loss: 0.0098147415\n",
      "[4736,     1] loss: 0.0098146796\n",
      "[4737,     1] loss: 0.0098146178\n",
      "[4738,     1] loss: 0.0098145567\n",
      "[4739,     1] loss: 0.0098144963\n",
      "[4740,     1] loss: 0.0098144345\n",
      "[4741,     1] loss: 0.0098143727\n",
      "[4742,     1] loss: 0.0098143123\n",
      "[4743,     1] loss: 0.0098142505\n",
      "[4744,     1] loss: 0.0098141894\n",
      "[4745,     1] loss: 0.0098141260\n",
      "[4746,     1] loss: 0.0098140664\n",
      "[4747,     1] loss: 0.0098140046\n",
      "[4748,     1] loss: 0.0098139435\n",
      "[4749,     1] loss: 0.0098138824\n",
      "[4750,     1] loss: 0.0098138213\n",
      "[4751,     1] loss: 0.0098137602\n",
      "[4752,     1] loss: 0.0098136991\n",
      "[4753,     1] loss: 0.0098136373\n",
      "[4754,     1] loss: 0.0098135769\n",
      "[4755,     1] loss: 0.0098135144\n",
      "[4756,     1] loss: 0.0098134533\n",
      "[4757,     1] loss: 0.0098133929\n",
      "[4758,     1] loss: 0.0098133311\n",
      "[4759,     1] loss: 0.0098132692\n",
      "[4760,     1] loss: 0.0098132081\n",
      "[4761,     1] loss: 0.0098131463\n",
      "[4762,     1] loss: 0.0098130859\n",
      "[4763,     1] loss: 0.0098130241\n",
      "[4764,     1] loss: 0.0098129638\n",
      "[4765,     1] loss: 0.0098129027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4766,     1] loss: 0.0098128408\n",
      "[4767,     1] loss: 0.0098127782\n",
      "[4768,     1] loss: 0.0098127171\n",
      "[4769,     1] loss: 0.0098126560\n",
      "[4770,     1] loss: 0.0098125957\n",
      "[4771,     1] loss: 0.0098125346\n",
      "[4772,     1] loss: 0.0098124720\n",
      "[4773,     1] loss: 0.0098124124\n",
      "[4774,     1] loss: 0.0098123513\n",
      "[4775,     1] loss: 0.0098122902\n",
      "[4776,     1] loss: 0.0098122269\n",
      "[4777,     1] loss: 0.0098121673\n",
      "[4778,     1] loss: 0.0098121047\n",
      "[4779,     1] loss: 0.0098120444\n",
      "[4780,     1] loss: 0.0098119847\n",
      "[4781,     1] loss: 0.0098119214\n",
      "[4782,     1] loss: 0.0098118611\n",
      "[4783,     1] loss: 0.0098118000\n",
      "[4784,     1] loss: 0.0098117374\n",
      "[4785,     1] loss: 0.0098116763\n",
      "[4786,     1] loss: 0.0098116152\n",
      "[4787,     1] loss: 0.0098115534\n",
      "[4788,     1] loss: 0.0098114923\n",
      "[4789,     1] loss: 0.0098114319\n",
      "[4790,     1] loss: 0.0098113701\n",
      "[4791,     1] loss: 0.0098113082\n",
      "[4792,     1] loss: 0.0098112471\n",
      "[4793,     1] loss: 0.0098111868\n",
      "[4794,     1] loss: 0.0098111242\n",
      "[4795,     1] loss: 0.0098110646\n",
      "[4796,     1] loss: 0.0098110028\n",
      "[4797,     1] loss: 0.0098109417\n",
      "[4798,     1] loss: 0.0098108791\n",
      "[4799,     1] loss: 0.0098108180\n",
      "[4800,     1] loss: 0.0098107576\n",
      "[4801,     1] loss: 0.0098106973\n",
      "[4802,     1] loss: 0.0098106347\n",
      "[4803,     1] loss: 0.0098105744\n",
      "[4804,     1] loss: 0.0098105125\n",
      "[4805,     1] loss: 0.0098104507\n",
      "[4806,     1] loss: 0.0098103903\n",
      "[4807,     1] loss: 0.0098103285\n",
      "[4808,     1] loss: 0.0098102674\n",
      "[4809,     1] loss: 0.0098102063\n",
      "[4810,     1] loss: 0.0098101452\n",
      "[4811,     1] loss: 0.0098100834\n",
      "[4812,     1] loss: 0.0098100215\n",
      "[4813,     1] loss: 0.0098099619\n",
      "[4814,     1] loss: 0.0098098993\n",
      "[4815,     1] loss: 0.0098098382\n",
      "[4816,     1] loss: 0.0098097757\n",
      "[4817,     1] loss: 0.0098097153\n",
      "[4818,     1] loss: 0.0098096542\n",
      "[4819,     1] loss: 0.0098095924\n",
      "[4820,     1] loss: 0.0098095313\n",
      "[4821,     1] loss: 0.0098094702\n",
      "[4822,     1] loss: 0.0098094091\n",
      "[4823,     1] loss: 0.0098093472\n",
      "[4824,     1] loss: 0.0098092869\n",
      "[4825,     1] loss: 0.0098092258\n",
      "[4826,     1] loss: 0.0098091632\n",
      "[4827,     1] loss: 0.0098091021\n",
      "[4828,     1] loss: 0.0098090410\n",
      "[4829,     1] loss: 0.0098089799\n",
      "[4830,     1] loss: 0.0098089181\n",
      "[4831,     1] loss: 0.0098088570\n",
      "[4832,     1] loss: 0.0098087959\n",
      "[4833,     1] loss: 0.0098087341\n",
      "[4834,     1] loss: 0.0098086722\n",
      "[4835,     1] loss: 0.0098086104\n",
      "[4836,     1] loss: 0.0098085515\n",
      "[4837,     1] loss: 0.0098084882\n",
      "[4838,     1] loss: 0.0098084278\n",
      "[4839,     1] loss: 0.0098083667\n",
      "[4840,     1] loss: 0.0098083064\n",
      "[4841,     1] loss: 0.0098082438\n",
      "[4842,     1] loss: 0.0098081827\n",
      "[4843,     1] loss: 0.0098081224\n",
      "[4844,     1] loss: 0.0098080598\n",
      "[4845,     1] loss: 0.0098079987\n",
      "[4846,     1] loss: 0.0098079368\n",
      "[4847,     1] loss: 0.0098078758\n",
      "[4848,     1] loss: 0.0098078147\n",
      "[4849,     1] loss: 0.0098077536\n",
      "[4850,     1] loss: 0.0098076932\n",
      "[4851,     1] loss: 0.0098076306\n",
      "[4852,     1] loss: 0.0098075695\n",
      "[4853,     1] loss: 0.0098075069\n",
      "[4854,     1] loss: 0.0098074466\n",
      "[4855,     1] loss: 0.0098073855\n",
      "[4856,     1] loss: 0.0098073237\n",
      "[4857,     1] loss: 0.0098072626\n",
      "[4858,     1] loss: 0.0098072000\n",
      "[4859,     1] loss: 0.0098071389\n",
      "[4860,     1] loss: 0.0098070771\n",
      "[4861,     1] loss: 0.0098070160\n",
      "[4862,     1] loss: 0.0098069556\n",
      "[4863,     1] loss: 0.0098068953\n",
      "[4864,     1] loss: 0.0098068312\n",
      "[4865,     1] loss: 0.0098067701\n",
      "[4866,     1] loss: 0.0098067090\n",
      "[4867,     1] loss: 0.0098066479\n",
      "[4868,     1] loss: 0.0098065861\n",
      "[4869,     1] loss: 0.0098065250\n",
      "[4870,     1] loss: 0.0098064639\n",
      "[4871,     1] loss: 0.0098064020\n",
      "[4872,     1] loss: 0.0098063402\n",
      "[4873,     1] loss: 0.0098062791\n",
      "[4874,     1] loss: 0.0098062173\n",
      "[4875,     1] loss: 0.0098061562\n",
      "[4876,     1] loss: 0.0098060936\n",
      "[4877,     1] loss: 0.0098060325\n",
      "[4878,     1] loss: 0.0098059721\n",
      "[4879,     1] loss: 0.0098059125\n",
      "[4880,     1] loss: 0.0098058484\n",
      "[4881,     1] loss: 0.0098057881\n",
      "[4882,     1] loss: 0.0098057278\n",
      "[4883,     1] loss: 0.0098056659\n",
      "[4884,     1] loss: 0.0098056041\n",
      "[4885,     1] loss: 0.0098055415\n",
      "[4886,     1] loss: 0.0098054811\n",
      "[4887,     1] loss: 0.0098054186\n",
      "[4888,     1] loss: 0.0098053567\n",
      "[4889,     1] loss: 0.0098052964\n",
      "[4890,     1] loss: 0.0098052353\n",
      "[4891,     1] loss: 0.0098051734\n",
      "[4892,     1] loss: 0.0098051123\n",
      "[4893,     1] loss: 0.0098050512\n",
      "[4894,     1] loss: 0.0098049887\n",
      "[4895,     1] loss: 0.0098049268\n",
      "[4896,     1] loss: 0.0098048650\n",
      "[4897,     1] loss: 0.0098048054\n",
      "[4898,     1] loss: 0.0098047428\n",
      "[4899,     1] loss: 0.0098046809\n",
      "[4900,     1] loss: 0.0098046206\n",
      "[4901,     1] loss: 0.0098045588\n",
      "[4902,     1] loss: 0.0098044969\n",
      "[4903,     1] loss: 0.0098044351\n",
      "[4904,     1] loss: 0.0098043747\n",
      "[4905,     1] loss: 0.0098043121\n",
      "[4906,     1] loss: 0.0098042503\n",
      "[4907,     1] loss: 0.0098041892\n",
      "[4908,     1] loss: 0.0098041281\n",
      "[4909,     1] loss: 0.0098040663\n",
      "[4910,     1] loss: 0.0098040044\n",
      "[4911,     1] loss: 0.0098039433\n",
      "[4912,     1] loss: 0.0098038808\n",
      "[4913,     1] loss: 0.0098038189\n",
      "[4914,     1] loss: 0.0098037578\n",
      "[4915,     1] loss: 0.0098036960\n",
      "[4916,     1] loss: 0.0098036356\n",
      "[4917,     1] loss: 0.0098035738\n",
      "[4918,     1] loss: 0.0098035127\n",
      "[4919,     1] loss: 0.0098034501\n",
      "[4920,     1] loss: 0.0098033890\n",
      "[4921,     1] loss: 0.0098033272\n",
      "[4922,     1] loss: 0.0098032661\n",
      "[4923,     1] loss: 0.0098032035\n",
      "[4924,     1] loss: 0.0098031424\n",
      "[4925,     1] loss: 0.0098030813\n",
      "[4926,     1] loss: 0.0098030202\n",
      "[4927,     1] loss: 0.0098029576\n",
      "[4928,     1] loss: 0.0098028973\n",
      "[4929,     1] loss: 0.0098028339\n",
      "[4930,     1] loss: 0.0098027728\n",
      "[4931,     1] loss: 0.0098027110\n",
      "[4932,     1] loss: 0.0098026477\n",
      "[4933,     1] loss: 0.0098025866\n",
      "[4934,     1] loss: 0.0098025255\n",
      "[4935,     1] loss: 0.0098024644\n",
      "[4936,     1] loss: 0.0098024026\n",
      "[4937,     1] loss: 0.0098023400\n",
      "[4938,     1] loss: 0.0098022796\n",
      "[4939,     1] loss: 0.0098022185\n",
      "[4940,     1] loss: 0.0098021567\n",
      "[4941,     1] loss: 0.0098020941\n",
      "[4942,     1] loss: 0.0098020338\n",
      "[4943,     1] loss: 0.0098019712\n",
      "[4944,     1] loss: 0.0098019101\n",
      "[4945,     1] loss: 0.0098018475\n",
      "[4946,     1] loss: 0.0098017856\n",
      "[4947,     1] loss: 0.0098017238\n",
      "[4948,     1] loss: 0.0098016627\n",
      "[4949,     1] loss: 0.0098016016\n",
      "[4950,     1] loss: 0.0098015383\n",
      "[4951,     1] loss: 0.0098014764\n",
      "[4952,     1] loss: 0.0098014161\n",
      "[4953,     1] loss: 0.0098013550\n",
      "[4954,     1] loss: 0.0098012917\n",
      "[4955,     1] loss: 0.0098012298\n",
      "[4956,     1] loss: 0.0098011687\n",
      "[4957,     1] loss: 0.0098011054\n",
      "[4958,     1] loss: 0.0098010443\n",
      "[4959,     1] loss: 0.0098009825\n",
      "[4960,     1] loss: 0.0098009214\n",
      "[4961,     1] loss: 0.0098008595\n",
      "[4962,     1] loss: 0.0098007977\n",
      "[4963,     1] loss: 0.0098007359\n",
      "[4964,     1] loss: 0.0098006740\n",
      "[4965,     1] loss: 0.0098006129\n",
      "[4966,     1] loss: 0.0098005511\n",
      "[4967,     1] loss: 0.0098004885\n",
      "[4968,     1] loss: 0.0098004267\n",
      "[4969,     1] loss: 0.0098003648\n",
      "[4970,     1] loss: 0.0098003022\n",
      "[4971,     1] loss: 0.0098002411\n",
      "[4972,     1] loss: 0.0098001808\n",
      "[4973,     1] loss: 0.0098001175\n",
      "[4974,     1] loss: 0.0098000556\n",
      "[4975,     1] loss: 0.0097999938\n",
      "[4976,     1] loss: 0.0097999319\n",
      "[4977,     1] loss: 0.0097998708\n",
      "[4978,     1] loss: 0.0097998083\n",
      "[4979,     1] loss: 0.0097997457\n",
      "[4980,     1] loss: 0.0097996831\n",
      "[4981,     1] loss: 0.0097996227\n",
      "[4982,     1] loss: 0.0097995609\n",
      "[4983,     1] loss: 0.0097994976\n",
      "[4984,     1] loss: 0.0097994372\n",
      "[4985,     1] loss: 0.0097993739\n",
      "[4986,     1] loss: 0.0097993128\n",
      "[4987,     1] loss: 0.0097992510\n",
      "[4988,     1] loss: 0.0097991876\n",
      "[4989,     1] loss: 0.0097991265\n",
      "[4990,     1] loss: 0.0097990640\n",
      "[4991,     1] loss: 0.0097990021\n",
      "[4992,     1] loss: 0.0097989410\n",
      "[4993,     1] loss: 0.0097988777\n",
      "[4994,     1] loss: 0.0097988166\n",
      "[4995,     1] loss: 0.0097987548\n",
      "[4996,     1] loss: 0.0097986944\n",
      "[4997,     1] loss: 0.0097986311\n",
      "[4998,     1] loss: 0.0097985685\n",
      "[4999,     1] loss: 0.0097985074\n",
      "[5000,     1] loss: 0.0097984448\n",
      "[5001,     1] loss: 0.0097983822\n",
      "[5002,     1] loss: 0.0097983211\n",
      "[5003,     1] loss: 0.0097982585\n",
      "[5004,     1] loss: 0.0097981960\n",
      "[5005,     1] loss: 0.0097981341\n",
      "[5006,     1] loss: 0.0097980715\n",
      "[5007,     1] loss: 0.0097980097\n",
      "[5008,     1] loss: 0.0097979479\n",
      "[5009,     1] loss: 0.0097978860\n",
      "[5010,     1] loss: 0.0097978234\n",
      "[5011,     1] loss: 0.0097977616\n",
      "[5012,     1] loss: 0.0097977005\n",
      "[5013,     1] loss: 0.0097976364\n",
      "[5014,     1] loss: 0.0097975753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5015,     1] loss: 0.0097975135\n",
      "[5016,     1] loss: 0.0097974502\n",
      "[5017,     1] loss: 0.0097973876\n",
      "[5018,     1] loss: 0.0097973250\n",
      "[5019,     1] loss: 0.0097972646\n",
      "[5020,     1] loss: 0.0097972028\n",
      "[5021,     1] loss: 0.0097971395\n",
      "[5022,     1] loss: 0.0097970776\n",
      "[5023,     1] loss: 0.0097970158\n",
      "[5024,     1] loss: 0.0097969525\n",
      "[5025,     1] loss: 0.0097968906\n",
      "[5026,     1] loss: 0.0097968295\n",
      "[5027,     1] loss: 0.0097967654\n",
      "[5028,     1] loss: 0.0097967036\n",
      "[5029,     1] loss: 0.0097966418\n",
      "[5030,     1] loss: 0.0097965799\n",
      "[5031,     1] loss: 0.0097965173\n",
      "[5032,     1] loss: 0.0097964555\n",
      "[5033,     1] loss: 0.0097963922\n",
      "[5034,     1] loss: 0.0097963303\n",
      "[5035,     1] loss: 0.0097962670\n",
      "[5036,     1] loss: 0.0097962052\n",
      "[5037,     1] loss: 0.0097961441\n",
      "[5038,     1] loss: 0.0097960815\n",
      "[5039,     1] loss: 0.0097960182\n",
      "[5040,     1] loss: 0.0097959556\n",
      "[5041,     1] loss: 0.0097958930\n",
      "[5042,     1] loss: 0.0097958311\n",
      "[5043,     1] loss: 0.0097957693\n",
      "[5044,     1] loss: 0.0097957060\n",
      "[5045,     1] loss: 0.0097956441\n",
      "[5046,     1] loss: 0.0097955815\n",
      "[5047,     1] loss: 0.0097955197\n",
      "[5048,     1] loss: 0.0097954564\n",
      "[5049,     1] loss: 0.0097953945\n",
      "[5050,     1] loss: 0.0097953305\n",
      "[5051,     1] loss: 0.0097952694\n",
      "[5052,     1] loss: 0.0097952075\n",
      "[5053,     1] loss: 0.0097951442\n",
      "[5054,     1] loss: 0.0097950824\n",
      "[5055,     1] loss: 0.0097950198\n",
      "[5056,     1] loss: 0.0097949572\n",
      "[5057,     1] loss: 0.0097948939\n",
      "[5058,     1] loss: 0.0097948320\n",
      "[5059,     1] loss: 0.0097947687\n",
      "[5060,     1] loss: 0.0097947083\n",
      "[5061,     1] loss: 0.0097946458\n",
      "[5062,     1] loss: 0.0097945809\n",
      "[5063,     1] loss: 0.0097945198\n",
      "[5064,     1] loss: 0.0097944573\n",
      "[5065,     1] loss: 0.0097943939\n",
      "[5066,     1] loss: 0.0097943313\n",
      "[5067,     1] loss: 0.0097942688\n",
      "[5068,     1] loss: 0.0097942054\n",
      "[5069,     1] loss: 0.0097941436\n",
      "[5070,     1] loss: 0.0097940803\n",
      "[5071,     1] loss: 0.0097940192\n",
      "[5072,     1] loss: 0.0097939558\n",
      "[5073,     1] loss: 0.0097938932\n",
      "[5074,     1] loss: 0.0097938307\n",
      "[5075,     1] loss: 0.0097937673\n",
      "[5076,     1] loss: 0.0097937047\n",
      "[5077,     1] loss: 0.0097936437\n",
      "[5078,     1] loss: 0.0097935788\n",
      "[5079,     1] loss: 0.0097935177\n",
      "[5080,     1] loss: 0.0097934544\n",
      "[5081,     1] loss: 0.0097933926\n",
      "[5082,     1] loss: 0.0097933285\n",
      "[5083,     1] loss: 0.0097932667\n",
      "[5084,     1] loss: 0.0097932041\n",
      "[5085,     1] loss: 0.0097931407\n",
      "[5086,     1] loss: 0.0097930782\n",
      "[5087,     1] loss: 0.0097930148\n",
      "[5088,     1] loss: 0.0097929515\n",
      "[5089,     1] loss: 0.0097928897\n",
      "[5090,     1] loss: 0.0097928271\n",
      "[5091,     1] loss: 0.0097927645\n",
      "[5092,     1] loss: 0.0097927004\n",
      "[5093,     1] loss: 0.0097926386\n",
      "[5094,     1] loss: 0.0097925760\n",
      "[5095,     1] loss: 0.0097925119\n",
      "[5096,     1] loss: 0.0097924516\n",
      "[5097,     1] loss: 0.0097923875\n",
      "[5098,     1] loss: 0.0097923242\n",
      "[5099,     1] loss: 0.0097922616\n",
      "[5100,     1] loss: 0.0097921990\n",
      "[5101,     1] loss: 0.0097921357\n",
      "[5102,     1] loss: 0.0097920738\n",
      "[5103,     1] loss: 0.0097920097\n",
      "[5104,     1] loss: 0.0097919472\n",
      "[5105,     1] loss: 0.0097918838\n",
      "[5106,     1] loss: 0.0097918220\n",
      "[5107,     1] loss: 0.0097917579\n",
      "[5108,     1] loss: 0.0097916953\n",
      "[5109,     1] loss: 0.0097916320\n",
      "[5110,     1] loss: 0.0097915709\n",
      "[5111,     1] loss: 0.0097915061\n",
      "[5112,     1] loss: 0.0097914428\n",
      "[5113,     1] loss: 0.0097913809\n",
      "[5114,     1] loss: 0.0097913176\n",
      "[5115,     1] loss: 0.0097912543\n",
      "[5116,     1] loss: 0.0097911909\n",
      "[5117,     1] loss: 0.0097911283\n",
      "[5118,     1] loss: 0.0097910643\n",
      "[5119,     1] loss: 0.0097910017\n",
      "[5120,     1] loss: 0.0097909398\n",
      "[5121,     1] loss: 0.0097908758\n",
      "[5122,     1] loss: 0.0097908132\n",
      "[5123,     1] loss: 0.0097907476\n",
      "[5124,     1] loss: 0.0097906858\n",
      "[5125,     1] loss: 0.0097906232\n",
      "[5126,     1] loss: 0.0097905613\n",
      "[5127,     1] loss: 0.0097904965\n",
      "[5128,     1] loss: 0.0097904332\n",
      "[5129,     1] loss: 0.0097903706\n",
      "[5130,     1] loss: 0.0097903080\n",
      "[5131,     1] loss: 0.0097902447\n",
      "[5132,     1] loss: 0.0097901814\n",
      "[5133,     1] loss: 0.0097901173\n",
      "[5134,     1] loss: 0.0097900547\n",
      "[5135,     1] loss: 0.0097899921\n",
      "[5136,     1] loss: 0.0097899280\n",
      "[5137,     1] loss: 0.0097898647\n",
      "[5138,     1] loss: 0.0097898029\n",
      "[5139,     1] loss: 0.0097897388\n",
      "[5140,     1] loss: 0.0097896755\n",
      "[5141,     1] loss: 0.0097896121\n",
      "[5142,     1] loss: 0.0097895481\n",
      "[5143,     1] loss: 0.0097894862\n",
      "[5144,     1] loss: 0.0097894222\n",
      "[5145,     1] loss: 0.0097893596\n",
      "[5146,     1] loss: 0.0097892955\n",
      "[5147,     1] loss: 0.0097892322\n",
      "[5148,     1] loss: 0.0097891688\n",
      "[5149,     1] loss: 0.0097891055\n",
      "[5150,     1] loss: 0.0097890429\n",
      "[5151,     1] loss: 0.0097889788\n",
      "[5152,     1] loss: 0.0097889155\n",
      "[5153,     1] loss: 0.0097888522\n",
      "[5154,     1] loss: 0.0097887896\n",
      "[5155,     1] loss: 0.0097887255\n",
      "[5156,     1] loss: 0.0097886629\n",
      "[5157,     1] loss: 0.0097885981\n",
      "[5158,     1] loss: 0.0097885355\n",
      "[5159,     1] loss: 0.0097884715\n",
      "[5160,     1] loss: 0.0097884074\n",
      "[5161,     1] loss: 0.0097883441\n",
      "[5162,     1] loss: 0.0097882807\n",
      "[5163,     1] loss: 0.0097882167\n",
      "[5164,     1] loss: 0.0097881548\n",
      "[5165,     1] loss: 0.0097880907\n",
      "[5166,     1] loss: 0.0097880274\n",
      "[5167,     1] loss: 0.0097879626\n",
      "[5168,     1] loss: 0.0097879000\n",
      "[5169,     1] loss: 0.0097878359\n",
      "[5170,     1] loss: 0.0097877726\n",
      "[5171,     1] loss: 0.0097877085\n",
      "[5172,     1] loss: 0.0097876459\n",
      "[5173,     1] loss: 0.0097875811\n",
      "[5174,     1] loss: 0.0097875185\n",
      "[5175,     1] loss: 0.0097874559\n",
      "[5176,     1] loss: 0.0097873911\n",
      "[5177,     1] loss: 0.0097873278\n",
      "[5178,     1] loss: 0.0097872630\n",
      "[5179,     1] loss: 0.0097871996\n",
      "[5180,     1] loss: 0.0097871371\n",
      "[5181,     1] loss: 0.0097870722\n",
      "[5182,     1] loss: 0.0097870089\n",
      "[5183,     1] loss: 0.0097869463\n",
      "[5184,     1] loss: 0.0097868823\n",
      "[5185,     1] loss: 0.0097868182\n",
      "[5186,     1] loss: 0.0097867548\n",
      "[5187,     1] loss: 0.0097866908\n",
      "[5188,     1] loss: 0.0097866282\n",
      "[5189,     1] loss: 0.0097865634\n",
      "[5190,     1] loss: 0.0097864993\n",
      "[5191,     1] loss: 0.0097864360\n",
      "[5192,     1] loss: 0.0097863711\n",
      "[5193,     1] loss: 0.0097863086\n",
      "[5194,     1] loss: 0.0097862452\n",
      "[5195,     1] loss: 0.0097861812\n",
      "[5196,     1] loss: 0.0097861178\n",
      "[5197,     1] loss: 0.0097860530\n",
      "[5198,     1] loss: 0.0097859897\n",
      "[5199,     1] loss: 0.0097859256\n",
      "[5200,     1] loss: 0.0097858623\n",
      "[5201,     1] loss: 0.0097857989\n",
      "[5202,     1] loss: 0.0097857334\n",
      "[5203,     1] loss: 0.0097856700\n",
      "[5204,     1] loss: 0.0097856067\n",
      "[5205,     1] loss: 0.0097855419\n",
      "[5206,     1] loss: 0.0097854778\n",
      "[5207,     1] loss: 0.0097854145\n",
      "[5208,     1] loss: 0.0097853504\n",
      "[5209,     1] loss: 0.0097852871\n",
      "[5210,     1] loss: 0.0097852230\n",
      "[5211,     1] loss: 0.0097851589\n",
      "[5212,     1] loss: 0.0097850949\n",
      "[5213,     1] loss: 0.0097850308\n",
      "[5214,     1] loss: 0.0097849682\n",
      "[5215,     1] loss: 0.0097849041\n",
      "[5216,     1] loss: 0.0097848393\n",
      "[5217,     1] loss: 0.0097847760\n",
      "[5218,     1] loss: 0.0097847112\n",
      "[5219,     1] loss: 0.0097846471\n",
      "[5220,     1] loss: 0.0097845837\n",
      "[5221,     1] loss: 0.0097845197\n",
      "[5222,     1] loss: 0.0097844549\n",
      "[5223,     1] loss: 0.0097843915\n",
      "[5224,     1] loss: 0.0097843274\n",
      "[5225,     1] loss: 0.0097842634\n",
      "[5226,     1] loss: 0.0097841986\n",
      "[5227,     1] loss: 0.0097841352\n",
      "[5228,     1] loss: 0.0097840719\n",
      "[5229,     1] loss: 0.0097840078\n",
      "[5230,     1] loss: 0.0097839423\n",
      "[5231,     1] loss: 0.0097838782\n",
      "[5232,     1] loss: 0.0097838148\n",
      "[5233,     1] loss: 0.0097837508\n",
      "[5234,     1] loss: 0.0097836860\n",
      "[5235,     1] loss: 0.0097836226\n",
      "[5236,     1] loss: 0.0097835578\n",
      "[5237,     1] loss: 0.0097834937\n",
      "[5238,     1] loss: 0.0097834289\n",
      "[5239,     1] loss: 0.0097833656\n",
      "[5240,     1] loss: 0.0097833008\n",
      "[5241,     1] loss: 0.0097832374\n",
      "[5242,     1] loss: 0.0097831734\n",
      "[5243,     1] loss: 0.0097831100\n",
      "[5244,     1] loss: 0.0097830445\n",
      "[5245,     1] loss: 0.0097829796\n",
      "[5246,     1] loss: 0.0097829163\n",
      "[5247,     1] loss: 0.0097828537\n",
      "[5248,     1] loss: 0.0097827882\n",
      "[5249,     1] loss: 0.0097827233\n",
      "[5250,     1] loss: 0.0097826593\n",
      "[5251,     1] loss: 0.0097825952\n",
      "[5252,     1] loss: 0.0097825296\n",
      "[5253,     1] loss: 0.0097824663\n",
      "[5254,     1] loss: 0.0097824030\n",
      "[5255,     1] loss: 0.0097823381\n",
      "[5256,     1] loss: 0.0097822733\n",
      "[5257,     1] loss: 0.0097822100\n",
      "[5258,     1] loss: 0.0097821459\n",
      "[5259,     1] loss: 0.0097820811\n",
      "[5260,     1] loss: 0.0097820163\n",
      "[5261,     1] loss: 0.0097819522\n",
      "[5262,     1] loss: 0.0097818889\n",
      "[5263,     1] loss: 0.0097818241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5264,     1] loss: 0.0097817592\n",
      "[5265,     1] loss: 0.0097816952\n",
      "[5266,     1] loss: 0.0097816318\n",
      "[5267,     1] loss: 0.0097815663\n",
      "[5268,     1] loss: 0.0097815022\n",
      "[5269,     1] loss: 0.0097814381\n",
      "[5270,     1] loss: 0.0097813733\n",
      "[5271,     1] loss: 0.0097813100\n",
      "[5272,     1] loss: 0.0097812444\n",
      "[5273,     1] loss: 0.0097811811\n",
      "[5274,     1] loss: 0.0097811148\n",
      "[5275,     1] loss: 0.0097810507\n",
      "[5276,     1] loss: 0.0097809874\n",
      "[5277,     1] loss: 0.0097809225\n",
      "[5278,     1] loss: 0.0097808592\n",
      "[5279,     1] loss: 0.0097807936\n",
      "[5280,     1] loss: 0.0097807296\n",
      "[5281,     1] loss: 0.0097806647\n",
      "[5282,     1] loss: 0.0097806014\n",
      "[5283,     1] loss: 0.0097805366\n",
      "[5284,     1] loss: 0.0097804710\n",
      "[5285,     1] loss: 0.0097804077\n",
      "[5286,     1] loss: 0.0097803444\n",
      "[5287,     1] loss: 0.0097802795\n",
      "[5288,     1] loss: 0.0097802147\n",
      "[5289,     1] loss: 0.0097801492\n",
      "[5290,     1] loss: 0.0097800843\n",
      "[5291,     1] loss: 0.0097800188\n",
      "[5292,     1] loss: 0.0097799562\n",
      "[5293,     1] loss: 0.0097798914\n",
      "[5294,     1] loss: 0.0097798273\n",
      "[5295,     1] loss: 0.0097797632\n",
      "[5296,     1] loss: 0.0097796984\n",
      "[5297,     1] loss: 0.0097796328\n",
      "[5298,     1] loss: 0.0097795688\n",
      "[5299,     1] loss: 0.0097795032\n",
      "[5300,     1] loss: 0.0097794406\n",
      "[5301,     1] loss: 0.0097793743\n",
      "[5302,     1] loss: 0.0097793110\n",
      "[5303,     1] loss: 0.0097792462\n",
      "[5304,     1] loss: 0.0097791821\n",
      "[5305,     1] loss: 0.0097791173\n",
      "[5306,     1] loss: 0.0097790517\n",
      "[5307,     1] loss: 0.0097789876\n",
      "[5308,     1] loss: 0.0097789228\n",
      "[5309,     1] loss: 0.0097788587\n",
      "[5310,     1] loss: 0.0097787932\n",
      "[5311,     1] loss: 0.0097787291\n",
      "[5312,     1] loss: 0.0097786650\n",
      "[5313,     1] loss: 0.0097786009\n",
      "[5314,     1] loss: 0.0097785354\n",
      "[5315,     1] loss: 0.0097784713\n",
      "[5316,     1] loss: 0.0097784065\n",
      "[5317,     1] loss: 0.0097783417\n",
      "[5318,     1] loss: 0.0097782768\n",
      "[5319,     1] loss: 0.0097782113\n",
      "[5320,     1] loss: 0.0097781472\n",
      "[5321,     1] loss: 0.0097780824\n",
      "[5322,     1] loss: 0.0097780168\n",
      "[5323,     1] loss: 0.0097779520\n",
      "[5324,     1] loss: 0.0097778879\n",
      "[5325,     1] loss: 0.0097778246\n",
      "[5326,     1] loss: 0.0097777590\n",
      "[5327,     1] loss: 0.0097776957\n",
      "[5328,     1] loss: 0.0097776301\n",
      "[5329,     1] loss: 0.0097775653\n",
      "[5330,     1] loss: 0.0097775005\n",
      "[5331,     1] loss: 0.0097774357\n",
      "[5332,     1] loss: 0.0097773708\n",
      "[5333,     1] loss: 0.0097773075\n",
      "[5334,     1] loss: 0.0097772427\n",
      "[5335,     1] loss: 0.0097771794\n",
      "[5336,     1] loss: 0.0097771131\n",
      "[5337,     1] loss: 0.0097770490\n",
      "[5338,     1] loss: 0.0097769842\n",
      "[5339,     1] loss: 0.0097769178\n",
      "[5340,     1] loss: 0.0097768545\n",
      "[5341,     1] loss: 0.0097767897\n",
      "[5342,     1] loss: 0.0097767249\n",
      "[5343,     1] loss: 0.0097766593\n",
      "[5344,     1] loss: 0.0097765952\n",
      "[5345,     1] loss: 0.0097765304\n",
      "[5346,     1] loss: 0.0097764656\n",
      "[5347,     1] loss: 0.0097764000\n",
      "[5348,     1] loss: 0.0097763367\n",
      "[5349,     1] loss: 0.0097762719\n",
      "[5350,     1] loss: 0.0097762063\n",
      "[5351,     1] loss: 0.0097761407\n",
      "[5352,     1] loss: 0.0097760782\n",
      "[5353,     1] loss: 0.0097760133\n",
      "[5354,     1] loss: 0.0097759485\n",
      "[5355,     1] loss: 0.0097758837\n",
      "[5356,     1] loss: 0.0097758189\n",
      "[5357,     1] loss: 0.0097757541\n",
      "[5358,     1] loss: 0.0097756892\n",
      "[5359,     1] loss: 0.0097756252\n",
      "[5360,     1] loss: 0.0097755596\n",
      "[5361,     1] loss: 0.0097754948\n",
      "[5362,     1] loss: 0.0097754307\n",
      "[5363,     1] loss: 0.0097753644\n",
      "[5364,     1] loss: 0.0097753011\n",
      "[5365,     1] loss: 0.0097752362\n",
      "[5366,     1] loss: 0.0097751707\n",
      "[5367,     1] loss: 0.0097751059\n",
      "[5368,     1] loss: 0.0097750410\n",
      "[5369,     1] loss: 0.0097749770\n",
      "[5370,     1] loss: 0.0097749114\n",
      "[5371,     1] loss: 0.0097748473\n",
      "[5372,     1] loss: 0.0097747825\n",
      "[5373,     1] loss: 0.0097747177\n",
      "[5374,     1] loss: 0.0097746521\n",
      "[5375,     1] loss: 0.0097745873\n",
      "[5376,     1] loss: 0.0097745232\n",
      "[5377,     1] loss: 0.0097744577\n",
      "[5378,     1] loss: 0.0097743936\n",
      "[5379,     1] loss: 0.0097743288\n",
      "[5380,     1] loss: 0.0097742647\n",
      "[5381,     1] loss: 0.0097741991\n",
      "[5382,     1] loss: 0.0097741351\n",
      "[5383,     1] loss: 0.0097740695\n",
      "[5384,     1] loss: 0.0097740062\n",
      "[5385,     1] loss: 0.0097739406\n",
      "[5386,     1] loss: 0.0097738765\n",
      "[5387,     1] loss: 0.0097738110\n",
      "[5388,     1] loss: 0.0097737476\n",
      "[5389,     1] loss: 0.0097736813\n",
      "[5390,     1] loss: 0.0097736165\n",
      "[5391,     1] loss: 0.0097735532\n",
      "[5392,     1] loss: 0.0097734876\n",
      "[5393,     1] loss: 0.0097734235\n",
      "[5394,     1] loss: 0.0097733587\n",
      "[5395,     1] loss: 0.0097732931\n",
      "[5396,     1] loss: 0.0097732276\n",
      "[5397,     1] loss: 0.0097731642\n",
      "[5398,     1] loss: 0.0097730994\n",
      "[5399,     1] loss: 0.0097730346\n",
      "[5400,     1] loss: 0.0097729698\n",
      "[5401,     1] loss: 0.0097729057\n",
      "[5402,     1] loss: 0.0097728401\n",
      "[5403,     1] loss: 0.0097727761\n",
      "[5404,     1] loss: 0.0097727105\n",
      "[5405,     1] loss: 0.0097726457\n",
      "[5406,     1] loss: 0.0097725809\n",
      "[5407,     1] loss: 0.0097725168\n",
      "[5408,     1] loss: 0.0097724505\n",
      "[5409,     1] loss: 0.0097723871\n",
      "[5410,     1] loss: 0.0097723223\n",
      "[5411,     1] loss: 0.0097722568\n",
      "[5412,     1] loss: 0.0097721934\n",
      "[5413,     1] loss: 0.0097721286\n",
      "[5414,     1] loss: 0.0097720645\n",
      "[5415,     1] loss: 0.0097719997\n",
      "[5416,     1] loss: 0.0097719342\n",
      "[5417,     1] loss: 0.0097718693\n",
      "[5418,     1] loss: 0.0097718045\n",
      "[5419,     1] loss: 0.0097717397\n",
      "[5420,     1] loss: 0.0097716749\n",
      "[5421,     1] loss: 0.0097716101\n",
      "[5422,     1] loss: 0.0097715452\n",
      "[5423,     1] loss: 0.0097714804\n",
      "[5424,     1] loss: 0.0097714156\n",
      "[5425,     1] loss: 0.0097713523\n",
      "[5426,     1] loss: 0.0097712874\n",
      "[5427,     1] loss: 0.0097712226\n",
      "[5428,     1] loss: 0.0097711578\n",
      "[5429,     1] loss: 0.0097710930\n",
      "[5430,     1] loss: 0.0097710282\n",
      "[5431,     1] loss: 0.0097709641\n",
      "[5432,     1] loss: 0.0097709000\n",
      "[5433,     1] loss: 0.0097708344\n",
      "[5434,     1] loss: 0.0097707689\n",
      "[5435,     1] loss: 0.0097707063\n",
      "[5436,     1] loss: 0.0097706400\n",
      "[5437,     1] loss: 0.0097705767\n",
      "[5438,     1] loss: 0.0097705111\n",
      "[5439,     1] loss: 0.0097704455\n",
      "[5440,     1] loss: 0.0097703822\n",
      "[5441,     1] loss: 0.0097703159\n",
      "[5442,     1] loss: 0.0097702511\n",
      "[5443,     1] loss: 0.0097701877\n",
      "[5444,     1] loss: 0.0097701237\n",
      "[5445,     1] loss: 0.0097700588\n",
      "[5446,     1] loss: 0.0097699940\n",
      "[5447,     1] loss: 0.0097699292\n",
      "[5448,     1] loss: 0.0097698659\n",
      "[5449,     1] loss: 0.0097698003\n",
      "[5450,     1] loss: 0.0097697362\n",
      "[5451,     1] loss: 0.0097696722\n",
      "[5452,     1] loss: 0.0097696073\n",
      "[5453,     1] loss: 0.0097695425\n",
      "[5454,     1] loss: 0.0097694777\n",
      "[5455,     1] loss: 0.0097694129\n",
      "[5456,     1] loss: 0.0097693481\n",
      "[5457,     1] loss: 0.0097692840\n",
      "[5458,     1] loss: 0.0097692192\n",
      "[5459,     1] loss: 0.0097691536\n",
      "[5460,     1] loss: 0.0097690895\n",
      "[5461,     1] loss: 0.0097690254\n",
      "[5462,     1] loss: 0.0097689614\n",
      "[5463,     1] loss: 0.0097688965\n",
      "[5464,     1] loss: 0.0097688332\n",
      "[5465,     1] loss: 0.0097687684\n",
      "[5466,     1] loss: 0.0097687013\n",
      "[5467,     1] loss: 0.0097686388\n",
      "[5468,     1] loss: 0.0097685739\n",
      "[5469,     1] loss: 0.0097685099\n",
      "[5470,     1] loss: 0.0097684458\n",
      "[5471,     1] loss: 0.0097683810\n",
      "[5472,     1] loss: 0.0097683169\n",
      "[5473,     1] loss: 0.0097682513\n",
      "[5474,     1] loss: 0.0097681873\n",
      "[5475,     1] loss: 0.0097681232\n",
      "[5476,     1] loss: 0.0097680584\n",
      "[5477,     1] loss: 0.0097679935\n",
      "[5478,     1] loss: 0.0097679287\n",
      "[5479,     1] loss: 0.0097678632\n",
      "[5480,     1] loss: 0.0097678006\n",
      "[5481,     1] loss: 0.0097677365\n",
      "[5482,     1] loss: 0.0097676717\n",
      "[5483,     1] loss: 0.0097676061\n",
      "[5484,     1] loss: 0.0097675435\n",
      "[5485,     1] loss: 0.0097674787\n",
      "[5486,     1] loss: 0.0097674139\n",
      "[5487,     1] loss: 0.0097673491\n",
      "[5488,     1] loss: 0.0097672857\n",
      "[5489,     1] loss: 0.0097672202\n",
      "[5490,     1] loss: 0.0097671561\n",
      "[5491,     1] loss: 0.0097670913\n",
      "[5492,     1] loss: 0.0097670272\n",
      "[5493,     1] loss: 0.0097669631\n",
      "[5494,     1] loss: 0.0097668983\n",
      "[5495,     1] loss: 0.0097668335\n",
      "[5496,     1] loss: 0.0097667702\n",
      "[5497,     1] loss: 0.0097667068\n",
      "[5498,     1] loss: 0.0097666413\n",
      "[5499,     1] loss: 0.0097665772\n",
      "[5500,     1] loss: 0.0097665131\n",
      "[5501,     1] loss: 0.0097664498\n",
      "[5502,     1] loss: 0.0097663850\n",
      "[5503,     1] loss: 0.0097663209\n",
      "[5504,     1] loss: 0.0097662561\n",
      "[5505,     1] loss: 0.0097661905\n",
      "[5506,     1] loss: 0.0097661257\n",
      "[5507,     1] loss: 0.0097660638\n",
      "[5508,     1] loss: 0.0097659975\n",
      "[5509,     1] loss: 0.0097659335\n",
      "[5510,     1] loss: 0.0097658709\n",
      "[5511,     1] loss: 0.0097658068\n",
      "[5512,     1] loss: 0.0097657435\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5513,     1] loss: 0.0097656786\n",
      "[5514,     1] loss: 0.0097656138\n",
      "[5515,     1] loss: 0.0097655505\n",
      "[5516,     1] loss: 0.0097654864\n",
      "[5517,     1] loss: 0.0097654216\n",
      "[5518,     1] loss: 0.0097653583\n",
      "[5519,     1] loss: 0.0097652934\n",
      "[5520,     1] loss: 0.0097652286\n",
      "[5521,     1] loss: 0.0097651646\n",
      "[5522,     1] loss: 0.0097650997\n",
      "[5523,     1] loss: 0.0097650371\n",
      "[5524,     1] loss: 0.0097649731\n",
      "[5525,     1] loss: 0.0097649083\n",
      "[5526,     1] loss: 0.0097648434\n",
      "[5527,     1] loss: 0.0097647794\n",
      "[5528,     1] loss: 0.0097647160\n",
      "[5529,     1] loss: 0.0097646512\n",
      "[5530,     1] loss: 0.0097645879\n",
      "[5531,     1] loss: 0.0097645238\n",
      "[5532,     1] loss: 0.0097644605\n",
      "[5533,     1] loss: 0.0097643957\n",
      "[5534,     1] loss: 0.0097643308\n",
      "[5535,     1] loss: 0.0097642675\n",
      "[5536,     1] loss: 0.0097642027\n",
      "[5537,     1] loss: 0.0097641386\n",
      "[5538,     1] loss: 0.0097640753\n",
      "[5539,     1] loss: 0.0097640119\n",
      "[5540,     1] loss: 0.0097639479\n",
      "[5541,     1] loss: 0.0097638838\n",
      "[5542,     1] loss: 0.0097638197\n",
      "[5543,     1] loss: 0.0097637549\n",
      "[5544,     1] loss: 0.0097636916\n",
      "[5545,     1] loss: 0.0097636268\n",
      "[5546,     1] loss: 0.0097635634\n",
      "[5547,     1] loss: 0.0097634986\n",
      "[5548,     1] loss: 0.0097634368\n",
      "[5549,     1] loss: 0.0097633712\n",
      "[5550,     1] loss: 0.0097633079\n",
      "[5551,     1] loss: 0.0097632445\n",
      "[5552,     1] loss: 0.0097631812\n",
      "[5553,     1] loss: 0.0097631164\n",
      "[5554,     1] loss: 0.0097630538\n",
      "[5555,     1] loss: 0.0097629882\n",
      "[5556,     1] loss: 0.0097629249\n",
      "[5557,     1] loss: 0.0097628608\n",
      "[5558,     1] loss: 0.0097627968\n",
      "[5559,     1] loss: 0.0097627334\n",
      "[5560,     1] loss: 0.0097626686\n",
      "[5561,     1] loss: 0.0097626053\n",
      "[5562,     1] loss: 0.0097625412\n",
      "[5563,     1] loss: 0.0097624779\n",
      "[5564,     1] loss: 0.0097624138\n",
      "[5565,     1] loss: 0.0097623497\n",
      "[5566,     1] loss: 0.0097622856\n",
      "[5567,     1] loss: 0.0097622231\n",
      "[5568,     1] loss: 0.0097621590\n",
      "[5569,     1] loss: 0.0097620949\n",
      "[5570,     1] loss: 0.0097620301\n",
      "[5571,     1] loss: 0.0097619668\n",
      "[5572,     1] loss: 0.0097619034\n",
      "[5573,     1] loss: 0.0097618401\n",
      "[5574,     1] loss: 0.0097617775\n",
      "[5575,     1] loss: 0.0097617127\n",
      "[5576,     1] loss: 0.0097616494\n",
      "[5577,     1] loss: 0.0097615853\n",
      "[5578,     1] loss: 0.0097615220\n",
      "[5579,     1] loss: 0.0097614586\n",
      "[5580,     1] loss: 0.0097613938\n",
      "[5581,     1] loss: 0.0097613312\n",
      "[5582,     1] loss: 0.0097612679\n",
      "[5583,     1] loss: 0.0097612038\n",
      "[5584,     1] loss: 0.0097611398\n",
      "[5585,     1] loss: 0.0097610764\n",
      "[5586,     1] loss: 0.0097610123\n",
      "[5587,     1] loss: 0.0097609498\n",
      "[5588,     1] loss: 0.0097608857\n",
      "[5589,     1] loss: 0.0097608231\n",
      "[5590,     1] loss: 0.0097607590\n",
      "[5591,     1] loss: 0.0097606950\n",
      "[5592,     1] loss: 0.0097606316\n",
      "[5593,     1] loss: 0.0097605683\n",
      "[5594,     1] loss: 0.0097605042\n",
      "[5595,     1] loss: 0.0097604416\n",
      "[5596,     1] loss: 0.0097603776\n",
      "[5597,     1] loss: 0.0097603142\n",
      "[5598,     1] loss: 0.0097602509\n",
      "[5599,     1] loss: 0.0097601868\n",
      "[5600,     1] loss: 0.0097601242\n",
      "[5601,     1] loss: 0.0097600609\n",
      "[5602,     1] loss: 0.0097599968\n",
      "[5603,     1] loss: 0.0097599342\n",
      "[5604,     1] loss: 0.0097598702\n",
      "[5605,     1] loss: 0.0097598061\n",
      "[5606,     1] loss: 0.0097597443\n",
      "[5607,     1] loss: 0.0097596802\n",
      "[5608,     1] loss: 0.0097596169\n",
      "[5609,     1] loss: 0.0097595535\n",
      "[5610,     1] loss: 0.0097594902\n",
      "[5611,     1] loss: 0.0097594269\n",
      "[5612,     1] loss: 0.0097593635\n",
      "[5613,     1] loss: 0.0097593009\n",
      "[5614,     1] loss: 0.0097592369\n",
      "[5615,     1] loss: 0.0097591728\n",
      "[5616,     1] loss: 0.0097591110\n",
      "[5617,     1] loss: 0.0097590461\n",
      "[5618,     1] loss: 0.0097589828\n",
      "[5619,     1] loss: 0.0097589202\n",
      "[5620,     1] loss: 0.0097588561\n",
      "[5621,     1] loss: 0.0097587936\n",
      "[5622,     1] loss: 0.0097587302\n",
      "[5623,     1] loss: 0.0097586669\n",
      "[5624,     1] loss: 0.0097586043\n",
      "[5625,     1] loss: 0.0097585417\n",
      "[5626,     1] loss: 0.0097584784\n",
      "[5627,     1] loss: 0.0097584136\n",
      "[5628,     1] loss: 0.0097583517\n",
      "[5629,     1] loss: 0.0097582884\n",
      "[5630,     1] loss: 0.0097582243\n",
      "[5631,     1] loss: 0.0097581625\n",
      "[5632,     1] loss: 0.0097581007\n",
      "[5633,     1] loss: 0.0097580366\n",
      "[5634,     1] loss: 0.0097579718\n",
      "[5635,     1] loss: 0.0097579092\n",
      "[5636,     1] loss: 0.0097578481\n",
      "[5637,     1] loss: 0.0097577833\n",
      "[5638,     1] loss: 0.0097577207\n",
      "[5639,     1] loss: 0.0097576573\n",
      "[5640,     1] loss: 0.0097575940\n",
      "[5641,     1] loss: 0.0097575322\n",
      "[5642,     1] loss: 0.0097574681\n",
      "[5643,     1] loss: 0.0097574055\n",
      "[5644,     1] loss: 0.0097573422\n",
      "[5645,     1] loss: 0.0097572789\n",
      "[5646,     1] loss: 0.0097572155\n",
      "[5647,     1] loss: 0.0097571544\n",
      "[5648,     1] loss: 0.0097570904\n",
      "[5649,     1] loss: 0.0097570278\n",
      "[5650,     1] loss: 0.0097569644\n",
      "[5651,     1] loss: 0.0097569011\n",
      "[5652,     1] loss: 0.0097568370\n",
      "[5653,     1] loss: 0.0097567759\n",
      "[5654,     1] loss: 0.0097567119\n",
      "[5655,     1] loss: 0.0097566493\n",
      "[5656,     1] loss: 0.0097565867\n",
      "[5657,     1] loss: 0.0097565241\n",
      "[5658,     1] loss: 0.0097564608\n",
      "[5659,     1] loss: 0.0097563982\n",
      "[5660,     1] loss: 0.0097563349\n",
      "[5661,     1] loss: 0.0097562723\n",
      "[5662,     1] loss: 0.0097562097\n",
      "[5663,     1] loss: 0.0097561471\n",
      "[5664,     1] loss: 0.0097560845\n",
      "[5665,     1] loss: 0.0097560212\n",
      "[5666,     1] loss: 0.0097559601\n",
      "[5667,     1] loss: 0.0097558968\n",
      "[5668,     1] loss: 0.0097558334\n",
      "[5669,     1] loss: 0.0097557731\n",
      "[5670,     1] loss: 0.0097557083\n",
      "[5671,     1] loss: 0.0097556457\n",
      "[5672,     1] loss: 0.0097555831\n",
      "[5673,     1] loss: 0.0097555213\n",
      "[5674,     1] loss: 0.0097554572\n",
      "[5675,     1] loss: 0.0097553946\n",
      "[5676,     1] loss: 0.0097553320\n",
      "[5677,     1] loss: 0.0097552687\n",
      "[5678,     1] loss: 0.0097552061\n",
      "[5679,     1] loss: 0.0097551443\n",
      "[5680,     1] loss: 0.0097550809\n",
      "[5681,     1] loss: 0.0097550176\n",
      "[5682,     1] loss: 0.0097549558\n",
      "[5683,     1] loss: 0.0097548932\n",
      "[5684,     1] loss: 0.0097548306\n",
      "[5685,     1] loss: 0.0097547680\n",
      "[5686,     1] loss: 0.0097547054\n",
      "[5687,     1] loss: 0.0097546428\n",
      "[5688,     1] loss: 0.0097545803\n",
      "[5689,     1] loss: 0.0097545184\n",
      "[5690,     1] loss: 0.0097544558\n",
      "[5691,     1] loss: 0.0097543932\n",
      "[5692,     1] loss: 0.0097543307\n",
      "[5693,     1] loss: 0.0097542696\n",
      "[5694,     1] loss: 0.0097542062\n",
      "[5695,     1] loss: 0.0097541429\n",
      "[5696,     1] loss: 0.0097540803\n",
      "[5697,     1] loss: 0.0097540192\n",
      "[5698,     1] loss: 0.0097539559\n",
      "[5699,     1] loss: 0.0097538933\n",
      "[5700,     1] loss: 0.0097538315\n",
      "[5701,     1] loss: 0.0097537689\n",
      "[5702,     1] loss: 0.0097537063\n",
      "[5703,     1] loss: 0.0097536445\n",
      "[5704,     1] loss: 0.0097535811\n",
      "[5705,     1] loss: 0.0097535193\n",
      "[5706,     1] loss: 0.0097534560\n",
      "[5707,     1] loss: 0.0097533941\n",
      "[5708,     1] loss: 0.0097533323\n",
      "[5709,     1] loss: 0.0097532690\n",
      "[5710,     1] loss: 0.0097532071\n",
      "[5711,     1] loss: 0.0097531460\n",
      "[5712,     1] loss: 0.0097530819\n",
      "[5713,     1] loss: 0.0097530194\n",
      "[5714,     1] loss: 0.0097529575\n",
      "[5715,     1] loss: 0.0097528942\n",
      "[5716,     1] loss: 0.0097528331\n",
      "[5717,     1] loss: 0.0097527720\n",
      "[5718,     1] loss: 0.0097527087\n",
      "[5719,     1] loss: 0.0097526461\n",
      "[5720,     1] loss: 0.0097525835\n",
      "[5721,     1] loss: 0.0097525232\n",
      "[5722,     1] loss: 0.0097524606\n",
      "[5723,     1] loss: 0.0097523980\n",
      "[5724,     1] loss: 0.0097523361\n",
      "[5725,     1] loss: 0.0097522743\n",
      "[5726,     1] loss: 0.0097522125\n",
      "[5727,     1] loss: 0.0097521499\n",
      "[5728,     1] loss: 0.0097520865\n",
      "[5729,     1] loss: 0.0097520255\n",
      "[5730,     1] loss: 0.0097519621\n",
      "[5731,     1] loss: 0.0097519010\n",
      "[5732,     1] loss: 0.0097518384\n",
      "[5733,     1] loss: 0.0097517774\n",
      "[5734,     1] loss: 0.0097517148\n",
      "[5735,     1] loss: 0.0097516514\n",
      "[5736,     1] loss: 0.0097515911\n",
      "[5737,     1] loss: 0.0097515278\n",
      "[5738,     1] loss: 0.0097514652\n",
      "[5739,     1] loss: 0.0097514056\n",
      "[5740,     1] loss: 0.0097513430\n",
      "[5741,     1] loss: 0.0097512811\n",
      "[5742,     1] loss: 0.0097512186\n",
      "[5743,     1] loss: 0.0097511567\n",
      "[5744,     1] loss: 0.0097510949\n",
      "[5745,     1] loss: 0.0097510330\n",
      "[5746,     1] loss: 0.0097509690\n",
      "[5747,     1] loss: 0.0097509086\n",
      "[5748,     1] loss: 0.0097508468\n",
      "[5749,     1] loss: 0.0097507842\n",
      "[5750,     1] loss: 0.0097507216\n",
      "[5751,     1] loss: 0.0097506598\n",
      "[5752,     1] loss: 0.0097505987\n",
      "[5753,     1] loss: 0.0097505368\n",
      "[5754,     1] loss: 0.0097504728\n",
      "[5755,     1] loss: 0.0097504117\n",
      "[5756,     1] loss: 0.0097503513\n",
      "[5757,     1] loss: 0.0097502895\n",
      "[5758,     1] loss: 0.0097502261\n",
      "[5759,     1] loss: 0.0097501650\n",
      "[5760,     1] loss: 0.0097501040\n",
      "[5761,     1] loss: 0.0097500414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5762,     1] loss: 0.0097499803\n",
      "[5763,     1] loss: 0.0097499184\n",
      "[5764,     1] loss: 0.0097498558\n",
      "[5765,     1] loss: 0.0097497940\n",
      "[5766,     1] loss: 0.0097497337\n",
      "[5767,     1] loss: 0.0097496711\n",
      "[5768,     1] loss: 0.0097496085\n",
      "[5769,     1] loss: 0.0097495474\n",
      "[5770,     1] loss: 0.0097494848\n",
      "[5771,     1] loss: 0.0097494245\n",
      "[5772,     1] loss: 0.0097493611\n",
      "[5773,     1] loss: 0.0097493008\n",
      "[5774,     1] loss: 0.0097492389\n",
      "[5775,     1] loss: 0.0097491764\n",
      "[5776,     1] loss: 0.0097491153\n",
      "[5777,     1] loss: 0.0097490542\n",
      "[5778,     1] loss: 0.0097489916\n",
      "[5779,     1] loss: 0.0097489297\n",
      "[5780,     1] loss: 0.0097488686\n",
      "[5781,     1] loss: 0.0097488068\n",
      "[5782,     1] loss: 0.0097487457\n",
      "[5783,     1] loss: 0.0097486839\n",
      "[5784,     1] loss: 0.0097486220\n",
      "[5785,     1] loss: 0.0097485609\n",
      "[5786,     1] loss: 0.0097484998\n",
      "[5787,     1] loss: 0.0097484373\n",
      "[5788,     1] loss: 0.0097483747\n",
      "[5789,     1] loss: 0.0097483143\n",
      "[5790,     1] loss: 0.0097482532\n",
      "[5791,     1] loss: 0.0097481914\n",
      "[5792,     1] loss: 0.0097481295\n",
      "[5793,     1] loss: 0.0097480685\n",
      "[5794,     1] loss: 0.0097480059\n",
      "[5795,     1] loss: 0.0097479448\n",
      "[5796,     1] loss: 0.0097478837\n",
      "[5797,     1] loss: 0.0097478218\n",
      "[5798,     1] loss: 0.0097477615\n",
      "[5799,     1] loss: 0.0097476996\n",
      "[5800,     1] loss: 0.0097476378\n",
      "[5801,     1] loss: 0.0097475767\n",
      "[5802,     1] loss: 0.0097475156\n",
      "[5803,     1] loss: 0.0097474530\n",
      "[5804,     1] loss: 0.0097473912\n",
      "[5805,     1] loss: 0.0097473301\n",
      "[5806,     1] loss: 0.0097472690\n",
      "[5807,     1] loss: 0.0097472079\n",
      "[5808,     1] loss: 0.0097471461\n",
      "[5809,     1] loss: 0.0097470835\n",
      "[5810,     1] loss: 0.0097470231\n",
      "[5811,     1] loss: 0.0097469613\n",
      "[5812,     1] loss: 0.0097469002\n",
      "[5813,     1] loss: 0.0097468384\n",
      "[5814,     1] loss: 0.0097467773\n",
      "[5815,     1] loss: 0.0097467162\n",
      "[5816,     1] loss: 0.0097466551\n",
      "[5817,     1] loss: 0.0097465925\n",
      "[5818,     1] loss: 0.0097465321\n",
      "[5819,     1] loss: 0.0097464703\n",
      "[5820,     1] loss: 0.0097464107\n",
      "[5821,     1] loss: 0.0097463496\n",
      "[5822,     1] loss: 0.0097462855\n",
      "[5823,     1] loss: 0.0097462252\n",
      "[5824,     1] loss: 0.0097461633\n",
      "[5825,     1] loss: 0.0097461030\n",
      "[5826,     1] loss: 0.0097460419\n",
      "[5827,     1] loss: 0.0097459808\n",
      "[5828,     1] loss: 0.0097459197\n",
      "[5829,     1] loss: 0.0097458586\n",
      "[5830,     1] loss: 0.0097457975\n",
      "[5831,     1] loss: 0.0097457357\n",
      "[5832,     1] loss: 0.0097456738\n",
      "[5833,     1] loss: 0.0097456120\n",
      "[5834,     1] loss: 0.0097455524\n",
      "[5835,     1] loss: 0.0097454906\n",
      "[5836,     1] loss: 0.0097454287\n",
      "[5837,     1] loss: 0.0097453691\n",
      "[5838,     1] loss: 0.0097453080\n",
      "[5839,     1] loss: 0.0097452454\n",
      "[5840,     1] loss: 0.0097451843\n",
      "[5841,     1] loss: 0.0097451247\n",
      "[5842,     1] loss: 0.0097450621\n",
      "[5843,     1] loss: 0.0097450010\n",
      "[5844,     1] loss: 0.0097449407\n",
      "[5845,     1] loss: 0.0097448803\n",
      "[5846,     1] loss: 0.0097448185\n",
      "[5847,     1] loss: 0.0097447559\n",
      "[5848,     1] loss: 0.0097446963\n",
      "[5849,     1] loss: 0.0097446352\n",
      "[5850,     1] loss: 0.0097445726\n",
      "[5851,     1] loss: 0.0097445123\n",
      "[5852,     1] loss: 0.0097444512\n",
      "[5853,     1] loss: 0.0097443908\n",
      "[5854,     1] loss: 0.0097443298\n",
      "[5855,     1] loss: 0.0097442687\n",
      "[5856,     1] loss: 0.0097442076\n",
      "[5857,     1] loss: 0.0097441457\n",
      "[5858,     1] loss: 0.0097440846\n",
      "[5859,     1] loss: 0.0097440235\n",
      "[5860,     1] loss: 0.0097439624\n",
      "[5861,     1] loss: 0.0097439028\n",
      "[5862,     1] loss: 0.0097438402\n",
      "[5863,     1] loss: 0.0097437792\n",
      "[5864,     1] loss: 0.0097437188\n",
      "[5865,     1] loss: 0.0097436577\n",
      "[5866,     1] loss: 0.0097435974\n",
      "[5867,     1] loss: 0.0097435363\n",
      "[5868,     1] loss: 0.0097434759\n",
      "[5869,     1] loss: 0.0097434156\n",
      "[5870,     1] loss: 0.0097433537\n",
      "[5871,     1] loss: 0.0097432919\n",
      "[5872,     1] loss: 0.0097432308\n",
      "[5873,     1] loss: 0.0097431719\n",
      "[5874,     1] loss: 0.0097431093\n",
      "[5875,     1] loss: 0.0097430475\n",
      "[5876,     1] loss: 0.0097429879\n",
      "[5877,     1] loss: 0.0097429268\n",
      "[5878,     1] loss: 0.0097428657\n",
      "[5879,     1] loss: 0.0097428054\n",
      "[5880,     1] loss: 0.0097427450\n",
      "[5881,     1] loss: 0.0097426824\n",
      "[5882,     1] loss: 0.0097426221\n",
      "[5883,     1] loss: 0.0097425610\n",
      "[5884,     1] loss: 0.0097425006\n",
      "[5885,     1] loss: 0.0097424380\n",
      "[5886,     1] loss: 0.0097423777\n",
      "[5887,     1] loss: 0.0097423181\n",
      "[5888,     1] loss: 0.0097422577\n",
      "[5889,     1] loss: 0.0097421966\n",
      "[5890,     1] loss: 0.0097421363\n",
      "[5891,     1] loss: 0.0097420752\n",
      "[5892,     1] loss: 0.0097420134\n",
      "[5893,     1] loss: 0.0097419538\n",
      "[5894,     1] loss: 0.0097418927\n",
      "[5895,     1] loss: 0.0097418316\n",
      "[5896,     1] loss: 0.0097417705\n",
      "[5897,     1] loss: 0.0097417109\n",
      "[5898,     1] loss: 0.0097416490\n",
      "[5899,     1] loss: 0.0097415887\n",
      "[5900,     1] loss: 0.0097415276\n",
      "[5901,     1] loss: 0.0097414672\n",
      "[5902,     1] loss: 0.0097414061\n",
      "[5903,     1] loss: 0.0097413465\n",
      "[5904,     1] loss: 0.0097412847\n",
      "[5905,     1] loss: 0.0097412229\n",
      "[5906,     1] loss: 0.0097411640\n",
      "[5907,     1] loss: 0.0097411029\n",
      "[5908,     1] loss: 0.0097410418\n",
      "[5909,     1] loss: 0.0097409822\n",
      "[5910,     1] loss: 0.0097409196\n",
      "[5911,     1] loss: 0.0097408600\n",
      "[5912,     1] loss: 0.0097407997\n",
      "[5913,     1] loss: 0.0097407386\n",
      "[5914,     1] loss: 0.0097406767\n",
      "[5915,     1] loss: 0.0097406164\n",
      "[5916,     1] loss: 0.0097405560\n",
      "[5917,     1] loss: 0.0097404957\n",
      "[5918,     1] loss: 0.0097404353\n",
      "[5919,     1] loss: 0.0097403757\n",
      "[5920,     1] loss: 0.0097403139\n",
      "[5921,     1] loss: 0.0097402535\n",
      "[5922,     1] loss: 0.0097401932\n",
      "[5923,     1] loss: 0.0097401328\n",
      "[5924,     1] loss: 0.0097400725\n",
      "[5925,     1] loss: 0.0097400099\n",
      "[5926,     1] loss: 0.0097399496\n",
      "[5927,     1] loss: 0.0097398899\n",
      "[5928,     1] loss: 0.0097398281\n",
      "[5929,     1] loss: 0.0097397678\n",
      "[5930,     1] loss: 0.0097397082\n",
      "[5931,     1] loss: 0.0097396463\n",
      "[5932,     1] loss: 0.0097395875\n",
      "[5933,     1] loss: 0.0097395264\n",
      "[5934,     1] loss: 0.0097394653\n",
      "[5935,     1] loss: 0.0097394057\n",
      "[5936,     1] loss: 0.0097393446\n",
      "[5937,     1] loss: 0.0097392842\n",
      "[5938,     1] loss: 0.0097392239\n",
      "[5939,     1] loss: 0.0097391628\n",
      "[5940,     1] loss: 0.0097391017\n",
      "[5941,     1] loss: 0.0097390413\n",
      "[5942,     1] loss: 0.0097389825\n",
      "[5943,     1] loss: 0.0097389214\n",
      "[5944,     1] loss: 0.0097388603\n",
      "[5945,     1] loss: 0.0097387999\n",
      "[5946,     1] loss: 0.0097387388\n",
      "[5947,     1] loss: 0.0097386792\n",
      "[5948,     1] loss: 0.0097386181\n",
      "[5949,     1] loss: 0.0097385578\n",
      "[5950,     1] loss: 0.0097384959\n",
      "[5951,     1] loss: 0.0097384371\n",
      "[5952,     1] loss: 0.0097383775\n",
      "[5953,     1] loss: 0.0097383164\n",
      "[5954,     1] loss: 0.0097382568\n",
      "[5955,     1] loss: 0.0097381942\n",
      "[5956,     1] loss: 0.0097381353\n",
      "[5957,     1] loss: 0.0097380742\n",
      "[5958,     1] loss: 0.0097380139\n",
      "[5959,     1] loss: 0.0097379535\n",
      "[5960,     1] loss: 0.0097378932\n",
      "[5961,     1] loss: 0.0097378328\n",
      "[5962,     1] loss: 0.0097377717\n",
      "[5963,     1] loss: 0.0097377114\n",
      "[5964,     1] loss: 0.0097376525\n",
      "[5965,     1] loss: 0.0097375900\n",
      "[5966,     1] loss: 0.0097375289\n",
      "[5967,     1] loss: 0.0097374700\n",
      "[5968,     1] loss: 0.0097374104\n",
      "[5969,     1] loss: 0.0097373493\n",
      "[5970,     1] loss: 0.0097372897\n",
      "[5971,     1] loss: 0.0097372286\n",
      "[5972,     1] loss: 0.0097371683\n",
      "[5973,     1] loss: 0.0097371086\n",
      "[5974,     1] loss: 0.0097370476\n",
      "[5975,     1] loss: 0.0097369872\n",
      "[5976,     1] loss: 0.0097369261\n",
      "[5977,     1] loss: 0.0097368658\n",
      "[5978,     1] loss: 0.0097368054\n",
      "[5979,     1] loss: 0.0097367443\n",
      "[5980,     1] loss: 0.0097366847\n",
      "[5981,     1] loss: 0.0097366251\n",
      "[5982,     1] loss: 0.0097365648\n",
      "[5983,     1] loss: 0.0097365037\n",
      "[5984,     1] loss: 0.0097364441\n",
      "[5985,     1] loss: 0.0097363830\n",
      "[5986,     1] loss: 0.0097363234\n",
      "[5987,     1] loss: 0.0097362623\n",
      "[5988,     1] loss: 0.0097362019\n",
      "[5989,     1] loss: 0.0097361416\n",
      "[5990,     1] loss: 0.0097360820\n",
      "[5991,     1] loss: 0.0097360209\n",
      "[5992,     1] loss: 0.0097359613\n",
      "[5993,     1] loss: 0.0097359009\n",
      "[5994,     1] loss: 0.0097358398\n",
      "[5995,     1] loss: 0.0097357802\n",
      "[5996,     1] loss: 0.0097357191\n",
      "[5997,     1] loss: 0.0097356595\n",
      "[5998,     1] loss: 0.0097355992\n",
      "[5999,     1] loss: 0.0097355388\n",
      "[6000,     1] loss: 0.0097354785\n",
      "[6001,     1] loss: 0.0097354189\n",
      "[6002,     1] loss: 0.0097353585\n",
      "[6003,     1] loss: 0.0097352967\n",
      "[6004,     1] loss: 0.0097352378\n",
      "[6005,     1] loss: 0.0097351767\n",
      "[6006,     1] loss: 0.0097351156\n",
      "[6007,     1] loss: 0.0097350560\n",
      "[6008,     1] loss: 0.0097349957\n",
      "[6009,     1] loss: 0.0097349361\n",
      "[6010,     1] loss: 0.0097348757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6011,     1] loss: 0.0097348146\n",
      "[6012,     1] loss: 0.0097347558\n",
      "[6013,     1] loss: 0.0097346954\n",
      "[6014,     1] loss: 0.0097346343\n",
      "[6015,     1] loss: 0.0097345740\n",
      "[6016,     1] loss: 0.0097345144\n",
      "[6017,     1] loss: 0.0097344540\n",
      "[6018,     1] loss: 0.0097343944\n",
      "[6019,     1] loss: 0.0097343341\n",
      "[6020,     1] loss: 0.0097342737\n",
      "[6021,     1] loss: 0.0097342134\n",
      "[6022,     1] loss: 0.0097341537\n",
      "[6023,     1] loss: 0.0097340927\n",
      "[6024,     1] loss: 0.0097340316\n",
      "[6025,     1] loss: 0.0097339712\n",
      "[6026,     1] loss: 0.0097339116\n",
      "[6027,     1] loss: 0.0097338513\n",
      "[6028,     1] loss: 0.0097337916\n",
      "[6029,     1] loss: 0.0097337313\n",
      "[6030,     1] loss: 0.0097336717\n",
      "[6031,     1] loss: 0.0097336113\n",
      "[6032,     1] loss: 0.0097335510\n",
      "[6033,     1] loss: 0.0097334906\n",
      "[6034,     1] loss: 0.0097334303\n",
      "[6035,     1] loss: 0.0097333699\n",
      "[6036,     1] loss: 0.0097333096\n",
      "[6037,     1] loss: 0.0097332500\n",
      "[6038,     1] loss: 0.0097331896\n",
      "[6039,     1] loss: 0.0097331293\n",
      "[6040,     1] loss: 0.0097330689\n",
      "[6041,     1] loss: 0.0097330093\n",
      "[6042,     1] loss: 0.0097329505\n",
      "[6043,     1] loss: 0.0097328886\n",
      "[6044,     1] loss: 0.0097328290\n",
      "[6045,     1] loss: 0.0097327687\n",
      "[6046,     1] loss: 0.0097327091\n",
      "[6047,     1] loss: 0.0097326480\n",
      "[6048,     1] loss: 0.0097325884\n",
      "[6049,     1] loss: 0.0097325288\n",
      "[6050,     1] loss: 0.0097324677\n",
      "[6051,     1] loss: 0.0097324081\n",
      "[6052,     1] loss: 0.0097323477\n",
      "[6053,     1] loss: 0.0097322874\n",
      "[6054,     1] loss: 0.0097322270\n",
      "[6055,     1] loss: 0.0097321667\n",
      "[6056,     1] loss: 0.0097321078\n",
      "[6057,     1] loss: 0.0097320475\n",
      "[6058,     1] loss: 0.0097319864\n",
      "[6059,     1] loss: 0.0097319260\n",
      "[6060,     1] loss: 0.0097318672\n",
      "[6061,     1] loss: 0.0097318061\n",
      "[6062,     1] loss: 0.0097317465\n",
      "[6063,     1] loss: 0.0097316869\n",
      "[6064,     1] loss: 0.0097316265\n",
      "[6065,     1] loss: 0.0097315654\n",
      "[6066,     1] loss: 0.0097315066\n",
      "[6067,     1] loss: 0.0097314455\n",
      "[6068,     1] loss: 0.0097313859\n",
      "[6069,     1] loss: 0.0097313248\n",
      "[6070,     1] loss: 0.0097312659\n",
      "[6071,     1] loss: 0.0097312048\n",
      "[6072,     1] loss: 0.0097311437\n",
      "[6073,     1] loss: 0.0097310841\n",
      "[6074,     1] loss: 0.0097310245\n",
      "[6075,     1] loss: 0.0097309649\n",
      "[6076,     1] loss: 0.0097309045\n",
      "[6077,     1] loss: 0.0097308442\n",
      "[6078,     1] loss: 0.0097307846\n",
      "[6079,     1] loss: 0.0097307235\n",
      "[6080,     1] loss: 0.0097306646\n",
      "[6081,     1] loss: 0.0097306035\n",
      "[6082,     1] loss: 0.0097305432\n",
      "[6083,     1] loss: 0.0097304836\n",
      "[6084,     1] loss: 0.0097304232\n",
      "[6085,     1] loss: 0.0097303629\n",
      "[6086,     1] loss: 0.0097303033\n",
      "[6087,     1] loss: 0.0097302437\n",
      "[6088,     1] loss: 0.0097301826\n",
      "[6089,     1] loss: 0.0097301230\n",
      "[6090,     1] loss: 0.0097300626\n",
      "[6091,     1] loss: 0.0097300030\n",
      "[6092,     1] loss: 0.0097299427\n",
      "[6093,     1] loss: 0.0097298816\n",
      "[6094,     1] loss: 0.0097298212\n",
      "[6095,     1] loss: 0.0097297616\n",
      "[6096,     1] loss: 0.0097297005\n",
      "[6097,     1] loss: 0.0097296417\n",
      "[6098,     1] loss: 0.0097295821\n",
      "[6099,     1] loss: 0.0097295210\n",
      "[6100,     1] loss: 0.0097294606\n",
      "[6101,     1] loss: 0.0097294003\n",
      "[6102,     1] loss: 0.0097293407\n",
      "[6103,     1] loss: 0.0097292803\n",
      "[6104,     1] loss: 0.0097292200\n",
      "[6105,     1] loss: 0.0097291611\n",
      "[6106,     1] loss: 0.0097291000\n",
      "[6107,     1] loss: 0.0097290397\n",
      "[6108,     1] loss: 0.0097289801\n",
      "[6109,     1] loss: 0.0097289197\n",
      "[6110,     1] loss: 0.0097288601\n",
      "[6111,     1] loss: 0.0097288005\n",
      "[6112,     1] loss: 0.0097287394\n",
      "[6113,     1] loss: 0.0097286798\n",
      "[6114,     1] loss: 0.0097286187\n",
      "[6115,     1] loss: 0.0097285606\n",
      "[6116,     1] loss: 0.0097284980\n",
      "[6117,     1] loss: 0.0097284384\n",
      "[6118,     1] loss: 0.0097283795\n",
      "[6119,     1] loss: 0.0097283185\n",
      "[6120,     1] loss: 0.0097282581\n",
      "[6121,     1] loss: 0.0097281992\n",
      "[6122,     1] loss: 0.0097281381\n",
      "[6123,     1] loss: 0.0097280785\n",
      "[6124,     1] loss: 0.0097280182\n",
      "[6125,     1] loss: 0.0097279586\n",
      "[6126,     1] loss: 0.0097278960\n",
      "[6127,     1] loss: 0.0097278386\n",
      "[6128,     1] loss: 0.0097277783\n",
      "[6129,     1] loss: 0.0097277179\n",
      "[6130,     1] loss: 0.0097276568\n",
      "[6131,     1] loss: 0.0097275972\n",
      "[6132,     1] loss: 0.0097275369\n",
      "[6133,     1] loss: 0.0097274765\n",
      "[6134,     1] loss: 0.0097274162\n",
      "[6135,     1] loss: 0.0097273566\n",
      "[6136,     1] loss: 0.0097272977\n",
      "[6137,     1] loss: 0.0097272366\n",
      "[6138,     1] loss: 0.0097271770\n",
      "[6139,     1] loss: 0.0097271159\n",
      "[6140,     1] loss: 0.0097270563\n",
      "[6141,     1] loss: 0.0097269975\n",
      "[6142,     1] loss: 0.0097269364\n",
      "[6143,     1] loss: 0.0097268760\n",
      "[6144,     1] loss: 0.0097268172\n",
      "[6145,     1] loss: 0.0097267553\n",
      "[6146,     1] loss: 0.0097266965\n",
      "[6147,     1] loss: 0.0097266361\n",
      "[6148,     1] loss: 0.0097265765\n",
      "[6149,     1] loss: 0.0097265154\n",
      "[6150,     1] loss: 0.0097264558\n",
      "[6151,     1] loss: 0.0097263947\n",
      "[6152,     1] loss: 0.0097263344\n",
      "[6153,     1] loss: 0.0097262740\n",
      "[6154,     1] loss: 0.0097262129\n",
      "[6155,     1] loss: 0.0097261541\n",
      "[6156,     1] loss: 0.0097260930\n",
      "[6157,     1] loss: 0.0097260341\n",
      "[6158,     1] loss: 0.0097259738\n",
      "[6159,     1] loss: 0.0097259142\n",
      "[6160,     1] loss: 0.0097258545\n",
      "[6161,     1] loss: 0.0097257935\n",
      "[6162,     1] loss: 0.0097257331\n",
      "[6163,     1] loss: 0.0097256735\n",
      "[6164,     1] loss: 0.0097256124\n",
      "[6165,     1] loss: 0.0097255528\n",
      "[6166,     1] loss: 0.0097254917\n",
      "[6167,     1] loss: 0.0097254321\n",
      "[6168,     1] loss: 0.0097253717\n",
      "[6169,     1] loss: 0.0097253114\n",
      "[6170,     1] loss: 0.0097252503\n",
      "[6171,     1] loss: 0.0097251914\n",
      "[6172,     1] loss: 0.0097251318\n",
      "[6173,     1] loss: 0.0097250722\n",
      "[6174,     1] loss: 0.0097250119\n",
      "[6175,     1] loss: 0.0097249515\n",
      "[6176,     1] loss: 0.0097248904\n",
      "[6177,     1] loss: 0.0097248308\n",
      "[6178,     1] loss: 0.0097247705\n",
      "[6179,     1] loss: 0.0097247101\n",
      "[6180,     1] loss: 0.0097246505\n",
      "[6181,     1] loss: 0.0097245894\n",
      "[6182,     1] loss: 0.0097245283\n",
      "[6183,     1] loss: 0.0097244702\n",
      "[6184,     1] loss: 0.0097244091\n",
      "[6185,     1] loss: 0.0097243488\n",
      "[6186,     1] loss: 0.0097242877\n",
      "[6187,     1] loss: 0.0097242281\n",
      "[6188,     1] loss: 0.0097241685\n",
      "[6189,     1] loss: 0.0097241081\n",
      "[6190,     1] loss: 0.0097240470\n",
      "[6191,     1] loss: 0.0097239874\n",
      "[6192,     1] loss: 0.0097239263\n",
      "[6193,     1] loss: 0.0097238682\n",
      "[6194,     1] loss: 0.0097238049\n",
      "[6195,     1] loss: 0.0097237460\n",
      "[6196,     1] loss: 0.0097236872\n",
      "[6197,     1] loss: 0.0097236261\n",
      "[6198,     1] loss: 0.0097235665\n",
      "[6199,     1] loss: 0.0097235054\n",
      "[6200,     1] loss: 0.0097234458\n",
      "[6201,     1] loss: 0.0097233862\n",
      "[6202,     1] loss: 0.0097233251\n",
      "[6203,     1] loss: 0.0097232647\n",
      "[6204,     1] loss: 0.0097232036\n",
      "[6205,     1] loss: 0.0097231433\n",
      "[6206,     1] loss: 0.0097230837\n",
      "[6207,     1] loss: 0.0097230233\n",
      "[6208,     1] loss: 0.0097229630\n",
      "[6209,     1] loss: 0.0097229026\n",
      "[6210,     1] loss: 0.0097228415\n",
      "[6211,     1] loss: 0.0097227827\n",
      "[6212,     1] loss: 0.0097227223\n",
      "[6213,     1] loss: 0.0097226620\n",
      "[6214,     1] loss: 0.0097226016\n",
      "[6215,     1] loss: 0.0097225413\n",
      "[6216,     1] loss: 0.0097224817\n",
      "[6217,     1] loss: 0.0097224213\n",
      "[6218,     1] loss: 0.0097223610\n",
      "[6219,     1] loss: 0.0097222999\n",
      "[6220,     1] loss: 0.0097222403\n",
      "[6221,     1] loss: 0.0097221792\n",
      "[6222,     1] loss: 0.0097221196\n",
      "[6223,     1] loss: 0.0097220585\n",
      "[6224,     1] loss: 0.0097219981\n",
      "[6225,     1] loss: 0.0097219378\n",
      "[6226,     1] loss: 0.0097218774\n",
      "[6227,     1] loss: 0.0097218186\n",
      "[6228,     1] loss: 0.0097217575\n",
      "[6229,     1] loss: 0.0097216971\n",
      "[6230,     1] loss: 0.0097216375\n",
      "[6231,     1] loss: 0.0097215757\n",
      "[6232,     1] loss: 0.0097215161\n",
      "[6233,     1] loss: 0.0097214550\n",
      "[6234,     1] loss: 0.0097213946\n",
      "[6235,     1] loss: 0.0097213335\n",
      "[6236,     1] loss: 0.0097212747\n",
      "[6237,     1] loss: 0.0097212143\n",
      "[6238,     1] loss: 0.0097211532\n",
      "[6239,     1] loss: 0.0097210921\n",
      "[6240,     1] loss: 0.0097210333\n",
      "[6241,     1] loss: 0.0097209729\n",
      "[6242,     1] loss: 0.0097209118\n",
      "[6243,     1] loss: 0.0097208507\n",
      "[6244,     1] loss: 0.0097207911\n",
      "[6245,     1] loss: 0.0097207308\n",
      "[6246,     1] loss: 0.0097206704\n",
      "[6247,     1] loss: 0.0097206101\n",
      "[6248,     1] loss: 0.0097205505\n",
      "[6249,     1] loss: 0.0097204894\n",
      "[6250,     1] loss: 0.0097204275\n",
      "[6251,     1] loss: 0.0097203687\n",
      "[6252,     1] loss: 0.0097203076\n",
      "[6253,     1] loss: 0.0097202480\n",
      "[6254,     1] loss: 0.0097201869\n",
      "[6255,     1] loss: 0.0097201265\n",
      "[6256,     1] loss: 0.0097200669\n",
      "[6257,     1] loss: 0.0097200066\n",
      "[6258,     1] loss: 0.0097199462\n",
      "[6259,     1] loss: 0.0097198851\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6260,     1] loss: 0.0097198248\n",
      "[6261,     1] loss: 0.0097197644\n",
      "[6262,     1] loss: 0.0097197041\n",
      "[6263,     1] loss: 0.0097196437\n",
      "[6264,     1] loss: 0.0097195819\n",
      "[6265,     1] loss: 0.0097195216\n",
      "[6266,     1] loss: 0.0097194612\n",
      "[6267,     1] loss: 0.0097194001\n",
      "[6268,     1] loss: 0.0097193412\n",
      "[6269,     1] loss: 0.0097192809\n",
      "[6270,     1] loss: 0.0097192198\n",
      "[6271,     1] loss: 0.0097191602\n",
      "[6272,     1] loss: 0.0097190991\n",
      "[6273,     1] loss: 0.0097190380\n",
      "[6274,     1] loss: 0.0097189777\n",
      "[6275,     1] loss: 0.0097189166\n",
      "[6276,     1] loss: 0.0097188570\n",
      "[6277,     1] loss: 0.0097187959\n",
      "[6278,     1] loss: 0.0097187348\n",
      "[6279,     1] loss: 0.0097186752\n",
      "[6280,     1] loss: 0.0097186133\n",
      "[6281,     1] loss: 0.0097185545\n",
      "[6282,     1] loss: 0.0097184941\n",
      "[6283,     1] loss: 0.0097184330\n",
      "[6284,     1] loss: 0.0097183727\n",
      "[6285,     1] loss: 0.0097183116\n",
      "[6286,     1] loss: 0.0097182520\n",
      "[6287,     1] loss: 0.0097181909\n",
      "[6288,     1] loss: 0.0097181298\n",
      "[6289,     1] loss: 0.0097180694\n",
      "[6290,     1] loss: 0.0097180098\n",
      "[6291,     1] loss: 0.0097179480\n",
      "[6292,     1] loss: 0.0097178876\n",
      "[6293,     1] loss: 0.0097178265\n",
      "[6294,     1] loss: 0.0097177662\n",
      "[6295,     1] loss: 0.0097177051\n",
      "[6296,     1] loss: 0.0097176462\n",
      "[6297,     1] loss: 0.0097175837\n",
      "[6298,     1] loss: 0.0097175233\n",
      "[6299,     1] loss: 0.0097174630\n",
      "[6300,     1] loss: 0.0097174019\n",
      "[6301,     1] loss: 0.0097173415\n",
      "[6302,     1] loss: 0.0097172812\n",
      "[6303,     1] loss: 0.0097172216\n",
      "[6304,     1] loss: 0.0097171590\n",
      "[6305,     1] loss: 0.0097170994\n",
      "[6306,     1] loss: 0.0097170375\n",
      "[6307,     1] loss: 0.0097169779\n",
      "[6308,     1] loss: 0.0097169168\n",
      "[6309,     1] loss: 0.0097168557\n",
      "[6310,     1] loss: 0.0097167946\n",
      "[6311,     1] loss: 0.0097167350\n",
      "[6312,     1] loss: 0.0097166739\n",
      "[6313,     1] loss: 0.0097166128\n",
      "[6314,     1] loss: 0.0097165525\n",
      "[6315,     1] loss: 0.0097164929\n",
      "[6316,     1] loss: 0.0097164325\n",
      "[6317,     1] loss: 0.0097163700\n",
      "[6318,     1] loss: 0.0097163096\n",
      "[6319,     1] loss: 0.0097162493\n",
      "[6320,     1] loss: 0.0097161882\n",
      "[6321,     1] loss: 0.0097161271\n",
      "[6322,     1] loss: 0.0097160667\n",
      "[6323,     1] loss: 0.0097160056\n",
      "[6324,     1] loss: 0.0097159453\n",
      "[6325,     1] loss: 0.0097158849\n",
      "[6326,     1] loss: 0.0097158238\n",
      "[6327,     1] loss: 0.0097157627\n",
      "[6328,     1] loss: 0.0097157024\n",
      "[6329,     1] loss: 0.0097156420\n",
      "[6330,     1] loss: 0.0097155817\n",
      "[6331,     1] loss: 0.0097155206\n",
      "[6332,     1] loss: 0.0097154595\n",
      "[6333,     1] loss: 0.0097153984\n",
      "[6334,     1] loss: 0.0097153381\n",
      "[6335,     1] loss: 0.0097152777\n",
      "[6336,     1] loss: 0.0097152166\n",
      "[6337,     1] loss: 0.0097151555\n",
      "[6338,     1] loss: 0.0097150944\n",
      "[6339,     1] loss: 0.0097150341\n",
      "[6340,     1] loss: 0.0097149730\n",
      "[6341,     1] loss: 0.0097149111\n",
      "[6342,     1] loss: 0.0097148515\n",
      "[6343,     1] loss: 0.0097147904\n",
      "[6344,     1] loss: 0.0097147286\n",
      "[6345,     1] loss: 0.0097146675\n",
      "[6346,     1] loss: 0.0097146064\n",
      "[6347,     1] loss: 0.0097145453\n",
      "[6348,     1] loss: 0.0097144850\n",
      "[6349,     1] loss: 0.0097144239\n",
      "[6350,     1] loss: 0.0097143628\n",
      "[6351,     1] loss: 0.0097143024\n",
      "[6352,     1] loss: 0.0097142413\n",
      "[6353,     1] loss: 0.0097141795\n",
      "[6354,     1] loss: 0.0097141176\n",
      "[6355,     1] loss: 0.0097140573\n",
      "[6356,     1] loss: 0.0097139962\n",
      "[6357,     1] loss: 0.0097139359\n",
      "[6358,     1] loss: 0.0097138755\n",
      "[6359,     1] loss: 0.0097138137\n",
      "[6360,     1] loss: 0.0097137526\n",
      "[6361,     1] loss: 0.0097136915\n",
      "[6362,     1] loss: 0.0097136304\n",
      "[6363,     1] loss: 0.0097135693\n",
      "[6364,     1] loss: 0.0097135082\n",
      "[6365,     1] loss: 0.0097134478\n",
      "[6366,     1] loss: 0.0097133875\n",
      "[6367,     1] loss: 0.0097133256\n",
      "[6368,     1] loss: 0.0097132631\n",
      "[6369,     1] loss: 0.0097132020\n",
      "[6370,     1] loss: 0.0097131416\n",
      "[6371,     1] loss: 0.0097130798\n",
      "[6372,     1] loss: 0.0097130194\n",
      "[6373,     1] loss: 0.0097129583\n",
      "[6374,     1] loss: 0.0097128980\n",
      "[6375,     1] loss: 0.0097128361\n",
      "[6376,     1] loss: 0.0097127758\n",
      "[6377,     1] loss: 0.0097127140\n",
      "[6378,     1] loss: 0.0097126521\n",
      "[6379,     1] loss: 0.0097125910\n",
      "[6380,     1] loss: 0.0097125307\n",
      "[6381,     1] loss: 0.0097124703\n",
      "[6382,     1] loss: 0.0097124092\n",
      "[6383,     1] loss: 0.0097123474\n",
      "[6384,     1] loss: 0.0097122870\n",
      "[6385,     1] loss: 0.0097122259\n",
      "[6386,     1] loss: 0.0097121641\n",
      "[6387,     1] loss: 0.0097121038\n",
      "[6388,     1] loss: 0.0097120419\n",
      "[6389,     1] loss: 0.0097119793\n",
      "[6390,     1] loss: 0.0097119205\n",
      "[6391,     1] loss: 0.0097118579\n",
      "[6392,     1] loss: 0.0097117968\n",
      "[6393,     1] loss: 0.0097117357\n",
      "[6394,     1] loss: 0.0097116739\n",
      "[6395,     1] loss: 0.0097116128\n",
      "[6396,     1] loss: 0.0097115517\n",
      "[6397,     1] loss: 0.0097114891\n",
      "[6398,     1] loss: 0.0097114280\n",
      "[6399,     1] loss: 0.0097113669\n",
      "[6400,     1] loss: 0.0097113065\n",
      "[6401,     1] loss: 0.0097112447\n",
      "[6402,     1] loss: 0.0097111836\n",
      "[6403,     1] loss: 0.0097111218\n",
      "[6404,     1] loss: 0.0097110607\n",
      "[6405,     1] loss: 0.0097109996\n",
      "[6406,     1] loss: 0.0097109385\n",
      "[6407,     1] loss: 0.0097108774\n",
      "[6408,     1] loss: 0.0097108155\n",
      "[6409,     1] loss: 0.0097107545\n",
      "[6410,     1] loss: 0.0097106926\n",
      "[6411,     1] loss: 0.0097106315\n",
      "[6412,     1] loss: 0.0097105697\n",
      "[6413,     1] loss: 0.0097105093\n",
      "[6414,     1] loss: 0.0097104475\n",
      "[6415,     1] loss: 0.0097103857\n",
      "[6416,     1] loss: 0.0097103246\n",
      "[6417,     1] loss: 0.0097102635\n",
      "[6418,     1] loss: 0.0097102009\n",
      "[6419,     1] loss: 0.0097101405\n",
      "[6420,     1] loss: 0.0097100772\n",
      "[6421,     1] loss: 0.0097100161\n",
      "[6422,     1] loss: 0.0097099550\n",
      "[6423,     1] loss: 0.0097098939\n",
      "[6424,     1] loss: 0.0097098321\n",
      "[6425,     1] loss: 0.0097097710\n",
      "[6426,     1] loss: 0.0097097106\n",
      "[6427,     1] loss: 0.0097096473\n",
      "[6428,     1] loss: 0.0097095862\n",
      "[6429,     1] loss: 0.0097095259\n",
      "[6430,     1] loss: 0.0097094625\n",
      "[6431,     1] loss: 0.0097094022\n",
      "[6432,     1] loss: 0.0097093403\n",
      "[6433,     1] loss: 0.0097092777\n",
      "[6434,     1] loss: 0.0097092167\n",
      "[6435,     1] loss: 0.0097091563\n",
      "[6436,     1] loss: 0.0097090937\n",
      "[6437,     1] loss: 0.0097090334\n",
      "[6438,     1] loss: 0.0097089715\n",
      "[6439,     1] loss: 0.0097089097\n",
      "[6440,     1] loss: 0.0097088471\n",
      "[6441,     1] loss: 0.0097087853\n",
      "[6442,     1] loss: 0.0097087249\n",
      "[6443,     1] loss: 0.0097086623\n",
      "[6444,     1] loss: 0.0097086020\n",
      "[6445,     1] loss: 0.0097085394\n",
      "[6446,     1] loss: 0.0097084776\n",
      "[6447,     1] loss: 0.0097084157\n",
      "[6448,     1] loss: 0.0097083539\n",
      "[6449,     1] loss: 0.0097082928\n",
      "[6450,     1] loss: 0.0097082302\n",
      "[6451,     1] loss: 0.0097081691\n",
      "[6452,     1] loss: 0.0097081065\n",
      "[6453,     1] loss: 0.0097080454\n",
      "[6454,     1] loss: 0.0097079843\n",
      "[6455,     1] loss: 0.0097079210\n",
      "[6456,     1] loss: 0.0097078606\n",
      "[6457,     1] loss: 0.0097077973\n",
      "[6458,     1] loss: 0.0097077355\n",
      "[6459,     1] loss: 0.0097076744\n",
      "[6460,     1] loss: 0.0097076125\n",
      "[6461,     1] loss: 0.0097075514\n",
      "[6462,     1] loss: 0.0097074889\n",
      "[6463,     1] loss: 0.0097074270\n",
      "[6464,     1] loss: 0.0097073659\n",
      "[6465,     1] loss: 0.0097073026\n",
      "[6466,     1] loss: 0.0097072423\n",
      "[6467,     1] loss: 0.0097071804\n",
      "[6468,     1] loss: 0.0097071178\n",
      "[6469,     1] loss: 0.0097070552\n",
      "[6470,     1] loss: 0.0097069934\n",
      "[6471,     1] loss: 0.0097069308\n",
      "[6472,     1] loss: 0.0097068697\n",
      "[6473,     1] loss: 0.0097068071\n",
      "[6474,     1] loss: 0.0097067460\n",
      "[6475,     1] loss: 0.0097066849\n",
      "[6476,     1] loss: 0.0097066224\n",
      "[6477,     1] loss: 0.0097065598\n",
      "[6478,     1] loss: 0.0097064979\n",
      "[6479,     1] loss: 0.0097064368\n",
      "[6480,     1] loss: 0.0097063743\n",
      "[6481,     1] loss: 0.0097063124\n",
      "[6482,     1] loss: 0.0097062506\n",
      "[6483,     1] loss: 0.0097061887\n",
      "[6484,     1] loss: 0.0097061269\n",
      "[6485,     1] loss: 0.0097060651\n",
      "[6486,     1] loss: 0.0097060025\n",
      "[6487,     1] loss: 0.0097059406\n",
      "[6488,     1] loss: 0.0097058780\n",
      "[6489,     1] loss: 0.0097058162\n",
      "[6490,     1] loss: 0.0097057536\n",
      "[6491,     1] loss: 0.0097056918\n",
      "[6492,     1] loss: 0.0097056299\n",
      "[6493,     1] loss: 0.0097055681\n",
      "[6494,     1] loss: 0.0097055070\n",
      "[6495,     1] loss: 0.0097054444\n",
      "[6496,     1] loss: 0.0097053811\n",
      "[6497,     1] loss: 0.0097053200\n",
      "[6498,     1] loss: 0.0097052582\n",
      "[6499,     1] loss: 0.0097051956\n",
      "[6500,     1] loss: 0.0097051330\n",
      "[6501,     1] loss: 0.0097050704\n",
      "[6502,     1] loss: 0.0097050093\n",
      "[6503,     1] loss: 0.0097049475\n",
      "[6504,     1] loss: 0.0097048841\n",
      "[6505,     1] loss: 0.0097048208\n",
      "[6506,     1] loss: 0.0097047605\n",
      "[6507,     1] loss: 0.0097046979\n",
      "[6508,     1] loss: 0.0097046353\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6509,     1] loss: 0.0097045735\n",
      "[6510,     1] loss: 0.0097045101\n",
      "[6511,     1] loss: 0.0097044490\n",
      "[6512,     1] loss: 0.0097043857\n",
      "[6513,     1] loss: 0.0097043231\n",
      "[6514,     1] loss: 0.0097042605\n",
      "[6515,     1] loss: 0.0097041987\n",
      "[6516,     1] loss: 0.0097041368\n",
      "[6517,     1] loss: 0.0097040743\n",
      "[6518,     1] loss: 0.0097040117\n",
      "[6519,     1] loss: 0.0097039498\n",
      "[6520,     1] loss: 0.0097038865\n",
      "[6521,     1] loss: 0.0097038247\n",
      "[6522,     1] loss: 0.0097037621\n",
      "[6523,     1] loss: 0.0097036988\n",
      "[6524,     1] loss: 0.0097036377\n",
      "[6525,     1] loss: 0.0097035743\n",
      "[6526,     1] loss: 0.0097035125\n",
      "[6527,     1] loss: 0.0097034492\n",
      "[6528,     1] loss: 0.0097033873\n",
      "[6529,     1] loss: 0.0097033247\n",
      "[6530,     1] loss: 0.0097032614\n",
      "[6531,     1] loss: 0.0097031996\n",
      "[6532,     1] loss: 0.0097031362\n",
      "[6533,     1] loss: 0.0097030744\n",
      "[6534,     1] loss: 0.0097030111\n",
      "[6535,     1] loss: 0.0097029492\n",
      "[6536,     1] loss: 0.0097028874\n",
      "[6537,     1] loss: 0.0097028233\n",
      "[6538,     1] loss: 0.0097027615\n",
      "[6539,     1] loss: 0.0097026989\n",
      "[6540,     1] loss: 0.0097026370\n",
      "[6541,     1] loss: 0.0097025722\n",
      "[6542,     1] loss: 0.0097025119\n",
      "[6543,     1] loss: 0.0097024485\n",
      "[6544,     1] loss: 0.0097023852\n",
      "[6545,     1] loss: 0.0097023234\n",
      "[6546,     1] loss: 0.0097022608\n",
      "[6547,     1] loss: 0.0097021975\n",
      "[6548,     1] loss: 0.0097021349\n",
      "[6549,     1] loss: 0.0097020730\n",
      "[6550,     1] loss: 0.0097020105\n",
      "[6551,     1] loss: 0.0097019471\n",
      "[6552,     1] loss: 0.0097018845\n",
      "[6553,     1] loss: 0.0097018220\n",
      "[6554,     1] loss: 0.0097017594\n",
      "[6555,     1] loss: 0.0097016953\n",
      "[6556,     1] loss: 0.0097016335\n",
      "[6557,     1] loss: 0.0097015709\n",
      "[6558,     1] loss: 0.0097015083\n",
      "[6559,     1] loss: 0.0097014450\n",
      "[6560,     1] loss: 0.0097013816\n",
      "[6561,     1] loss: 0.0097013190\n",
      "[6562,     1] loss: 0.0097012557\n",
      "[6563,     1] loss: 0.0097011939\n",
      "[6564,     1] loss: 0.0097011305\n",
      "[6565,     1] loss: 0.0097010687\n",
      "[6566,     1] loss: 0.0097010054\n",
      "[6567,     1] loss: 0.0097009420\n",
      "[6568,     1] loss: 0.0097008802\n",
      "[6569,     1] loss: 0.0097008169\n",
      "[6570,     1] loss: 0.0097007535\n",
      "[6571,     1] loss: 0.0097006902\n",
      "[6572,     1] loss: 0.0097006276\n",
      "[6573,     1] loss: 0.0097005650\n",
      "[6574,     1] loss: 0.0097005017\n",
      "[6575,     1] loss: 0.0097004391\n",
      "[6576,     1] loss: 0.0097003758\n",
      "[6577,     1] loss: 0.0097003132\n",
      "[6578,     1] loss: 0.0097002499\n",
      "[6579,     1] loss: 0.0097001880\n",
      "[6580,     1] loss: 0.0097001240\n",
      "[6581,     1] loss: 0.0097000606\n",
      "[6582,     1] loss: 0.0096999981\n",
      "[6583,     1] loss: 0.0096999340\n",
      "[6584,     1] loss: 0.0096998714\n",
      "[6585,     1] loss: 0.0096998081\n",
      "[6586,     1] loss: 0.0096997462\n",
      "[6587,     1] loss: 0.0096996829\n",
      "[6588,     1] loss: 0.0096996196\n",
      "[6589,     1] loss: 0.0096995562\n",
      "[6590,     1] loss: 0.0096994929\n",
      "[6591,     1] loss: 0.0096994303\n",
      "[6592,     1] loss: 0.0096993670\n",
      "[6593,     1] loss: 0.0096993037\n",
      "[6594,     1] loss: 0.0096992396\n",
      "[6595,     1] loss: 0.0096991770\n",
      "[6596,     1] loss: 0.0096991152\n",
      "[6597,     1] loss: 0.0096990518\n",
      "[6598,     1] loss: 0.0096989870\n",
      "[6599,     1] loss: 0.0096989267\n",
      "[6600,     1] loss: 0.0096988611\n",
      "[6601,     1] loss: 0.0096987963\n",
      "[6602,     1] loss: 0.0096987344\n",
      "[6603,     1] loss: 0.0096986711\n",
      "[6604,     1] loss: 0.0096986070\n",
      "[6605,     1] loss: 0.0096985452\n",
      "[6606,     1] loss: 0.0096984811\n",
      "[6607,     1] loss: 0.0096984185\n",
      "[6608,     1] loss: 0.0096983545\n",
      "[6609,     1] loss: 0.0096982904\n",
      "[6610,     1] loss: 0.0096982278\n",
      "[6611,     1] loss: 0.0096981637\n",
      "[6612,     1] loss: 0.0096980996\n",
      "[6613,     1] loss: 0.0096980371\n",
      "[6614,     1] loss: 0.0096979730\n",
      "[6615,     1] loss: 0.0096979104\n",
      "[6616,     1] loss: 0.0096978471\n",
      "[6617,     1] loss: 0.0096977845\n",
      "[6618,     1] loss: 0.0096977204\n",
      "[6619,     1] loss: 0.0096976563\n",
      "[6620,     1] loss: 0.0096975930\n",
      "[6621,     1] loss: 0.0096975304\n",
      "[6622,     1] loss: 0.0096974656\n",
      "[6623,     1] loss: 0.0096974030\n",
      "[6624,     1] loss: 0.0096973382\n",
      "[6625,     1] loss: 0.0096972749\n",
      "[6626,     1] loss: 0.0096972115\n",
      "[6627,     1] loss: 0.0096971467\n",
      "[6628,     1] loss: 0.0096970849\n",
      "[6629,     1] loss: 0.0096970201\n",
      "[6630,     1] loss: 0.0096969567\n",
      "[6631,     1] loss: 0.0096968934\n",
      "[6632,     1] loss: 0.0096968301\n",
      "[6633,     1] loss: 0.0096967667\n",
      "[6634,     1] loss: 0.0096967041\n",
      "[6635,     1] loss: 0.0096966386\n",
      "[6636,     1] loss: 0.0096965753\n",
      "[6637,     1] loss: 0.0096965119\n",
      "[6638,     1] loss: 0.0096964471\n",
      "[6639,     1] loss: 0.0096963838\n",
      "[6640,     1] loss: 0.0096963197\n",
      "[6641,     1] loss: 0.0096962564\n",
      "[6642,     1] loss: 0.0096961923\n",
      "[6643,     1] loss: 0.0096961297\n",
      "[6644,     1] loss: 0.0096960656\n",
      "[6645,     1] loss: 0.0096960008\n",
      "[6646,     1] loss: 0.0096959360\n",
      "[6647,     1] loss: 0.0096958742\n",
      "[6648,     1] loss: 0.0096958093\n",
      "[6649,     1] loss: 0.0096957460\n",
      "[6650,     1] loss: 0.0096956834\n",
      "[6651,     1] loss: 0.0096956186\n",
      "[6652,     1] loss: 0.0096955545\n",
      "[6653,     1] loss: 0.0096954904\n",
      "[6654,     1] loss: 0.0096954264\n",
      "[6655,     1] loss: 0.0096953630\n",
      "[6656,     1] loss: 0.0096952997\n",
      "[6657,     1] loss: 0.0096952364\n",
      "[6658,     1] loss: 0.0096951716\n",
      "[6659,     1] loss: 0.0096951075\n",
      "[6660,     1] loss: 0.0096950427\n",
      "[6661,     1] loss: 0.0096949801\n",
      "[6662,     1] loss: 0.0096949138\n",
      "[6663,     1] loss: 0.0096948504\n",
      "[6664,     1] loss: 0.0096947864\n",
      "[6665,     1] loss: 0.0096947238\n",
      "[6666,     1] loss: 0.0096946582\n",
      "[6667,     1] loss: 0.0096945941\n",
      "[6668,     1] loss: 0.0096945308\n",
      "[6669,     1] loss: 0.0096944667\n",
      "[6670,     1] loss: 0.0096944027\n",
      "[6671,     1] loss: 0.0096943393\n",
      "[6672,     1] loss: 0.0096942753\n",
      "[6673,     1] loss: 0.0096942104\n",
      "[6674,     1] loss: 0.0096941479\n",
      "[6675,     1] loss: 0.0096940823\n",
      "[6676,     1] loss: 0.0096940182\n",
      "[6677,     1] loss: 0.0096939541\n",
      "[6678,     1] loss: 0.0096938893\n",
      "[6679,     1] loss: 0.0096938245\n",
      "[6680,     1] loss: 0.0096937634\n",
      "[6681,     1] loss: 0.0096936963\n",
      "[6682,     1] loss: 0.0096936338\n",
      "[6683,     1] loss: 0.0096935689\n",
      "[6684,     1] loss: 0.0096935049\n",
      "[6685,     1] loss: 0.0096934393\n",
      "[6686,     1] loss: 0.0096933760\n",
      "[6687,     1] loss: 0.0096933112\n",
      "[6688,     1] loss: 0.0096932463\n",
      "[6689,     1] loss: 0.0096931823\n",
      "[6690,     1] loss: 0.0096931182\n",
      "[6691,     1] loss: 0.0096930549\n",
      "[6692,     1] loss: 0.0096929900\n",
      "[6693,     1] loss: 0.0096929252\n",
      "[6694,     1] loss: 0.0096928611\n",
      "[6695,     1] loss: 0.0096927956\n",
      "[6696,     1] loss: 0.0096927308\n",
      "[6697,     1] loss: 0.0096926674\n",
      "[6698,     1] loss: 0.0096926019\n",
      "[6699,     1] loss: 0.0096925378\n",
      "[6700,     1] loss: 0.0096924730\n",
      "[6701,     1] loss: 0.0096924096\n",
      "[6702,     1] loss: 0.0096923456\n",
      "[6703,     1] loss: 0.0096922807\n",
      "[6704,     1] loss: 0.0096922152\n",
      "[6705,     1] loss: 0.0096921511\n",
      "[6706,     1] loss: 0.0096920863\n",
      "[6707,     1] loss: 0.0096920229\n",
      "[6708,     1] loss: 0.0096919581\n",
      "[6709,     1] loss: 0.0096918941\n",
      "[6710,     1] loss: 0.0096918292\n",
      "[6711,     1] loss: 0.0096917644\n",
      "[6712,     1] loss: 0.0096917003\n",
      "[6713,     1] loss: 0.0096916348\n",
      "[6714,     1] loss: 0.0096915700\n",
      "[6715,     1] loss: 0.0096915051\n",
      "[6716,     1] loss: 0.0096914411\n",
      "[6717,     1] loss: 0.0096913762\n",
      "[6718,     1] loss: 0.0096913107\n",
      "[6719,     1] loss: 0.0096912466\n",
      "[6720,     1] loss: 0.0096911840\n",
      "[6721,     1] loss: 0.0096911177\n",
      "[6722,     1] loss: 0.0096910514\n",
      "[6723,     1] loss: 0.0096909881\n",
      "[6724,     1] loss: 0.0096909240\n",
      "[6725,     1] loss: 0.0096908584\n",
      "[6726,     1] loss: 0.0096907936\n",
      "[6727,     1] loss: 0.0096907288\n",
      "[6728,     1] loss: 0.0096906640\n",
      "[6729,     1] loss: 0.0096905991\n",
      "[6730,     1] loss: 0.0096905343\n",
      "[6731,     1] loss: 0.0096904688\n",
      "[6732,     1] loss: 0.0096904047\n",
      "[6733,     1] loss: 0.0096903391\n",
      "[6734,     1] loss: 0.0096902743\n",
      "[6735,     1] loss: 0.0096902102\n",
      "[6736,     1] loss: 0.0096901447\n",
      "[6737,     1] loss: 0.0096900798\n",
      "[6738,     1] loss: 0.0096900135\n",
      "[6739,     1] loss: 0.0096899487\n",
      "[6740,     1] loss: 0.0096898839\n",
      "[6741,     1] loss: 0.0096898191\n",
      "[6742,     1] loss: 0.0096897535\n",
      "[6743,     1] loss: 0.0096896894\n",
      "[6744,     1] loss: 0.0096896239\n",
      "[6745,     1] loss: 0.0096895590\n",
      "[6746,     1] loss: 0.0096894942\n",
      "[6747,     1] loss: 0.0096894287\n",
      "[6748,     1] loss: 0.0096893638\n",
      "[6749,     1] loss: 0.0096892990\n",
      "[6750,     1] loss: 0.0096892342\n",
      "[6751,     1] loss: 0.0096891686\n",
      "[6752,     1] loss: 0.0096891031\n",
      "[6753,     1] loss: 0.0096890382\n",
      "[6754,     1] loss: 0.0096889727\n",
      "[6755,     1] loss: 0.0096889079\n",
      "[6756,     1] loss: 0.0096888430\n",
      "[6757,     1] loss: 0.0096887775\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6758,     1] loss: 0.0096887127\n",
      "[6759,     1] loss: 0.0096886486\n",
      "[6760,     1] loss: 0.0096885830\n",
      "[6761,     1] loss: 0.0096885160\n",
      "[6762,     1] loss: 0.0096884511\n",
      "[6763,     1] loss: 0.0096883856\n",
      "[6764,     1] loss: 0.0096883185\n",
      "[6765,     1] loss: 0.0096882559\n",
      "[6766,     1] loss: 0.0096881904\n",
      "[6767,     1] loss: 0.0096881241\n",
      "[6768,     1] loss: 0.0096880585\n",
      "[6769,     1] loss: 0.0096879937\n",
      "[6770,     1] loss: 0.0096879289\n",
      "[6771,     1] loss: 0.0096878625\n",
      "[6772,     1] loss: 0.0096877977\n",
      "[6773,     1] loss: 0.0096877329\n",
      "[6774,     1] loss: 0.0096876673\n",
      "[6775,     1] loss: 0.0096876018\n",
      "[6776,     1] loss: 0.0096875355\n",
      "[6777,     1] loss: 0.0096874714\n",
      "[6778,     1] loss: 0.0096874051\n",
      "[6779,     1] loss: 0.0096873395\n",
      "[6780,     1] loss: 0.0096872732\n",
      "[6781,     1] loss: 0.0096872084\n",
      "[6782,     1] loss: 0.0096871436\n",
      "[6783,     1] loss: 0.0096870765\n",
      "[6784,     1] loss: 0.0096870109\n",
      "[6785,     1] loss: 0.0096869469\n",
      "[6786,     1] loss: 0.0096868806\n",
      "[6787,     1] loss: 0.0096868157\n",
      "[6788,     1] loss: 0.0096867494\n",
      "[6789,     1] loss: 0.0096866839\n",
      "[6790,     1] loss: 0.0096866176\n",
      "[6791,     1] loss: 0.0096865520\n",
      "[6792,     1] loss: 0.0096864864\n",
      "[6793,     1] loss: 0.0096864201\n",
      "[6794,     1] loss: 0.0096863545\n",
      "[6795,     1] loss: 0.0096862905\n",
      "[6796,     1] loss: 0.0096862234\n",
      "[6797,     1] loss: 0.0096861586\n",
      "[6798,     1] loss: 0.0096860915\n",
      "[6799,     1] loss: 0.0096860267\n",
      "[6800,     1] loss: 0.0096859597\n",
      "[6801,     1] loss: 0.0096858948\n",
      "[6802,     1] loss: 0.0096858293\n",
      "[6803,     1] loss: 0.0096857637\n",
      "[6804,     1] loss: 0.0096856974\n",
      "[6805,     1] loss: 0.0096856311\n",
      "[6806,     1] loss: 0.0096855655\n",
      "[6807,     1] loss: 0.0096854992\n",
      "[6808,     1] loss: 0.0096854337\n",
      "[6809,     1] loss: 0.0096853681\n",
      "[6810,     1] loss: 0.0096853018\n",
      "[6811,     1] loss: 0.0096852362\n",
      "[6812,     1] loss: 0.0096851699\n",
      "[6813,     1] loss: 0.0096851029\n",
      "[6814,     1] loss: 0.0096850380\n",
      "[6815,     1] loss: 0.0096849717\n",
      "[6816,     1] loss: 0.0096849054\n",
      "[6817,     1] loss: 0.0096848391\n",
      "[6818,     1] loss: 0.0096847728\n",
      "[6819,     1] loss: 0.0096847080\n",
      "[6820,     1] loss: 0.0096846431\n",
      "[6821,     1] loss: 0.0096845761\n",
      "[6822,     1] loss: 0.0096845090\n",
      "[6823,     1] loss: 0.0096844435\n",
      "[6824,     1] loss: 0.0096843772\n",
      "[6825,     1] loss: 0.0096843109\n",
      "[6826,     1] loss: 0.0096842453\n",
      "[6827,     1] loss: 0.0096841790\n",
      "[6828,     1] loss: 0.0096841119\n",
      "[6829,     1] loss: 0.0096840464\n",
      "[6830,     1] loss: 0.0096839815\n",
      "[6831,     1] loss: 0.0096839145\n",
      "[6832,     1] loss: 0.0096838474\n",
      "[6833,     1] loss: 0.0096837819\n",
      "[6834,     1] loss: 0.0096837156\n",
      "[6835,     1] loss: 0.0096836485\n",
      "[6836,     1] loss: 0.0096835837\n",
      "[6837,     1] loss: 0.0096835166\n",
      "[6838,     1] loss: 0.0096834496\n",
      "[6839,     1] loss: 0.0096833833\n",
      "[6840,     1] loss: 0.0096833162\n",
      "[6841,     1] loss: 0.0096832514\n",
      "[6842,     1] loss: 0.0096831836\n",
      "[6843,     1] loss: 0.0096831173\n",
      "[6844,     1] loss: 0.0096830510\n",
      "[6845,     1] loss: 0.0096829854\n",
      "[6846,     1] loss: 0.0096829191\n",
      "[6847,     1] loss: 0.0096828528\n",
      "[6848,     1] loss: 0.0096827865\n",
      "[6849,     1] loss: 0.0096827202\n",
      "[6850,     1] loss: 0.0096826524\n",
      "[6851,     1] loss: 0.0096825846\n",
      "[6852,     1] loss: 0.0096825190\n",
      "[6853,     1] loss: 0.0096824534\n",
      "[6854,     1] loss: 0.0096823864\n",
      "[6855,     1] loss: 0.0096823193\n",
      "[6856,     1] loss: 0.0096822530\n",
      "[6857,     1] loss: 0.0096821859\n",
      "[6858,     1] loss: 0.0096821196\n",
      "[6859,     1] loss: 0.0096820541\n",
      "[6860,     1] loss: 0.0096819863\n",
      "[6861,     1] loss: 0.0096819192\n",
      "[6862,     1] loss: 0.0096818529\n",
      "[6863,     1] loss: 0.0096817859\n",
      "[6864,     1] loss: 0.0096817195\n",
      "[6865,     1] loss: 0.0096816517\n",
      "[6866,     1] loss: 0.0096815869\n",
      "[6867,     1] loss: 0.0096815206\n",
      "[6868,     1] loss: 0.0096814528\n",
      "[6869,     1] loss: 0.0096813865\n",
      "[6870,     1] loss: 0.0096813180\n",
      "[6871,     1] loss: 0.0096812531\n",
      "[6872,     1] loss: 0.0096811868\n",
      "[6873,     1] loss: 0.0096811198\n",
      "[6874,     1] loss: 0.0096810527\n",
      "[6875,     1] loss: 0.0096809857\n",
      "[6876,     1] loss: 0.0096809193\n",
      "[6877,     1] loss: 0.0096808508\n",
      "[6878,     1] loss: 0.0096807852\n",
      "[6879,     1] loss: 0.0096807182\n",
      "[6880,     1] loss: 0.0096806504\n",
      "[6881,     1] loss: 0.0096805841\n",
      "[6882,     1] loss: 0.0096805178\n",
      "[6883,     1] loss: 0.0096804507\n",
      "[6884,     1] loss: 0.0096803829\n",
      "[6885,     1] loss: 0.0096803166\n",
      "[6886,     1] loss: 0.0096802481\n",
      "[6887,     1] loss: 0.0096801825\n",
      "[6888,     1] loss: 0.0096801154\n",
      "[6889,     1] loss: 0.0096800476\n",
      "[6890,     1] loss: 0.0096799813\n",
      "[6891,     1] loss: 0.0096799143\n",
      "[6892,     1] loss: 0.0096798457\n",
      "[6893,     1] loss: 0.0096797802\n",
      "[6894,     1] loss: 0.0096797138\n",
      "[6895,     1] loss: 0.0096796453\n",
      "[6896,     1] loss: 0.0096795782\n",
      "[6897,     1] loss: 0.0096795104\n",
      "[6898,     1] loss: 0.0096794449\n",
      "[6899,     1] loss: 0.0096793778\n",
      "[6900,     1] loss: 0.0096793108\n",
      "[6901,     1] loss: 0.0096792430\n",
      "[6902,     1] loss: 0.0096791752\n",
      "[6903,     1] loss: 0.0096791081\n",
      "[6904,     1] loss: 0.0096790411\n",
      "[6905,     1] loss: 0.0096789733\n",
      "[6906,     1] loss: 0.0096789055\n",
      "[6907,     1] loss: 0.0096788391\n",
      "[6908,     1] loss: 0.0096787721\n",
      "[6909,     1] loss: 0.0096787050\n",
      "[6910,     1] loss: 0.0096786380\n",
      "[6911,     1] loss: 0.0096785694\n",
      "[6912,     1] loss: 0.0096785031\n",
      "[6913,     1] loss: 0.0096784361\n",
      "[6914,     1] loss: 0.0096783675\n",
      "[6915,     1] loss: 0.0096783012\n",
      "[6916,     1] loss: 0.0096782334\n",
      "[6917,     1] loss: 0.0096781664\n",
      "[6918,     1] loss: 0.0096780986\n",
      "[6919,     1] loss: 0.0096780315\n",
      "[6920,     1] loss: 0.0096779637\n",
      "[6921,     1] loss: 0.0096778966\n",
      "[6922,     1] loss: 0.0096778288\n",
      "[6923,     1] loss: 0.0096777625\n",
      "[6924,     1] loss: 0.0096776940\n",
      "[6925,     1] loss: 0.0096776262\n",
      "[6926,     1] loss: 0.0096775591\n",
      "[6927,     1] loss: 0.0096774906\n",
      "[6928,     1] loss: 0.0096774228\n",
      "[6929,     1] loss: 0.0096773550\n",
      "[6930,     1] loss: 0.0096772879\n",
      "[6931,     1] loss: 0.0096772209\n",
      "[6932,     1] loss: 0.0096771523\n",
      "[6933,     1] loss: 0.0096770853\n",
      "[6934,     1] loss: 0.0096770175\n",
      "[6935,     1] loss: 0.0096769497\n",
      "[6936,     1] loss: 0.0096768811\n",
      "[6937,     1] loss: 0.0096768141\n",
      "[6938,     1] loss: 0.0096767463\n",
      "[6939,     1] loss: 0.0096766792\n",
      "[6940,     1] loss: 0.0096766107\n",
      "[6941,     1] loss: 0.0096765429\n",
      "[6942,     1] loss: 0.0096764758\n",
      "[6943,     1] loss: 0.0096764088\n",
      "[6944,     1] loss: 0.0096763417\n",
      "[6945,     1] loss: 0.0096762724\n",
      "[6946,     1] loss: 0.0096762039\n",
      "[6947,     1] loss: 0.0096761368\n",
      "[6948,     1] loss: 0.0096760683\n",
      "[6949,     1] loss: 0.0096760012\n",
      "[6950,     1] loss: 0.0096759327\n",
      "[6951,     1] loss: 0.0096758649\n",
      "[6952,     1] loss: 0.0096757978\n",
      "[6953,     1] loss: 0.0096757293\n",
      "[6954,     1] loss: 0.0096756615\n",
      "[6955,     1] loss: 0.0096755929\n",
      "[6956,     1] loss: 0.0096755251\n",
      "[6957,     1] loss: 0.0096754566\n",
      "[6958,     1] loss: 0.0096753888\n",
      "[6959,     1] loss: 0.0096753217\n",
      "[6960,     1] loss: 0.0096752532\n",
      "[6961,     1] loss: 0.0096751854\n",
      "[6962,     1] loss: 0.0096751161\n",
      "[6963,     1] loss: 0.0096750483\n",
      "[6964,     1] loss: 0.0096749812\n",
      "[6965,     1] loss: 0.0096749119\n",
      "[6966,     1] loss: 0.0096748449\n",
      "[6967,     1] loss: 0.0096747756\n",
      "[6968,     1] loss: 0.0096747078\n",
      "[6969,     1] loss: 0.0096746393\n",
      "[6970,     1] loss: 0.0096745729\n",
      "[6971,     1] loss: 0.0096745037\n",
      "[6972,     1] loss: 0.0096744344\n",
      "[6973,     1] loss: 0.0096743673\n",
      "[6974,     1] loss: 0.0096742995\n",
      "[6975,     1] loss: 0.0096742302\n",
      "[6976,     1] loss: 0.0096741632\n",
      "[6977,     1] loss: 0.0096740939\n",
      "[6978,     1] loss: 0.0096740261\n",
      "[6979,     1] loss: 0.0096739575\n",
      "[6980,     1] loss: 0.0096738882\n",
      "[6981,     1] loss: 0.0096738197\n",
      "[6982,     1] loss: 0.0096737526\n",
      "[6983,     1] loss: 0.0096736833\n",
      "[6984,     1] loss: 0.0096736155\n",
      "[6985,     1] loss: 0.0096735470\n",
      "[6986,     1] loss: 0.0096734792\n",
      "[6987,     1] loss: 0.0096734099\n",
      "[6988,     1] loss: 0.0096733421\n",
      "[6989,     1] loss: 0.0096732736\n",
      "[6990,     1] loss: 0.0096732050\n",
      "[6991,     1] loss: 0.0096731357\n",
      "[6992,     1] loss: 0.0096730687\n",
      "[6993,     1] loss: 0.0096730009\n",
      "[6994,     1] loss: 0.0096729308\n",
      "[6995,     1] loss: 0.0096728623\n",
      "[6996,     1] loss: 0.0096727930\n",
      "[6997,     1] loss: 0.0096727259\n",
      "[6998,     1] loss: 0.0096726567\n",
      "[6999,     1] loss: 0.0096725896\n",
      "[7000,     1] loss: 0.0096725196\n",
      "[7001,     1] loss: 0.0096724518\n",
      "[7002,     1] loss: 0.0096723832\n",
      "[7003,     1] loss: 0.0096723139\n",
      "[7004,     1] loss: 0.0096722454\n",
      "[7005,     1] loss: 0.0096721776\n",
      "[7006,     1] loss: 0.0096721090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7007,     1] loss: 0.0096720397\n",
      "[7008,     1] loss: 0.0096719705\n",
      "[7009,     1] loss: 0.0096719012\n",
      "[7010,     1] loss: 0.0096718341\n",
      "[7011,     1] loss: 0.0096717648\n",
      "[7012,     1] loss: 0.0096716955\n",
      "[7013,     1] loss: 0.0096716262\n",
      "[7014,     1] loss: 0.0096715569\n",
      "[7015,     1] loss: 0.0096714891\n",
      "[7016,     1] loss: 0.0096714206\n",
      "[7017,     1] loss: 0.0096713521\n",
      "[7018,     1] loss: 0.0096712828\n",
      "[7019,     1] loss: 0.0096712135\n",
      "[7020,     1] loss: 0.0096711449\n",
      "[7021,     1] loss: 0.0096710764\n",
      "[7022,     1] loss: 0.0096710078\n",
      "[7023,     1] loss: 0.0096709378\n",
      "[7024,     1] loss: 0.0096708693\n",
      "[7025,     1] loss: 0.0096707985\n",
      "[7026,     1] loss: 0.0096707299\n",
      "[7027,     1] loss: 0.0096706621\n",
      "[7028,     1] loss: 0.0096705928\n",
      "[7029,     1] loss: 0.0096705236\n",
      "[7030,     1] loss: 0.0096704550\n",
      "[7031,     1] loss: 0.0096703857\n",
      "[7032,     1] loss: 0.0096703164\n",
      "[7033,     1] loss: 0.0096702471\n",
      "[7034,     1] loss: 0.0096701786\n",
      "[7035,     1] loss: 0.0096701100\n",
      "[7036,     1] loss: 0.0096700408\n",
      "[7037,     1] loss: 0.0096699722\n",
      "[7038,     1] loss: 0.0096699014\n",
      "[7039,     1] loss: 0.0096698336\n",
      "[7040,     1] loss: 0.0096697628\n",
      "[7041,     1] loss: 0.0096696936\n",
      "[7042,     1] loss: 0.0096696258\n",
      "[7043,     1] loss: 0.0096695557\n",
      "[7044,     1] loss: 0.0096694857\n",
      "[7045,     1] loss: 0.0096694179\n",
      "[7046,     1] loss: 0.0096693486\n",
      "[7047,     1] loss: 0.0096692793\n",
      "[7048,     1] loss: 0.0096692100\n",
      "[7049,     1] loss: 0.0096691400\n",
      "[7050,     1] loss: 0.0096690699\n",
      "[7051,     1] loss: 0.0096690014\n",
      "[7052,     1] loss: 0.0096689314\n",
      "[7053,     1] loss: 0.0096688628\n",
      "[7054,     1] loss: 0.0096687935\n",
      "[7055,     1] loss: 0.0096687250\n",
      "[7056,     1] loss: 0.0096686557\n",
      "[7057,     1] loss: 0.0096685857\n",
      "[7058,     1] loss: 0.0096685171\n",
      "[7059,     1] loss: 0.0096684478\n",
      "[7060,     1] loss: 0.0096683770\n",
      "[7061,     1] loss: 0.0096683070\n",
      "[7062,     1] loss: 0.0096682385\n",
      "[7063,     1] loss: 0.0096681684\n",
      "[7064,     1] loss: 0.0096680999\n",
      "[7065,     1] loss: 0.0096680291\n",
      "[7066,     1] loss: 0.0096679598\n",
      "[7067,     1] loss: 0.0096678905\n",
      "[7068,     1] loss: 0.0096678205\n",
      "[7069,     1] loss: 0.0096677512\n",
      "[7070,     1] loss: 0.0096676819\n",
      "[7071,     1] loss: 0.0096676126\n",
      "[7072,     1] loss: 0.0096675418\n",
      "[7073,     1] loss: 0.0096674733\n",
      "[7074,     1] loss: 0.0096674033\n",
      "[7075,     1] loss: 0.0096673332\n",
      "[7076,     1] loss: 0.0096672647\n",
      "[7077,     1] loss: 0.0096671946\n",
      "[7078,     1] loss: 0.0096671246\n",
      "[7079,     1] loss: 0.0096670553\n",
      "[7080,     1] loss: 0.0096669845\n",
      "[7081,     1] loss: 0.0096669145\n",
      "[7082,     1] loss: 0.0096668452\n",
      "[7083,     1] loss: 0.0096667752\n",
      "[7084,     1] loss: 0.0096667051\n",
      "[7085,     1] loss: 0.0096666358\n",
      "[7086,     1] loss: 0.0096665666\n",
      "[7087,     1] loss: 0.0096664965\n",
      "[7088,     1] loss: 0.0096664272\n",
      "[7089,     1] loss: 0.0096663564\n",
      "[7090,     1] loss: 0.0096662879\n",
      "[7091,     1] loss: 0.0096662164\n",
      "[7092,     1] loss: 0.0096661471\n",
      "[7093,     1] loss: 0.0096660778\n",
      "[7094,     1] loss: 0.0096660078\n",
      "[7095,     1] loss: 0.0096659377\n",
      "[7096,     1] loss: 0.0096658669\n",
      "[7097,     1] loss: 0.0096657969\n",
      "[7098,     1] loss: 0.0096657269\n",
      "[7099,     1] loss: 0.0096656583\n",
      "[7100,     1] loss: 0.0096655868\n",
      "[7101,     1] loss: 0.0096655175\n",
      "[7102,     1] loss: 0.0096654475\n",
      "[7103,     1] loss: 0.0096653767\n",
      "[7104,     1] loss: 0.0096653074\n",
      "[7105,     1] loss: 0.0096652366\n",
      "[7106,     1] loss: 0.0096651666\n",
      "[7107,     1] loss: 0.0096650951\n",
      "[7108,     1] loss: 0.0096650265\n",
      "[7109,     1] loss: 0.0096649565\n",
      "[7110,     1] loss: 0.0096648864\n",
      "[7111,     1] loss: 0.0096648164\n",
      "[7112,     1] loss: 0.0096647456\n",
      "[7113,     1] loss: 0.0096646756\n",
      "[7114,     1] loss: 0.0096646056\n",
      "[7115,     1] loss: 0.0096645355\n",
      "[7116,     1] loss: 0.0096644640\n",
      "[7117,     1] loss: 0.0096643940\n",
      "[7118,     1] loss: 0.0096643247\n",
      "[7119,     1] loss: 0.0096642524\n",
      "[7120,     1] loss: 0.0096641824\n",
      "[7121,     1] loss: 0.0096641131\n",
      "[7122,     1] loss: 0.0096640438\n",
      "[7123,     1] loss: 0.0096639737\n",
      "[7124,     1] loss: 0.0096639022\n",
      "[7125,     1] loss: 0.0096638314\n",
      "[7126,     1] loss: 0.0096637614\n",
      "[7127,     1] loss: 0.0096636899\n",
      "[7128,     1] loss: 0.0096636206\n",
      "[7129,     1] loss: 0.0096635498\n",
      "[7130,     1] loss: 0.0096634790\n",
      "[7131,     1] loss: 0.0096634090\n",
      "[7132,     1] loss: 0.0096633382\n",
      "[7133,     1] loss: 0.0096632674\n",
      "[7134,     1] loss: 0.0096631967\n",
      "[7135,     1] loss: 0.0096631274\n",
      "[7136,     1] loss: 0.0096630558\n",
      "[7137,     1] loss: 0.0096629865\n",
      "[7138,     1] loss: 0.0096629158\n",
      "[7139,     1] loss: 0.0096628442\n",
      "[7140,     1] loss: 0.0096627735\n",
      "[7141,     1] loss: 0.0096627027\n",
      "[7142,     1] loss: 0.0096626319\n",
      "[7143,     1] loss: 0.0096625611\n",
      "[7144,     1] loss: 0.0096624903\n",
      "[7145,     1] loss: 0.0096624210\n",
      "[7146,     1] loss: 0.0096623518\n",
      "[7147,     1] loss: 0.0096622787\n",
      "[7148,     1] loss: 0.0096622087\n",
      "[7149,     1] loss: 0.0096621379\n",
      "[7150,     1] loss: 0.0096620664\n",
      "[7151,     1] loss: 0.0096619964\n",
      "[7152,     1] loss: 0.0096619248\n",
      "[7153,     1] loss: 0.0096618548\n",
      "[7154,     1] loss: 0.0096617848\n",
      "[7155,     1] loss: 0.0096617132\n",
      "[7156,     1] loss: 0.0096616425\n",
      "[7157,     1] loss: 0.0096615717\n",
      "[7158,     1] loss: 0.0096615016\n",
      "[7159,     1] loss: 0.0096614301\n",
      "[7160,     1] loss: 0.0096613601\n",
      "[7161,     1] loss: 0.0096612893\n",
      "[7162,     1] loss: 0.0096612178\n",
      "[7163,     1] loss: 0.0096611455\n",
      "[7164,     1] loss: 0.0096610755\n",
      "[7165,     1] loss: 0.0096610047\n",
      "[7166,     1] loss: 0.0096609324\n",
      "[7167,     1] loss: 0.0096608624\n",
      "[7168,     1] loss: 0.0096607916\n",
      "[7169,     1] loss: 0.0096607208\n",
      "[7170,     1] loss: 0.0096606500\n",
      "[7171,     1] loss: 0.0096605778\n",
      "[7172,     1] loss: 0.0096605070\n",
      "[7173,     1] loss: 0.0096604370\n",
      "[7174,     1] loss: 0.0096603647\n",
      "[7175,     1] loss: 0.0096602939\n",
      "[7176,     1] loss: 0.0096602246\n",
      "[7177,     1] loss: 0.0096601523\n",
      "[7178,     1] loss: 0.0096600808\n",
      "[7179,     1] loss: 0.0096600100\n",
      "[7180,     1] loss: 0.0096599385\n",
      "[7181,     1] loss: 0.0096598677\n",
      "[7182,     1] loss: 0.0096597970\n",
      "[7183,     1] loss: 0.0096597254\n",
      "[7184,     1] loss: 0.0096596539\n",
      "[7185,     1] loss: 0.0096595831\n",
      "[7186,     1] loss: 0.0096595116\n",
      "[7187,     1] loss: 0.0096594401\n",
      "[7188,     1] loss: 0.0096593685\n",
      "[7189,     1] loss: 0.0096592978\n",
      "[7190,     1] loss: 0.0096592262\n",
      "[7191,     1] loss: 0.0096591540\n",
      "[7192,     1] loss: 0.0096590839\n",
      "[7193,     1] loss: 0.0096590117\n",
      "[7194,     1] loss: 0.0096589416\n",
      "[7195,     1] loss: 0.0096588701\n",
      "[7196,     1] loss: 0.0096587986\n",
      "[7197,     1] loss: 0.0096587270\n",
      "[7198,     1] loss: 0.0096586555\n",
      "[7199,     1] loss: 0.0096585840\n",
      "[7200,     1] loss: 0.0096585125\n",
      "[7201,     1] loss: 0.0096584417\n",
      "[7202,     1] loss: 0.0096583702\n",
      "[7203,     1] loss: 0.0096582979\n",
      "[7204,     1] loss: 0.0096582271\n",
      "[7205,     1] loss: 0.0096581556\n",
      "[7206,     1] loss: 0.0096580833\n",
      "[7207,     1] loss: 0.0096580118\n",
      "[7208,     1] loss: 0.0096579410\n",
      "[7209,     1] loss: 0.0096578687\n",
      "[7210,     1] loss: 0.0096577987\n",
      "[7211,     1] loss: 0.0096577257\n",
      "[7212,     1] loss: 0.0096576549\n",
      "[7213,     1] loss: 0.0096575826\n",
      "[7214,     1] loss: 0.0096575119\n",
      "[7215,     1] loss: 0.0096574396\n",
      "[7216,     1] loss: 0.0096573681\n",
      "[7217,     1] loss: 0.0096572965\n",
      "[7218,     1] loss: 0.0096572250\n",
      "[7219,     1] loss: 0.0096571527\n",
      "[7220,     1] loss: 0.0096570820\n",
      "[7221,     1] loss: 0.0096570089\n",
      "[7222,     1] loss: 0.0096569382\n",
      "[7223,     1] loss: 0.0096568659\n",
      "[7224,     1] loss: 0.0096567936\n",
      "[7225,     1] loss: 0.0096567221\n",
      "[7226,     1] loss: 0.0096566506\n",
      "[7227,     1] loss: 0.0096565790\n",
      "[7228,     1] loss: 0.0096565068\n",
      "[7229,     1] loss: 0.0096564360\n",
      "[7230,     1] loss: 0.0096563637\n",
      "[7231,     1] loss: 0.0096562915\n",
      "[7232,     1] loss: 0.0096562199\n",
      "[7233,     1] loss: 0.0096561477\n",
      "[7234,     1] loss: 0.0096560746\n",
      "[7235,     1] loss: 0.0096560039\n",
      "[7236,     1] loss: 0.0096559331\n",
      "[7237,     1] loss: 0.0096558593\n",
      "[7238,     1] loss: 0.0096557885\n",
      "[7239,     1] loss: 0.0096557163\n",
      "[7240,     1] loss: 0.0096556447\n",
      "[7241,     1] loss: 0.0096555725\n",
      "[7242,     1] loss: 0.0096555009\n",
      "[7243,     1] loss: 0.0096554279\n",
      "[7244,     1] loss: 0.0096553549\n",
      "[7245,     1] loss: 0.0096552841\n",
      "[7246,     1] loss: 0.0096552104\n",
      "[7247,     1] loss: 0.0096551396\n",
      "[7248,     1] loss: 0.0096550681\n",
      "[7249,     1] loss: 0.0096549951\n",
      "[7250,     1] loss: 0.0096549228\n",
      "[7251,     1] loss: 0.0096548520\n",
      "[7252,     1] loss: 0.0096547797\n",
      "[7253,     1] loss: 0.0096547060\n",
      "[7254,     1] loss: 0.0096546337\n",
      "[7255,     1] loss: 0.0096545622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7256,     1] loss: 0.0096544906\n",
      "[7257,     1] loss: 0.0096544176\n",
      "[7258,     1] loss: 0.0096543454\n",
      "[7259,     1] loss: 0.0096542738\n",
      "[7260,     1] loss: 0.0096542008\n",
      "[7261,     1] loss: 0.0096541278\n",
      "[7262,     1] loss: 0.0096540570\n",
      "[7263,     1] loss: 0.0096539840\n",
      "[7264,     1] loss: 0.0096539110\n",
      "[7265,     1] loss: 0.0096538395\n",
      "[7266,     1] loss: 0.0096537657\n",
      "[7267,     1] loss: 0.0096536942\n",
      "[7268,     1] loss: 0.0096536212\n",
      "[7269,     1] loss: 0.0096535489\n",
      "[7270,     1] loss: 0.0096534766\n",
      "[7271,     1] loss: 0.0096534051\n",
      "[7272,     1] loss: 0.0096533321\n",
      "[7273,     1] loss: 0.0096532598\n",
      "[7274,     1] loss: 0.0096531868\n",
      "[7275,     1] loss: 0.0096531153\n",
      "[7276,     1] loss: 0.0096530423\n",
      "[7277,     1] loss: 0.0096529700\n",
      "[7278,     1] loss: 0.0096528970\n",
      "[7279,     1] loss: 0.0096528247\n",
      "[7280,     1] loss: 0.0096527532\n",
      "[7281,     1] loss: 0.0096526794\n",
      "[7282,     1] loss: 0.0096526079\n",
      "[7283,     1] loss: 0.0096525356\n",
      "[7284,     1] loss: 0.0096524626\n",
      "[7285,     1] loss: 0.0096523903\n",
      "[7286,     1] loss: 0.0096523173\n",
      "[7287,     1] loss: 0.0096522443\n",
      "[7288,     1] loss: 0.0096521720\n",
      "[7289,     1] loss: 0.0096520990\n",
      "[7290,     1] loss: 0.0096520275\n",
      "[7291,     1] loss: 0.0096519537\n",
      "[7292,     1] loss: 0.0096518807\n",
      "[7293,     1] loss: 0.0096518092\n",
      "[7294,     1] loss: 0.0096517362\n",
      "[7295,     1] loss: 0.0096516632\n",
      "[7296,     1] loss: 0.0096515901\n",
      "[7297,     1] loss: 0.0096515179\n",
      "[7298,     1] loss: 0.0096514449\n",
      "[7299,     1] loss: 0.0096513726\n",
      "[7300,     1] loss: 0.0096512988\n",
      "[7301,     1] loss: 0.0096512258\n",
      "[7302,     1] loss: 0.0096511535\n",
      "[7303,     1] loss: 0.0096510813\n",
      "[7304,     1] loss: 0.0096510082\n",
      "[7305,     1] loss: 0.0096509345\n",
      "[7306,     1] loss: 0.0096508637\n",
      "[7307,     1] loss: 0.0096507892\n",
      "[7308,     1] loss: 0.0096507162\n",
      "[7309,     1] loss: 0.0096506432\n",
      "[7310,     1] loss: 0.0096505702\n",
      "[7311,     1] loss: 0.0096504971\n",
      "[7312,     1] loss: 0.0096504256\n",
      "[7313,     1] loss: 0.0096503541\n",
      "[7314,     1] loss: 0.0096502796\n",
      "[7315,     1] loss: 0.0096502066\n",
      "[7316,     1] loss: 0.0096501328\n",
      "[7317,     1] loss: 0.0096500590\n",
      "[7318,     1] loss: 0.0096499860\n",
      "[7319,     1] loss: 0.0096499145\n",
      "[7320,     1] loss: 0.0096498407\n",
      "[7321,     1] loss: 0.0096497677\n",
      "[7322,     1] loss: 0.0096496947\n",
      "[7323,     1] loss: 0.0096496217\n",
      "[7324,     1] loss: 0.0096495487\n",
      "[7325,     1] loss: 0.0096494742\n",
      "[7326,     1] loss: 0.0096494026\n",
      "[7327,     1] loss: 0.0096493274\n",
      "[7328,     1] loss: 0.0096492551\n",
      "[7329,     1] loss: 0.0096491829\n",
      "[7330,     1] loss: 0.0096491098\n",
      "[7331,     1] loss: 0.0096490361\n",
      "[7332,     1] loss: 0.0096489623\n",
      "[7333,     1] loss: 0.0096488893\n",
      "[7334,     1] loss: 0.0096488163\n",
      "[7335,     1] loss: 0.0096487433\n",
      "[7336,     1] loss: 0.0096486695\n",
      "[7337,     1] loss: 0.0096485972\n",
      "[7338,     1] loss: 0.0096485235\n",
      "[7339,     1] loss: 0.0096484490\n",
      "[7340,     1] loss: 0.0096483774\n",
      "[7341,     1] loss: 0.0096483029\n",
      "[7342,     1] loss: 0.0096482299\n",
      "[7343,     1] loss: 0.0096481562\n",
      "[7344,     1] loss: 0.0096480832\n",
      "[7345,     1] loss: 0.0096480094\n",
      "[7346,     1] loss: 0.0096479371\n",
      "[7347,     1] loss: 0.0096478626\n",
      "[7348,     1] loss: 0.0096477903\n",
      "[7349,     1] loss: 0.0096477173\n",
      "[7350,     1] loss: 0.0096476443\n",
      "[7351,     1] loss: 0.0096475698\n",
      "[7352,     1] loss: 0.0096474975\n",
      "[7353,     1] loss: 0.0096474230\n",
      "[7354,     1] loss: 0.0096473485\n",
      "[7355,     1] loss: 0.0096472763\n",
      "[7356,     1] loss: 0.0096472025\n",
      "[7357,     1] loss: 0.0096471287\n",
      "[7358,     1] loss: 0.0096470550\n",
      "[7359,     1] loss: 0.0096469820\n",
      "[7360,     1] loss: 0.0096469074\n",
      "[7361,     1] loss: 0.0096468337\n",
      "[7362,     1] loss: 0.0096467614\n",
      "[7363,     1] loss: 0.0096466877\n",
      "[7364,     1] loss: 0.0096466124\n",
      "[7365,     1] loss: 0.0096465394\n",
      "[7366,     1] loss: 0.0096464656\n",
      "[7367,     1] loss: 0.0096463926\n",
      "[7368,     1] loss: 0.0096463181\n",
      "[7369,     1] loss: 0.0096462443\n",
      "[7370,     1] loss: 0.0096461713\n",
      "[7371,     1] loss: 0.0096460991\n",
      "[7372,     1] loss: 0.0096460238\n",
      "[7373,     1] loss: 0.0096459508\n",
      "[7374,     1] loss: 0.0096458770\n",
      "[7375,     1] loss: 0.0096458033\n",
      "[7376,     1] loss: 0.0096457295\n",
      "[7377,     1] loss: 0.0096456550\n",
      "[7378,     1] loss: 0.0096455812\n",
      "[7379,     1] loss: 0.0096455082\n",
      "[7380,     1] loss: 0.0096454352\n",
      "[7381,     1] loss: 0.0096453600\n",
      "[7382,     1] loss: 0.0096452862\n",
      "[7383,     1] loss: 0.0096452124\n",
      "[7384,     1] loss: 0.0096451379\n",
      "[7385,     1] loss: 0.0096450642\n",
      "[7386,     1] loss: 0.0096449912\n",
      "[7387,     1] loss: 0.0096449167\n",
      "[7388,     1] loss: 0.0096448429\n",
      "[7389,     1] loss: 0.0096447691\n",
      "[7390,     1] loss: 0.0096446954\n",
      "[7391,     1] loss: 0.0096446201\n",
      "[7392,     1] loss: 0.0096445471\n",
      "[7393,     1] loss: 0.0096444741\n",
      "[7394,     1] loss: 0.0096444003\n",
      "[7395,     1] loss: 0.0096443251\n",
      "[7396,     1] loss: 0.0096442513\n",
      "[7397,     1] loss: 0.0096441783\n",
      "[7398,     1] loss: 0.0096441031\n",
      "[7399,     1] loss: 0.0096440293\n",
      "[7400,     1] loss: 0.0096439563\n",
      "[7401,     1] loss: 0.0096438818\n",
      "[7402,     1] loss: 0.0096438088\n",
      "[7403,     1] loss: 0.0096437342\n",
      "[7404,     1] loss: 0.0096436605\n",
      "[7405,     1] loss: 0.0096435845\n",
      "[7406,     1] loss: 0.0096435107\n",
      "[7407,     1] loss: 0.0096434370\n",
      "[7408,     1] loss: 0.0096433617\n",
      "[7409,     1] loss: 0.0096432880\n",
      "[7410,     1] loss: 0.0096432142\n",
      "[7411,     1] loss: 0.0096431404\n",
      "[7412,     1] loss: 0.0096430659\n",
      "[7413,     1] loss: 0.0096429922\n",
      "[7414,     1] loss: 0.0096429177\n",
      "[7415,     1] loss: 0.0096428439\n",
      "[7416,     1] loss: 0.0096427679\n",
      "[7417,     1] loss: 0.0096426956\n",
      "[7418,     1] loss: 0.0096426211\n",
      "[7419,     1] loss: 0.0096425466\n",
      "[7420,     1] loss: 0.0096424714\n",
      "[7421,     1] loss: 0.0096423969\n",
      "[7422,     1] loss: 0.0096423246\n",
      "[7423,     1] loss: 0.0096422493\n",
      "[7424,     1] loss: 0.0096421741\n",
      "[7425,     1] loss: 0.0096421003\n",
      "[7426,     1] loss: 0.0096420266\n",
      "[7427,     1] loss: 0.0096419513\n",
      "[7428,     1] loss: 0.0096418768\n",
      "[7429,     1] loss: 0.0096418023\n",
      "[7430,     1] loss: 0.0096417278\n",
      "[7431,     1] loss: 0.0096416540\n",
      "[7432,     1] loss: 0.0096415788\n",
      "[7433,     1] loss: 0.0096415050\n",
      "[7434,     1] loss: 0.0096414305\n",
      "[7435,     1] loss: 0.0096413568\n",
      "[7436,     1] loss: 0.0096412808\n",
      "[7437,     1] loss: 0.0096412070\n",
      "[7438,     1] loss: 0.0096411332\n",
      "[7439,     1] loss: 0.0096410580\n",
      "[7440,     1] loss: 0.0096409835\n",
      "[7441,     1] loss: 0.0096409082\n",
      "[7442,     1] loss: 0.0096408345\n",
      "[7443,     1] loss: 0.0096407592\n",
      "[7444,     1] loss: 0.0096406862\n",
      "[7445,     1] loss: 0.0096406102\n",
      "[7446,     1] loss: 0.0096405372\n",
      "[7447,     1] loss: 0.0096404620\n",
      "[7448,     1] loss: 0.0096403867\n",
      "[7449,     1] loss: 0.0096403122\n",
      "[7450,     1] loss: 0.0096402377\n",
      "[7451,     1] loss: 0.0096401624\n",
      "[7452,     1] loss: 0.0096400887\n",
      "[7453,     1] loss: 0.0096400134\n",
      "[7454,     1] loss: 0.0096399374\n",
      "[7455,     1] loss: 0.0096398637\n",
      "[7456,     1] loss: 0.0096397869\n",
      "[7457,     1] loss: 0.0096397139\n",
      "[7458,     1] loss: 0.0096396394\n",
      "[7459,     1] loss: 0.0096395649\n",
      "[7460,     1] loss: 0.0096394897\n",
      "[7461,     1] loss: 0.0096394159\n",
      "[7462,     1] loss: 0.0096393414\n",
      "[7463,     1] loss: 0.0096392654\n",
      "[7464,     1] loss: 0.0096391909\n",
      "[7465,     1] loss: 0.0096391164\n",
      "[7466,     1] loss: 0.0096390404\n",
      "[7467,     1] loss: 0.0096389666\n",
      "[7468,     1] loss: 0.0096388914\n",
      "[7469,     1] loss: 0.0096388161\n",
      "[7470,     1] loss: 0.0096387409\n",
      "[7471,     1] loss: 0.0096386671\n",
      "[7472,     1] loss: 0.0096385926\n",
      "[7473,     1] loss: 0.0096385159\n",
      "[7474,     1] loss: 0.0096384414\n",
      "[7475,     1] loss: 0.0096383654\n",
      "[7476,     1] loss: 0.0096382909\n",
      "[7477,     1] loss: 0.0096382163\n",
      "[7478,     1] loss: 0.0096381418\n",
      "[7479,     1] loss: 0.0096380658\n",
      "[7480,     1] loss: 0.0096379913\n",
      "[7481,     1] loss: 0.0096379168\n",
      "[7482,     1] loss: 0.0096378431\n",
      "[7483,     1] loss: 0.0096377678\n",
      "[7484,     1] loss: 0.0096376918\n",
      "[7485,     1] loss: 0.0096376158\n",
      "[7486,     1] loss: 0.0096375428\n",
      "[7487,     1] loss: 0.0096374661\n",
      "[7488,     1] loss: 0.0096373908\n",
      "[7489,     1] loss: 0.0096373171\n",
      "[7490,     1] loss: 0.0096372411\n",
      "[7491,     1] loss: 0.0096371666\n",
      "[7492,     1] loss: 0.0096370906\n",
      "[7493,     1] loss: 0.0096370161\n",
      "[7494,     1] loss: 0.0096369408\n",
      "[7495,     1] loss: 0.0096368656\n",
      "[7496,     1] loss: 0.0096367903\n",
      "[7497,     1] loss: 0.0096367151\n",
      "[7498,     1] loss: 0.0096366391\n",
      "[7499,     1] loss: 0.0096365646\n",
      "[7500,     1] loss: 0.0096364886\n",
      "[7501,     1] loss: 0.0096364141\n",
      "[7502,     1] loss: 0.0096363388\n",
      "[7503,     1] loss: 0.0096362643\n",
      "[7504,     1] loss: 0.0096361876\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7505,     1] loss: 0.0096361130\n",
      "[7506,     1] loss: 0.0096360378\n",
      "[7507,     1] loss: 0.0096359618\n",
      "[7508,     1] loss: 0.0096358873\n",
      "[7509,     1] loss: 0.0096358120\n",
      "[7510,     1] loss: 0.0096357368\n",
      "[7511,     1] loss: 0.0096356601\n",
      "[7512,     1] loss: 0.0096355848\n",
      "[7513,     1] loss: 0.0096355103\n",
      "[7514,     1] loss: 0.0096354336\n",
      "[7515,     1] loss: 0.0096353598\n",
      "[7516,     1] loss: 0.0096352823\n",
      "[7517,     1] loss: 0.0096352085\n",
      "[7518,     1] loss: 0.0096351326\n",
      "[7519,     1] loss: 0.0096350573\n",
      "[7520,     1] loss: 0.0096349813\n",
      "[7521,     1] loss: 0.0096349061\n",
      "[7522,     1] loss: 0.0096348315\n",
      "[7523,     1] loss: 0.0096347556\n",
      "[7524,     1] loss: 0.0096346803\n",
      "[7525,     1] loss: 0.0096346051\n",
      "[7526,     1] loss: 0.0096345291\n",
      "[7527,     1] loss: 0.0096344531\n",
      "[7528,     1] loss: 0.0096343778\n",
      "[7529,     1] loss: 0.0096343018\n",
      "[7530,     1] loss: 0.0096342266\n",
      "[7531,     1] loss: 0.0096341513\n",
      "[7532,     1] loss: 0.0096340761\n",
      "[7533,     1] loss: 0.0096339993\n",
      "[7534,     1] loss: 0.0096339248\n",
      "[7535,     1] loss: 0.0096338496\n",
      "[7536,     1] loss: 0.0096337736\n",
      "[7537,     1] loss: 0.0096336983\n",
      "[7538,     1] loss: 0.0096336231\n",
      "[7539,     1] loss: 0.0096335471\n",
      "[7540,     1] loss: 0.0096334711\n",
      "[7541,     1] loss: 0.0096333958\n",
      "[7542,     1] loss: 0.0096333198\n",
      "[7543,     1] loss: 0.0096332438\n",
      "[7544,     1] loss: 0.0096331678\n",
      "[7545,     1] loss: 0.0096330926\n",
      "[7546,     1] loss: 0.0096330173\n",
      "[7547,     1] loss: 0.0096329421\n",
      "[7548,     1] loss: 0.0096328661\n",
      "[7549,     1] loss: 0.0096327893\n",
      "[7550,     1] loss: 0.0096327141\n",
      "[7551,     1] loss: 0.0096326388\n",
      "[7552,     1] loss: 0.0096325628\n",
      "[7553,     1] loss: 0.0096324876\n",
      "[7554,     1] loss: 0.0096324109\n",
      "[7555,     1] loss: 0.0096323349\n",
      "[7556,     1] loss: 0.0096322604\n",
      "[7557,     1] loss: 0.0096321836\n",
      "[7558,     1] loss: 0.0096321069\n",
      "[7559,     1] loss: 0.0096320316\n",
      "[7560,     1] loss: 0.0096319564\n",
      "[7561,     1] loss: 0.0096318804\n",
      "[7562,     1] loss: 0.0096318044\n",
      "[7563,     1] loss: 0.0096317291\n",
      "[7564,     1] loss: 0.0096316524\n",
      "[7565,     1] loss: 0.0096315764\n",
      "[7566,     1] loss: 0.0096315004\n",
      "[7567,     1] loss: 0.0096314237\n",
      "[7568,     1] loss: 0.0096313477\n",
      "[7569,     1] loss: 0.0096312724\n",
      "[7570,     1] loss: 0.0096311972\n",
      "[7571,     1] loss: 0.0096311219\n",
      "[7572,     1] loss: 0.0096310452\n",
      "[7573,     1] loss: 0.0096309684\n",
      "[7574,     1] loss: 0.0096308917\n",
      "[7575,     1] loss: 0.0096308164\n",
      "[7576,     1] loss: 0.0096307412\n",
      "[7577,     1] loss: 0.0096306644\n",
      "[7578,     1] loss: 0.0096305884\n",
      "[7579,     1] loss: 0.0096305132\n",
      "[7580,     1] loss: 0.0096304372\n",
      "[7581,     1] loss: 0.0096303612\n",
      "[7582,     1] loss: 0.0096302837\n",
      "[7583,     1] loss: 0.0096302085\n",
      "[7584,     1] loss: 0.0096301332\n",
      "[7585,     1] loss: 0.0096300565\n",
      "[7586,     1] loss: 0.0096299790\n",
      "[7587,     1] loss: 0.0096299052\n",
      "[7588,     1] loss: 0.0096298277\n",
      "[7589,     1] loss: 0.0096297517\n",
      "[7590,     1] loss: 0.0096296750\n",
      "[7591,     1] loss: 0.0096295983\n",
      "[7592,     1] loss: 0.0096295245\n",
      "[7593,     1] loss: 0.0096294470\n",
      "[7594,     1] loss: 0.0096293695\n",
      "[7595,     1] loss: 0.0096292950\n",
      "[7596,     1] loss: 0.0096292175\n",
      "[7597,     1] loss: 0.0096291430\n",
      "[7598,     1] loss: 0.0096290655\n",
      "[7599,     1] loss: 0.0096289895\n",
      "[7600,     1] loss: 0.0096289136\n",
      "[7601,     1] loss: 0.0096288383\n",
      "[7602,     1] loss: 0.0096287601\n",
      "[7603,     1] loss: 0.0096286848\n",
      "[7604,     1] loss: 0.0096286088\n",
      "[7605,     1] loss: 0.0096285321\n",
      "[7606,     1] loss: 0.0096284561\n",
      "[7607,     1] loss: 0.0096283801\n",
      "[7608,     1] loss: 0.0096283033\n",
      "[7609,     1] loss: 0.0096282274\n",
      "[7610,     1] loss: 0.0096281499\n",
      "[7611,     1] loss: 0.0096280761\n",
      "[7612,     1] loss: 0.0096279971\n",
      "[7613,     1] loss: 0.0096279226\n",
      "[7614,     1] loss: 0.0096278451\n",
      "[7615,     1] loss: 0.0096277691\n",
      "[7616,     1] loss: 0.0096276909\n",
      "[7617,     1] loss: 0.0096276157\n",
      "[7618,     1] loss: 0.0096275389\n",
      "[7619,     1] loss: 0.0096274629\n",
      "[7620,     1] loss: 0.0096273869\n",
      "[7621,     1] loss: 0.0096273117\n",
      "[7622,     1] loss: 0.0096272342\n",
      "[7623,     1] loss: 0.0096271582\n",
      "[7624,     1] loss: 0.0096270800\n",
      "[7625,     1] loss: 0.0096270040\n",
      "[7626,     1] loss: 0.0096269280\n",
      "[7627,     1] loss: 0.0096268505\n",
      "[7628,     1] loss: 0.0096267752\n",
      "[7629,     1] loss: 0.0096266985\n",
      "[7630,     1] loss: 0.0096266218\n",
      "[7631,     1] loss: 0.0096265458\n",
      "[7632,     1] loss: 0.0096264690\n",
      "[7633,     1] loss: 0.0096263908\n",
      "[7634,     1] loss: 0.0096263133\n",
      "[7635,     1] loss: 0.0096262373\n",
      "[7636,     1] loss: 0.0096261621\n",
      "[7637,     1] loss: 0.0096260861\n",
      "[7638,     1] loss: 0.0096260093\n",
      "[7639,     1] loss: 0.0096259318\n",
      "[7640,     1] loss: 0.0096258558\n",
      "[7641,     1] loss: 0.0096257783\n",
      "[7642,     1] loss: 0.0096257016\n",
      "[7643,     1] loss: 0.0096256256\n",
      "[7644,     1] loss: 0.0096255481\n",
      "[7645,     1] loss: 0.0096254714\n",
      "[7646,     1] loss: 0.0096253954\n",
      "[7647,     1] loss: 0.0096253194\n",
      "[7648,     1] loss: 0.0096252419\n",
      "[7649,     1] loss: 0.0096251644\n",
      "[7650,     1] loss: 0.0096250877\n",
      "[7651,     1] loss: 0.0096250109\n",
      "[7652,     1] loss: 0.0096249342\n",
      "[7653,     1] loss: 0.0096248575\n",
      "[7654,     1] loss: 0.0096247807\n",
      "[7655,     1] loss: 0.0096247032\n",
      "[7656,     1] loss: 0.0096246272\n",
      "[7657,     1] loss: 0.0096245505\n",
      "[7658,     1] loss: 0.0096244745\n",
      "[7659,     1] loss: 0.0096243970\n",
      "[7660,     1] loss: 0.0096243188\n",
      "[7661,     1] loss: 0.0096242435\n",
      "[7662,     1] loss: 0.0096241668\n",
      "[7663,     1] loss: 0.0096240900\n",
      "[7664,     1] loss: 0.0096240126\n",
      "[7665,     1] loss: 0.0096239358\n",
      "[7666,     1] loss: 0.0096238591\n",
      "[7667,     1] loss: 0.0096237816\n",
      "[7668,     1] loss: 0.0096237056\n",
      "[7669,     1] loss: 0.0096236289\n",
      "[7670,     1] loss: 0.0096235521\n",
      "[7671,     1] loss: 0.0096234754\n",
      "[7672,     1] loss: 0.0096233971\n",
      "[7673,     1] loss: 0.0096233211\n",
      "[7674,     1] loss: 0.0096232429\n",
      "[7675,     1] loss: 0.0096231669\n",
      "[7676,     1] loss: 0.0096230902\n",
      "[7677,     1] loss: 0.0096230127\n",
      "[7678,     1] loss: 0.0096229352\n",
      "[7679,     1] loss: 0.0096228585\n",
      "[7680,     1] loss: 0.0096227810\n",
      "[7681,     1] loss: 0.0096227050\n",
      "[7682,     1] loss: 0.0096226282\n",
      "[7683,     1] loss: 0.0096225500\n",
      "[7684,     1] loss: 0.0096224740\n",
      "[7685,     1] loss: 0.0096223973\n",
      "[7686,     1] loss: 0.0096223190\n",
      "[7687,     1] loss: 0.0096222423\n",
      "[7688,     1] loss: 0.0096221656\n",
      "[7689,     1] loss: 0.0096220881\n",
      "[7690,     1] loss: 0.0096220113\n",
      "[7691,     1] loss: 0.0096219338\n",
      "[7692,     1] loss: 0.0096218571\n",
      "[7693,     1] loss: 0.0096217796\n",
      "[7694,     1] loss: 0.0096217029\n",
      "[7695,     1] loss: 0.0096216261\n",
      "[7696,     1] loss: 0.0096215487\n",
      "[7697,     1] loss: 0.0096214719\n",
      "[7698,     1] loss: 0.0096213929\n",
      "[7699,     1] loss: 0.0096213169\n",
      "[7700,     1] loss: 0.0096212402\n",
      "[7701,     1] loss: 0.0096211627\n",
      "[7702,     1] loss: 0.0096210845\n",
      "[7703,     1] loss: 0.0096210100\n",
      "[7704,     1] loss: 0.0096209325\n",
      "[7705,     1] loss: 0.0096208535\n",
      "[7706,     1] loss: 0.0096207768\n",
      "[7707,     1] loss: 0.0096206993\n",
      "[7708,     1] loss: 0.0096206225\n",
      "[7709,     1] loss: 0.0096205443\n",
      "[7710,     1] loss: 0.0096204668\n",
      "[7711,     1] loss: 0.0096203893\n",
      "[7712,     1] loss: 0.0096203133\n",
      "[7713,     1] loss: 0.0096202351\n",
      "[7714,     1] loss: 0.0096201576\n",
      "[7715,     1] loss: 0.0096200816\n",
      "[7716,     1] loss: 0.0096200034\n",
      "[7717,     1] loss: 0.0096199267\n",
      "[7718,     1] loss: 0.0096198484\n",
      "[7719,     1] loss: 0.0096197717\n",
      "[7720,     1] loss: 0.0096196935\n",
      "[7721,     1] loss: 0.0096196167\n",
      "[7722,     1] loss: 0.0096195385\n",
      "[7723,     1] loss: 0.0096194617\n",
      "[7724,     1] loss: 0.0096193857\n",
      "[7725,     1] loss: 0.0096193068\n",
      "[7726,     1] loss: 0.0096192293\n",
      "[7727,     1] loss: 0.0096191533\n",
      "[7728,     1] loss: 0.0096190758\n",
      "[7729,     1] loss: 0.0096189991\n",
      "[7730,     1] loss: 0.0096189193\n",
      "[7731,     1] loss: 0.0096188433\n",
      "[7732,     1] loss: 0.0096187651\n",
      "[7733,     1] loss: 0.0096186884\n",
      "[7734,     1] loss: 0.0096186116\n",
      "[7735,     1] loss: 0.0096185327\n",
      "[7736,     1] loss: 0.0096184559\n",
      "[7737,     1] loss: 0.0096183792\n",
      "[7738,     1] loss: 0.0096183017\n",
      "[7739,     1] loss: 0.0096182235\n",
      "[7740,     1] loss: 0.0096181460\n",
      "[7741,     1] loss: 0.0096180685\n",
      "[7742,     1] loss: 0.0096179917\n",
      "[7743,     1] loss: 0.0096179135\n",
      "[7744,     1] loss: 0.0096178360\n",
      "[7745,     1] loss: 0.0096177600\n",
      "[7746,     1] loss: 0.0096176818\n",
      "[7747,     1] loss: 0.0096176036\n",
      "[7748,     1] loss: 0.0096175261\n",
      "[7749,     1] loss: 0.0096174486\n",
      "[7750,     1] loss: 0.0096173704\n",
      "[7751,     1] loss: 0.0096172936\n",
      "[7752,     1] loss: 0.0096172161\n",
      "[7753,     1] loss: 0.0096171379\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7754,     1] loss: 0.0096170619\n",
      "[7755,     1] loss: 0.0096169822\n",
      "[7756,     1] loss: 0.0096169055\n",
      "[7757,     1] loss: 0.0096168280\n",
      "[7758,     1] loss: 0.0096167497\n",
      "[7759,     1] loss: 0.0096166722\n",
      "[7760,     1] loss: 0.0096165940\n",
      "[7761,     1] loss: 0.0096165173\n",
      "[7762,     1] loss: 0.0096164383\n",
      "[7763,     1] loss: 0.0096163616\n",
      "[7764,     1] loss: 0.0096162833\n",
      "[7765,     1] loss: 0.0096162073\n",
      "[7766,     1] loss: 0.0096161298\n",
      "[7767,     1] loss: 0.0096160524\n",
      "[7768,     1] loss: 0.0096159749\n",
      "[7769,     1] loss: 0.0096158959\n",
      "[7770,     1] loss: 0.0096158192\n",
      "[7771,     1] loss: 0.0096157402\n",
      "[7772,     1] loss: 0.0096156619\n",
      "[7773,     1] loss: 0.0096155852\n",
      "[7774,     1] loss: 0.0096155085\n",
      "[7775,     1] loss: 0.0096154302\n",
      "[7776,     1] loss: 0.0096153505\n",
      "[7777,     1] loss: 0.0096152730\n",
      "[7778,     1] loss: 0.0096151963\n",
      "[7779,     1] loss: 0.0096151173\n",
      "[7780,     1] loss: 0.0096150398\n",
      "[7781,     1] loss: 0.0096149623\n",
      "[7782,     1] loss: 0.0096148849\n",
      "[7783,     1] loss: 0.0096148074\n",
      "[7784,     1] loss: 0.0096147291\n",
      "[7785,     1] loss: 0.0096146517\n",
      "[7786,     1] loss: 0.0096145742\n",
      "[7787,     1] loss: 0.0096144967\n",
      "[7788,     1] loss: 0.0096144177\n",
      "[7789,     1] loss: 0.0096143410\n",
      "[7790,     1] loss: 0.0096142627\n",
      "[7791,     1] loss: 0.0096141852\n",
      "[7792,     1] loss: 0.0096141063\n",
      "[7793,     1] loss: 0.0096140288\n",
      "[7794,     1] loss: 0.0096139520\n",
      "[7795,     1] loss: 0.0096138738\n",
      "[7796,     1] loss: 0.0096137956\n",
      "[7797,     1] loss: 0.0096137173\n",
      "[7798,     1] loss: 0.0096136399\n",
      "[7799,     1] loss: 0.0096135616\n",
      "[7800,     1] loss: 0.0096134849\n",
      "[7801,     1] loss: 0.0096134059\n",
      "[7802,     1] loss: 0.0096133277\n",
      "[7803,     1] loss: 0.0096132495\n",
      "[7804,     1] loss: 0.0096131720\n",
      "[7805,     1] loss: 0.0096130945\n",
      "[7806,     1] loss: 0.0096130162\n",
      "[7807,     1] loss: 0.0096129365\n",
      "[7808,     1] loss: 0.0096128598\n",
      "[7809,     1] loss: 0.0096127816\n",
      "[7810,     1] loss: 0.0096127026\n",
      "[7811,     1] loss: 0.0096126258\n",
      "[7812,     1] loss: 0.0096125484\n",
      "[7813,     1] loss: 0.0096124709\n",
      "[7814,     1] loss: 0.0096123926\n",
      "[7815,     1] loss: 0.0096123144\n",
      "[7816,     1] loss: 0.0096122369\n",
      "[7817,     1] loss: 0.0096121594\n",
      "[7818,     1] loss: 0.0096120805\n",
      "[7819,     1] loss: 0.0096120030\n",
      "[7820,     1] loss: 0.0096119255\n",
      "[7821,     1] loss: 0.0096118450\n",
      "[7822,     1] loss: 0.0096117683\n",
      "[7823,     1] loss: 0.0096116908\n",
      "[7824,     1] loss: 0.0096116126\n",
      "[7825,     1] loss: 0.0096115343\n",
      "[7826,     1] loss: 0.0096114568\n",
      "[7827,     1] loss: 0.0096113779\n",
      "[7828,     1] loss: 0.0096112996\n",
      "[7829,     1] loss: 0.0096112221\n",
      "[7830,     1] loss: 0.0096111439\n",
      "[7831,     1] loss: 0.0096110657\n",
      "[7832,     1] loss: 0.0096109875\n",
      "[7833,     1] loss: 0.0096109085\n",
      "[7834,     1] loss: 0.0096108317\n",
      "[7835,     1] loss: 0.0096107535\n",
      "[7836,     1] loss: 0.0096106745\n",
      "[7837,     1] loss: 0.0096105978\n",
      "[7838,     1] loss: 0.0096105188\n",
      "[7839,     1] loss: 0.0096104406\n",
      "[7840,     1] loss: 0.0096103616\n",
      "[7841,     1] loss: 0.0096102834\n",
      "[7842,     1] loss: 0.0096102059\n",
      "[7843,     1] loss: 0.0096101284\n",
      "[7844,     1] loss: 0.0096100494\n",
      "[7845,     1] loss: 0.0096099719\n",
      "[7846,     1] loss: 0.0096098930\n",
      "[7847,     1] loss: 0.0096098140\n",
      "[7848,     1] loss: 0.0096097358\n",
      "[7849,     1] loss: 0.0096096575\n",
      "[7850,     1] loss: 0.0096095800\n",
      "[7851,     1] loss: 0.0096095018\n",
      "[7852,     1] loss: 0.0096094236\n",
      "[7853,     1] loss: 0.0096093446\n",
      "[7854,     1] loss: 0.0096092671\n",
      "[7855,     1] loss: 0.0096091889\n",
      "[7856,     1] loss: 0.0096091107\n",
      "[7857,     1] loss: 0.0096090332\n",
      "[7858,     1] loss: 0.0096089542\n",
      "[7859,     1] loss: 0.0096088760\n",
      "[7860,     1] loss: 0.0096087977\n",
      "[7861,     1] loss: 0.0096087180\n",
      "[7862,     1] loss: 0.0096086398\n",
      "[7863,     1] loss: 0.0096085623\n",
      "[7864,     1] loss: 0.0096084848\n",
      "[7865,     1] loss: 0.0096084051\n",
      "[7866,     1] loss: 0.0096083276\n",
      "[7867,     1] loss: 0.0096082494\n",
      "[7868,     1] loss: 0.0096081696\n",
      "[7869,     1] loss: 0.0096080929\n",
      "[7870,     1] loss: 0.0096080139\n",
      "[7871,     1] loss: 0.0096079342\n",
      "[7872,     1] loss: 0.0096078560\n",
      "[7873,     1] loss: 0.0096077792\n",
      "[7874,     1] loss: 0.0096076995\n",
      "[7875,     1] loss: 0.0096076220\n",
      "[7876,     1] loss: 0.0096075431\n",
      "[7877,     1] loss: 0.0096074648\n",
      "[7878,     1] loss: 0.0096073866\n",
      "[7879,     1] loss: 0.0096073091\n",
      "[7880,     1] loss: 0.0096072286\n",
      "[7881,     1] loss: 0.0096071512\n",
      "[7882,     1] loss: 0.0096070729\n",
      "[7883,     1] loss: 0.0096069932\n",
      "[7884,     1] loss: 0.0096069157\n",
      "[7885,     1] loss: 0.0096068375\n",
      "[7886,     1] loss: 0.0096067585\n",
      "[7887,     1] loss: 0.0096066810\n",
      "[7888,     1] loss: 0.0096066028\n",
      "[7889,     1] loss: 0.0096065246\n",
      "[7890,     1] loss: 0.0096064448\n",
      "[7891,     1] loss: 0.0096063681\n",
      "[7892,     1] loss: 0.0096062891\n",
      "[7893,     1] loss: 0.0096062101\n",
      "[7894,     1] loss: 0.0096061319\n",
      "[7895,     1] loss: 0.0096060537\n",
      "[7896,     1] loss: 0.0096059754\n",
      "[7897,     1] loss: 0.0096058965\n",
      "[7898,     1] loss: 0.0096058182\n",
      "[7899,     1] loss: 0.0096057393\n",
      "[7900,     1] loss: 0.0096056618\n",
      "[7901,     1] loss: 0.0096055835\n",
      "[7902,     1] loss: 0.0096055046\n",
      "[7903,     1] loss: 0.0096054263\n",
      "[7904,     1] loss: 0.0096053481\n",
      "[7905,     1] loss: 0.0096052684\n",
      "[7906,     1] loss: 0.0096051909\n",
      "[7907,     1] loss: 0.0096051119\n",
      "[7908,     1] loss: 0.0096050337\n",
      "[7909,     1] loss: 0.0096049547\n",
      "[7910,     1] loss: 0.0096048765\n",
      "[7911,     1] loss: 0.0096047975\n",
      "[7912,     1] loss: 0.0096047185\n",
      "[7913,     1] loss: 0.0096046396\n",
      "[7914,     1] loss: 0.0096045621\n",
      "[7915,     1] loss: 0.0096044824\n",
      "[7916,     1] loss: 0.0096044041\n",
      "[7917,     1] loss: 0.0096043259\n",
      "[7918,     1] loss: 0.0096042484\n",
      "[7919,     1] loss: 0.0096041687\n",
      "[7920,     1] loss: 0.0096040919\n",
      "[7921,     1] loss: 0.0096040122\n",
      "[7922,     1] loss: 0.0096039340\n",
      "[7923,     1] loss: 0.0096038543\n",
      "[7924,     1] loss: 0.0096037760\n",
      "[7925,     1] loss: 0.0096036963\n",
      "[7926,     1] loss: 0.0096036188\n",
      "[7927,     1] loss: 0.0096035399\n",
      "[7928,     1] loss: 0.0096034616\n",
      "[7929,     1] loss: 0.0096033834\n",
      "[7930,     1] loss: 0.0096033052\n",
      "[7931,     1] loss: 0.0096032254\n",
      "[7932,     1] loss: 0.0096031472\n",
      "[7933,     1] loss: 0.0096030690\n",
      "[7934,     1] loss: 0.0096029907\n",
      "[7935,     1] loss: 0.0096029110\n",
      "[7936,     1] loss: 0.0096028328\n",
      "[7937,     1] loss: 0.0096027531\n",
      "[7938,     1] loss: 0.0096026748\n",
      "[7939,     1] loss: 0.0096025959\n",
      "[7940,     1] loss: 0.0096025184\n",
      "[7941,     1] loss: 0.0096024387\n",
      "[7942,     1] loss: 0.0096023604\n",
      "[7943,     1] loss: 0.0096022807\n",
      "[7944,     1] loss: 0.0096022025\n",
      "[7945,     1] loss: 0.0096021242\n",
      "[7946,     1] loss: 0.0096020445\n",
      "[7947,     1] loss: 0.0096019670\n",
      "[7948,     1] loss: 0.0096018881\n",
      "[7949,     1] loss: 0.0096018098\n",
      "[7950,     1] loss: 0.0096017316\n",
      "[7951,     1] loss: 0.0096016526\n",
      "[7952,     1] loss: 0.0096015736\n",
      "[7953,     1] loss: 0.0096014954\n",
      "[7954,     1] loss: 0.0096014164\n",
      "[7955,     1] loss: 0.0096013375\n",
      "[7956,     1] loss: 0.0096012585\n",
      "[7957,     1] loss: 0.0096011803\n",
      "[7958,     1] loss: 0.0096011013\n",
      "[7959,     1] loss: 0.0096010230\n",
      "[7960,     1] loss: 0.0096009448\n",
      "[7961,     1] loss: 0.0096008666\n",
      "[7962,     1] loss: 0.0096007869\n",
      "[7963,     1] loss: 0.0096007079\n",
      "[7964,     1] loss: 0.0096006289\n",
      "[7965,     1] loss: 0.0096005507\n",
      "[7966,     1] loss: 0.0096004710\n",
      "[7967,     1] loss: 0.0096003912\n",
      "[7968,     1] loss: 0.0096003138\n",
      "[7969,     1] loss: 0.0096002348\n",
      "[7970,     1] loss: 0.0096001551\n",
      "[7971,     1] loss: 0.0096000783\n",
      "[7972,     1] loss: 0.0095999986\n",
      "[7973,     1] loss: 0.0095999189\n",
      "[7974,     1] loss: 0.0095998406\n",
      "[7975,     1] loss: 0.0095997617\n",
      "[7976,     1] loss: 0.0095996827\n",
      "[7977,     1] loss: 0.0095996052\n",
      "[7978,     1] loss: 0.0095995255\n",
      "[7979,     1] loss: 0.0095994458\n",
      "[7980,     1] loss: 0.0095993668\n",
      "[7981,     1] loss: 0.0095992893\n",
      "[7982,     1] loss: 0.0095992088\n",
      "[7983,     1] loss: 0.0095991313\n",
      "[7984,     1] loss: 0.0095990524\n",
      "[7985,     1] loss: 0.0095989734\n",
      "[7986,     1] loss: 0.0095988944\n",
      "[7987,     1] loss: 0.0095988162\n",
      "[7988,     1] loss: 0.0095987372\n",
      "[7989,     1] loss: 0.0095986575\n",
      "[7990,     1] loss: 0.0095985793\n",
      "[7991,     1] loss: 0.0095985003\n",
      "[7992,     1] loss: 0.0095984206\n",
      "[7993,     1] loss: 0.0095983416\n",
      "[7994,     1] loss: 0.0095982634\n",
      "[7995,     1] loss: 0.0095981844\n",
      "[7996,     1] loss: 0.0095981061\n",
      "[7997,     1] loss: 0.0095980279\n",
      "[7998,     1] loss: 0.0095979489\n",
      "[7999,     1] loss: 0.0095978692\n",
      "[8000,     1] loss: 0.0095977910\n",
      "[8001,     1] loss: 0.0095977113\n",
      "[8002,     1] loss: 0.0095976315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8003,     1] loss: 0.0095975541\n",
      "[8004,     1] loss: 0.0095974751\n",
      "[8005,     1] loss: 0.0095973946\n",
      "[8006,     1] loss: 0.0095973164\n",
      "[8007,     1] loss: 0.0095972382\n",
      "[8008,     1] loss: 0.0095971584\n",
      "[8009,     1] loss: 0.0095970795\n",
      "[8010,     1] loss: 0.0095970005\n",
      "[8011,     1] loss: 0.0095969208\n",
      "[8012,     1] loss: 0.0095968433\n",
      "[8013,     1] loss: 0.0095967650\n",
      "[8014,     1] loss: 0.0095966853\n",
      "[8015,     1] loss: 0.0095966056\n",
      "[8016,     1] loss: 0.0095965274\n",
      "[8017,     1] loss: 0.0095964476\n",
      "[8018,     1] loss: 0.0095963702\n",
      "[8019,     1] loss: 0.0095962904\n",
      "[8020,     1] loss: 0.0095962122\n",
      "[8021,     1] loss: 0.0095961332\n",
      "[8022,     1] loss: 0.0095960543\n",
      "[8023,     1] loss: 0.0095959753\n",
      "[8024,     1] loss: 0.0095958963\n",
      "[8025,     1] loss: 0.0095958166\n",
      "[8026,     1] loss: 0.0095957391\n",
      "[8027,     1] loss: 0.0095956586\n",
      "[8028,     1] loss: 0.0095955811\n",
      "[8029,     1] loss: 0.0095955007\n",
      "[8030,     1] loss: 0.0095954217\n",
      "[8031,     1] loss: 0.0095953442\n",
      "[8032,     1] loss: 0.0095952645\n",
      "[8033,     1] loss: 0.0095951855\n",
      "[8034,     1] loss: 0.0095951065\n",
      "[8035,     1] loss: 0.0095950283\n",
      "[8036,     1] loss: 0.0095949486\n",
      "[8037,     1] loss: 0.0095948704\n",
      "[8038,     1] loss: 0.0095947914\n",
      "[8039,     1] loss: 0.0095947117\n",
      "[8040,     1] loss: 0.0095946334\n",
      "[8041,     1] loss: 0.0095945545\n",
      "[8042,     1] loss: 0.0095944770\n",
      "[8043,     1] loss: 0.0095943958\n",
      "[8044,     1] loss: 0.0095943160\n",
      "[8045,     1] loss: 0.0095942378\n",
      "[8046,     1] loss: 0.0095941588\n",
      "[8047,     1] loss: 0.0095940806\n",
      "[8048,     1] loss: 0.0095940001\n",
      "[8049,     1] loss: 0.0095939204\n",
      "[8050,     1] loss: 0.0095938429\n",
      "[8051,     1] loss: 0.0095937639\n",
      "[8052,     1] loss: 0.0095936842\n",
      "[8053,     1] loss: 0.0095936060\n",
      "[8054,     1] loss: 0.0095935270\n",
      "[8055,     1] loss: 0.0095934480\n",
      "[8056,     1] loss: 0.0095933668\n",
      "[8057,     1] loss: 0.0095932901\n",
      "[8058,     1] loss: 0.0095932096\n",
      "[8059,     1] loss: 0.0095931306\n",
      "[8060,     1] loss: 0.0095930532\n",
      "[8061,     1] loss: 0.0095929734\n",
      "[8062,     1] loss: 0.0095928937\n",
      "[8063,     1] loss: 0.0095928140\n",
      "[8064,     1] loss: 0.0095927358\n",
      "[8065,     1] loss: 0.0095926575\n",
      "[8066,     1] loss: 0.0095925778\n",
      "[8067,     1] loss: 0.0095925003\n",
      "[8068,     1] loss: 0.0095924206\n",
      "[8069,     1] loss: 0.0095923409\n",
      "[8070,     1] loss: 0.0095922619\n",
      "[8071,     1] loss: 0.0095921822\n",
      "[8072,     1] loss: 0.0095921032\n",
      "[8073,     1] loss: 0.0095920242\n",
      "[8074,     1] loss: 0.0095919460\n",
      "[8075,     1] loss: 0.0095918678\n",
      "[8076,     1] loss: 0.0095917881\n",
      "[8077,     1] loss: 0.0095917091\n",
      "[8078,     1] loss: 0.0095916294\n",
      "[8079,     1] loss: 0.0095915511\n",
      "[8080,     1] loss: 0.0095914729\n",
      "[8081,     1] loss: 0.0095913924\n",
      "[8082,     1] loss: 0.0095913127\n",
      "[8083,     1] loss: 0.0095912345\n",
      "[8084,     1] loss: 0.0095911562\n",
      "[8085,     1] loss: 0.0095910750\n",
      "[8086,     1] loss: 0.0095909975\n",
      "[8087,     1] loss: 0.0095909186\n",
      "[8088,     1] loss: 0.0095908381\n",
      "[8089,     1] loss: 0.0095907599\n",
      "[8090,     1] loss: 0.0095906809\n",
      "[8091,     1] loss: 0.0095906004\n",
      "[8092,     1] loss: 0.0095905237\n",
      "[8093,     1] loss: 0.0095904432\n",
      "[8094,     1] loss: 0.0095903650\n",
      "[8095,     1] loss: 0.0095902853\n",
      "[8096,     1] loss: 0.0095902048\n",
      "[8097,     1] loss: 0.0095901258\n",
      "[8098,     1] loss: 0.0095900469\n",
      "[8099,     1] loss: 0.0095899694\n",
      "[8100,     1] loss: 0.0095898911\n",
      "[8101,     1] loss: 0.0095898107\n",
      "[8102,     1] loss: 0.0095897309\n",
      "[8103,     1] loss: 0.0095896527\n",
      "[8104,     1] loss: 0.0095895730\n",
      "[8105,     1] loss: 0.0095894933\n",
      "[8106,     1] loss: 0.0095894143\n",
      "[8107,     1] loss: 0.0095893361\n",
      "[8108,     1] loss: 0.0095892563\n",
      "[8109,     1] loss: 0.0095891774\n",
      "[8110,     1] loss: 0.0095890984\n",
      "[8111,     1] loss: 0.0095890194\n",
      "[8112,     1] loss: 0.0095889397\n",
      "[8113,     1] loss: 0.0095888607\n",
      "[8114,     1] loss: 0.0095887810\n",
      "[8115,     1] loss: 0.0095887013\n",
      "[8116,     1] loss: 0.0095886230\n",
      "[8117,     1] loss: 0.0095885456\n",
      "[8118,     1] loss: 0.0095884643\n",
      "[8119,     1] loss: 0.0095883869\n",
      "[8120,     1] loss: 0.0095883079\n",
      "[8121,     1] loss: 0.0095882282\n",
      "[8122,     1] loss: 0.0095881484\n",
      "[8123,     1] loss: 0.0095880702\n",
      "[8124,     1] loss: 0.0095879905\n",
      "[8125,     1] loss: 0.0095879100\n",
      "[8126,     1] loss: 0.0095878333\n",
      "[8127,     1] loss: 0.0095877536\n",
      "[8128,     1] loss: 0.0095876746\n",
      "[8129,     1] loss: 0.0095875956\n",
      "[8130,     1] loss: 0.0095875166\n",
      "[8131,     1] loss: 0.0095874377\n",
      "[8132,     1] loss: 0.0095873587\n",
      "[8133,     1] loss: 0.0095872775\n",
      "[8134,     1] loss: 0.0095872000\n",
      "[8135,     1] loss: 0.0095871203\n",
      "[8136,     1] loss: 0.0095870413\n",
      "[8137,     1] loss: 0.0095869616\n",
      "[8138,     1] loss: 0.0095868826\n",
      "[8139,     1] loss: 0.0095868036\n",
      "[8140,     1] loss: 0.0095867246\n",
      "[8141,     1] loss: 0.0095866457\n",
      "[8142,     1] loss: 0.0095865659\n",
      "[8143,     1] loss: 0.0095864877\n",
      "[8144,     1] loss: 0.0095864080\n",
      "[8145,     1] loss: 0.0095863290\n",
      "[8146,     1] loss: 0.0095862493\n",
      "[8147,     1] loss: 0.0095861711\n",
      "[8148,     1] loss: 0.0095860928\n",
      "[8149,     1] loss: 0.0095860124\n",
      "[8150,     1] loss: 0.0095859334\n",
      "[8151,     1] loss: 0.0095858544\n",
      "[8152,     1] loss: 0.0095857747\n",
      "[8153,     1] loss: 0.0095856950\n",
      "[8154,     1] loss: 0.0095856167\n",
      "[8155,     1] loss: 0.0095855378\n",
      "[8156,     1] loss: 0.0095854580\n",
      "[8157,     1] loss: 0.0095853791\n",
      "[8158,     1] loss: 0.0095853008\n",
      "[8159,     1] loss: 0.0095852219\n",
      "[8160,     1] loss: 0.0095851414\n",
      "[8161,     1] loss: 0.0095850624\n",
      "[8162,     1] loss: 0.0095849834\n",
      "[8163,     1] loss: 0.0095849030\n",
      "[8164,     1] loss: 0.0095848240\n",
      "[8165,     1] loss: 0.0095847465\n",
      "[8166,     1] loss: 0.0095846675\n",
      "[8167,     1] loss: 0.0095845878\n",
      "[8168,     1] loss: 0.0095845096\n",
      "[8169,     1] loss: 0.0095844291\n",
      "[8170,     1] loss: 0.0095843501\n",
      "[8171,     1] loss: 0.0095842697\n",
      "[8172,     1] loss: 0.0095841907\n",
      "[8173,     1] loss: 0.0095841125\n",
      "[8174,     1] loss: 0.0095840342\n",
      "[8175,     1] loss: 0.0095839545\n",
      "[8176,     1] loss: 0.0095838755\n",
      "[8177,     1] loss: 0.0095837966\n",
      "[8178,     1] loss: 0.0095837176\n",
      "[8179,     1] loss: 0.0095836386\n",
      "[8180,     1] loss: 0.0095835589\n",
      "[8181,     1] loss: 0.0095834814\n",
      "[8182,     1] loss: 0.0095834017\n",
      "[8183,     1] loss: 0.0095833220\n",
      "[8184,     1] loss: 0.0095832430\n",
      "[8185,     1] loss: 0.0095831633\n",
      "[8186,     1] loss: 0.0095830843\n",
      "[8187,     1] loss: 0.0095830038\n",
      "[8188,     1] loss: 0.0095829256\n",
      "[8189,     1] loss: 0.0095828466\n",
      "[8190,     1] loss: 0.0095827691\n",
      "[8191,     1] loss: 0.0095826894\n",
      "[8192,     1] loss: 0.0095826097\n",
      "[8193,     1] loss: 0.0095825307\n",
      "[8194,     1] loss: 0.0095824510\n",
      "[8195,     1] loss: 0.0095823720\n",
      "[8196,     1] loss: 0.0095822945\n",
      "[8197,     1] loss: 0.0095822133\n",
      "[8198,     1] loss: 0.0095821358\n",
      "[8199,     1] loss: 0.0095820561\n",
      "[8200,     1] loss: 0.0095819756\n",
      "[8201,     1] loss: 0.0095818974\n",
      "[8202,     1] loss: 0.0095818192\n",
      "[8203,     1] loss: 0.0095817395\n",
      "[8204,     1] loss: 0.0095816605\n",
      "[8205,     1] loss: 0.0095815800\n",
      "[8206,     1] loss: 0.0095815025\n",
      "[8207,     1] loss: 0.0095814221\n",
      "[8208,     1] loss: 0.0095813438\n",
      "[8209,     1] loss: 0.0095812656\n",
      "[8210,     1] loss: 0.0095811844\n",
      "[8211,     1] loss: 0.0095811062\n",
      "[8212,     1] loss: 0.0095810264\n",
      "[8213,     1] loss: 0.0095809482\n",
      "[8214,     1] loss: 0.0095808677\n",
      "[8215,     1] loss: 0.0095807903\n",
      "[8216,     1] loss: 0.0095807113\n",
      "[8217,     1] loss: 0.0095806316\n",
      "[8218,     1] loss: 0.0095805518\n",
      "[8219,     1] loss: 0.0095804736\n",
      "[8220,     1] loss: 0.0095803931\n",
      "[8221,     1] loss: 0.0095803149\n",
      "[8222,     1] loss: 0.0095802367\n",
      "[8223,     1] loss: 0.0095801562\n",
      "[8224,     1] loss: 0.0095800787\n",
      "[8225,     1] loss: 0.0095799983\n",
      "[8226,     1] loss: 0.0095799208\n",
      "[8227,     1] loss: 0.0095798403\n",
      "[8228,     1] loss: 0.0095797613\n",
      "[8229,     1] loss: 0.0095796824\n",
      "[8230,     1] loss: 0.0095796041\n",
      "[8231,     1] loss: 0.0095795244\n",
      "[8232,     1] loss: 0.0095794454\n",
      "[8233,     1] loss: 0.0095793650\n",
      "[8234,     1] loss: 0.0095792882\n",
      "[8235,     1] loss: 0.0095792077\n",
      "[8236,     1] loss: 0.0095791288\n",
      "[8237,     1] loss: 0.0095790498\n",
      "[8238,     1] loss: 0.0095789716\n",
      "[8239,     1] loss: 0.0095788904\n",
      "[8240,     1] loss: 0.0095788114\n",
      "[8241,     1] loss: 0.0095787331\n",
      "[8242,     1] loss: 0.0095786542\n",
      "[8243,     1] loss: 0.0095785737\n",
      "[8244,     1] loss: 0.0095784977\n",
      "[8245,     1] loss: 0.0095784187\n",
      "[8246,     1] loss: 0.0095783383\n",
      "[8247,     1] loss: 0.0095782593\n",
      "[8248,     1] loss: 0.0095781818\n",
      "[8249,     1] loss: 0.0095781013\n",
      "[8250,     1] loss: 0.0095780224\n",
      "[8251,     1] loss: 0.0095779434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8252,     1] loss: 0.0095778659\n",
      "[8253,     1] loss: 0.0095777847\n",
      "[8254,     1] loss: 0.0095777057\n",
      "[8255,     1] loss: 0.0095776275\n",
      "[8256,     1] loss: 0.0095775478\n",
      "[8257,     1] loss: 0.0095774680\n",
      "[8258,     1] loss: 0.0095773891\n",
      "[8259,     1] loss: 0.0095773108\n",
      "[8260,     1] loss: 0.0095772311\n",
      "[8261,     1] loss: 0.0095771529\n",
      "[8262,     1] loss: 0.0095770746\n",
      "[8263,     1] loss: 0.0095769949\n",
      "[8264,     1] loss: 0.0095769145\n",
      "[8265,     1] loss: 0.0095768370\n",
      "[8266,     1] loss: 0.0095767573\n",
      "[8267,     1] loss: 0.0095766783\n",
      "[8268,     1] loss: 0.0095766000\n",
      "[8269,     1] loss: 0.0095765218\n",
      "[8270,     1] loss: 0.0095764413\n",
      "[8271,     1] loss: 0.0095763631\n",
      "[8272,     1] loss: 0.0095762841\n",
      "[8273,     1] loss: 0.0095762044\n",
      "[8274,     1] loss: 0.0095761254\n",
      "[8275,     1] loss: 0.0095760472\n",
      "[8276,     1] loss: 0.0095759682\n",
      "[8277,     1] loss: 0.0095758893\n",
      "[8278,     1] loss: 0.0095758095\n",
      "[8279,     1] loss: 0.0095757313\n",
      "[8280,     1] loss: 0.0095756508\n",
      "[8281,     1] loss: 0.0095755726\n",
      "[8282,     1] loss: 0.0095754951\n",
      "[8283,     1] loss: 0.0095754169\n",
      "[8284,     1] loss: 0.0095753372\n",
      "[8285,     1] loss: 0.0095752575\n",
      "[8286,     1] loss: 0.0095751777\n",
      "[8287,     1] loss: 0.0095751002\n",
      "[8288,     1] loss: 0.0095750205\n",
      "[8289,     1] loss: 0.0095749415\n",
      "[8290,     1] loss: 0.0095748641\n",
      "[8291,     1] loss: 0.0095747843\n",
      "[8292,     1] loss: 0.0095747046\n",
      "[8293,     1] loss: 0.0095746256\n",
      "[8294,     1] loss: 0.0095745474\n",
      "[8295,     1] loss: 0.0095744677\n",
      "[8296,     1] loss: 0.0095743895\n",
      "[8297,     1] loss: 0.0095743112\n",
      "[8298,     1] loss: 0.0095742315\n",
      "[8299,     1] loss: 0.0095741533\n",
      "[8300,     1] loss: 0.0095740736\n",
      "[8301,     1] loss: 0.0095739953\n",
      "[8302,     1] loss: 0.0095739171\n",
      "[8303,     1] loss: 0.0095738374\n",
      "[8304,     1] loss: 0.0095737591\n",
      "[8305,     1] loss: 0.0095736794\n",
      "[8306,     1] loss: 0.0095736004\n",
      "[8307,     1] loss: 0.0095735222\n",
      "[8308,     1] loss: 0.0095734417\n",
      "[8309,     1] loss: 0.0095733628\n",
      "[8310,     1] loss: 0.0095732838\n",
      "[8311,     1] loss: 0.0095732056\n",
      "[8312,     1] loss: 0.0095731273\n",
      "[8313,     1] loss: 0.0095730491\n",
      "[8314,     1] loss: 0.0095729679\n",
      "[8315,     1] loss: 0.0095728911\n",
      "[8316,     1] loss: 0.0095728114\n",
      "[8317,     1] loss: 0.0095727324\n",
      "[8318,     1] loss: 0.0095726535\n",
      "[8319,     1] loss: 0.0095725745\n",
      "[8320,     1] loss: 0.0095724978\n",
      "[8321,     1] loss: 0.0095724180\n",
      "[8322,     1] loss: 0.0095723383\n",
      "[8323,     1] loss: 0.0095722593\n",
      "[8324,     1] loss: 0.0095721811\n",
      "[8325,     1] loss: 0.0095721014\n",
      "[8326,     1] loss: 0.0095720224\n",
      "[8327,     1] loss: 0.0095719449\n",
      "[8328,     1] loss: 0.0095718659\n",
      "[8329,     1] loss: 0.0095717862\n",
      "[8330,     1] loss: 0.0095717087\n",
      "[8331,     1] loss: 0.0095716283\n",
      "[8332,     1] loss: 0.0095715493\n",
      "[8333,     1] loss: 0.0095714718\n",
      "[8334,     1] loss: 0.0095713928\n",
      "[8335,     1] loss: 0.0095713131\n",
      "[8336,     1] loss: 0.0095712341\n",
      "[8337,     1] loss: 0.0095711559\n",
      "[8338,     1] loss: 0.0095710762\n",
      "[8339,     1] loss: 0.0095709987\n",
      "[8340,     1] loss: 0.0095709190\n",
      "[8341,     1] loss: 0.0095708407\n",
      "[8342,     1] loss: 0.0095707610\n",
      "[8343,     1] loss: 0.0095706828\n",
      "[8344,     1] loss: 0.0095706046\n",
      "[8345,     1] loss: 0.0095705256\n",
      "[8346,     1] loss: 0.0095704459\n",
      "[8347,     1] loss: 0.0095703669\n",
      "[8348,     1] loss: 0.0095702879\n",
      "[8349,     1] loss: 0.0095702112\n",
      "[8350,     1] loss: 0.0095701322\n",
      "[8351,     1] loss: 0.0095700525\n",
      "[8352,     1] loss: 0.0095699742\n",
      "[8353,     1] loss: 0.0095698953\n",
      "[8354,     1] loss: 0.0095698163\n",
      "[8355,     1] loss: 0.0095697388\n",
      "[8356,     1] loss: 0.0095696598\n",
      "[8357,     1] loss: 0.0095695816\n",
      "[8358,     1] loss: 0.0095695011\n",
      "[8359,     1] loss: 0.0095694222\n",
      "[8360,     1] loss: 0.0095693454\n",
      "[8361,     1] loss: 0.0095692657\n",
      "[8362,     1] loss: 0.0095691875\n",
      "[8363,     1] loss: 0.0095691085\n",
      "[8364,     1] loss: 0.0095690310\n",
      "[8365,     1] loss: 0.0095689520\n",
      "[8366,     1] loss: 0.0095688716\n",
      "[8367,     1] loss: 0.0095687933\n",
      "[8368,     1] loss: 0.0095687158\n",
      "[8369,     1] loss: 0.0095686361\n",
      "[8370,     1] loss: 0.0095685579\n",
      "[8371,     1] loss: 0.0095684789\n",
      "[8372,     1] loss: 0.0095684029\n",
      "[8373,     1] loss: 0.0095683224\n",
      "[8374,     1] loss: 0.0095682442\n",
      "[8375,     1] loss: 0.0095681660\n",
      "[8376,     1] loss: 0.0095680863\n",
      "[8377,     1] loss: 0.0095680065\n",
      "[8378,     1] loss: 0.0095679291\n",
      "[8379,     1] loss: 0.0095678508\n",
      "[8380,     1] loss: 0.0095677733\n",
      "[8381,     1] loss: 0.0095676936\n",
      "[8382,     1] loss: 0.0095676146\n",
      "[8383,     1] loss: 0.0095675357\n",
      "[8384,     1] loss: 0.0095674574\n",
      "[8385,     1] loss: 0.0095673785\n",
      "[8386,     1] loss: 0.0095672987\n",
      "[8387,     1] loss: 0.0095672213\n",
      "[8388,     1] loss: 0.0095671438\n",
      "[8389,     1] loss: 0.0095670655\n",
      "[8390,     1] loss: 0.0095669866\n",
      "[8391,     1] loss: 0.0095669076\n",
      "[8392,     1] loss: 0.0095668286\n",
      "[8393,     1] loss: 0.0095667504\n",
      "[8394,     1] loss: 0.0095666721\n",
      "[8395,     1] loss: 0.0095665954\n",
      "[8396,     1] loss: 0.0095665149\n",
      "[8397,     1] loss: 0.0095664360\n",
      "[8398,     1] loss: 0.0095663577\n",
      "[8399,     1] loss: 0.0095662795\n",
      "[8400,     1] loss: 0.0095662013\n",
      "[8401,     1] loss: 0.0095661230\n",
      "[8402,     1] loss: 0.0095660441\n",
      "[8403,     1] loss: 0.0095659666\n",
      "[8404,     1] loss: 0.0095658876\n",
      "[8405,     1] loss: 0.0095658086\n",
      "[8406,     1] loss: 0.0095657296\n",
      "[8407,     1] loss: 0.0095656522\n",
      "[8408,     1] loss: 0.0095655739\n",
      "[8409,     1] loss: 0.0095654957\n",
      "[8410,     1] loss: 0.0095654152\n",
      "[8411,     1] loss: 0.0095653377\n",
      "[8412,     1] loss: 0.0095652595\n",
      "[8413,     1] loss: 0.0095651820\n",
      "[8414,     1] loss: 0.0095651038\n",
      "[8415,     1] loss: 0.0095650256\n",
      "[8416,     1] loss: 0.0095649473\n",
      "[8417,     1] loss: 0.0095648676\n",
      "[8418,     1] loss: 0.0095647894\n",
      "[8419,     1] loss: 0.0095647119\n",
      "[8420,     1] loss: 0.0095646322\n",
      "[8421,     1] loss: 0.0095645547\n",
      "[8422,     1] loss: 0.0095644750\n",
      "[8423,     1] loss: 0.0095643975\n",
      "[8424,     1] loss: 0.0095643193\n",
      "[8425,     1] loss: 0.0095642418\n",
      "[8426,     1] loss: 0.0095641635\n",
      "[8427,     1] loss: 0.0095640853\n",
      "[8428,     1] loss: 0.0095640063\n",
      "[8429,     1] loss: 0.0095639266\n",
      "[8430,     1] loss: 0.0095638506\n",
      "[8431,     1] loss: 0.0095637701\n",
      "[8432,     1] loss: 0.0095636927\n",
      "[8433,     1] loss: 0.0095636152\n",
      "[8434,     1] loss: 0.0095635362\n",
      "[8435,     1] loss: 0.0095634572\n",
      "[8436,     1] loss: 0.0095633797\n",
      "[8437,     1] loss: 0.0095633022\n",
      "[8438,     1] loss: 0.0095632225\n",
      "[8439,     1] loss: 0.0095631450\n",
      "[8440,     1] loss: 0.0095630661\n",
      "[8441,     1] loss: 0.0095629886\n",
      "[8442,     1] loss: 0.0095629096\n",
      "[8443,     1] loss: 0.0095628314\n",
      "[8444,     1] loss: 0.0095627531\n",
      "[8445,     1] loss: 0.0095626757\n",
      "[8446,     1] loss: 0.0095625982\n",
      "[8447,     1] loss: 0.0095625192\n",
      "[8448,     1] loss: 0.0095624425\n",
      "[8449,     1] loss: 0.0095623635\n",
      "[8450,     1] loss: 0.0095622845\n",
      "[8451,     1] loss: 0.0095622063\n",
      "[8452,     1] loss: 0.0095621295\n",
      "[8453,     1] loss: 0.0095620498\n",
      "[8454,     1] loss: 0.0095619716\n",
      "[8455,     1] loss: 0.0095618941\n",
      "[8456,     1] loss: 0.0095618151\n",
      "[8457,     1] loss: 0.0095617376\n",
      "[8458,     1] loss: 0.0095616587\n",
      "[8459,     1] loss: 0.0095615804\n",
      "[8460,     1] loss: 0.0095615037\n",
      "[8461,     1] loss: 0.0095614254\n",
      "[8462,     1] loss: 0.0095613465\n",
      "[8463,     1] loss: 0.0095612690\n",
      "[8464,     1] loss: 0.0095611900\n",
      "[8465,     1] loss: 0.0095611118\n",
      "[8466,     1] loss: 0.0095610335\n",
      "[8467,     1] loss: 0.0095609561\n",
      "[8468,     1] loss: 0.0095608786\n",
      "[8469,     1] loss: 0.0095607996\n",
      "[8470,     1] loss: 0.0095607221\n",
      "[8471,     1] loss: 0.0095606446\n",
      "[8472,     1] loss: 0.0095605657\n",
      "[8473,     1] loss: 0.0095604867\n",
      "[8474,     1] loss: 0.0095604092\n",
      "[8475,     1] loss: 0.0095603317\n",
      "[8476,     1] loss: 0.0095602542\n",
      "[8477,     1] loss: 0.0095601760\n",
      "[8478,     1] loss: 0.0095600963\n",
      "[8479,     1] loss: 0.0095600188\n",
      "[8480,     1] loss: 0.0095599413\n",
      "[8481,     1] loss: 0.0095598638\n",
      "[8482,     1] loss: 0.0095597856\n",
      "[8483,     1] loss: 0.0095597088\n",
      "[8484,     1] loss: 0.0095596299\n",
      "[8485,     1] loss: 0.0095595509\n",
      "[8486,     1] loss: 0.0095594741\n",
      "[8487,     1] loss: 0.0095593967\n",
      "[8488,     1] loss: 0.0095593177\n",
      "[8489,     1] loss: 0.0095592402\n",
      "[8490,     1] loss: 0.0095591620\n",
      "[8491,     1] loss: 0.0095590830\n",
      "[8492,     1] loss: 0.0095590077\n",
      "[8493,     1] loss: 0.0095589295\n",
      "[8494,     1] loss: 0.0095588520\n",
      "[8495,     1] loss: 0.0095587730\n",
      "[8496,     1] loss: 0.0095586941\n",
      "[8497,     1] loss: 0.0095586181\n",
      "[8498,     1] loss: 0.0095585391\n",
      "[8499,     1] loss: 0.0095584616\n",
      "[8500,     1] loss: 0.0095583849\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8501,     1] loss: 0.0095583066\n",
      "[8502,     1] loss: 0.0095582277\n",
      "[8503,     1] loss: 0.0095581509\n",
      "[8504,     1] loss: 0.0095580727\n",
      "[8505,     1] loss: 0.0095579945\n",
      "[8506,     1] loss: 0.0095579170\n",
      "[8507,     1] loss: 0.0095578395\n",
      "[8508,     1] loss: 0.0095577620\n",
      "[8509,     1] loss: 0.0095576845\n",
      "[8510,     1] loss: 0.0095576055\n",
      "[8511,     1] loss: 0.0095575288\n",
      "[8512,     1] loss: 0.0095574513\n",
      "[8513,     1] loss: 0.0095573716\n",
      "[8514,     1] loss: 0.0095572941\n",
      "[8515,     1] loss: 0.0095572188\n",
      "[8516,     1] loss: 0.0095571384\n",
      "[8517,     1] loss: 0.0095570616\n",
      "[8518,     1] loss: 0.0095569849\n",
      "[8519,     1] loss: 0.0095569059\n",
      "[8520,     1] loss: 0.0095568292\n",
      "[8521,     1] loss: 0.0095567510\n",
      "[8522,     1] loss: 0.0095566720\n",
      "[8523,     1] loss: 0.0095565952\n",
      "[8524,     1] loss: 0.0095565170\n",
      "[8525,     1] loss: 0.0095564395\n",
      "[8526,     1] loss: 0.0095563613\n",
      "[8527,     1] loss: 0.0095562853\n",
      "[8528,     1] loss: 0.0095562071\n",
      "[8529,     1] loss: 0.0095561281\n",
      "[8530,     1] loss: 0.0095560513\n",
      "[8531,     1] loss: 0.0095559731\n",
      "[8532,     1] loss: 0.0095558964\n",
      "[8533,     1] loss: 0.0095558174\n",
      "[8534,     1] loss: 0.0095557407\n",
      "[8535,     1] loss: 0.0095556639\n",
      "[8536,     1] loss: 0.0095555849\n",
      "[8537,     1] loss: 0.0095555075\n",
      "[8538,     1] loss: 0.0095554315\n",
      "[8539,     1] loss: 0.0095553532\n",
      "[8540,     1] loss: 0.0095552757\n",
      "[8541,     1] loss: 0.0095551983\n",
      "[8542,     1] loss: 0.0095551200\n",
      "[8543,     1] loss: 0.0095550418\n",
      "[8544,     1] loss: 0.0095549650\n",
      "[8545,     1] loss: 0.0095548868\n",
      "[8546,     1] loss: 0.0095548108\n",
      "[8547,     1] loss: 0.0095547333\n",
      "[8548,     1] loss: 0.0095546551\n",
      "[8549,     1] loss: 0.0095545776\n",
      "[8550,     1] loss: 0.0095545009\n",
      "[8551,     1] loss: 0.0095544241\n",
      "[8552,     1] loss: 0.0095543452\n",
      "[8553,     1] loss: 0.0095542669\n",
      "[8554,     1] loss: 0.0095541909\n",
      "[8555,     1] loss: 0.0095541142\n",
      "[8556,     1] loss: 0.0095540360\n",
      "[8557,     1] loss: 0.0095539577\n",
      "[8558,     1] loss: 0.0095538817\n",
      "[8559,     1] loss: 0.0095538035\n",
      "[8560,     1] loss: 0.0095537275\n",
      "[8561,     1] loss: 0.0095536500\n",
      "[8562,     1] loss: 0.0095535733\n",
      "[8563,     1] loss: 0.0095534950\n",
      "[8564,     1] loss: 0.0095534191\n",
      "[8565,     1] loss: 0.0095533408\n",
      "[8566,     1] loss: 0.0095532641\n",
      "[8567,     1] loss: 0.0095531859\n",
      "[8568,     1] loss: 0.0095531084\n",
      "[8569,     1] loss: 0.0095530309\n",
      "[8570,     1] loss: 0.0095529541\n",
      "[8571,     1] loss: 0.0095528759\n",
      "[8572,     1] loss: 0.0095527984\n",
      "[8573,     1] loss: 0.0095527224\n",
      "[8574,     1] loss: 0.0095526449\n",
      "[8575,     1] loss: 0.0095525675\n",
      "[8576,     1] loss: 0.0095524907\n",
      "[8577,     1] loss: 0.0095524147\n",
      "[8578,     1] loss: 0.0095523357\n",
      "[8579,     1] loss: 0.0095522583\n",
      "[8580,     1] loss: 0.0095521815\n",
      "[8581,     1] loss: 0.0095521040\n",
      "[8582,     1] loss: 0.0095520265\n",
      "[8583,     1] loss: 0.0095519498\n",
      "[8584,     1] loss: 0.0095518731\n",
      "[8585,     1] loss: 0.0095517963\n",
      "[8586,     1] loss: 0.0095517181\n",
      "[8587,     1] loss: 0.0095516406\n",
      "[8588,     1] loss: 0.0095515653\n",
      "[8589,     1] loss: 0.0095514879\n",
      "[8590,     1] loss: 0.0095514104\n",
      "[8591,     1] loss: 0.0095513336\n",
      "[8592,     1] loss: 0.0095512562\n",
      "[8593,     1] loss: 0.0095511802\n",
      "[8594,     1] loss: 0.0095511027\n",
      "[8595,     1] loss: 0.0095510252\n",
      "[8596,     1] loss: 0.0095509477\n",
      "[8597,     1] loss: 0.0095508717\n",
      "[8598,     1] loss: 0.0095507950\n",
      "[8599,     1] loss: 0.0095507182\n",
      "[8600,     1] loss: 0.0095506407\n",
      "[8601,     1] loss: 0.0095505640\n",
      "[8602,     1] loss: 0.0095504865\n",
      "[8603,     1] loss: 0.0095504098\n",
      "[8604,     1] loss: 0.0095503323\n",
      "[8605,     1] loss: 0.0095502555\n",
      "[8606,     1] loss: 0.0095501795\n",
      "[8607,     1] loss: 0.0095501013\n",
      "[8608,     1] loss: 0.0095500238\n",
      "[8609,     1] loss: 0.0095499471\n",
      "[8610,     1] loss: 0.0095498711\n",
      "[8611,     1] loss: 0.0095497943\n",
      "[8612,     1] loss: 0.0095497176\n",
      "[8613,     1] loss: 0.0095496409\n",
      "[8614,     1] loss: 0.0095495634\n",
      "[8615,     1] loss: 0.0095494866\n",
      "[8616,     1] loss: 0.0095494099\n",
      "[8617,     1] loss: 0.0095493332\n",
      "[8618,     1] loss: 0.0095492557\n",
      "[8619,     1] loss: 0.0095491789\n",
      "[8620,     1] loss: 0.0095491029\n",
      "[8621,     1] loss: 0.0095490254\n",
      "[8622,     1] loss: 0.0095489495\n",
      "[8623,     1] loss: 0.0095488720\n",
      "[8624,     1] loss: 0.0095487952\n",
      "[8625,     1] loss: 0.0095487185\n",
      "[8626,     1] loss: 0.0095486425\n",
      "[8627,     1] loss: 0.0095485657\n",
      "[8628,     1] loss: 0.0095484883\n",
      "[8629,     1] loss: 0.0095484123\n",
      "[8630,     1] loss: 0.0095483363\n",
      "[8631,     1] loss: 0.0095482595\n",
      "[8632,     1] loss: 0.0095481828\n",
      "[8633,     1] loss: 0.0095481060\n",
      "[8634,     1] loss: 0.0095480300\n",
      "[8635,     1] loss: 0.0095479511\n",
      "[8636,     1] loss: 0.0095478758\n",
      "[8637,     1] loss: 0.0095477998\n",
      "[8638,     1] loss: 0.0095477223\n",
      "[8639,     1] loss: 0.0095476471\n",
      "[8640,     1] loss: 0.0095475696\n",
      "[8641,     1] loss: 0.0095474929\n",
      "[8642,     1] loss: 0.0095474169\n",
      "[8643,     1] loss: 0.0095473394\n",
      "[8644,     1] loss: 0.0095472634\n",
      "[8645,     1] loss: 0.0095471866\n",
      "[8646,     1] loss: 0.0095471106\n",
      "[8647,     1] loss: 0.0095470332\n",
      "[8648,     1] loss: 0.0095469579\n",
      "[8649,     1] loss: 0.0095468804\n",
      "[8650,     1] loss: 0.0095468037\n",
      "[8651,     1] loss: 0.0095467284\n",
      "[8652,     1] loss: 0.0095466502\n",
      "[8653,     1] loss: 0.0095465735\n",
      "[8654,     1] loss: 0.0095464990\n",
      "[8655,     1] loss: 0.0095464230\n",
      "[8656,     1] loss: 0.0095463455\n",
      "[8657,     1] loss: 0.0095462710\n",
      "[8658,     1] loss: 0.0095461950\n",
      "[8659,     1] loss: 0.0095461175\n",
      "[8660,     1] loss: 0.0095460407\n",
      "[8661,     1] loss: 0.0095459655\n",
      "[8662,     1] loss: 0.0095458880\n",
      "[8663,     1] loss: 0.0095458120\n",
      "[8664,     1] loss: 0.0095457360\n",
      "[8665,     1] loss: 0.0095456593\n",
      "[8666,     1] loss: 0.0095455818\n",
      "[8667,     1] loss: 0.0095455058\n",
      "[8668,     1] loss: 0.0095454313\n",
      "[8669,     1] loss: 0.0095453545\n",
      "[8670,     1] loss: 0.0095452778\n",
      "[8671,     1] loss: 0.0095452026\n",
      "[8672,     1] loss: 0.0095451251\n",
      "[8673,     1] loss: 0.0095450498\n",
      "[8674,     1] loss: 0.0095449738\n",
      "[8675,     1] loss: 0.0095448971\n",
      "[8676,     1] loss: 0.0095448218\n",
      "[8677,     1] loss: 0.0095447451\n",
      "[8678,     1] loss: 0.0095446676\n",
      "[8679,     1] loss: 0.0095445938\n",
      "[8680,     1] loss: 0.0095445164\n",
      "[8681,     1] loss: 0.0095444404\n",
      "[8682,     1] loss: 0.0095443644\n",
      "[8683,     1] loss: 0.0095442891\n",
      "[8684,     1] loss: 0.0095442124\n",
      "[8685,     1] loss: 0.0095441364\n",
      "[8686,     1] loss: 0.0095440604\n",
      "[8687,     1] loss: 0.0095439844\n",
      "[8688,     1] loss: 0.0095439084\n",
      "[8689,     1] loss: 0.0095438316\n",
      "[8690,     1] loss: 0.0095437571\n",
      "[8691,     1] loss: 0.0095436811\n",
      "[8692,     1] loss: 0.0095436051\n",
      "[8693,     1] loss: 0.0095435284\n",
      "[8694,     1] loss: 0.0095434524\n",
      "[8695,     1] loss: 0.0095433779\n",
      "[8696,     1] loss: 0.0095433012\n",
      "[8697,     1] loss: 0.0095432259\n",
      "[8698,     1] loss: 0.0095431492\n",
      "[8699,     1] loss: 0.0095430739\n",
      "[8700,     1] loss: 0.0095429972\n",
      "[8701,     1] loss: 0.0095429227\n",
      "[8702,     1] loss: 0.0095428474\n",
      "[8703,     1] loss: 0.0095427707\n",
      "[8704,     1] loss: 0.0095426954\n",
      "[8705,     1] loss: 0.0095426194\n",
      "[8706,     1] loss: 0.0095425434\n",
      "[8707,     1] loss: 0.0095424674\n",
      "[8708,     1] loss: 0.0095423907\n",
      "[8709,     1] loss: 0.0095423155\n",
      "[8710,     1] loss: 0.0095422402\n",
      "[8711,     1] loss: 0.0095421642\n",
      "[8712,     1] loss: 0.0095420882\n",
      "[8713,     1] loss: 0.0095420122\n",
      "[8714,     1] loss: 0.0095419377\n",
      "[8715,     1] loss: 0.0095418602\n",
      "[8716,     1] loss: 0.0095417850\n",
      "[8717,     1] loss: 0.0095417097\n",
      "[8718,     1] loss: 0.0095416322\n",
      "[8719,     1] loss: 0.0095415577\n",
      "[8720,     1] loss: 0.0095414832\n",
      "[8721,     1] loss: 0.0095414080\n",
      "[8722,     1] loss: 0.0095413297\n",
      "[8723,     1] loss: 0.0095412560\n",
      "[8724,     1] loss: 0.0095411807\n",
      "[8725,     1] loss: 0.0095411047\n",
      "[8726,     1] loss: 0.0095410310\n",
      "[8727,     1] loss: 0.0095409542\n",
      "[8728,     1] loss: 0.0095408790\n",
      "[8729,     1] loss: 0.0095408030\n",
      "[8730,     1] loss: 0.0095407285\n",
      "[8731,     1] loss: 0.0095406532\n",
      "[8732,     1] loss: 0.0095405772\n",
      "[8733,     1] loss: 0.0095405012\n",
      "[8734,     1] loss: 0.0095404282\n",
      "[8735,     1] loss: 0.0095403507\n",
      "[8736,     1] loss: 0.0095402762\n",
      "[8737,     1] loss: 0.0095402002\n",
      "[8738,     1] loss: 0.0095401242\n",
      "[8739,     1] loss: 0.0095400490\n",
      "[8740,     1] loss: 0.0095399752\n",
      "[8741,     1] loss: 0.0095398992\n",
      "[8742,     1] loss: 0.0095398225\n",
      "[8743,     1] loss: 0.0095397487\n",
      "[8744,     1] loss: 0.0095396735\n",
      "[8745,     1] loss: 0.0095395975\n",
      "[8746,     1] loss: 0.0095395230\n",
      "[8747,     1] loss: 0.0095394477\n",
      "[8748,     1] loss: 0.0095393725\n",
      "[8749,     1] loss: 0.0095392980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8750,     1] loss: 0.0095392235\n",
      "[8751,     1] loss: 0.0095391467\n",
      "[8752,     1] loss: 0.0095390722\n",
      "[8753,     1] loss: 0.0095389992\n",
      "[8754,     1] loss: 0.0095389217\n",
      "[8755,     1] loss: 0.0095388480\n",
      "[8756,     1] loss: 0.0095387727\n",
      "[8757,     1] loss: 0.0095386960\n",
      "[8758,     1] loss: 0.0095386237\n",
      "[8759,     1] loss: 0.0095385477\n",
      "[8760,     1] loss: 0.0095384710\n",
      "[8761,     1] loss: 0.0095383972\n",
      "[8762,     1] loss: 0.0095383212\n",
      "[8763,     1] loss: 0.0095382459\n",
      "[8764,     1] loss: 0.0095381714\n",
      "[8765,     1] loss: 0.0095380977\n",
      "[8766,     1] loss: 0.0095380232\n",
      "[8767,     1] loss: 0.0095379472\n",
      "[8768,     1] loss: 0.0095378727\n",
      "[8769,     1] loss: 0.0095377974\n",
      "[8770,     1] loss: 0.0095377252\n",
      "[8771,     1] loss: 0.0095376484\n",
      "[8772,     1] loss: 0.0095375732\n",
      "[8773,     1] loss: 0.0095374987\n",
      "[8774,     1] loss: 0.0095374234\n",
      "[8775,     1] loss: 0.0095373489\n",
      "[8776,     1] loss: 0.0095372744\n",
      "[8777,     1] loss: 0.0095371984\n",
      "[8778,     1] loss: 0.0095371239\n",
      "[8779,     1] loss: 0.0095370501\n",
      "[8780,     1] loss: 0.0095369749\n",
      "[8781,     1] loss: 0.0095369004\n",
      "[8782,     1] loss: 0.0095368259\n",
      "[8783,     1] loss: 0.0095367506\n",
      "[8784,     1] loss: 0.0095366761\n",
      "[8785,     1] loss: 0.0095366009\n",
      "[8786,     1] loss: 0.0095365278\n",
      "[8787,     1] loss: 0.0095364518\n",
      "[8788,     1] loss: 0.0095363766\n",
      "[8789,     1] loss: 0.0095363028\n",
      "[8790,     1] loss: 0.0095362291\n",
      "[8791,     1] loss: 0.0095361546\n",
      "[8792,     1] loss: 0.0095360793\n",
      "[8793,     1] loss: 0.0095360048\n",
      "[8794,     1] loss: 0.0095359303\n",
      "[8795,     1] loss: 0.0095358558\n",
      "[8796,     1] loss: 0.0095357813\n",
      "[8797,     1] loss: 0.0095357068\n",
      "[8798,     1] loss: 0.0095356323\n",
      "[8799,     1] loss: 0.0095355578\n",
      "[8800,     1] loss: 0.0095354840\n",
      "[8801,     1] loss: 0.0095354095\n",
      "[8802,     1] loss: 0.0095353357\n",
      "[8803,     1] loss: 0.0095352612\n",
      "[8804,     1] loss: 0.0095351860\n",
      "[8805,     1] loss: 0.0095351115\n",
      "[8806,     1] loss: 0.0095350385\n",
      "[8807,     1] loss: 0.0095349625\n",
      "[8808,     1] loss: 0.0095348895\n",
      "[8809,     1] loss: 0.0095348157\n",
      "[8810,     1] loss: 0.0095347404\n",
      "[8811,     1] loss: 0.0095346667\n",
      "[8812,     1] loss: 0.0095345922\n",
      "[8813,     1] loss: 0.0095345169\n",
      "[8814,     1] loss: 0.0095344439\n",
      "[8815,     1] loss: 0.0095343702\n",
      "[8816,     1] loss: 0.0095342964\n",
      "[8817,     1] loss: 0.0095342211\n",
      "[8818,     1] loss: 0.0095341474\n",
      "[8819,     1] loss: 0.0095340736\n",
      "[8820,     1] loss: 0.0095339984\n",
      "[8821,     1] loss: 0.0095339246\n",
      "[8822,     1] loss: 0.0095338501\n",
      "[8823,     1] loss: 0.0095337771\n",
      "[8824,     1] loss: 0.0095337026\n",
      "[8825,     1] loss: 0.0095336296\n",
      "[8826,     1] loss: 0.0095335551\n",
      "[8827,     1] loss: 0.0095334813\n",
      "[8828,     1] loss: 0.0095334083\n",
      "[8829,     1] loss: 0.0095333338\n",
      "[8830,     1] loss: 0.0095332600\n",
      "[8831,     1] loss: 0.0095331870\n",
      "[8832,     1] loss: 0.0095331125\n",
      "[8833,     1] loss: 0.0095330387\n",
      "[8834,     1] loss: 0.0095329650\n",
      "[8835,     1] loss: 0.0095328905\n",
      "[8836,     1] loss: 0.0095328182\n",
      "[8837,     1] loss: 0.0095327429\n",
      "[8838,     1] loss: 0.0095326699\n",
      "[8839,     1] loss: 0.0095325947\n",
      "[8840,     1] loss: 0.0095325224\n",
      "[8841,     1] loss: 0.0095324486\n",
      "[8842,     1] loss: 0.0095323741\n",
      "[8843,     1] loss: 0.0095322996\n",
      "[8844,     1] loss: 0.0095322274\n",
      "[8845,     1] loss: 0.0095321544\n",
      "[8846,     1] loss: 0.0095320798\n",
      "[8847,     1] loss: 0.0095320068\n",
      "[8848,     1] loss: 0.0095319331\n",
      "[8849,     1] loss: 0.0095318593\n",
      "[8850,     1] loss: 0.0095317855\n",
      "[8851,     1] loss: 0.0095317118\n",
      "[8852,     1] loss: 0.0095316380\n",
      "[8853,     1] loss: 0.0095315650\n",
      "[8854,     1] loss: 0.0095314912\n",
      "[8855,     1] loss: 0.0095314175\n",
      "[8856,     1] loss: 0.0095313437\n",
      "[8857,     1] loss: 0.0095312707\n",
      "[8858,     1] loss: 0.0095311970\n",
      "[8859,     1] loss: 0.0095311239\n",
      "[8860,     1] loss: 0.0095310502\n",
      "[8861,     1] loss: 0.0095309764\n",
      "[8862,     1] loss: 0.0095309034\n",
      "[8863,     1] loss: 0.0095308311\n",
      "[8864,     1] loss: 0.0095307574\n",
      "[8865,     1] loss: 0.0095306851\n",
      "[8866,     1] loss: 0.0095306113\n",
      "[8867,     1] loss: 0.0095305376\n",
      "[8868,     1] loss: 0.0095304646\n",
      "[8869,     1] loss: 0.0095303915\n",
      "[8870,     1] loss: 0.0095303178\n",
      "[8871,     1] loss: 0.0095302448\n",
      "[8872,     1] loss: 0.0095301718\n",
      "[8873,     1] loss: 0.0095300995\n",
      "[8874,     1] loss: 0.0095300257\n",
      "[8875,     1] loss: 0.0095299527\n",
      "[8876,     1] loss: 0.0095298797\n",
      "[8877,     1] loss: 0.0095298067\n",
      "[8878,     1] loss: 0.0095297329\n",
      "[8879,     1] loss: 0.0095296614\n",
      "[8880,     1] loss: 0.0095295876\n",
      "[8881,     1] loss: 0.0095295146\n",
      "[8882,     1] loss: 0.0095294409\n",
      "[8883,     1] loss: 0.0095293686\n",
      "[8884,     1] loss: 0.0095292956\n",
      "[8885,     1] loss: 0.0095292218\n",
      "[8886,     1] loss: 0.0095291503\n",
      "[8887,     1] loss: 0.0095290773\n",
      "[8888,     1] loss: 0.0095290035\n",
      "[8889,     1] loss: 0.0095289312\n",
      "[8890,     1] loss: 0.0095288590\n",
      "[8891,     1] loss: 0.0095287852\n",
      "[8892,     1] loss: 0.0095287137\n",
      "[8893,     1] loss: 0.0095286407\n",
      "[8894,     1] loss: 0.0095285676\n",
      "[8895,     1] loss: 0.0095284954\n",
      "[8896,     1] loss: 0.0095284231\n",
      "[8897,     1] loss: 0.0095283493\n",
      "[8898,     1] loss: 0.0095282763\n",
      "[8899,     1] loss: 0.0095282048\n",
      "[8900,     1] loss: 0.0095281318\n",
      "[8901,     1] loss: 0.0095280580\n",
      "[8902,     1] loss: 0.0095279858\n",
      "[8903,     1] loss: 0.0095279150\n",
      "[8904,     1] loss: 0.0095278420\n",
      "[8905,     1] loss: 0.0095277712\n",
      "[8906,     1] loss: 0.0095276982\n",
      "[8907,     1] loss: 0.0095276244\n",
      "[8908,     1] loss: 0.0095275514\n",
      "[8909,     1] loss: 0.0095274791\n",
      "[8910,     1] loss: 0.0095274068\n",
      "[8911,     1] loss: 0.0095273346\n",
      "[8912,     1] loss: 0.0095272623\n",
      "[8913,     1] loss: 0.0095271908\n",
      "[8914,     1] loss: 0.0095271185\n",
      "[8915,     1] loss: 0.0095270462\n",
      "[8916,     1] loss: 0.0095269740\n",
      "[8917,     1] loss: 0.0095269009\n",
      "[8918,     1] loss: 0.0095268279\n",
      "[8919,     1] loss: 0.0095267564\n",
      "[8920,     1] loss: 0.0095266841\n",
      "[8921,     1] loss: 0.0095266104\n",
      "[8922,     1] loss: 0.0095265388\n",
      "[8923,     1] loss: 0.0095264681\n",
      "[8924,     1] loss: 0.0095263943\n",
      "[8925,     1] loss: 0.0095263228\n",
      "[8926,     1] loss: 0.0095262513\n",
      "[8927,     1] loss: 0.0095261790\n",
      "[8928,     1] loss: 0.0095261075\n",
      "[8929,     1] loss: 0.0095260352\n",
      "[8930,     1] loss: 0.0095259644\n",
      "[8931,     1] loss: 0.0095258906\n",
      "[8932,     1] loss: 0.0095258199\n",
      "[8933,     1] loss: 0.0095257476\n",
      "[8934,     1] loss: 0.0095256746\n",
      "[8935,     1] loss: 0.0095256031\n",
      "[8936,     1] loss: 0.0095255308\n",
      "[8937,     1] loss: 0.0095254593\n",
      "[8938,     1] loss: 0.0095253877\n",
      "[8939,     1] loss: 0.0095253162\n",
      "[8940,     1] loss: 0.0095252439\n",
      "[8941,     1] loss: 0.0095251717\n",
      "[8942,     1] loss: 0.0095250994\n",
      "[8943,     1] loss: 0.0095250286\n",
      "[8944,     1] loss: 0.0095249571\n",
      "[8945,     1] loss: 0.0095248848\n",
      "[8946,     1] loss: 0.0095248125\n",
      "[8947,     1] loss: 0.0095247418\n",
      "[8948,     1] loss: 0.0095246680\n",
      "[8949,     1] loss: 0.0095245980\n",
      "[8950,     1] loss: 0.0095245272\n",
      "[8951,     1] loss: 0.0095244542\n",
      "[8952,     1] loss: 0.0095243841\n",
      "[8953,     1] loss: 0.0095243126\n",
      "[8954,     1] loss: 0.0095242389\n",
      "[8955,     1] loss: 0.0095241696\n",
      "[8956,     1] loss: 0.0095240973\n",
      "[8957,     1] loss: 0.0095240265\n",
      "[8958,     1] loss: 0.0095239542\n",
      "[8959,     1] loss: 0.0095238835\n",
      "[8960,     1] loss: 0.0095238112\n",
      "[8961,     1] loss: 0.0095237404\n",
      "[8962,     1] loss: 0.0095236689\n",
      "[8963,     1] loss: 0.0095235966\n",
      "[8964,     1] loss: 0.0095235266\n",
      "[8965,     1] loss: 0.0095234551\n",
      "[8966,     1] loss: 0.0095233835\n",
      "[8967,     1] loss: 0.0095233135\n",
      "[8968,     1] loss: 0.0095232412\n",
      "[8969,     1] loss: 0.0095231704\n",
      "[8970,     1] loss: 0.0095230982\n",
      "[8971,     1] loss: 0.0095230274\n",
      "[8972,     1] loss: 0.0095229574\n",
      "[8973,     1] loss: 0.0095228866\n",
      "[8974,     1] loss: 0.0095228136\n",
      "[8975,     1] loss: 0.0095227435\n",
      "[8976,     1] loss: 0.0095226720\n",
      "[8977,     1] loss: 0.0095226005\n",
      "[8978,     1] loss: 0.0095225289\n",
      "[8979,     1] loss: 0.0095224589\n",
      "[8980,     1] loss: 0.0095223881\n",
      "[8981,     1] loss: 0.0095223166\n",
      "[8982,     1] loss: 0.0095222458\n",
      "[8983,     1] loss: 0.0095221750\n",
      "[8984,     1] loss: 0.0095221035\n",
      "[8985,     1] loss: 0.0095220327\n",
      "[8986,     1] loss: 0.0095219620\n",
      "[8987,     1] loss: 0.0095218912\n",
      "[8988,     1] loss: 0.0095218204\n",
      "[8989,     1] loss: 0.0095217504\n",
      "[8990,     1] loss: 0.0095216788\n",
      "[8991,     1] loss: 0.0095216088\n",
      "[8992,     1] loss: 0.0095215380\n",
      "[8993,     1] loss: 0.0095214665\n",
      "[8994,     1] loss: 0.0095213965\n",
      "[8995,     1] loss: 0.0095213264\n",
      "[8996,     1] loss: 0.0095212549\n",
      "[8997,     1] loss: 0.0095211841\n",
      "[8998,     1] loss: 0.0095211141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8999,     1] loss: 0.0095210440\n",
      "[9000,     1] loss: 0.0095209748\n",
      "[9001,     1] loss: 0.0095209017\n",
      "[9002,     1] loss: 0.0095208324\n",
      "[9003,     1] loss: 0.0095207617\n",
      "[9004,     1] loss: 0.0095206909\n",
      "[9005,     1] loss: 0.0095206194\n",
      "[9006,     1] loss: 0.0095205501\n",
      "[9007,     1] loss: 0.0095204785\n",
      "[9008,     1] loss: 0.0095204100\n",
      "[9009,     1] loss: 0.0095203377\n",
      "[9010,     1] loss: 0.0095202670\n",
      "[9011,     1] loss: 0.0095201991\n",
      "[9012,     1] loss: 0.0095201276\n",
      "[9013,     1] loss: 0.0095200561\n",
      "[9014,     1] loss: 0.0095199876\n",
      "[9015,     1] loss: 0.0095199160\n",
      "[9016,     1] loss: 0.0095198460\n",
      "[9017,     1] loss: 0.0095197774\n",
      "[9018,     1] loss: 0.0095197067\n",
      "[9019,     1] loss: 0.0095196359\n",
      "[9020,     1] loss: 0.0095195659\n",
      "[9021,     1] loss: 0.0095194958\n",
      "[9022,     1] loss: 0.0095194258\n",
      "[9023,     1] loss: 0.0095193557\n",
      "[9024,     1] loss: 0.0095192850\n",
      "[9025,     1] loss: 0.0095192164\n",
      "[9026,     1] loss: 0.0095191456\n",
      "[9027,     1] loss: 0.0095190763\n",
      "[9028,     1] loss: 0.0095190063\n",
      "[9029,     1] loss: 0.0095189370\n",
      "[9030,     1] loss: 0.0095188662\n",
      "[9031,     1] loss: 0.0095187962\n",
      "[9032,     1] loss: 0.0095187254\n",
      "[9033,     1] loss: 0.0095186561\n",
      "[9034,     1] loss: 0.0095185854\n",
      "[9035,     1] loss: 0.0095185168\n",
      "[9036,     1] loss: 0.0095184475\n",
      "[9037,     1] loss: 0.0095183767\n",
      "[9038,     1] loss: 0.0095183074\n",
      "[9039,     1] loss: 0.0095182374\n",
      "[9040,     1] loss: 0.0095181681\n",
      "[9041,     1] loss: 0.0095180981\n",
      "[9042,     1] loss: 0.0095180281\n",
      "[9043,     1] loss: 0.0095179580\n",
      "[9044,     1] loss: 0.0095178887\n",
      "[9045,     1] loss: 0.0095178194\n",
      "[9046,     1] loss: 0.0095177501\n",
      "[9047,     1] loss: 0.0095176809\n",
      "[9048,     1] loss: 0.0095176116\n",
      "[9049,     1] loss: 0.0095175423\n",
      "[9050,     1] loss: 0.0095174730\n",
      "[9051,     1] loss: 0.0095174029\n",
      "[9052,     1] loss: 0.0095173337\n",
      "[9053,     1] loss: 0.0095172636\n",
      "[9054,     1] loss: 0.0095171951\n",
      "[9055,     1] loss: 0.0095171258\n",
      "[9056,     1] loss: 0.0095170565\n",
      "[9057,     1] loss: 0.0095169872\n",
      "[9058,     1] loss: 0.0095169187\n",
      "[9059,     1] loss: 0.0095168509\n",
      "[9060,     1] loss: 0.0095167801\n",
      "[9061,     1] loss: 0.0095167108\n",
      "[9062,     1] loss: 0.0095166415\n",
      "[9063,     1] loss: 0.0095165737\n",
      "[9064,     1] loss: 0.0095165037\n",
      "[9065,     1] loss: 0.0095164359\n",
      "[9066,     1] loss: 0.0095163666\n",
      "[9067,     1] loss: 0.0095162980\n",
      "[9068,     1] loss: 0.0095162280\n",
      "[9069,     1] loss: 0.0095161587\n",
      "[9070,     1] loss: 0.0095160894\n",
      "[9071,     1] loss: 0.0095160209\n",
      "[9072,     1] loss: 0.0095159523\n",
      "[9073,     1] loss: 0.0095158830\n",
      "[9074,     1] loss: 0.0095158130\n",
      "[9075,     1] loss: 0.0095157459\n",
      "[9076,     1] loss: 0.0095156781\n",
      "[9077,     1] loss: 0.0095156096\n",
      "[9078,     1] loss: 0.0095155396\n",
      "[9079,     1] loss: 0.0095154725\n",
      "[9080,     1] loss: 0.0095154025\n",
      "[9081,     1] loss: 0.0095153339\n",
      "[9082,     1] loss: 0.0095152661\n",
      "[9083,     1] loss: 0.0095151991\n",
      "[9084,     1] loss: 0.0095151275\n",
      "[9085,     1] loss: 0.0095150605\n",
      "[9086,     1] loss: 0.0095149927\n",
      "[9087,     1] loss: 0.0095149241\n",
      "[9088,     1] loss: 0.0095148556\n",
      "[9089,     1] loss: 0.0095147878\n",
      "[9090,     1] loss: 0.0095147200\n",
      "[9091,     1] loss: 0.0095146507\n",
      "[9092,     1] loss: 0.0095145814\n",
      "[9093,     1] loss: 0.0095145136\n",
      "[9094,     1] loss: 0.0095144451\n",
      "[9095,     1] loss: 0.0095143758\n",
      "[9096,     1] loss: 0.0095143080\n",
      "[9097,     1] loss: 0.0095142409\n",
      "[9098,     1] loss: 0.0095141731\n",
      "[9099,     1] loss: 0.0095141046\n",
      "[9100,     1] loss: 0.0095140368\n",
      "[9101,     1] loss: 0.0095139682\n",
      "[9102,     1] loss: 0.0095138997\n",
      "[9103,     1] loss: 0.0095138311\n",
      "[9104,     1] loss: 0.0095137626\n",
      "[9105,     1] loss: 0.0095136940\n",
      "[9106,     1] loss: 0.0095136277\n",
      "[9107,     1] loss: 0.0095135584\n",
      "[9108,     1] loss: 0.0095134914\n",
      "[9109,     1] loss: 0.0095134236\n",
      "[9110,     1] loss: 0.0095133550\n",
      "[9111,     1] loss: 0.0095132865\n",
      "[9112,     1] loss: 0.0095132202\n",
      "[9113,     1] loss: 0.0095131516\n",
      "[9114,     1] loss: 0.0095130831\n",
      "[9115,     1] loss: 0.0095130160\n",
      "[9116,     1] loss: 0.0095129490\n",
      "[9117,     1] loss: 0.0095128812\n",
      "[9118,     1] loss: 0.0095128134\n",
      "[9119,     1] loss: 0.0095127456\n",
      "[9120,     1] loss: 0.0095126778\n",
      "[9121,     1] loss: 0.0095126085\n",
      "[9122,     1] loss: 0.0095125414\n",
      "[9123,     1] loss: 0.0095124736\n",
      "[9124,     1] loss: 0.0095124058\n",
      "[9125,     1] loss: 0.0095123403\n",
      "[9126,     1] loss: 0.0095122725\n",
      "[9127,     1] loss: 0.0095122054\n",
      "[9128,     1] loss: 0.0095121361\n",
      "[9129,     1] loss: 0.0095120698\n",
      "[9130,     1] loss: 0.0095120013\n",
      "[9131,     1] loss: 0.0095119335\n",
      "[9132,     1] loss: 0.0095118672\n",
      "[9133,     1] loss: 0.0095117986\n",
      "[9134,     1] loss: 0.0095117316\n",
      "[9135,     1] loss: 0.0095116653\n",
      "[9136,     1] loss: 0.0095115975\n",
      "[9137,     1] loss: 0.0095115304\n",
      "[9138,     1] loss: 0.0095114641\n",
      "[9139,     1] loss: 0.0095113970\n",
      "[9140,     1] loss: 0.0095113300\n",
      "[9141,     1] loss: 0.0095112637\n",
      "[9142,     1] loss: 0.0095111966\n",
      "[9143,     1] loss: 0.0095111288\n",
      "[9144,     1] loss: 0.0095110618\n",
      "[9145,     1] loss: 0.0095109954\n",
      "[9146,     1] loss: 0.0095109276\n",
      "[9147,     1] loss: 0.0095108598\n",
      "[9148,     1] loss: 0.0095107928\n",
      "[9149,     1] loss: 0.0095107272\n",
      "[9150,     1] loss: 0.0095106609\n",
      "[9151,     1] loss: 0.0095105931\n",
      "[9152,     1] loss: 0.0095105253\n",
      "[9153,     1] loss: 0.0095104605\n",
      "[9154,     1] loss: 0.0095103927\n",
      "[9155,     1] loss: 0.0095103249\n",
      "[9156,     1] loss: 0.0095102593\n",
      "[9157,     1] loss: 0.0095101930\n",
      "[9158,     1] loss: 0.0095101252\n",
      "[9159,     1] loss: 0.0095100597\n",
      "[9160,     1] loss: 0.0095099926\n",
      "[9161,     1] loss: 0.0095099255\n",
      "[9162,     1] loss: 0.0095098592\n",
      "[9163,     1] loss: 0.0095097937\n",
      "[9164,     1] loss: 0.0095097259\n",
      "[9165,     1] loss: 0.0095096588\n",
      "[9166,     1] loss: 0.0095095940\n",
      "[9167,     1] loss: 0.0095095269\n",
      "[9168,     1] loss: 0.0095094606\n",
      "[9169,     1] loss: 0.0095093943\n",
      "[9170,     1] loss: 0.0095093280\n",
      "[9171,     1] loss: 0.0095092624\n",
      "[9172,     1] loss: 0.0095091961\n",
      "[9173,     1] loss: 0.0095091291\n",
      "[9174,     1] loss: 0.0095090628\n",
      "[9175,     1] loss: 0.0095089979\n",
      "[9176,     1] loss: 0.0095089309\n",
      "[9177,     1] loss: 0.0095088661\n",
      "[9178,     1] loss: 0.0095087990\n",
      "[9179,     1] loss: 0.0095087335\n",
      "[9180,     1] loss: 0.0095086664\n",
      "[9181,     1] loss: 0.0095086001\n",
      "[9182,     1] loss: 0.0095085345\n",
      "[9183,     1] loss: 0.0095084690\n",
      "[9184,     1] loss: 0.0095084026\n",
      "[9185,     1] loss: 0.0095083378\n",
      "[9186,     1] loss: 0.0095082723\n",
      "[9187,     1] loss: 0.0095082052\n",
      "[9188,     1] loss: 0.0095081404\n",
      "[9189,     1] loss: 0.0095080741\n",
      "[9190,     1] loss: 0.0095080085\n",
      "[9191,     1] loss: 0.0095079437\n",
      "[9192,     1] loss: 0.0095078781\n",
      "[9193,     1] loss: 0.0095078111\n",
      "[9194,     1] loss: 0.0095077462\n",
      "[9195,     1] loss: 0.0095076807\n",
      "[9196,     1] loss: 0.0095076151\n",
      "[9197,     1] loss: 0.0095075488\n",
      "[9198,     1] loss: 0.0095074840\n",
      "[9199,     1] loss: 0.0095074177\n",
      "[9200,     1] loss: 0.0095073536\n",
      "[9201,     1] loss: 0.0095072888\n",
      "[9202,     1] loss: 0.0095072232\n",
      "[9203,     1] loss: 0.0095071569\n",
      "[9204,     1] loss: 0.0095070936\n",
      "[9205,     1] loss: 0.0095070265\n",
      "[9206,     1] loss: 0.0095069617\n",
      "[9207,     1] loss: 0.0095068969\n",
      "[9208,     1] loss: 0.0095068306\n",
      "[9209,     1] loss: 0.0095067665\n",
      "[9210,     1] loss: 0.0095067002\n",
      "[9211,     1] loss: 0.0095066361\n",
      "[9212,     1] loss: 0.0095065705\n",
      "[9213,     1] loss: 0.0095065050\n",
      "[9214,     1] loss: 0.0095064409\n",
      "[9215,     1] loss: 0.0095063746\n",
      "[9216,     1] loss: 0.0095063105\n",
      "[9217,     1] loss: 0.0095062450\n",
      "[9218,     1] loss: 0.0095061801\n",
      "[9219,     1] loss: 0.0095061146\n",
      "[9220,     1] loss: 0.0095060498\n",
      "[9221,     1] loss: 0.0095059849\n",
      "[9222,     1] loss: 0.0095059209\n",
      "[9223,     1] loss: 0.0095058568\n",
      "[9224,     1] loss: 0.0095057912\n",
      "[9225,     1] loss: 0.0095057286\n",
      "[9226,     1] loss: 0.0095056623\n",
      "[9227,     1] loss: 0.0095055975\n",
      "[9228,     1] loss: 0.0095055342\n",
      "[9229,     1] loss: 0.0095054694\n",
      "[9230,     1] loss: 0.0095054038\n",
      "[9231,     1] loss: 0.0095053390\n",
      "[9232,     1] loss: 0.0095052741\n",
      "[9233,     1] loss: 0.0095052101\n",
      "[9234,     1] loss: 0.0095051453\n",
      "[9235,     1] loss: 0.0095050812\n",
      "[9236,     1] loss: 0.0095050178\n",
      "[9237,     1] loss: 0.0095049523\n",
      "[9238,     1] loss: 0.0095048882\n",
      "[9239,     1] loss: 0.0095048234\n",
      "[9240,     1] loss: 0.0095047586\n",
      "[9241,     1] loss: 0.0095046930\n",
      "[9242,     1] loss: 0.0095046312\n",
      "[9243,     1] loss: 0.0095045656\n",
      "[9244,     1] loss: 0.0095045023\n",
      "[9245,     1] loss: 0.0095044389\n",
      "[9246,     1] loss: 0.0095043741\n",
      "[9247,     1] loss: 0.0095043100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9248,     1] loss: 0.0095042452\n",
      "[9249,     1] loss: 0.0095041811\n",
      "[9250,     1] loss: 0.0095041171\n",
      "[9251,     1] loss: 0.0095040523\n",
      "[9252,     1] loss: 0.0095039904\n",
      "[9253,     1] loss: 0.0095039256\n",
      "[9254,     1] loss: 0.0095038615\n",
      "[9255,     1] loss: 0.0095037989\n",
      "[9256,     1] loss: 0.0095037349\n",
      "[9257,     1] loss: 0.0095036715\n",
      "[9258,     1] loss: 0.0095036060\n",
      "[9259,     1] loss: 0.0095035434\n",
      "[9260,     1] loss: 0.0095034793\n",
      "[9261,     1] loss: 0.0095034160\n",
      "[9262,     1] loss: 0.0095033519\n",
      "[9263,     1] loss: 0.0095032886\n",
      "[9264,     1] loss: 0.0095032260\n",
      "[9265,     1] loss: 0.0095031627\n",
      "[9266,     1] loss: 0.0095030986\n",
      "[9267,     1] loss: 0.0095030345\n",
      "[9268,     1] loss: 0.0095029704\n",
      "[9269,     1] loss: 0.0095029071\n",
      "[9270,     1] loss: 0.0095028430\n",
      "[9271,     1] loss: 0.0095027812\n",
      "[9272,     1] loss: 0.0095027171\n",
      "[9273,     1] loss: 0.0095026538\n",
      "[9274,     1] loss: 0.0095025904\n",
      "[9275,     1] loss: 0.0095025279\n",
      "[9276,     1] loss: 0.0095024638\n",
      "[9277,     1] loss: 0.0095023997\n",
      "[9278,     1] loss: 0.0095023371\n",
      "[9279,     1] loss: 0.0095022731\n",
      "[9280,     1] loss: 0.0095022097\n",
      "[9281,     1] loss: 0.0095021479\n",
      "[9282,     1] loss: 0.0095020846\n",
      "[9283,     1] loss: 0.0095020227\n",
      "[9284,     1] loss: 0.0095019579\n",
      "[9285,     1] loss: 0.0095018961\n",
      "[9286,     1] loss: 0.0095018335\n",
      "[9287,     1] loss: 0.0095017709\n",
      "[9288,     1] loss: 0.0095017076\n",
      "[9289,     1] loss: 0.0095016450\n",
      "[9290,     1] loss: 0.0095015831\n",
      "[9291,     1] loss: 0.0095015198\n",
      "[9292,     1] loss: 0.0095014565\n",
      "[9293,     1] loss: 0.0095013939\n",
      "[9294,     1] loss: 0.0095013306\n",
      "[9295,     1] loss: 0.0095012687\n",
      "[9296,     1] loss: 0.0095012054\n",
      "[9297,     1] loss: 0.0095011428\n",
      "[9298,     1] loss: 0.0095010802\n",
      "[9299,     1] loss: 0.0095010169\n",
      "[9300,     1] loss: 0.0095009543\n",
      "[9301,     1] loss: 0.0095008917\n",
      "[9302,     1] loss: 0.0095008306\n",
      "[9303,     1] loss: 0.0095007680\n",
      "[9304,     1] loss: 0.0095007055\n",
      "[9305,     1] loss: 0.0095006429\n",
      "[9306,     1] loss: 0.0095005810\n",
      "[9307,     1] loss: 0.0095005184\n",
      "[9308,     1] loss: 0.0095004551\n",
      "[9309,     1] loss: 0.0095003948\n",
      "[9310,     1] loss: 0.0095003322\n",
      "[9311,     1] loss: 0.0095002696\n",
      "[9312,     1] loss: 0.0095002070\n",
      "[9313,     1] loss: 0.0095001452\n",
      "[9314,     1] loss: 0.0095000841\n",
      "[9315,     1] loss: 0.0095000207\n",
      "[9316,     1] loss: 0.0094999596\n",
      "[9317,     1] loss: 0.0094998978\n",
      "[9318,     1] loss: 0.0094998352\n",
      "[9319,     1] loss: 0.0094997726\n",
      "[9320,     1] loss: 0.0094997101\n",
      "[9321,     1] loss: 0.0094996497\n",
      "[9322,     1] loss: 0.0094995864\n",
      "[9323,     1] loss: 0.0094995260\n",
      "[9324,     1] loss: 0.0094994649\n",
      "[9325,     1] loss: 0.0094994031\n",
      "[9326,     1] loss: 0.0094993398\n",
      "[9327,     1] loss: 0.0094992787\n",
      "[9328,     1] loss: 0.0094992176\n",
      "[9329,     1] loss: 0.0094991565\n",
      "[9330,     1] loss: 0.0094990946\n",
      "[9331,     1] loss: 0.0094990328\n",
      "[9332,     1] loss: 0.0094989710\n",
      "[9333,     1] loss: 0.0094989091\n",
      "[9334,     1] loss: 0.0094988480\n",
      "[9335,     1] loss: 0.0094987869\n",
      "[9336,     1] loss: 0.0094987243\n",
      "[9337,     1] loss: 0.0094986632\n",
      "[9338,     1] loss: 0.0094986022\n",
      "[9339,     1] loss: 0.0094985418\n",
      "[9340,     1] loss: 0.0094984792\n",
      "[9341,     1] loss: 0.0094984189\n",
      "[9342,     1] loss: 0.0094983570\n",
      "[9343,     1] loss: 0.0094982952\n",
      "[9344,     1] loss: 0.0094982356\n",
      "[9345,     1] loss: 0.0094981737\n",
      "[9346,     1] loss: 0.0094981141\n",
      "[9347,     1] loss: 0.0094980523\n",
      "[9348,     1] loss: 0.0094979912\n",
      "[9349,     1] loss: 0.0094979309\n",
      "[9350,     1] loss: 0.0094978698\n",
      "[9351,     1] loss: 0.0094978079\n",
      "[9352,     1] loss: 0.0094977468\n",
      "[9353,     1] loss: 0.0094976872\n",
      "[9354,     1] loss: 0.0094976261\n",
      "[9355,     1] loss: 0.0094975650\n",
      "[9356,     1] loss: 0.0094975039\n",
      "[9357,     1] loss: 0.0094974428\n",
      "[9358,     1] loss: 0.0094973825\n",
      "[9359,     1] loss: 0.0094973229\n",
      "[9360,     1] loss: 0.0094972610\n",
      "[9361,     1] loss: 0.0094972007\n",
      "[9362,     1] loss: 0.0094971396\n",
      "[9363,     1] loss: 0.0094970793\n",
      "[9364,     1] loss: 0.0094970182\n",
      "[9365,     1] loss: 0.0094969571\n",
      "[9366,     1] loss: 0.0094968989\n",
      "[9367,     1] loss: 0.0094968379\n",
      "[9368,     1] loss: 0.0094967768\n",
      "[9369,     1] loss: 0.0094967186\n",
      "[9370,     1] loss: 0.0094966568\n",
      "[9371,     1] loss: 0.0094965957\n",
      "[9372,     1] loss: 0.0094965346\n",
      "[9373,     1] loss: 0.0094964758\n",
      "[9374,     1] loss: 0.0094964147\n",
      "[9375,     1] loss: 0.0094963551\n",
      "[9376,     1] loss: 0.0094962947\n",
      "[9377,     1] loss: 0.0094962351\n",
      "[9378,     1] loss: 0.0094961740\n",
      "[9379,     1] loss: 0.0094961144\n",
      "[9380,     1] loss: 0.0094960555\n",
      "[9381,     1] loss: 0.0094959952\n",
      "[9382,     1] loss: 0.0094959348\n",
      "[9383,     1] loss: 0.0094958760\n",
      "[9384,     1] loss: 0.0094958149\n",
      "[9385,     1] loss: 0.0094957553\n",
      "[9386,     1] loss: 0.0094956949\n",
      "[9387,     1] loss: 0.0094956346\n",
      "[9388,     1] loss: 0.0094955757\n",
      "[9389,     1] loss: 0.0094955154\n",
      "[9390,     1] loss: 0.0094954558\n",
      "[9391,     1] loss: 0.0094953969\n",
      "[9392,     1] loss: 0.0094953351\n",
      "[9393,     1] loss: 0.0094952777\n",
      "[9394,     1] loss: 0.0094952174\n",
      "[9395,     1] loss: 0.0094951585\n",
      "[9396,     1] loss: 0.0094950981\n",
      "[9397,     1] loss: 0.0094950385\n",
      "[9398,     1] loss: 0.0094949797\n",
      "[9399,     1] loss: 0.0094949208\n",
      "[9400,     1] loss: 0.0094948620\n",
      "[9401,     1] loss: 0.0094948024\n",
      "[9402,     1] loss: 0.0094947428\n",
      "[9403,     1] loss: 0.0094946839\n",
      "[9404,     1] loss: 0.0094946235\n",
      "[9405,     1] loss: 0.0094945654\n",
      "[9406,     1] loss: 0.0094945066\n",
      "[9407,     1] loss: 0.0094944470\n",
      "[9408,     1] loss: 0.0094943859\n",
      "[9409,     1] loss: 0.0094943285\n",
      "[9410,     1] loss: 0.0094942696\n",
      "[9411,     1] loss: 0.0094942093\n",
      "[9412,     1] loss: 0.0094941504\n",
      "[9413,     1] loss: 0.0094940923\n",
      "[9414,     1] loss: 0.0094940335\n",
      "[9415,     1] loss: 0.0094939739\n",
      "[9416,     1] loss: 0.0094939150\n",
      "[9417,     1] loss: 0.0094938569\n",
      "[9418,     1] loss: 0.0094937980\n",
      "[9419,     1] loss: 0.0094937377\n",
      "[9420,     1] loss: 0.0094936796\n",
      "[9421,     1] loss: 0.0094936207\n",
      "[9422,     1] loss: 0.0094935618\n",
      "[9423,     1] loss: 0.0094935037\n",
      "[9424,     1] loss: 0.0094934449\n",
      "[9425,     1] loss: 0.0094933867\n",
      "[9426,     1] loss: 0.0094933264\n",
      "[9427,     1] loss: 0.0094932698\n",
      "[9428,     1] loss: 0.0094932102\n",
      "[9429,     1] loss: 0.0094931521\n",
      "[9430,     1] loss: 0.0094930947\n",
      "[9431,     1] loss: 0.0094930366\n",
      "[9432,     1] loss: 0.0094929777\n",
      "[9433,     1] loss: 0.0094929196\n",
      "[9434,     1] loss: 0.0094928607\n",
      "[9435,     1] loss: 0.0094928026\n",
      "[9436,     1] loss: 0.0094927438\n",
      "[9437,     1] loss: 0.0094926856\n",
      "[9438,     1] loss: 0.0094926283\n",
      "[9439,     1] loss: 0.0094925709\n",
      "[9440,     1] loss: 0.0094925120\n",
      "[9441,     1] loss: 0.0094924532\n",
      "[9442,     1] loss: 0.0094923943\n",
      "[9443,     1] loss: 0.0094923377\n",
      "[9444,     1] loss: 0.0094922803\n",
      "[9445,     1] loss: 0.0094922207\n",
      "[9446,     1] loss: 0.0094921626\n",
      "[9447,     1] loss: 0.0094921060\n",
      "[9448,     1] loss: 0.0094920479\n",
      "[9449,     1] loss: 0.0094919898\n",
      "[9450,     1] loss: 0.0094919316\n",
      "[9451,     1] loss: 0.0094918743\n",
      "[9452,     1] loss: 0.0094918169\n",
      "[9453,     1] loss: 0.0094917580\n",
      "[9454,     1] loss: 0.0094917007\n",
      "[9455,     1] loss: 0.0094916448\n",
      "[9456,     1] loss: 0.0094915845\n",
      "[9457,     1] loss: 0.0094915278\n",
      "[9458,     1] loss: 0.0094914712\n",
      "[9459,     1] loss: 0.0094914131\n",
      "[9460,     1] loss: 0.0094913550\n",
      "[9461,     1] loss: 0.0094912976\n",
      "[9462,     1] loss: 0.0094912410\n",
      "[9463,     1] loss: 0.0094911829\n",
      "[9464,     1] loss: 0.0094911270\n",
      "[9465,     1] loss: 0.0094910689\n",
      "[9466,     1] loss: 0.0094910115\n",
      "[9467,     1] loss: 0.0094909541\n",
      "[9468,     1] loss: 0.0094908975\n",
      "[9469,     1] loss: 0.0094908379\n",
      "[9470,     1] loss: 0.0094907820\n",
      "[9471,     1] loss: 0.0094907239\n",
      "[9472,     1] loss: 0.0094906665\n",
      "[9473,     1] loss: 0.0094906107\n",
      "[9474,     1] loss: 0.0094905540\n",
      "[9475,     1] loss: 0.0094904959\n",
      "[9476,     1] loss: 0.0094904393\n",
      "[9477,     1] loss: 0.0094903834\n",
      "[9478,     1] loss: 0.0094903260\n",
      "[9479,     1] loss: 0.0094902672\n",
      "[9480,     1] loss: 0.0094902113\n",
      "[9481,     1] loss: 0.0094901547\n",
      "[9482,     1] loss: 0.0094900981\n",
      "[9483,     1] loss: 0.0094900399\n",
      "[9484,     1] loss: 0.0094899848\n",
      "[9485,     1] loss: 0.0094899289\n",
      "[9486,     1] loss: 0.0094898708\n",
      "[9487,     1] loss: 0.0094898157\n",
      "[9488,     1] loss: 0.0094897591\n",
      "[9489,     1] loss: 0.0094897009\n",
      "[9490,     1] loss: 0.0094896451\n",
      "[9491,     1] loss: 0.0094895884\n",
      "[9492,     1] loss: 0.0094895318\n",
      "[9493,     1] loss: 0.0094894752\n",
      "[9494,     1] loss: 0.0094894178\n",
      "[9495,     1] loss: 0.0094893619\n",
      "[9496,     1] loss: 0.0094893061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9497,     1] loss: 0.0094892509\n",
      "[9498,     1] loss: 0.0094891928\n",
      "[9499,     1] loss: 0.0094891384\n",
      "[9500,     1] loss: 0.0094890811\n",
      "[9501,     1] loss: 0.0094890237\n",
      "[9502,     1] loss: 0.0094889686\n",
      "[9503,     1] loss: 0.0094889119\n",
      "[9504,     1] loss: 0.0094888568\n",
      "[9505,     1] loss: 0.0094887994\n",
      "[9506,     1] loss: 0.0094887435\n",
      "[9507,     1] loss: 0.0094886869\n",
      "[9508,     1] loss: 0.0094886325\n",
      "[9509,     1] loss: 0.0094885759\n",
      "[9510,     1] loss: 0.0094885200\n",
      "[9511,     1] loss: 0.0094884641\n",
      "[9512,     1] loss: 0.0094884083\n",
      "[9513,     1] loss: 0.0094883509\n",
      "[9514,     1] loss: 0.0094882965\n",
      "[9515,     1] loss: 0.0094882414\n",
      "[9516,     1] loss: 0.0094881855\n",
      "[9517,     1] loss: 0.0094881296\n",
      "[9518,     1] loss: 0.0094880737\n",
      "[9519,     1] loss: 0.0094880179\n",
      "[9520,     1] loss: 0.0094879620\n",
      "[9521,     1] loss: 0.0094879083\n",
      "[9522,     1] loss: 0.0094878525\n",
      "[9523,     1] loss: 0.0094877951\n",
      "[9524,     1] loss: 0.0094877407\n",
      "[9525,     1] loss: 0.0094876848\n",
      "[9526,     1] loss: 0.0094876282\n",
      "[9527,     1] loss: 0.0094875738\n",
      "[9528,     1] loss: 0.0094875187\n",
      "[9529,     1] loss: 0.0094874643\n",
      "[9530,     1] loss: 0.0094874084\n",
      "[9531,     1] loss: 0.0094873540\n",
      "[9532,     1] loss: 0.0094872981\n",
      "[9533,     1] loss: 0.0094872430\n",
      "[9534,     1] loss: 0.0094871871\n",
      "[9535,     1] loss: 0.0094871320\n",
      "[9536,     1] loss: 0.0094870761\n",
      "[9537,     1] loss: 0.0094870210\n",
      "[9538,     1] loss: 0.0094869658\n",
      "[9539,     1] loss: 0.0094869107\n",
      "[9540,     1] loss: 0.0094868556\n",
      "[9541,     1] loss: 0.0094868012\n",
      "[9542,     1] loss: 0.0094867468\n",
      "[9543,     1] loss: 0.0094866924\n",
      "[9544,     1] loss: 0.0094866365\n",
      "[9545,     1] loss: 0.0094865829\n",
      "[9546,     1] loss: 0.0094865277\n",
      "[9547,     1] loss: 0.0094864734\n",
      "[9548,     1] loss: 0.0094864175\n",
      "[9549,     1] loss: 0.0094863623\n",
      "[9550,     1] loss: 0.0094863087\n",
      "[9551,     1] loss: 0.0094862536\n",
      "[9552,     1] loss: 0.0094861992\n",
      "[9553,     1] loss: 0.0094861440\n",
      "[9554,     1] loss: 0.0094860889\n",
      "[9555,     1] loss: 0.0094860360\n",
      "[9556,     1] loss: 0.0094859801\n",
      "[9557,     1] loss: 0.0094859257\n",
      "[9558,     1] loss: 0.0094858713\n",
      "[9559,     1] loss: 0.0094858177\n",
      "[9560,     1] loss: 0.0094857633\n",
      "[9561,     1] loss: 0.0094857097\n",
      "[9562,     1] loss: 0.0094856545\n",
      "[9563,     1] loss: 0.0094855994\n",
      "[9564,     1] loss: 0.0094855458\n",
      "[9565,     1] loss: 0.0094854914\n",
      "[9566,     1] loss: 0.0094854377\n",
      "[9567,     1] loss: 0.0094853833\n",
      "[9568,     1] loss: 0.0094853289\n",
      "[9569,     1] loss: 0.0094852738\n",
      "[9570,     1] loss: 0.0094852217\n",
      "[9571,     1] loss: 0.0094851665\n",
      "[9572,     1] loss: 0.0094851129\n",
      "[9573,     1] loss: 0.0094850592\n",
      "[9574,     1] loss: 0.0094850041\n",
      "[9575,     1] loss: 0.0094849505\n",
      "[9576,     1] loss: 0.0094848961\n",
      "[9577,     1] loss: 0.0094848432\n",
      "[9578,     1] loss: 0.0094847903\n",
      "[9579,     1] loss: 0.0094847344\n",
      "[9580,     1] loss: 0.0094846815\n",
      "[9581,     1] loss: 0.0094846286\n",
      "[9582,     1] loss: 0.0094845749\n",
      "[9583,     1] loss: 0.0094845191\n",
      "[9584,     1] loss: 0.0094844669\n",
      "[9585,     1] loss: 0.0094844148\n",
      "[9586,     1] loss: 0.0094843589\n",
      "[9587,     1] loss: 0.0094843075\n",
      "[9588,     1] loss: 0.0094842538\n",
      "[9589,     1] loss: 0.0094842009\n",
      "[9590,     1] loss: 0.0094841465\n",
      "[9591,     1] loss: 0.0094840936\n",
      "[9592,     1] loss: 0.0094840400\n",
      "[9593,     1] loss: 0.0094839856\n",
      "[9594,     1] loss: 0.0094839327\n",
      "[9595,     1] loss: 0.0094838805\n",
      "[9596,     1] loss: 0.0094838254\n",
      "[9597,     1] loss: 0.0094837733\n",
      "[9598,     1] loss: 0.0094837204\n",
      "[9599,     1] loss: 0.0094836667\n",
      "[9600,     1] loss: 0.0094836131\n",
      "[9601,     1] loss: 0.0094835617\n",
      "[9602,     1] loss: 0.0094835080\n",
      "[9603,     1] loss: 0.0094834544\n",
      "[9604,     1] loss: 0.0094834007\n",
      "[9605,     1] loss: 0.0094833486\n",
      "[9606,     1] loss: 0.0094832949\n",
      "[9607,     1] loss: 0.0094832435\n",
      "[9608,     1] loss: 0.0094831906\n",
      "[9609,     1] loss: 0.0094831377\n",
      "[9610,     1] loss: 0.0094830863\n",
      "[9611,     1] loss: 0.0094830319\n",
      "[9612,     1] loss: 0.0094829790\n",
      "[9613,     1] loss: 0.0094829269\n",
      "[9614,     1] loss: 0.0094828740\n",
      "[9615,     1] loss: 0.0094828211\n",
      "[9616,     1] loss: 0.0094827682\n",
      "[9617,     1] loss: 0.0094827160\n",
      "[9618,     1] loss: 0.0094826631\n",
      "[9619,     1] loss: 0.0094826117\n",
      "[9620,     1] loss: 0.0094825581\n",
      "[9621,     1] loss: 0.0094825059\n",
      "[9622,     1] loss: 0.0094824538\n",
      "[9623,     1] loss: 0.0094824001\n",
      "[9624,     1] loss: 0.0094823480\n",
      "[9625,     1] loss: 0.0094822973\n",
      "[9626,     1] loss: 0.0094822444\n",
      "[9627,     1] loss: 0.0094821908\n",
      "[9628,     1] loss: 0.0094821386\n",
      "[9629,     1] loss: 0.0094820872\n",
      "[9630,     1] loss: 0.0094820350\n",
      "[9631,     1] loss: 0.0094819829\n",
      "[9632,     1] loss: 0.0094819307\n",
      "[9633,     1] loss: 0.0094818793\n",
      "[9634,     1] loss: 0.0094818279\n",
      "[9635,     1] loss: 0.0094817750\n",
      "[9636,     1] loss: 0.0094817229\n",
      "[9637,     1] loss: 0.0094816700\n",
      "[9638,     1] loss: 0.0094816186\n",
      "[9639,     1] loss: 0.0094815671\n",
      "[9640,     1] loss: 0.0094815150\n",
      "[9641,     1] loss: 0.0094814628\n",
      "[9642,     1] loss: 0.0094814107\n",
      "[9643,     1] loss: 0.0094813608\n",
      "[9644,     1] loss: 0.0094813071\n",
      "[9645,     1] loss: 0.0094812565\n",
      "[9646,     1] loss: 0.0094812036\n",
      "[9647,     1] loss: 0.0094811529\n",
      "[9648,     1] loss: 0.0094811022\n",
      "[9649,     1] loss: 0.0094810501\n",
      "[9650,     1] loss: 0.0094809987\n",
      "[9651,     1] loss: 0.0094809465\n",
      "[9652,     1] loss: 0.0094808944\n",
      "[9653,     1] loss: 0.0094808444\n",
      "[9654,     1] loss: 0.0094807923\n",
      "[9655,     1] loss: 0.0094807401\n",
      "[9656,     1] loss: 0.0094806895\n",
      "[9657,     1] loss: 0.0094806381\n",
      "[9658,     1] loss: 0.0094805859\n",
      "[9659,     1] loss: 0.0094805345\n",
      "[9660,     1] loss: 0.0094804831\n",
      "[9661,     1] loss: 0.0094804324\n",
      "[9662,     1] loss: 0.0094803795\n",
      "[9663,     1] loss: 0.0094803289\n",
      "[9664,     1] loss: 0.0094802774\n",
      "[9665,     1] loss: 0.0094802268\n",
      "[9666,     1] loss: 0.0094801746\n",
      "[9667,     1] loss: 0.0094801240\n",
      "[9668,     1] loss: 0.0094800733\n",
      "[9669,     1] loss: 0.0094800226\n",
      "[9670,     1] loss: 0.0094799705\n",
      "[9671,     1] loss: 0.0094799191\n",
      "[9672,     1] loss: 0.0094798699\n",
      "[9673,     1] loss: 0.0094798170\n",
      "[9674,     1] loss: 0.0094797671\n",
      "[9675,     1] loss: 0.0094797157\n",
      "[9676,     1] loss: 0.0094796650\n",
      "[9677,     1] loss: 0.0094796143\n",
      "[9678,     1] loss: 0.0094795637\n",
      "[9679,     1] loss: 0.0094795123\n",
      "[9680,     1] loss: 0.0094794616\n",
      "[9681,     1] loss: 0.0094794109\n",
      "[9682,     1] loss: 0.0094793603\n",
      "[9683,     1] loss: 0.0094793104\n",
      "[9684,     1] loss: 0.0094792567\n",
      "[9685,     1] loss: 0.0094792083\n",
      "[9686,     1] loss: 0.0094791584\n",
      "[9687,     1] loss: 0.0094791085\n",
      "[9688,     1] loss: 0.0094790570\n",
      "[9689,     1] loss: 0.0094790079\n",
      "[9690,     1] loss: 0.0094789572\n",
      "[9691,     1] loss: 0.0094789058\n",
      "[9692,     1] loss: 0.0094788566\n",
      "[9693,     1] loss: 0.0094788067\n",
      "[9694,     1] loss: 0.0094787546\n",
      "[9695,     1] loss: 0.0094787039\n",
      "[9696,     1] loss: 0.0094786555\n",
      "[9697,     1] loss: 0.0094786048\n",
      "[9698,     1] loss: 0.0094785526\n",
      "[9699,     1] loss: 0.0094785057\n",
      "[9700,     1] loss: 0.0094784543\n",
      "[9701,     1] loss: 0.0094784036\n",
      "[9702,     1] loss: 0.0094783530\n",
      "[9703,     1] loss: 0.0094783023\n",
      "[9704,     1] loss: 0.0094782516\n",
      "[9705,     1] loss: 0.0094782025\n",
      "[9706,     1] loss: 0.0094781518\n",
      "[9707,     1] loss: 0.0094781026\n",
      "[9708,     1] loss: 0.0094780520\n",
      "[9709,     1] loss: 0.0094780028\n",
      "[9710,     1] loss: 0.0094779536\n",
      "[9711,     1] loss: 0.0094779037\n",
      "[9712,     1] loss: 0.0094778523\n",
      "[9713,     1] loss: 0.0094778024\n",
      "[9714,     1] loss: 0.0094777524\n",
      "[9715,     1] loss: 0.0094777025\n",
      "[9716,     1] loss: 0.0094776541\n",
      "[9717,     1] loss: 0.0094776027\n",
      "[9718,     1] loss: 0.0094775543\n",
      "[9719,     1] loss: 0.0094775051\n",
      "[9720,     1] loss: 0.0094774537\n",
      "[9721,     1] loss: 0.0094774082\n",
      "[9722,     1] loss: 0.0094773568\n",
      "[9723,     1] loss: 0.0094773054\n",
      "[9724,     1] loss: 0.0094772592\n",
      "[9725,     1] loss: 0.0094772086\n",
      "[9726,     1] loss: 0.0094771579\n",
      "[9727,     1] loss: 0.0094771087\n",
      "[9728,     1] loss: 0.0094770581\n",
      "[9729,     1] loss: 0.0094770089\n",
      "[9730,     1] loss: 0.0094769590\n",
      "[9731,     1] loss: 0.0094769113\n",
      "[9732,     1] loss: 0.0094768614\n",
      "[9733,     1] loss: 0.0094768129\n",
      "[9734,     1] loss: 0.0094767638\n",
      "[9735,     1] loss: 0.0094767153\n",
      "[9736,     1] loss: 0.0094766662\n",
      "[9737,     1] loss: 0.0094766162\n",
      "[9738,     1] loss: 0.0094765671\n",
      "[9739,     1] loss: 0.0094765164\n",
      "[9740,     1] loss: 0.0094764687\n",
      "[9741,     1] loss: 0.0094764188\n",
      "[9742,     1] loss: 0.0094763696\n",
      "[9743,     1] loss: 0.0094763212\n",
      "[9744,     1] loss: 0.0094762728\n",
      "[9745,     1] loss: 0.0094762243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9746,     1] loss: 0.0094761744\n",
      "[9747,     1] loss: 0.0094761267\n",
      "[9748,     1] loss: 0.0094760776\n",
      "[9749,     1] loss: 0.0094760284\n",
      "[9750,     1] loss: 0.0094759800\n",
      "[9751,     1] loss: 0.0094759315\n",
      "[9752,     1] loss: 0.0094758824\n",
      "[9753,     1] loss: 0.0094758332\n",
      "[9754,     1] loss: 0.0094757833\n",
      "[9755,     1] loss: 0.0094757348\n",
      "[9756,     1] loss: 0.0094756857\n",
      "[9757,     1] loss: 0.0094756387\n",
      "[9758,     1] loss: 0.0094755903\n",
      "[9759,     1] loss: 0.0094755419\n",
      "[9760,     1] loss: 0.0094754934\n",
      "[9761,     1] loss: 0.0094754435\n",
      "[9762,     1] loss: 0.0094753958\n",
      "[9763,     1] loss: 0.0094753467\n",
      "[9764,     1] loss: 0.0094752982\n",
      "[9765,     1] loss: 0.0094752505\n",
      "[9766,     1] loss: 0.0094752029\n",
      "[9767,     1] loss: 0.0094751537\n",
      "[9768,     1] loss: 0.0094751053\n",
      "[9769,     1] loss: 0.0094750576\n",
      "[9770,     1] loss: 0.0094750091\n",
      "[9771,     1] loss: 0.0094749600\n",
      "[9772,     1] loss: 0.0094749130\n",
      "[9773,     1] loss: 0.0094748646\n",
      "[9774,     1] loss: 0.0094748162\n",
      "[9775,     1] loss: 0.0094747685\n",
      "[9776,     1] loss: 0.0094747201\n",
      "[9777,     1] loss: 0.0094746716\n",
      "[9778,     1] loss: 0.0094746232\n",
      "[9779,     1] loss: 0.0094745748\n",
      "[9780,     1] loss: 0.0094745271\n",
      "[9781,     1] loss: 0.0094744802\n",
      "[9782,     1] loss: 0.0094744317\n",
      "[9783,     1] loss: 0.0094743833\n",
      "[9784,     1] loss: 0.0094743364\n",
      "[9785,     1] loss: 0.0094742879\n",
      "[9786,     1] loss: 0.0094742410\n",
      "[9787,     1] loss: 0.0094741926\n",
      "[9788,     1] loss: 0.0094741441\n",
      "[9789,     1] loss: 0.0094740950\n",
      "[9790,     1] loss: 0.0094740488\n",
      "[9791,     1] loss: 0.0094740003\n",
      "[9792,     1] loss: 0.0094739527\n",
      "[9793,     1] loss: 0.0094739065\n",
      "[9794,     1] loss: 0.0094738595\n",
      "[9795,     1] loss: 0.0094738111\n",
      "[9796,     1] loss: 0.0094737634\n",
      "[9797,     1] loss: 0.0094737165\n",
      "[9798,     1] loss: 0.0094736688\n",
      "[9799,     1] loss: 0.0094736211\n",
      "[9800,     1] loss: 0.0094735742\n",
      "[9801,     1] loss: 0.0094735257\n",
      "[9802,     1] loss: 0.0094734780\n",
      "[9803,     1] loss: 0.0094734311\n",
      "[9804,     1] loss: 0.0094733842\n",
      "[9805,     1] loss: 0.0094733372\n",
      "[9806,     1] loss: 0.0094732903\n",
      "[9807,     1] loss: 0.0094732426\n",
      "[9808,     1] loss: 0.0094731957\n",
      "[9809,     1] loss: 0.0094731480\n",
      "[9810,     1] loss: 0.0094731003\n",
      "[9811,     1] loss: 0.0094730534\n",
      "[9812,     1] loss: 0.0094730087\n",
      "[9813,     1] loss: 0.0094729602\n",
      "[9814,     1] loss: 0.0094729118\n",
      "[9815,     1] loss: 0.0094728641\n",
      "[9816,     1] loss: 0.0094728179\n",
      "[9817,     1] loss: 0.0094727695\n",
      "[9818,     1] loss: 0.0094727218\n",
      "[9819,     1] loss: 0.0094726771\n",
      "[9820,     1] loss: 0.0094726294\n",
      "[9821,     1] loss: 0.0094725825\n",
      "[9822,     1] loss: 0.0094725370\n",
      "[9823,     1] loss: 0.0094724901\n",
      "[9824,     1] loss: 0.0094724417\n",
      "[9825,     1] loss: 0.0094723947\n",
      "[9826,     1] loss: 0.0094723485\n",
      "[9827,     1] loss: 0.0094723031\n",
      "[9828,     1] loss: 0.0094722547\n",
      "[9829,     1] loss: 0.0094722077\n",
      "[9830,     1] loss: 0.0094721600\n",
      "[9831,     1] loss: 0.0094721138\n",
      "[9832,     1] loss: 0.0094720677\n",
      "[9833,     1] loss: 0.0094720207\n",
      "[9834,     1] loss: 0.0094719745\n",
      "[9835,     1] loss: 0.0094719276\n",
      "[9836,     1] loss: 0.0094718814\n",
      "[9837,     1] loss: 0.0094718367\n",
      "[9838,     1] loss: 0.0094717890\n",
      "[9839,     1] loss: 0.0094717413\n",
      "[9840,     1] loss: 0.0094716974\n",
      "[9841,     1] loss: 0.0094716497\n",
      "[9842,     1] loss: 0.0094716042\n",
      "[9843,     1] loss: 0.0094715558\n",
      "[9844,     1] loss: 0.0094715104\n",
      "[9845,     1] loss: 0.0094714634\n",
      "[9846,     1] loss: 0.0094714187\n",
      "[9847,     1] loss: 0.0094713710\n",
      "[9848,     1] loss: 0.0094713256\n",
      "[9849,     1] loss: 0.0094712801\n",
      "[9850,     1] loss: 0.0094712332\n",
      "[9851,     1] loss: 0.0094711885\n",
      "[9852,     1] loss: 0.0094711423\n",
      "[9853,     1] loss: 0.0094710946\n",
      "[9854,     1] loss: 0.0094710506\n",
      "[9855,     1] loss: 0.0094710030\n",
      "[9856,     1] loss: 0.0094709575\n",
      "[9857,     1] loss: 0.0094709113\n",
      "[9858,     1] loss: 0.0094708636\n",
      "[9859,     1] loss: 0.0094708182\n",
      "[9860,     1] loss: 0.0094707727\n",
      "[9861,     1] loss: 0.0094707265\n",
      "[9862,     1] loss: 0.0094706804\n",
      "[9863,     1] loss: 0.0094706342\n",
      "[9864,     1] loss: 0.0094705887\n",
      "[9865,     1] loss: 0.0094705433\n",
      "[9866,     1] loss: 0.0094704963\n",
      "[9867,     1] loss: 0.0094704516\n",
      "[9868,     1] loss: 0.0094704054\n",
      "[9869,     1] loss: 0.0094703607\n",
      "[9870,     1] loss: 0.0094703138\n",
      "[9871,     1] loss: 0.0094702691\n",
      "[9872,     1] loss: 0.0094702229\n",
      "[9873,     1] loss: 0.0094701767\n",
      "[9874,     1] loss: 0.0094701312\n",
      "[9875,     1] loss: 0.0094700836\n",
      "[9876,     1] loss: 0.0094700396\n",
      "[9877,     1] loss: 0.0094699934\n",
      "[9878,     1] loss: 0.0094699480\n",
      "[9879,     1] loss: 0.0094699025\n",
      "[9880,     1] loss: 0.0094698578\n",
      "[9881,     1] loss: 0.0094698139\n",
      "[9882,     1] loss: 0.0094697669\n",
      "[9883,     1] loss: 0.0094697200\n",
      "[9884,     1] loss: 0.0094696760\n",
      "[9885,     1] loss: 0.0094696291\n",
      "[9886,     1] loss: 0.0094695844\n",
      "[9887,     1] loss: 0.0094695397\n",
      "[9888,     1] loss: 0.0094694950\n",
      "[9889,     1] loss: 0.0094694488\n",
      "[9890,     1] loss: 0.0094694033\n",
      "[9891,     1] loss: 0.0094693594\n",
      "[9892,     1] loss: 0.0094693132\n",
      "[9893,     1] loss: 0.0094692677\n",
      "[9894,     1] loss: 0.0094692245\n",
      "[9895,     1] loss: 0.0094691783\n",
      "[9896,     1] loss: 0.0094691314\n",
      "[9897,     1] loss: 0.0094690867\n",
      "[9898,     1] loss: 0.0094690427\n",
      "[9899,     1] loss: 0.0094689965\n",
      "[9900,     1] loss: 0.0094689518\n",
      "[9901,     1] loss: 0.0094689079\n",
      "[9902,     1] loss: 0.0094688632\n",
      "[9903,     1] loss: 0.0094688177\n",
      "[9904,     1] loss: 0.0094687745\n",
      "[9905,     1] loss: 0.0094687276\n",
      "[9906,     1] loss: 0.0094686843\n",
      "[9907,     1] loss: 0.0094686382\n",
      "[9908,     1] loss: 0.0094685920\n",
      "[9909,     1] loss: 0.0094685487\n",
      "[9910,     1] loss: 0.0094685040\n",
      "[9911,     1] loss: 0.0094684586\n",
      "[9912,     1] loss: 0.0094684146\n",
      "[9913,     1] loss: 0.0094683707\n",
      "[9914,     1] loss: 0.0094683267\n",
      "[9915,     1] loss: 0.0094682805\n",
      "[9916,     1] loss: 0.0094682366\n",
      "[9917,     1] loss: 0.0094681926\n",
      "[9918,     1] loss: 0.0094681472\n",
      "[9919,     1] loss: 0.0094681025\n",
      "[9920,     1] loss: 0.0094680585\n",
      "[9921,     1] loss: 0.0094680138\n",
      "[9922,     1] loss: 0.0094679683\n",
      "[9923,     1] loss: 0.0094679229\n",
      "[9924,     1] loss: 0.0094678774\n",
      "[9925,     1] loss: 0.0094678350\n",
      "[9926,     1] loss: 0.0094677910\n",
      "[9927,     1] loss: 0.0094677463\n",
      "[9928,     1] loss: 0.0094677038\n",
      "[9929,     1] loss: 0.0094676577\n",
      "[9930,     1] loss: 0.0094676130\n",
      "[9931,     1] loss: 0.0094675690\n",
      "[9932,     1] loss: 0.0094675258\n",
      "[9933,     1] loss: 0.0094674803\n",
      "[9934,     1] loss: 0.0094674349\n",
      "[9935,     1] loss: 0.0094673924\n",
      "[9936,     1] loss: 0.0094673485\n",
      "[9937,     1] loss: 0.0094673030\n",
      "[9938,     1] loss: 0.0094672583\n",
      "[9939,     1] loss: 0.0094672143\n",
      "[9940,     1] loss: 0.0094671704\n",
      "[9941,     1] loss: 0.0094671279\n",
      "[9942,     1] loss: 0.0094670840\n",
      "[9943,     1] loss: 0.0094670400\n",
      "[9944,     1] loss: 0.0094669946\n",
      "[9945,     1] loss: 0.0094669506\n",
      "[9946,     1] loss: 0.0094669066\n",
      "[9947,     1] loss: 0.0094668627\n",
      "[9948,     1] loss: 0.0094668195\n",
      "[9949,     1] loss: 0.0094667755\n",
      "[9950,     1] loss: 0.0094667308\n",
      "[9951,     1] loss: 0.0094666868\n",
      "[9952,     1] loss: 0.0094666444\n",
      "[9953,     1] loss: 0.0094665997\n",
      "[9954,     1] loss: 0.0094665557\n",
      "[9955,     1] loss: 0.0094665118\n",
      "[9956,     1] loss: 0.0094664685\n",
      "[9957,     1] loss: 0.0094664253\n",
      "[9958,     1] loss: 0.0094663814\n",
      "[9959,     1] loss: 0.0094663382\n",
      "[9960,     1] loss: 0.0094662942\n",
      "[9961,     1] loss: 0.0094662510\n",
      "[9962,     1] loss: 0.0094662063\n",
      "[9963,     1] loss: 0.0094661631\n",
      "[9964,     1] loss: 0.0094661191\n",
      "[9965,     1] loss: 0.0094660737\n",
      "[9966,     1] loss: 0.0094660312\n",
      "[9967,     1] loss: 0.0094659887\n",
      "[9968,     1] loss: 0.0094659455\n",
      "[9969,     1] loss: 0.0094659008\n",
      "[9970,     1] loss: 0.0094658569\n",
      "[9971,     1] loss: 0.0094658144\n",
      "[9972,     1] loss: 0.0094657712\n",
      "[9973,     1] loss: 0.0094657280\n",
      "[9974,     1] loss: 0.0094656847\n",
      "[9975,     1] loss: 0.0094656415\n",
      "[9976,     1] loss: 0.0094655968\n",
      "[9977,     1] loss: 0.0094655544\n",
      "[9978,     1] loss: 0.0094655111\n",
      "[9979,     1] loss: 0.0094654672\n",
      "[9980,     1] loss: 0.0094654232\n",
      "[9981,     1] loss: 0.0094653800\n",
      "[9982,     1] loss: 0.0094653383\n",
      "[9983,     1] loss: 0.0094652943\n",
      "[9984,     1] loss: 0.0094652519\n",
      "[9985,     1] loss: 0.0094652086\n",
      "[9986,     1] loss: 0.0094651654\n",
      "[9987,     1] loss: 0.0094651222\n",
      "[9988,     1] loss: 0.0094650783\n",
      "[9989,     1] loss: 0.0094650373\n",
      "[9990,     1] loss: 0.0094649926\n",
      "[9991,     1] loss: 0.0094649494\n",
      "[9992,     1] loss: 0.0094649069\n",
      "[9993,     1] loss: 0.0094648644\n",
      "[9994,     1] loss: 0.0094648212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9995,     1] loss: 0.0094647773\n",
      "[9996,     1] loss: 0.0094647340\n",
      "[9997,     1] loss: 0.0094646908\n",
      "[9998,     1] loss: 0.0094646484\n",
      "[9999,     1] loss: 0.0094646059\n",
      "[10000,     1] loss: 0.0094645634\n",
      "[10001,     1] loss: 0.0094645195\n",
      "[10002,     1] loss: 0.0094644777\n",
      "[10003,     1] loss: 0.0094644353\n",
      "[10004,     1] loss: 0.0094643921\n",
      "[10005,     1] loss: 0.0094643489\n",
      "[10006,     1] loss: 0.0094643064\n",
      "[10007,     1] loss: 0.0094642647\n",
      "[10008,     1] loss: 0.0094642200\n",
      "[10009,     1] loss: 0.0094641775\n",
      "[10010,     1] loss: 0.0094641358\n",
      "[10011,     1] loss: 0.0094640948\n",
      "[10012,     1] loss: 0.0094640501\n",
      "[10013,     1] loss: 0.0094640084\n",
      "[10014,     1] loss: 0.0094639651\n",
      "[10015,     1] loss: 0.0094639227\n",
      "[10016,     1] loss: 0.0094638810\n",
      "[10017,     1] loss: 0.0094638377\n",
      "[10018,     1] loss: 0.0094637960\n",
      "[10019,     1] loss: 0.0094637543\n",
      "[10020,     1] loss: 0.0094637111\n",
      "[10021,     1] loss: 0.0094636679\n",
      "[10022,     1] loss: 0.0094636261\n",
      "[10023,     1] loss: 0.0094635852\n",
      "[10024,     1] loss: 0.0094635420\n",
      "[10025,     1] loss: 0.0094634987\n",
      "[10026,     1] loss: 0.0094634570\n",
      "[10027,     1] loss: 0.0094634145\n",
      "[10028,     1] loss: 0.0094633728\n",
      "[10029,     1] loss: 0.0094633296\n",
      "[10030,     1] loss: 0.0094632886\n",
      "[10031,     1] loss: 0.0094632462\n",
      "[10032,     1] loss: 0.0094632044\n",
      "[10033,     1] loss: 0.0094631605\n",
      "[10034,     1] loss: 0.0094631203\n",
      "[10035,     1] loss: 0.0094630778\n",
      "[10036,     1] loss: 0.0094630361\n",
      "[10037,     1] loss: 0.0094629928\n",
      "[10038,     1] loss: 0.0094629519\n",
      "[10039,     1] loss: 0.0094629094\n",
      "[10040,     1] loss: 0.0094628677\n",
      "[10041,     1] loss: 0.0094628267\n",
      "[10042,     1] loss: 0.0094627827\n",
      "[10043,     1] loss: 0.0094627418\n",
      "[10044,     1] loss: 0.0094627000\n",
      "[10045,     1] loss: 0.0094626576\n",
      "[10046,     1] loss: 0.0094626151\n",
      "[10047,     1] loss: 0.0094625749\n",
      "[10048,     1] loss: 0.0094625324\n",
      "[10049,     1] loss: 0.0094624914\n",
      "[10050,     1] loss: 0.0094624490\n",
      "[10051,     1] loss: 0.0094624072\n",
      "[10052,     1] loss: 0.0094623655\n",
      "[10053,     1] loss: 0.0094623245\n",
      "[10054,     1] loss: 0.0094622821\n",
      "[10055,     1] loss: 0.0094622411\n",
      "[10056,     1] loss: 0.0094621994\n",
      "[10057,     1] loss: 0.0094621584\n",
      "[10058,     1] loss: 0.0094621152\n",
      "[10059,     1] loss: 0.0094620727\n",
      "[10060,     1] loss: 0.0094620310\n",
      "[10061,     1] loss: 0.0094619900\n",
      "[10062,     1] loss: 0.0094619475\n",
      "[10063,     1] loss: 0.0094619080\n",
      "[10064,     1] loss: 0.0094618656\n",
      "[10065,     1] loss: 0.0094618239\n",
      "[10066,     1] loss: 0.0094617814\n",
      "[10067,     1] loss: 0.0094617397\n",
      "[10068,     1] loss: 0.0094616994\n",
      "[10069,     1] loss: 0.0094616584\n",
      "[10070,     1] loss: 0.0094616175\n",
      "[10071,     1] loss: 0.0094615743\n",
      "[10072,     1] loss: 0.0094615340\n",
      "[10073,     1] loss: 0.0094614923\n",
      "[10074,     1] loss: 0.0094614498\n",
      "[10075,     1] loss: 0.0094614074\n",
      "[10076,     1] loss: 0.0094613679\n",
      "[10077,     1] loss: 0.0094613247\n",
      "[10078,     1] loss: 0.0094612837\n",
      "[10079,     1] loss: 0.0094612435\n",
      "[10080,     1] loss: 0.0094612010\n",
      "[10081,     1] loss: 0.0094611600\n",
      "[10082,     1] loss: 0.0094611190\n",
      "[10083,     1] loss: 0.0094610788\n",
      "[10084,     1] loss: 0.0094610363\n",
      "[10085,     1] loss: 0.0094609961\n",
      "[10086,     1] loss: 0.0094609544\n",
      "[10087,     1] loss: 0.0094609134\n",
      "[10088,     1] loss: 0.0094608724\n",
      "[10089,     1] loss: 0.0094608322\n",
      "[10090,     1] loss: 0.0094607897\n",
      "[10091,     1] loss: 0.0094607502\n",
      "[10092,     1] loss: 0.0094607070\n",
      "[10093,     1] loss: 0.0094606660\n",
      "[10094,     1] loss: 0.0094606251\n",
      "[10095,     1] loss: 0.0094605833\n",
      "[10096,     1] loss: 0.0094605424\n",
      "[10097,     1] loss: 0.0094605029\n",
      "[10098,     1] loss: 0.0094604619\n",
      "[10099,     1] loss: 0.0094604202\n",
      "[10100,     1] loss: 0.0094603792\n",
      "[10101,     1] loss: 0.0094603397\n",
      "[10102,     1] loss: 0.0094602980\n",
      "[10103,     1] loss: 0.0094602562\n",
      "[10104,     1] loss: 0.0094602168\n",
      "[10105,     1] loss: 0.0094601743\n",
      "[10106,     1] loss: 0.0094601348\n",
      "[10107,     1] loss: 0.0094600946\n",
      "[10108,     1] loss: 0.0094600551\n",
      "[10109,     1] loss: 0.0094600134\n",
      "[10110,     1] loss: 0.0094599724\n",
      "[10111,     1] loss: 0.0094599314\n",
      "[10112,     1] loss: 0.0094598912\n",
      "[10113,     1] loss: 0.0094598487\n",
      "[10114,     1] loss: 0.0094598092\n",
      "[10115,     1] loss: 0.0094597682\n",
      "[10116,     1] loss: 0.0094597280\n",
      "[10117,     1] loss: 0.0094596878\n",
      "[10118,     1] loss: 0.0094596483\n",
      "[10119,     1] loss: 0.0094596058\n",
      "[10120,     1] loss: 0.0094595656\n",
      "[10121,     1] loss: 0.0094595239\n",
      "[10122,     1] loss: 0.0094594851\n",
      "[10123,     1] loss: 0.0094594434\n",
      "[10124,     1] loss: 0.0094594039\n",
      "[10125,     1] loss: 0.0094593637\n",
      "[10126,     1] loss: 0.0094593227\n",
      "[10127,     1] loss: 0.0094592839\n",
      "[10128,     1] loss: 0.0094592415\n",
      "[10129,     1] loss: 0.0094592005\n",
      "[10130,     1] loss: 0.0094591610\n",
      "[10131,     1] loss: 0.0094591208\n",
      "[10132,     1] loss: 0.0094590805\n",
      "[10133,     1] loss: 0.0094590403\n",
      "[10134,     1] loss: 0.0094589993\n",
      "[10135,     1] loss: 0.0094589613\n",
      "[10136,     1] loss: 0.0094589196\n",
      "[10137,     1] loss: 0.0094588794\n",
      "[10138,     1] loss: 0.0094588391\n",
      "[10139,     1] loss: 0.0094587989\n",
      "[10140,     1] loss: 0.0094587587\n",
      "[10141,     1] loss: 0.0094587192\n",
      "[10142,     1] loss: 0.0094586790\n",
      "[10143,     1] loss: 0.0094586395\n",
      "[10144,     1] loss: 0.0094585985\n",
      "[10145,     1] loss: 0.0094585598\n",
      "[10146,     1] loss: 0.0094585188\n",
      "[10147,     1] loss: 0.0094584793\n",
      "[10148,     1] loss: 0.0094584383\n",
      "[10149,     1] loss: 0.0094583996\n",
      "[10150,     1] loss: 0.0094583586\n",
      "[10151,     1] loss: 0.0094583191\n",
      "[10152,     1] loss: 0.0094582789\n",
      "[10153,     1] loss: 0.0094582379\n",
      "[10154,     1] loss: 0.0094582006\n",
      "[10155,     1] loss: 0.0094581597\n",
      "[10156,     1] loss: 0.0094581194\n",
      "[10157,     1] loss: 0.0094580792\n",
      "[10158,     1] loss: 0.0094580382\n",
      "[10159,     1] loss: 0.0094579987\n",
      "[10160,     1] loss: 0.0094579592\n",
      "[10161,     1] loss: 0.0094579183\n",
      "[10162,     1] loss: 0.0094578803\n",
      "[10163,     1] loss: 0.0094578400\n",
      "[10164,     1] loss: 0.0094577983\n",
      "[10165,     1] loss: 0.0094577588\n",
      "[10166,     1] loss: 0.0094577201\n",
      "[10167,     1] loss: 0.0094576806\n",
      "[10168,     1] loss: 0.0094576403\n",
      "[10169,     1] loss: 0.0094576016\n",
      "[10170,     1] loss: 0.0094575629\n",
      "[10171,     1] loss: 0.0094575241\n",
      "[10172,     1] loss: 0.0094574824\n",
      "[10173,     1] loss: 0.0094574437\n",
      "[10174,     1] loss: 0.0094574049\n",
      "[10175,     1] loss: 0.0094573639\n",
      "[10176,     1] loss: 0.0094573252\n",
      "[10177,     1] loss: 0.0094572842\n",
      "[10178,     1] loss: 0.0094572447\n",
      "[10179,     1] loss: 0.0094572060\n",
      "[10180,     1] loss: 0.0094571657\n",
      "[10181,     1] loss: 0.0094571263\n",
      "[10182,     1] loss: 0.0094570883\n",
      "[10183,     1] loss: 0.0094570488\n",
      "[10184,     1] loss: 0.0094570093\n",
      "[10185,     1] loss: 0.0094569691\n",
      "[10186,     1] loss: 0.0094569311\n",
      "[10187,     1] loss: 0.0094568908\n",
      "[10188,     1] loss: 0.0094568513\n",
      "[10189,     1] loss: 0.0094568118\n",
      "[10190,     1] loss: 0.0094567716\n",
      "[10191,     1] loss: 0.0094567329\n",
      "[10192,     1] loss: 0.0094566934\n",
      "[10193,     1] loss: 0.0094566539\n",
      "[10194,     1] loss: 0.0094566151\n",
      "[10195,     1] loss: 0.0094565757\n",
      "[10196,     1] loss: 0.0094565347\n",
      "[10197,     1] loss: 0.0094564967\n",
      "[10198,     1] loss: 0.0094564579\n",
      "[10199,     1] loss: 0.0094564177\n",
      "[10200,     1] loss: 0.0094563797\n",
      "[10201,     1] loss: 0.0094563410\n",
      "[10202,     1] loss: 0.0094563015\n",
      "[10203,     1] loss: 0.0094562612\n",
      "[10204,     1] loss: 0.0094562218\n",
      "[10205,     1] loss: 0.0094561815\n",
      "[10206,     1] loss: 0.0094561435\n",
      "[10207,     1] loss: 0.0094561055\n",
      "[10208,     1] loss: 0.0094560660\n",
      "[10209,     1] loss: 0.0094560258\n",
      "[10210,     1] loss: 0.0094559871\n",
      "[10211,     1] loss: 0.0094559491\n",
      "[10212,     1] loss: 0.0094559096\n",
      "[10213,     1] loss: 0.0094558708\n",
      "[10214,     1] loss: 0.0094558313\n",
      "[10215,     1] loss: 0.0094557926\n",
      "[10216,     1] loss: 0.0094557531\n",
      "[10217,     1] loss: 0.0094557144\n",
      "[10218,     1] loss: 0.0094556749\n",
      "[10219,     1] loss: 0.0094556361\n",
      "[10220,     1] loss: 0.0094555974\n",
      "[10221,     1] loss: 0.0094555601\n",
      "[10222,     1] loss: 0.0094555207\n",
      "[10223,     1] loss: 0.0094554827\n",
      "[10224,     1] loss: 0.0094554432\n",
      "[10225,     1] loss: 0.0094554052\n",
      "[10226,     1] loss: 0.0094553664\n",
      "[10227,     1] loss: 0.0094553277\n",
      "[10228,     1] loss: 0.0094552867\n",
      "[10229,     1] loss: 0.0094552495\n",
      "[10230,     1] loss: 0.0094552107\n",
      "[10231,     1] loss: 0.0094551712\n",
      "[10232,     1] loss: 0.0094551347\n",
      "[10233,     1] loss: 0.0094550945\n",
      "[10234,     1] loss: 0.0094550565\n",
      "[10235,     1] loss: 0.0094550177\n",
      "[10236,     1] loss: 0.0094549790\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10237,     1] loss: 0.0094549388\n",
      "[10238,     1] loss: 0.0094549023\n",
      "[10239,     1] loss: 0.0094548635\n",
      "[10240,     1] loss: 0.0094548248\n",
      "[10241,     1] loss: 0.0094547845\n",
      "[10242,     1] loss: 0.0094547473\n",
      "[10243,     1] loss: 0.0094547085\n",
      "[10244,     1] loss: 0.0094546683\n",
      "[10245,     1] loss: 0.0094546311\n",
      "[10246,     1] loss: 0.0094545931\n",
      "[10247,     1] loss: 0.0094545536\n",
      "[10248,     1] loss: 0.0094545163\n",
      "[10249,     1] loss: 0.0094544776\n",
      "[10250,     1] loss: 0.0094544396\n",
      "[10251,     1] loss: 0.0094544001\n",
      "[10252,     1] loss: 0.0094543628\n",
      "[10253,     1] loss: 0.0094543248\n",
      "[10254,     1] loss: 0.0094542861\n",
      "[10255,     1] loss: 0.0094542474\n",
      "[10256,     1] loss: 0.0094542086\n",
      "[10257,     1] loss: 0.0094541699\n",
      "[10258,     1] loss: 0.0094541319\n",
      "[10259,     1] loss: 0.0094540939\n",
      "[10260,     1] loss: 0.0094540551\n",
      "[10261,     1] loss: 0.0094540171\n",
      "[10262,     1] loss: 0.0094539784\n",
      "[10263,     1] loss: 0.0094539404\n",
      "[10264,     1] loss: 0.0094539031\n",
      "[10265,     1] loss: 0.0094538637\n",
      "[10266,     1] loss: 0.0094538257\n",
      "[10267,     1] loss: 0.0094537884\n",
      "[10268,     1] loss: 0.0094537497\n",
      "[10269,     1] loss: 0.0094537109\n",
      "[10270,     1] loss: 0.0094536722\n",
      "[10271,     1] loss: 0.0094536349\n",
      "[10272,     1] loss: 0.0094535969\n",
      "[10273,     1] loss: 0.0094535589\n",
      "[10274,     1] loss: 0.0094535202\n",
      "[10275,     1] loss: 0.0094534829\n",
      "[10276,     1] loss: 0.0094534449\n",
      "[10277,     1] loss: 0.0094534069\n",
      "[10278,     1] loss: 0.0094533682\n",
      "[10279,     1] loss: 0.0094533309\n",
      "[10280,     1] loss: 0.0094532944\n",
      "[10281,     1] loss: 0.0094532557\n",
      "[10282,     1] loss: 0.0094532177\n",
      "[10283,     1] loss: 0.0094531797\n",
      "[10284,     1] loss: 0.0094531424\n",
      "[10285,     1] loss: 0.0094531037\n",
      "[10286,     1] loss: 0.0094530657\n",
      "[10287,     1] loss: 0.0094530277\n",
      "[10288,     1] loss: 0.0094529897\n",
      "[10289,     1] loss: 0.0094529517\n",
      "[10290,     1] loss: 0.0094529137\n",
      "[10291,     1] loss: 0.0094528772\n",
      "[10292,     1] loss: 0.0094528392\n",
      "[10293,     1] loss: 0.0094528005\n",
      "[10294,     1] loss: 0.0094527639\n",
      "[10295,     1] loss: 0.0094527245\n",
      "[10296,     1] loss: 0.0094526865\n",
      "[10297,     1] loss: 0.0094526492\n",
      "[10298,     1] loss: 0.0094526105\n",
      "[10299,     1] loss: 0.0094525740\n",
      "[10300,     1] loss: 0.0094525367\n",
      "[10301,     1] loss: 0.0094524972\n",
      "[10302,     1] loss: 0.0094524600\n",
      "[10303,     1] loss: 0.0094524235\n",
      "[10304,     1] loss: 0.0094523847\n",
      "[10305,     1] loss: 0.0094523475\n",
      "[10306,     1] loss: 0.0094523102\n",
      "[10307,     1] loss: 0.0094522722\n",
      "[10308,     1] loss: 0.0094522342\n",
      "[10309,     1] loss: 0.0094521977\n",
      "[10310,     1] loss: 0.0094521604\n",
      "[10311,     1] loss: 0.0094521217\n",
      "[10312,     1] loss: 0.0094520852\n",
      "[10313,     1] loss: 0.0094520465\n",
      "[10314,     1] loss: 0.0094520092\n",
      "[10315,     1] loss: 0.0094519727\n",
      "[10316,     1] loss: 0.0094519354\n",
      "[10317,     1] loss: 0.0094518952\n",
      "[10318,     1] loss: 0.0094518594\n",
      "[10319,     1] loss: 0.0094518214\n",
      "[10320,     1] loss: 0.0094517834\n",
      "[10321,     1] loss: 0.0094517469\n",
      "[10322,     1] loss: 0.0094517097\n",
      "[10323,     1] loss: 0.0094516724\n",
      "[10324,     1] loss: 0.0094516344\n",
      "[10325,     1] loss: 0.0094515979\n",
      "[10326,     1] loss: 0.0094515607\n",
      "[10327,     1] loss: 0.0094515227\n",
      "[10328,     1] loss: 0.0094514847\n",
      "[10329,     1] loss: 0.0094514482\n",
      "[10330,     1] loss: 0.0094514087\n",
      "[10331,     1] loss: 0.0094513729\n",
      "[10332,     1] loss: 0.0094513349\n",
      "[10333,     1] loss: 0.0094512984\n",
      "[10334,     1] loss: 0.0094512612\n",
      "[10335,     1] loss: 0.0094512224\n",
      "[10336,     1] loss: 0.0094511867\n",
      "[10337,     1] loss: 0.0094511494\n",
      "[10338,     1] loss: 0.0094511114\n",
      "[10339,     1] loss: 0.0094510771\n",
      "[10340,     1] loss: 0.0094510399\n",
      "[10341,     1] loss: 0.0094510011\n",
      "[10342,     1] loss: 0.0094509654\n",
      "[10343,     1] loss: 0.0094509281\n",
      "[10344,     1] loss: 0.0094508894\n",
      "[10345,     1] loss: 0.0094508536\n",
      "[10346,     1] loss: 0.0094508156\n",
      "[10347,     1] loss: 0.0094507791\n",
      "[10348,     1] loss: 0.0094507419\n",
      "[10349,     1] loss: 0.0094507061\n",
      "[10350,     1] loss: 0.0094506681\n",
      "[10351,     1] loss: 0.0094506301\n",
      "[10352,     1] loss: 0.0094505936\n",
      "[10353,     1] loss: 0.0094505563\n",
      "[10354,     1] loss: 0.0094505198\n",
      "[10355,     1] loss: 0.0094504826\n",
      "[10356,     1] loss: 0.0094504461\n",
      "[10357,     1] loss: 0.0094504088\n",
      "[10358,     1] loss: 0.0094503731\n",
      "[10359,     1] loss: 0.0094503351\n",
      "[10360,     1] loss: 0.0094502993\n",
      "[10361,     1] loss: 0.0094502620\n",
      "[10362,     1] loss: 0.0094502255\n",
      "[10363,     1] loss: 0.0094501890\n",
      "[10364,     1] loss: 0.0094501518\n",
      "[10365,     1] loss: 0.0094501145\n",
      "[10366,     1] loss: 0.0094500780\n",
      "[10367,     1] loss: 0.0094500408\n",
      "[10368,     1] loss: 0.0094500042\n",
      "[10369,     1] loss: 0.0094499677\n",
      "[10370,     1] loss: 0.0094499305\n",
      "[10371,     1] loss: 0.0094498940\n",
      "[10372,     1] loss: 0.0094498567\n",
      "[10373,     1] loss: 0.0094498202\n",
      "[10374,     1] loss: 0.0094497830\n",
      "[10375,     1] loss: 0.0094497465\n",
      "[10376,     1] loss: 0.0094497100\n",
      "[10377,     1] loss: 0.0094496742\n",
      "[10378,     1] loss: 0.0094496377\n",
      "[10379,     1] loss: 0.0094496012\n",
      "[10380,     1] loss: 0.0094495639\n",
      "[10381,     1] loss: 0.0094495274\n",
      "[10382,     1] loss: 0.0094494902\n",
      "[10383,     1] loss: 0.0094494544\n",
      "[10384,     1] loss: 0.0094494194\n",
      "[10385,     1] loss: 0.0094493829\n",
      "[10386,     1] loss: 0.0094493449\n",
      "[10387,     1] loss: 0.0094493076\n",
      "[10388,     1] loss: 0.0094492711\n",
      "[10389,     1] loss: 0.0094492346\n",
      "[10390,     1] loss: 0.0094491974\n",
      "[10391,     1] loss: 0.0094491616\n",
      "[10392,     1] loss: 0.0094491243\n",
      "[10393,     1] loss: 0.0094490871\n",
      "[10394,     1] loss: 0.0094490528\n",
      "[10395,     1] loss: 0.0094490148\n",
      "[10396,     1] loss: 0.0094489798\n",
      "[10397,     1] loss: 0.0094489433\n",
      "[10398,     1] loss: 0.0094489060\n",
      "[10399,     1] loss: 0.0094488695\n",
      "[10400,     1] loss: 0.0094488323\n",
      "[10401,     1] loss: 0.0094487973\n",
      "[10402,     1] loss: 0.0094487600\n",
      "[10403,     1] loss: 0.0094487242\n",
      "[10404,     1] loss: 0.0094486892\n",
      "[10405,     1] loss: 0.0094486527\n",
      "[10406,     1] loss: 0.0094486162\n",
      "[10407,     1] loss: 0.0094485797\n",
      "[10408,     1] loss: 0.0094485432\n",
      "[10409,     1] loss: 0.0094485074\n",
      "[10410,     1] loss: 0.0094484694\n",
      "[10411,     1] loss: 0.0094484344\n",
      "[10412,     1] loss: 0.0094483979\n",
      "[10413,     1] loss: 0.0094483614\n",
      "[10414,     1] loss: 0.0094483241\n",
      "[10415,     1] loss: 0.0094482876\n",
      "[10416,     1] loss: 0.0094482534\n",
      "[10417,     1] loss: 0.0094482169\n",
      "[10418,     1] loss: 0.0094481803\n",
      "[10419,     1] loss: 0.0094481446\n",
      "[10420,     1] loss: 0.0094481088\n",
      "[10421,     1] loss: 0.0094480738\n",
      "[10422,     1] loss: 0.0094480366\n",
      "[10423,     1] loss: 0.0094479993\n",
      "[10424,     1] loss: 0.0094479635\n",
      "[10425,     1] loss: 0.0094479278\n",
      "[10426,     1] loss: 0.0094478920\n",
      "[10427,     1] loss: 0.0094478570\n",
      "[10428,     1] loss: 0.0094478197\n",
      "[10429,     1] loss: 0.0094477840\n",
      "[10430,     1] loss: 0.0094477460\n",
      "[10431,     1] loss: 0.0094477125\n",
      "[10432,     1] loss: 0.0094476759\n",
      "[10433,     1] loss: 0.0094476387\n",
      "[10434,     1] loss: 0.0094476037\n",
      "[10435,     1] loss: 0.0094475694\n",
      "[10436,     1] loss: 0.0094475314\n",
      "[10437,     1] loss: 0.0094474949\n",
      "[10438,     1] loss: 0.0094474599\n",
      "[10439,     1] loss: 0.0094474226\n",
      "[10440,     1] loss: 0.0094473869\n",
      "[10441,     1] loss: 0.0094473533\n",
      "[10442,     1] loss: 0.0094473168\n",
      "[10443,     1] loss: 0.0094472803\n",
      "[10444,     1] loss: 0.0094472438\n",
      "[10445,     1] loss: 0.0094472080\n",
      "[10446,     1] loss: 0.0094471730\n",
      "[10447,     1] loss: 0.0094471373\n",
      "[10448,     1] loss: 0.0094471015\n",
      "[10449,     1] loss: 0.0094470665\n",
      "[10450,     1] loss: 0.0094470292\n",
      "[10451,     1] loss: 0.0094469942\n",
      "[10452,     1] loss: 0.0094469592\n",
      "[10453,     1] loss: 0.0094469234\n",
      "[10454,     1] loss: 0.0094468854\n",
      "[10455,     1] loss: 0.0094468512\n",
      "[10456,     1] loss: 0.0094468139\n",
      "[10457,     1] loss: 0.0094467796\n",
      "[10458,     1] loss: 0.0094467431\n",
      "[10459,     1] loss: 0.0094467074\n",
      "[10460,     1] loss: 0.0094466709\n",
      "[10461,     1] loss: 0.0094466366\n",
      "[10462,     1] loss: 0.0094466001\n",
      "[10463,     1] loss: 0.0094465651\n",
      "[10464,     1] loss: 0.0094465300\n",
      "[10465,     1] loss: 0.0094464935\n",
      "[10466,     1] loss: 0.0094464585\n",
      "[10467,     1] loss: 0.0094464228\n",
      "[10468,     1] loss: 0.0094463862\n",
      "[10469,     1] loss: 0.0094463505\n",
      "[10470,     1] loss: 0.0094463162\n",
      "[10471,     1] loss: 0.0094462804\n",
      "[10472,     1] loss: 0.0094462454\n",
      "[10473,     1] loss: 0.0094462104\n",
      "[10474,     1] loss: 0.0094461739\n",
      "[10475,     1] loss: 0.0094461396\n",
      "[10476,     1] loss: 0.0094461031\n",
      "[10477,     1] loss: 0.0094460681\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10478,     1] loss: 0.0094460316\n",
      "[10479,     1] loss: 0.0094459958\n",
      "[10480,     1] loss: 0.0094459601\n",
      "[10481,     1] loss: 0.0094459251\n",
      "[10482,     1] loss: 0.0094458885\n",
      "[10483,     1] loss: 0.0094458543\n",
      "[10484,     1] loss: 0.0094458193\n",
      "[10485,     1] loss: 0.0094457828\n",
      "[10486,     1] loss: 0.0094457485\n",
      "[10487,     1] loss: 0.0094457135\n",
      "[10488,     1] loss: 0.0094456777\n",
      "[10489,     1] loss: 0.0094456427\n",
      "[10490,     1] loss: 0.0094456062\n",
      "[10491,     1] loss: 0.0094455704\n",
      "[10492,     1] loss: 0.0094455354\n",
      "[10493,     1] loss: 0.0094455011\n",
      "[10494,     1] loss: 0.0094454654\n",
      "[10495,     1] loss: 0.0094454296\n",
      "[10496,     1] loss: 0.0094453938\n",
      "[10497,     1] loss: 0.0094453596\n",
      "[10498,     1] loss: 0.0094453238\n",
      "[10499,     1] loss: 0.0094452880\n",
      "[10500,     1] loss: 0.0094452523\n",
      "[10501,     1] loss: 0.0094452180\n",
      "[10502,     1] loss: 0.0094451807\n",
      "[10503,     1] loss: 0.0094451472\n",
      "[10504,     1] loss: 0.0094451115\n",
      "[10505,     1] loss: 0.0094450757\n",
      "[10506,     1] loss: 0.0094450422\n",
      "[10507,     1] loss: 0.0094450064\n",
      "[10508,     1] loss: 0.0094449699\n",
      "[10509,     1] loss: 0.0094449356\n",
      "[10510,     1] loss: 0.0094449013\n",
      "[10511,     1] loss: 0.0094448656\n",
      "[10512,     1] loss: 0.0094448298\n",
      "[10513,     1] loss: 0.0094447948\n",
      "[10514,     1] loss: 0.0094447605\n",
      "[10515,     1] loss: 0.0094447263\n",
      "[10516,     1] loss: 0.0094446890\n",
      "[10517,     1] loss: 0.0094446540\n",
      "[10518,     1] loss: 0.0094446197\n",
      "[10519,     1] loss: 0.0094445832\n",
      "[10520,     1] loss: 0.0094445474\n",
      "[10521,     1] loss: 0.0094445132\n",
      "[10522,     1] loss: 0.0094444774\n",
      "[10523,     1] loss: 0.0094444431\n",
      "[10524,     1] loss: 0.0094444081\n",
      "[10525,     1] loss: 0.0094443731\n",
      "[10526,     1] loss: 0.0094443373\n",
      "[10527,     1] loss: 0.0094443031\n",
      "[10528,     1] loss: 0.0094442688\n",
      "[10529,     1] loss: 0.0094442323\n",
      "[10530,     1] loss: 0.0094441988\n",
      "[10531,     1] loss: 0.0094441630\n",
      "[10532,     1] loss: 0.0094441272\n",
      "[10533,     1] loss: 0.0094440915\n",
      "[10534,     1] loss: 0.0094440587\n",
      "[10535,     1] loss: 0.0094440222\n",
      "[10536,     1] loss: 0.0094439887\n",
      "[10537,     1] loss: 0.0094439529\n",
      "[10538,     1] loss: 0.0094439186\n",
      "[10539,     1] loss: 0.0094438836\n",
      "[10540,     1] loss: 0.0094438486\n",
      "[10541,     1] loss: 0.0094438143\n",
      "[10542,     1] loss: 0.0094437793\n",
      "[10543,     1] loss: 0.0094437435\n",
      "[10544,     1] loss: 0.0094437093\n",
      "[10545,     1] loss: 0.0094436757\n",
      "[10546,     1] loss: 0.0094436392\n",
      "[10547,     1] loss: 0.0094436057\n",
      "[10548,     1] loss: 0.0094435707\n",
      "[10549,     1] loss: 0.0094435342\n",
      "[10550,     1] loss: 0.0094435014\n",
      "[10551,     1] loss: 0.0094434664\n",
      "[10552,     1] loss: 0.0094434321\n",
      "[10553,     1] loss: 0.0094433963\n",
      "[10554,     1] loss: 0.0094433613\n",
      "[10555,     1] loss: 0.0094433255\n",
      "[10556,     1] loss: 0.0094432905\n",
      "[10557,     1] loss: 0.0094432570\n",
      "[10558,     1] loss: 0.0094432227\n",
      "[10559,     1] loss: 0.0094431877\n",
      "[10560,     1] loss: 0.0094431527\n",
      "[10561,     1] loss: 0.0094431192\n",
      "[10562,     1] loss: 0.0094430834\n",
      "[10563,     1] loss: 0.0094430491\n",
      "[10564,     1] loss: 0.0094430149\n",
      "[10565,     1] loss: 0.0094429798\n",
      "[10566,     1] loss: 0.0094429433\n",
      "[10567,     1] loss: 0.0094429091\n",
      "[10568,     1] loss: 0.0094428755\n",
      "[10569,     1] loss: 0.0094428405\n",
      "[10570,     1] loss: 0.0094428062\n",
      "[10571,     1] loss: 0.0094427712\n",
      "[10572,     1] loss: 0.0094427362\n",
      "[10573,     1] loss: 0.0094427019\n",
      "[10574,     1] loss: 0.0094426669\n",
      "[10575,     1] loss: 0.0094426326\n",
      "[10576,     1] loss: 0.0094425976\n",
      "[10577,     1] loss: 0.0094425626\n",
      "[10578,     1] loss: 0.0094425291\n",
      "[10579,     1] loss: 0.0094424941\n",
      "[10580,     1] loss: 0.0094424590\n",
      "[10581,     1] loss: 0.0094424255\n",
      "[10582,     1] loss: 0.0094423905\n",
      "[10583,     1] loss: 0.0094423562\n",
      "[10584,     1] loss: 0.0094423227\n",
      "[10585,     1] loss: 0.0094422884\n",
      "[10586,     1] loss: 0.0094422534\n",
      "[10587,     1] loss: 0.0094422191\n",
      "[10588,     1] loss: 0.0094421841\n",
      "[10589,     1] loss: 0.0094421506\n",
      "[10590,     1] loss: 0.0094421148\n",
      "[10591,     1] loss: 0.0094420813\n",
      "[10592,     1] loss: 0.0094420470\n",
      "[10593,     1] loss: 0.0094420135\n",
      "[10594,     1] loss: 0.0094419777\n",
      "[10595,     1] loss: 0.0094419442\n",
      "[10596,     1] loss: 0.0094419092\n",
      "[10597,     1] loss: 0.0094418742\n",
      "[10598,     1] loss: 0.0094418414\n",
      "[10599,     1] loss: 0.0094418071\n",
      "[10600,     1] loss: 0.0094417736\n",
      "[10601,     1] loss: 0.0094417393\n",
      "[10602,     1] loss: 0.0094417036\n",
      "[10603,     1] loss: 0.0094416685\n",
      "[10604,     1] loss: 0.0094416350\n",
      "[10605,     1] loss: 0.0094416007\n",
      "[10606,     1] loss: 0.0094415680\n",
      "[10607,     1] loss: 0.0094415329\n",
      "[10608,     1] loss: 0.0094414987\n",
      "[10609,     1] loss: 0.0094414636\n",
      "[10610,     1] loss: 0.0094414301\n",
      "[10611,     1] loss: 0.0094413951\n",
      "[10612,     1] loss: 0.0094413608\n",
      "[10613,     1] loss: 0.0094413273\n",
      "[10614,     1] loss: 0.0094412923\n",
      "[10615,     1] loss: 0.0094412588\n",
      "[10616,     1] loss: 0.0094412252\n",
      "[10617,     1] loss: 0.0094411895\n",
      "[10618,     1] loss: 0.0094411559\n",
      "[10619,     1] loss: 0.0094411217\n",
      "[10620,     1] loss: 0.0094410874\n",
      "[10621,     1] loss: 0.0094410531\n",
      "[10622,     1] loss: 0.0094410181\n",
      "[10623,     1] loss: 0.0094409846\n",
      "[10624,     1] loss: 0.0094409503\n",
      "[10625,     1] loss: 0.0094409160\n",
      "[10626,     1] loss: 0.0094408818\n",
      "[10627,     1] loss: 0.0094408482\n",
      "[10628,     1] loss: 0.0094408140\n",
      "[10629,     1] loss: 0.0094407804\n",
      "[10630,     1] loss: 0.0094407462\n",
      "[10631,     1] loss: 0.0094407119\n",
      "[10632,     1] loss: 0.0094406776\n",
      "[10633,     1] loss: 0.0094406433\n",
      "[10634,     1] loss: 0.0094406106\n",
      "[10635,     1] loss: 0.0094405763\n",
      "[10636,     1] loss: 0.0094405413\n",
      "[10637,     1] loss: 0.0094405070\n",
      "[10638,     1] loss: 0.0094404735\n",
      "[10639,     1] loss: 0.0094404384\n",
      "[10640,     1] loss: 0.0094404057\n",
      "[10641,     1] loss: 0.0094403721\n",
      "[10642,     1] loss: 0.0094403371\n",
      "[10643,     1] loss: 0.0094403036\n",
      "[10644,     1] loss: 0.0094402678\n",
      "[10645,     1] loss: 0.0094402343\n",
      "[10646,     1] loss: 0.0094401993\n",
      "[10647,     1] loss: 0.0094401665\n",
      "[10648,     1] loss: 0.0094401315\n",
      "[10649,     1] loss: 0.0094400980\n",
      "[10650,     1] loss: 0.0094400652\n",
      "[10651,     1] loss: 0.0094400302\n",
      "[10652,     1] loss: 0.0094399974\n",
      "[10653,     1] loss: 0.0094399638\n",
      "[10654,     1] loss: 0.0094399288\n",
      "[10655,     1] loss: 0.0094398953\n",
      "[10656,     1] loss: 0.0094398610\n",
      "[10657,     1] loss: 0.0094398260\n",
      "[10658,     1] loss: 0.0094397910\n",
      "[10659,     1] loss: 0.0094397582\n",
      "[10660,     1] loss: 0.0094397239\n",
      "[10661,     1] loss: 0.0094396904\n",
      "[10662,     1] loss: 0.0094396569\n",
      "[10663,     1] loss: 0.0094396226\n",
      "[10664,     1] loss: 0.0094395891\n",
      "[10665,     1] loss: 0.0094395541\n",
      "[10666,     1] loss: 0.0094395220\n",
      "[10667,     1] loss: 0.0094394878\n",
      "[10668,     1] loss: 0.0094394535\n",
      "[10669,     1] loss: 0.0094394214\n",
      "[10670,     1] loss: 0.0094393879\n",
      "[10671,     1] loss: 0.0094393529\n",
      "[10672,     1] loss: 0.0094393186\n",
      "[10673,     1] loss: 0.0094392844\n",
      "[10674,     1] loss: 0.0094392508\n",
      "[10675,     1] loss: 0.0094392166\n",
      "[10676,     1] loss: 0.0094391830\n",
      "[10677,     1] loss: 0.0094391495\n",
      "[10678,     1] loss: 0.0094391160\n",
      "[10679,     1] loss: 0.0094390824\n",
      "[10680,     1] loss: 0.0094390474\n",
      "[10681,     1] loss: 0.0094390146\n",
      "[10682,     1] loss: 0.0094389811\n",
      "[10683,     1] loss: 0.0094389461\n",
      "[10684,     1] loss: 0.0094389133\n",
      "[10685,     1] loss: 0.0094388805\n",
      "[10686,     1] loss: 0.0094388455\n",
      "[10687,     1] loss: 0.0094388112\n",
      "[10688,     1] loss: 0.0094387770\n",
      "[10689,     1] loss: 0.0094387434\n",
      "[10690,     1] loss: 0.0094387121\n",
      "[10691,     1] loss: 0.0094386771\n",
      "[10692,     1] loss: 0.0094386436\n",
      "[10693,     1] loss: 0.0094386108\n",
      "[10694,     1] loss: 0.0094385765\n",
      "[10695,     1] loss: 0.0094385445\n",
      "[10696,     1] loss: 0.0094385087\n",
      "[10697,     1] loss: 0.0094384767\n",
      "[10698,     1] loss: 0.0094384417\n",
      "[10699,     1] loss: 0.0094384082\n",
      "[10700,     1] loss: 0.0094383739\n",
      "[10701,     1] loss: 0.0094383389\n",
      "[10702,     1] loss: 0.0094383068\n",
      "[10703,     1] loss: 0.0094382733\n",
      "[10704,     1] loss: 0.0094382398\n",
      "[10705,     1] loss: 0.0094382055\n",
      "[10706,     1] loss: 0.0094381735\n",
      "[10707,     1] loss: 0.0094381399\n",
      "[10708,     1] loss: 0.0094381049\n",
      "[10709,     1] loss: 0.0094380721\n",
      "[10710,     1] loss: 0.0094380394\n",
      "[10711,     1] loss: 0.0094380043\n",
      "[10712,     1] loss: 0.0094379716\n",
      "[10713,     1] loss: 0.0094379358\n",
      "[10714,     1] loss: 0.0094379030\n",
      "[10715,     1] loss: 0.0094378695\n",
      "[10716,     1] loss: 0.0094378360\n",
      "[10717,     1] loss: 0.0094378017\n",
      "[10718,     1] loss: 0.0094377697\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10719,     1] loss: 0.0094377361\n",
      "[10720,     1] loss: 0.0094377026\n",
      "[10721,     1] loss: 0.0094376706\n",
      "[10722,     1] loss: 0.0094376363\n",
      "[10723,     1] loss: 0.0094376020\n",
      "[10724,     1] loss: 0.0094375685\n",
      "[10725,     1] loss: 0.0094375372\n",
      "[10726,     1] loss: 0.0094375022\n",
      "[10727,     1] loss: 0.0094374679\n",
      "[10728,     1] loss: 0.0094374351\n",
      "[10729,     1] loss: 0.0094374023\n",
      "[10730,     1] loss: 0.0094373681\n",
      "[10731,     1] loss: 0.0094373345\n",
      "[10732,     1] loss: 0.0094373010\n",
      "[10733,     1] loss: 0.0094372682\n",
      "[10734,     1] loss: 0.0094372340\n",
      "[10735,     1] loss: 0.0094372004\n",
      "[10736,     1] loss: 0.0094371676\n",
      "[10737,     1] loss: 0.0094371334\n",
      "[10738,     1] loss: 0.0094370998\n",
      "[10739,     1] loss: 0.0094370663\n",
      "[10740,     1] loss: 0.0094370335\n",
      "[10741,     1] loss: 0.0094369993\n",
      "[10742,     1] loss: 0.0094369657\n",
      "[10743,     1] loss: 0.0094369337\n",
      "[10744,     1] loss: 0.0094368972\n",
      "[10745,     1] loss: 0.0094368652\n",
      "[10746,     1] loss: 0.0094368316\n",
      "[10747,     1] loss: 0.0094367981\n",
      "[10748,     1] loss: 0.0094367661\n",
      "[10749,     1] loss: 0.0094367310\n",
      "[10750,     1] loss: 0.0094366975\n",
      "[10751,     1] loss: 0.0094366662\n",
      "[10752,     1] loss: 0.0094366319\n",
      "[10753,     1] loss: 0.0094365984\n",
      "[10754,     1] loss: 0.0094365656\n",
      "[10755,     1] loss: 0.0094365314\n",
      "[10756,     1] loss: 0.0094364986\n",
      "[10757,     1] loss: 0.0094364643\n",
      "[10758,     1] loss: 0.0094364315\n",
      "[10759,     1] loss: 0.0094363987\n",
      "[10760,     1] loss: 0.0094363645\n",
      "[10761,     1] loss: 0.0094363324\n",
      "[10762,     1] loss: 0.0094362982\n",
      "[10763,     1] loss: 0.0094362639\n",
      "[10764,     1] loss: 0.0094362326\n",
      "[10765,     1] loss: 0.0094361991\n",
      "[10766,     1] loss: 0.0094361648\n",
      "[10767,     1] loss: 0.0094361313\n",
      "[10768,     1] loss: 0.0094360985\n",
      "[10769,     1] loss: 0.0094360650\n",
      "[10770,     1] loss: 0.0094360322\n",
      "[10771,     1] loss: 0.0094359986\n",
      "[10772,     1] loss: 0.0094359644\n",
      "[10773,     1] loss: 0.0094359331\n",
      "[10774,     1] loss: 0.0094358988\n",
      "[10775,     1] loss: 0.0094358653\n",
      "[10776,     1] loss: 0.0094358332\n",
      "[10777,     1] loss: 0.0094357990\n",
      "[10778,     1] loss: 0.0094357647\n",
      "[10779,     1] loss: 0.0094357334\n",
      "[10780,     1] loss: 0.0094356991\n",
      "[10781,     1] loss: 0.0094356664\n",
      "[10782,     1] loss: 0.0094356328\n",
      "[10783,     1] loss: 0.0094356000\n",
      "[10784,     1] loss: 0.0094355673\n",
      "[10785,     1] loss: 0.0094355337\n",
      "[10786,     1] loss: 0.0094354995\n",
      "[10787,     1] loss: 0.0094354682\n",
      "[10788,     1] loss: 0.0094354339\n",
      "[10789,     1] loss: 0.0094354019\n",
      "[10790,     1] loss: 0.0094353683\n",
      "[10791,     1] loss: 0.0094353348\n",
      "[10792,     1] loss: 0.0094353028\n",
      "[10793,     1] loss: 0.0094352700\n",
      "[10794,     1] loss: 0.0094352350\n",
      "[10795,     1] loss: 0.0094352037\n",
      "[10796,     1] loss: 0.0094351701\n",
      "[10797,     1] loss: 0.0094351374\n",
      "[10798,     1] loss: 0.0094351038\n",
      "[10799,     1] loss: 0.0094350718\n",
      "[10800,     1] loss: 0.0094350383\n",
      "[10801,     1] loss: 0.0094350047\n",
      "[10802,     1] loss: 0.0094349712\n",
      "[10803,     1] loss: 0.0094349377\n",
      "[10804,     1] loss: 0.0094349049\n",
      "[10805,     1] loss: 0.0094348721\n",
      "[10806,     1] loss: 0.0094348393\n",
      "[10807,     1] loss: 0.0094348066\n",
      "[10808,     1] loss: 0.0094347738\n",
      "[10809,     1] loss: 0.0094347402\n",
      "[10810,     1] loss: 0.0094347075\n",
      "[10811,     1] loss: 0.0094346747\n",
      "[10812,     1] loss: 0.0094346412\n",
      "[10813,     1] loss: 0.0094346091\n",
      "[10814,     1] loss: 0.0094345756\n",
      "[10815,     1] loss: 0.0094345436\n",
      "[10816,     1] loss: 0.0094345100\n",
      "[10817,     1] loss: 0.0094344772\n",
      "[10818,     1] loss: 0.0094344430\n",
      "[10819,     1] loss: 0.0094344102\n",
      "[10820,     1] loss: 0.0094343774\n",
      "[10821,     1] loss: 0.0094343439\n",
      "[10822,     1] loss: 0.0094343111\n",
      "[10823,     1] loss: 0.0094342776\n",
      "[10824,     1] loss: 0.0094342455\n",
      "[10825,     1] loss: 0.0094342120\n",
      "[10826,     1] loss: 0.0094341785\n",
      "[10827,     1] loss: 0.0094341472\n",
      "[10828,     1] loss: 0.0094341137\n",
      "[10829,     1] loss: 0.0094340794\n",
      "[10830,     1] loss: 0.0094340466\n",
      "[10831,     1] loss: 0.0094340146\n",
      "[10832,     1] loss: 0.0094339818\n",
      "[10833,     1] loss: 0.0094339482\n",
      "[10834,     1] loss: 0.0094339147\n",
      "[10835,     1] loss: 0.0094338804\n",
      "[10836,     1] loss: 0.0094338477\n",
      "[10837,     1] loss: 0.0094338156\n",
      "[10838,     1] loss: 0.0094337821\n",
      "[10839,     1] loss: 0.0094337486\n",
      "[10840,     1] loss: 0.0094337158\n",
      "[10841,     1] loss: 0.0094336838\n",
      "[10842,     1] loss: 0.0094336495\n",
      "[10843,     1] loss: 0.0094336174\n",
      "[10844,     1] loss: 0.0094335839\n",
      "[10845,     1] loss: 0.0094335519\n",
      "[10846,     1] loss: 0.0094335191\n",
      "[10847,     1] loss: 0.0094334856\n",
      "[10848,     1] loss: 0.0094334520\n",
      "[10849,     1] loss: 0.0094334193\n",
      "[10850,     1] loss: 0.0094333872\n",
      "[10851,     1] loss: 0.0094333537\n",
      "[10852,     1] loss: 0.0094333202\n",
      "[10853,     1] loss: 0.0094332881\n",
      "[10854,     1] loss: 0.0094332546\n",
      "[10855,     1] loss: 0.0094332211\n",
      "[10856,     1] loss: 0.0094331890\n",
      "[10857,     1] loss: 0.0094331548\n",
      "[10858,     1] loss: 0.0094331220\n",
      "[10859,     1] loss: 0.0094330899\n",
      "[10860,     1] loss: 0.0094330564\n",
      "[10861,     1] loss: 0.0094330244\n",
      "[10862,     1] loss: 0.0094329923\n",
      "[10863,     1] loss: 0.0094329596\n",
      "[10864,     1] loss: 0.0094329260\n",
      "[10865,     1] loss: 0.0094328932\n",
      "[10866,     1] loss: 0.0094328597\n",
      "[10867,     1] loss: 0.0094328269\n",
      "[10868,     1] loss: 0.0094327949\n",
      "[10869,     1] loss: 0.0094327621\n",
      "[10870,     1] loss: 0.0094327286\n",
      "[10871,     1] loss: 0.0094326958\n",
      "[10872,     1] loss: 0.0094326630\n",
      "[10873,     1] loss: 0.0094326302\n",
      "[10874,     1] loss: 0.0094325975\n",
      "[10875,     1] loss: 0.0094325647\n",
      "[10876,     1] loss: 0.0094325311\n",
      "[10877,     1] loss: 0.0094324999\n",
      "[10878,     1] loss: 0.0094324663\n",
      "[10879,     1] loss: 0.0094324343\n",
      "[10880,     1] loss: 0.0094324015\n",
      "[10881,     1] loss: 0.0094323687\n",
      "[10882,     1] loss: 0.0094323352\n",
      "[10883,     1] loss: 0.0094323017\n",
      "[10884,     1] loss: 0.0094322704\n",
      "[10885,     1] loss: 0.0094322369\n",
      "[10886,     1] loss: 0.0094322041\n",
      "[10887,     1] loss: 0.0094321705\n",
      "[10888,     1] loss: 0.0094321385\n",
      "[10889,     1] loss: 0.0094321050\n",
      "[10890,     1] loss: 0.0094320722\n",
      "[10891,     1] loss: 0.0094320409\n",
      "[10892,     1] loss: 0.0094320074\n",
      "[10893,     1] loss: 0.0094319746\n",
      "[10894,     1] loss: 0.0094319426\n",
      "[10895,     1] loss: 0.0094319105\n",
      "[10896,     1] loss: 0.0094318762\n",
      "[10897,     1] loss: 0.0094318449\n",
      "[10898,     1] loss: 0.0094318122\n",
      "[10899,     1] loss: 0.0094317779\n",
      "[10900,     1] loss: 0.0094317459\n",
      "[10901,     1] loss: 0.0094317123\n",
      "[10902,     1] loss: 0.0094316788\n",
      "[10903,     1] loss: 0.0094316475\n",
      "[10904,     1] loss: 0.0094316132\n",
      "[10905,     1] loss: 0.0094315812\n",
      "[10906,     1] loss: 0.0094315477\n",
      "[10907,     1] loss: 0.0094315149\n",
      "[10908,     1] loss: 0.0094314829\n",
      "[10909,     1] loss: 0.0094314508\n",
      "[10910,     1] loss: 0.0094314165\n",
      "[10911,     1] loss: 0.0094313852\n",
      "[10912,     1] loss: 0.0094313510\n",
      "[10913,     1] loss: 0.0094313189\n",
      "[10914,     1] loss: 0.0094312862\n",
      "[10915,     1] loss: 0.0094312526\n",
      "[10916,     1] loss: 0.0094312213\n",
      "[10917,     1] loss: 0.0094311878\n",
      "[10918,     1] loss: 0.0094311550\n",
      "[10919,     1] loss: 0.0094311230\n",
      "[10920,     1] loss: 0.0094310887\n",
      "[10921,     1] loss: 0.0094310574\n",
      "[10922,     1] loss: 0.0094310239\n",
      "[10923,     1] loss: 0.0094309911\n",
      "[10924,     1] loss: 0.0094309583\n",
      "[10925,     1] loss: 0.0094309263\n",
      "[10926,     1] loss: 0.0094308928\n",
      "[10927,     1] loss: 0.0094308600\n",
      "[10928,     1] loss: 0.0094308279\n",
      "[10929,     1] loss: 0.0094307959\n",
      "[10930,     1] loss: 0.0094307631\n",
      "[10931,     1] loss: 0.0094307303\n",
      "[10932,     1] loss: 0.0094306983\n",
      "[10933,     1] loss: 0.0094306663\n",
      "[10934,     1] loss: 0.0094306327\n",
      "[10935,     1] loss: 0.0094306000\n",
      "[10936,     1] loss: 0.0094305664\n",
      "[10937,     1] loss: 0.0094305336\n",
      "[10938,     1] loss: 0.0094305016\n",
      "[10939,     1] loss: 0.0094304681\n",
      "[10940,     1] loss: 0.0094304368\n",
      "[10941,     1] loss: 0.0094304033\n",
      "[10942,     1] loss: 0.0094303720\n",
      "[10943,     1] loss: 0.0094303384\n",
      "[10944,     1] loss: 0.0094303057\n",
      "[10945,     1] loss: 0.0094302729\n",
      "[10946,     1] loss: 0.0094302401\n",
      "[10947,     1] loss: 0.0094302088\n",
      "[10948,     1] loss: 0.0094301753\n",
      "[10949,     1] loss: 0.0094301410\n",
      "[10950,     1] loss: 0.0094301082\n",
      "[10951,     1] loss: 0.0094300769\n",
      "[10952,     1] loss: 0.0094300434\n",
      "[10953,     1] loss: 0.0094300114\n",
      "[10954,     1] loss: 0.0094299793\n",
      "[10955,     1] loss: 0.0094299465\n",
      "[10956,     1] loss: 0.0094299138\n",
      "[10957,     1] loss: 0.0094298810\n",
      "[10958,     1] loss: 0.0094298482\n",
      "[10959,     1] loss: 0.0094298147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10960,     1] loss: 0.0094297819\n",
      "[10961,     1] loss: 0.0094297476\n",
      "[10962,     1] loss: 0.0094297171\n",
      "[10963,     1] loss: 0.0094296843\n",
      "[10964,     1] loss: 0.0094296508\n",
      "[10965,     1] loss: 0.0094296172\n",
      "[10966,     1] loss: 0.0094295852\n",
      "[10967,     1] loss: 0.0094295532\n",
      "[10968,     1] loss: 0.0094295211\n",
      "[10969,     1] loss: 0.0094294891\n",
      "[10970,     1] loss: 0.0094294548\n",
      "[10971,     1] loss: 0.0094294228\n",
      "[10972,     1] loss: 0.0094293922\n",
      "[10973,     1] loss: 0.0094293572\n",
      "[10974,     1] loss: 0.0094293252\n",
      "[10975,     1] loss: 0.0094292939\n",
      "[10976,     1] loss: 0.0094292603\n",
      "[10977,     1] loss: 0.0094292276\n",
      "[10978,     1] loss: 0.0094291940\n",
      "[10979,     1] loss: 0.0094291620\n",
      "[10980,     1] loss: 0.0094291285\n",
      "[10981,     1] loss: 0.0094290964\n",
      "[10982,     1] loss: 0.0094290644\n",
      "[10983,     1] loss: 0.0094290316\n",
      "[10984,     1] loss: 0.0094289973\n",
      "[10985,     1] loss: 0.0094289660\n",
      "[10986,     1] loss: 0.0094289333\n",
      "[10987,     1] loss: 0.0094289005\n",
      "[10988,     1] loss: 0.0094288684\n",
      "[10989,     1] loss: 0.0094288357\n",
      "[10990,     1] loss: 0.0094288021\n",
      "[10991,     1] loss: 0.0094287694\n",
      "[10992,     1] loss: 0.0094287373\n",
      "[10993,     1] loss: 0.0094287053\n",
      "[10994,     1] loss: 0.0094286717\n",
      "[10995,     1] loss: 0.0094286382\n",
      "[10996,     1] loss: 0.0094286069\n",
      "[10997,     1] loss: 0.0094285727\n",
      "[10998,     1] loss: 0.0094285421\n",
      "[10999,     1] loss: 0.0094285086\n",
      "[11000,     1] loss: 0.0094284765\n",
      "[11001,     1] loss: 0.0094284438\n",
      "[11002,     1] loss: 0.0094284117\n",
      "[11003,     1] loss: 0.0094283782\n",
      "[11004,     1] loss: 0.0094283469\n",
      "[11005,     1] loss: 0.0094283126\n",
      "[11006,     1] loss: 0.0094282813\n",
      "[11007,     1] loss: 0.0094282486\n",
      "[11008,     1] loss: 0.0094282173\n",
      "[11009,     1] loss: 0.0094281845\n",
      "[11010,     1] loss: 0.0094281517\n",
      "[11011,     1] loss: 0.0094281174\n",
      "[11012,     1] loss: 0.0094280839\n",
      "[11013,     1] loss: 0.0094280526\n",
      "[11014,     1] loss: 0.0094280206\n",
      "[11015,     1] loss: 0.0094279878\n",
      "[11016,     1] loss: 0.0094279550\n",
      "[11017,     1] loss: 0.0094279207\n",
      "[11018,     1] loss: 0.0094278894\n",
      "[11019,     1] loss: 0.0094278567\n",
      "[11020,     1] loss: 0.0094278246\n",
      "[11021,     1] loss: 0.0094277911\n",
      "[11022,     1] loss: 0.0094277591\n",
      "[11023,     1] loss: 0.0094277270\n",
      "[11024,     1] loss: 0.0094276942\n",
      "[11025,     1] loss: 0.0094276622\n",
      "[11026,     1] loss: 0.0094276279\n",
      "[11027,     1] loss: 0.0094275974\n",
      "[11028,     1] loss: 0.0094275653\n",
      "[11029,     1] loss: 0.0094275318\n",
      "[11030,     1] loss: 0.0094275005\n",
      "[11031,     1] loss: 0.0094274670\n",
      "[11032,     1] loss: 0.0094274327\n",
      "[11033,     1] loss: 0.0094273999\n",
      "[11034,     1] loss: 0.0094273679\n",
      "[11035,     1] loss: 0.0094273351\n",
      "[11036,     1] loss: 0.0094273038\n",
      "[11037,     1] loss: 0.0094272703\n",
      "[11038,     1] loss: 0.0094272375\n",
      "[11039,     1] loss: 0.0094272055\n",
      "[11040,     1] loss: 0.0094271727\n",
      "[11041,     1] loss: 0.0094271407\n",
      "[11042,     1] loss: 0.0094271071\n",
      "[11043,     1] loss: 0.0094270743\n",
      "[11044,     1] loss: 0.0094270423\n",
      "[11045,     1] loss: 0.0094270095\n",
      "[11046,     1] loss: 0.0094269775\n",
      "[11047,     1] loss: 0.0094269454\n",
      "[11048,     1] loss: 0.0094269127\n",
      "[11049,     1] loss: 0.0094268806\n",
      "[11050,     1] loss: 0.0094268471\n",
      "[11051,     1] loss: 0.0094268151\n",
      "[11052,     1] loss: 0.0094267823\n",
      "[11053,     1] loss: 0.0094267502\n",
      "[11054,     1] loss: 0.0094267167\n",
      "[11055,     1] loss: 0.0094266824\n",
      "[11056,     1] loss: 0.0094266519\n",
      "[11057,     1] loss: 0.0094266176\n",
      "[11058,     1] loss: 0.0094265856\n",
      "[11059,     1] loss: 0.0094265528\n",
      "[11060,     1] loss: 0.0094265200\n",
      "[11061,     1] loss: 0.0094264872\n",
      "[11062,     1] loss: 0.0094264552\n",
      "[11063,     1] loss: 0.0094264224\n",
      "[11064,     1] loss: 0.0094263904\n",
      "[11065,     1] loss: 0.0094263576\n",
      "[11066,     1] loss: 0.0094263256\n",
      "[11067,     1] loss: 0.0094262920\n",
      "[11068,     1] loss: 0.0094262592\n",
      "[11069,     1] loss: 0.0094262272\n",
      "[11070,     1] loss: 0.0094261929\n",
      "[11071,     1] loss: 0.0094261624\n",
      "[11072,     1] loss: 0.0094261281\n",
      "[11073,     1] loss: 0.0094260953\n",
      "[11074,     1] loss: 0.0094260648\n",
      "[11075,     1] loss: 0.0094260305\n",
      "[11076,     1] loss: 0.0094259977\n",
      "[11077,     1] loss: 0.0094259642\n",
      "[11078,     1] loss: 0.0094259322\n",
      "[11079,     1] loss: 0.0094259001\n",
      "[11080,     1] loss: 0.0094258666\n",
      "[11081,     1] loss: 0.0094258331\n",
      "[11082,     1] loss: 0.0094258018\n",
      "[11083,     1] loss: 0.0094257683\n",
      "[11084,     1] loss: 0.0094257355\n",
      "[11085,     1] loss: 0.0094257042\n",
      "[11086,     1] loss: 0.0094256729\n",
      "[11087,     1] loss: 0.0094256394\n",
      "[11088,     1] loss: 0.0094256058\n",
      "[11089,     1] loss: 0.0094255738\n",
      "[11090,     1] loss: 0.0094255418\n",
      "[11091,     1] loss: 0.0094255090\n",
      "[11092,     1] loss: 0.0094254769\n",
      "[11093,     1] loss: 0.0094254442\n",
      "[11094,     1] loss: 0.0094254121\n",
      "[11095,     1] loss: 0.0094253786\n",
      "[11096,     1] loss: 0.0094253451\n",
      "[11097,     1] loss: 0.0094253138\n",
      "[11098,     1] loss: 0.0094252810\n",
      "[11099,     1] loss: 0.0094252475\n",
      "[11100,     1] loss: 0.0094252169\n",
      "[11101,     1] loss: 0.0094251841\n",
      "[11102,     1] loss: 0.0094251499\n",
      "[11103,     1] loss: 0.0094251178\n",
      "[11104,     1] loss: 0.0094250843\n",
      "[11105,     1] loss: 0.0094250523\n",
      "[11106,     1] loss: 0.0094250217\n",
      "[11107,     1] loss: 0.0094249882\n",
      "[11108,     1] loss: 0.0094249532\n",
      "[11109,     1] loss: 0.0094249226\n",
      "[11110,     1] loss: 0.0094248898\n",
      "[11111,     1] loss: 0.0094248571\n",
      "[11112,     1] loss: 0.0094248243\n",
      "[11113,     1] loss: 0.0094247930\n",
      "[11114,     1] loss: 0.0094247594\n",
      "[11115,     1] loss: 0.0094247267\n",
      "[11116,     1] loss: 0.0094246939\n",
      "[11117,     1] loss: 0.0094246611\n",
      "[11118,     1] loss: 0.0094246276\n",
      "[11119,     1] loss: 0.0094245955\n",
      "[11120,     1] loss: 0.0094245628\n",
      "[11121,     1] loss: 0.0094245307\n",
      "[11122,     1] loss: 0.0094244972\n",
      "[11123,     1] loss: 0.0094244651\n",
      "[11124,     1] loss: 0.0094244324\n",
      "[11125,     1] loss: 0.0094243988\n",
      "[11126,     1] loss: 0.0094243661\n",
      "[11127,     1] loss: 0.0094243333\n",
      "[11128,     1] loss: 0.0094243005\n",
      "[11129,     1] loss: 0.0094242685\n",
      "[11130,     1] loss: 0.0094242349\n",
      "[11131,     1] loss: 0.0094242021\n",
      "[11132,     1] loss: 0.0094241701\n",
      "[11133,     1] loss: 0.0094241373\n",
      "[11134,     1] loss: 0.0094241038\n",
      "[11135,     1] loss: 0.0094240710\n",
      "[11136,     1] loss: 0.0094240390\n",
      "[11137,     1] loss: 0.0094240054\n",
      "[11138,     1] loss: 0.0094239734\n",
      "[11139,     1] loss: 0.0094239399\n",
      "[11140,     1] loss: 0.0094239078\n",
      "[11141,     1] loss: 0.0094238743\n",
      "[11142,     1] loss: 0.0094238423\n",
      "[11143,     1] loss: 0.0094238102\n",
      "[11144,     1] loss: 0.0094237760\n",
      "[11145,     1] loss: 0.0094237439\n",
      "[11146,     1] loss: 0.0094237112\n",
      "[11147,     1] loss: 0.0094236784\n",
      "[11148,     1] loss: 0.0094236456\n",
      "[11149,     1] loss: 0.0094236135\n",
      "[11150,     1] loss: 0.0094235800\n",
      "[11151,     1] loss: 0.0094235487\n",
      "[11152,     1] loss: 0.0094235145\n",
      "[11153,     1] loss: 0.0094234817\n",
      "[11154,     1] loss: 0.0094234489\n",
      "[11155,     1] loss: 0.0094234176\n",
      "[11156,     1] loss: 0.0094233841\n",
      "[11157,     1] loss: 0.0094233505\n",
      "[11158,     1] loss: 0.0094233185\n",
      "[11159,     1] loss: 0.0094232842\n",
      "[11160,     1] loss: 0.0094232522\n",
      "[11161,     1] loss: 0.0094232202\n",
      "[11162,     1] loss: 0.0094231881\n",
      "[11163,     1] loss: 0.0094231546\n",
      "[11164,     1] loss: 0.0094231211\n",
      "[11165,     1] loss: 0.0094230883\n",
      "[11166,     1] loss: 0.0094230562\n",
      "[11167,     1] loss: 0.0094230242\n",
      "[11168,     1] loss: 0.0094229907\n",
      "[11169,     1] loss: 0.0094229586\n",
      "[11170,     1] loss: 0.0094229266\n",
      "[11171,     1] loss: 0.0094228923\n",
      "[11172,     1] loss: 0.0094228595\n",
      "[11173,     1] loss: 0.0094228275\n",
      "[11174,     1] loss: 0.0094227940\n",
      "[11175,     1] loss: 0.0094227605\n",
      "[11176,     1] loss: 0.0094227292\n",
      "[11177,     1] loss: 0.0094226964\n",
      "[11178,     1] loss: 0.0094226643\n",
      "[11179,     1] loss: 0.0094226308\n",
      "[11180,     1] loss: 0.0094225995\n",
      "[11181,     1] loss: 0.0094225660\n",
      "[11182,     1] loss: 0.0094225332\n",
      "[11183,     1] loss: 0.0094225019\n",
      "[11184,     1] loss: 0.0094224676\n",
      "[11185,     1] loss: 0.0094224349\n",
      "[11186,     1] loss: 0.0094224021\n",
      "[11187,     1] loss: 0.0094223693\n",
      "[11188,     1] loss: 0.0094223350\n",
      "[11189,     1] loss: 0.0094223037\n",
      "[11190,     1] loss: 0.0094222695\n",
      "[11191,     1] loss: 0.0094222389\n",
      "[11192,     1] loss: 0.0094222046\n",
      "[11193,     1] loss: 0.0094221719\n",
      "[11194,     1] loss: 0.0094221383\n",
      "[11195,     1] loss: 0.0094221048\n",
      "[11196,     1] loss: 0.0094220728\n",
      "[11197,     1] loss: 0.0094220400\n",
      "[11198,     1] loss: 0.0094220072\n",
      "[11199,     1] loss: 0.0094219744\n",
      "[11200,     1] loss: 0.0094219409\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11201,     1] loss: 0.0094219081\n",
      "[11202,     1] loss: 0.0094218746\n",
      "[11203,     1] loss: 0.0094218425\n",
      "[11204,     1] loss: 0.0094218098\n",
      "[11205,     1] loss: 0.0094217777\n",
      "[11206,     1] loss: 0.0094217442\n",
      "[11207,     1] loss: 0.0094217107\n",
      "[11208,     1] loss: 0.0094216786\n",
      "[11209,     1] loss: 0.0094216458\n",
      "[11210,     1] loss: 0.0094216131\n",
      "[11211,     1] loss: 0.0094215803\n",
      "[11212,     1] loss: 0.0094215468\n",
      "[11213,     1] loss: 0.0094215147\n",
      "[11214,     1] loss: 0.0094214804\n",
      "[11215,     1] loss: 0.0094214477\n",
      "[11216,     1] loss: 0.0094214156\n",
      "[11217,     1] loss: 0.0094213836\n",
      "[11218,     1] loss: 0.0094213508\n",
      "[11219,     1] loss: 0.0094213158\n",
      "[11220,     1] loss: 0.0094212838\n",
      "[11221,     1] loss: 0.0094212517\n",
      "[11222,     1] loss: 0.0094212189\n",
      "[11223,     1] loss: 0.0094211854\n",
      "[11224,     1] loss: 0.0094211511\n",
      "[11225,     1] loss: 0.0094211198\n",
      "[11226,     1] loss: 0.0094210863\n",
      "[11227,     1] loss: 0.0094210535\n",
      "[11228,     1] loss: 0.0094210215\n",
      "[11229,     1] loss: 0.0094209872\n",
      "[11230,     1] loss: 0.0094209552\n",
      "[11231,     1] loss: 0.0094209217\n",
      "[11232,     1] loss: 0.0094208896\n",
      "[11233,     1] loss: 0.0094208568\n",
      "[11234,     1] loss: 0.0094208241\n",
      "[11235,     1] loss: 0.0094207913\n",
      "[11236,     1] loss: 0.0094207585\n",
      "[11237,     1] loss: 0.0094207257\n",
      "[11238,     1] loss: 0.0094206922\n",
      "[11239,     1] loss: 0.0094206594\n",
      "[11240,     1] loss: 0.0094206266\n",
      "[11241,     1] loss: 0.0094205931\n",
      "[11242,     1] loss: 0.0094205603\n",
      "[11243,     1] loss: 0.0094205268\n",
      "[11244,     1] loss: 0.0094204940\n",
      "[11245,     1] loss: 0.0094204620\n",
      "[11246,     1] loss: 0.0094204277\n",
      "[11247,     1] loss: 0.0094203949\n",
      "[11248,     1] loss: 0.0094203621\n",
      "[11249,     1] loss: 0.0094203293\n",
      "[11250,     1] loss: 0.0094202951\n",
      "[11251,     1] loss: 0.0094202638\n",
      "[11252,     1] loss: 0.0094202310\n",
      "[11253,     1] loss: 0.0094201967\n",
      "[11254,     1] loss: 0.0094201647\n",
      "[11255,     1] loss: 0.0094201319\n",
      "[11256,     1] loss: 0.0094200976\n",
      "[11257,     1] loss: 0.0094200641\n",
      "[11258,     1] loss: 0.0094200328\n",
      "[11259,     1] loss: 0.0094199985\n",
      "[11260,     1] loss: 0.0094199643\n",
      "[11261,     1] loss: 0.0094199322\n",
      "[11262,     1] loss: 0.0094198987\n",
      "[11263,     1] loss: 0.0094198667\n",
      "[11264,     1] loss: 0.0094198331\n",
      "[11265,     1] loss: 0.0094198011\n",
      "[11266,     1] loss: 0.0094197676\n",
      "[11267,     1] loss: 0.0094197333\n",
      "[11268,     1] loss: 0.0094196998\n",
      "[11269,     1] loss: 0.0094196670\n",
      "[11270,     1] loss: 0.0094196342\n",
      "[11271,     1] loss: 0.0094196007\n",
      "[11272,     1] loss: 0.0094195679\n",
      "[11273,     1] loss: 0.0094195351\n",
      "[11274,     1] loss: 0.0094195016\n",
      "[11275,     1] loss: 0.0094194688\n",
      "[11276,     1] loss: 0.0094194360\n",
      "[11277,     1] loss: 0.0094194025\n",
      "[11278,     1] loss: 0.0094193690\n",
      "[11279,     1] loss: 0.0094193354\n",
      "[11280,     1] loss: 0.0094193026\n",
      "[11281,     1] loss: 0.0094192706\n",
      "[11282,     1] loss: 0.0094192363\n",
      "[11283,     1] loss: 0.0094192035\n",
      "[11284,     1] loss: 0.0094191708\n",
      "[11285,     1] loss: 0.0094191380\n",
      "[11286,     1] loss: 0.0094191052\n",
      "[11287,     1] loss: 0.0094190717\n",
      "[11288,     1] loss: 0.0094190389\n",
      "[11289,     1] loss: 0.0094190069\n",
      "[11290,     1] loss: 0.0094189733\n",
      "[11291,     1] loss: 0.0094189405\n",
      "[11292,     1] loss: 0.0094189070\n",
      "[11293,     1] loss: 0.0094188727\n",
      "[11294,     1] loss: 0.0094188407\n",
      "[11295,     1] loss: 0.0094188072\n",
      "[11296,     1] loss: 0.0094187737\n",
      "[11297,     1] loss: 0.0094187409\n",
      "[11298,     1] loss: 0.0094187096\n",
      "[11299,     1] loss: 0.0094186746\n",
      "[11300,     1] loss: 0.0094186418\n",
      "[11301,     1] loss: 0.0094186090\n",
      "[11302,     1] loss: 0.0094185755\n",
      "[11303,     1] loss: 0.0094185419\n",
      "[11304,     1] loss: 0.0094185092\n",
      "[11305,     1] loss: 0.0094184756\n",
      "[11306,     1] loss: 0.0094184421\n",
      "[11307,     1] loss: 0.0094184093\n",
      "[11308,     1] loss: 0.0094183773\n",
      "[11309,     1] loss: 0.0094183438\n",
      "[11310,     1] loss: 0.0094183117\n",
      "[11311,     1] loss: 0.0094182789\n",
      "[11312,     1] loss: 0.0094182432\n",
      "[11313,     1] loss: 0.0094182126\n",
      "[11314,     1] loss: 0.0094181776\n",
      "[11315,     1] loss: 0.0094181433\n",
      "[11316,     1] loss: 0.0094181105\n",
      "[11317,     1] loss: 0.0094180778\n",
      "[11318,     1] loss: 0.0094180435\n",
      "[11319,     1] loss: 0.0094180107\n",
      "[11320,     1] loss: 0.0094179779\n",
      "[11321,     1] loss: 0.0094179437\n",
      "[11322,     1] loss: 0.0094179124\n",
      "[11323,     1] loss: 0.0094178781\n",
      "[11324,     1] loss: 0.0094178438\n",
      "[11325,     1] loss: 0.0094178110\n",
      "[11326,     1] loss: 0.0094177783\n",
      "[11327,     1] loss: 0.0094177440\n",
      "[11328,     1] loss: 0.0094177105\n",
      "[11329,     1] loss: 0.0094176777\n",
      "[11330,     1] loss: 0.0094176449\n",
      "[11331,     1] loss: 0.0094176121\n",
      "[11332,     1] loss: 0.0094175786\n",
      "[11333,     1] loss: 0.0094175451\n",
      "[11334,     1] loss: 0.0094175115\n",
      "[11335,     1] loss: 0.0094174780\n",
      "[11336,     1] loss: 0.0094174452\n",
      "[11337,     1] loss: 0.0094174124\n",
      "[11338,     1] loss: 0.0094173782\n",
      "[11339,     1] loss: 0.0094173454\n",
      "[11340,     1] loss: 0.0094173118\n",
      "[11341,     1] loss: 0.0094172776\n",
      "[11342,     1] loss: 0.0094172448\n",
      "[11343,     1] loss: 0.0094172113\n",
      "[11344,     1] loss: 0.0094171777\n",
      "[11345,     1] loss: 0.0094171442\n",
      "[11346,     1] loss: 0.0094171114\n",
      "[11347,     1] loss: 0.0094170772\n",
      "[11348,     1] loss: 0.0094170444\n",
      "[11349,     1] loss: 0.0094170116\n",
      "[11350,     1] loss: 0.0094169773\n",
      "[11351,     1] loss: 0.0094169438\n",
      "[11352,     1] loss: 0.0094169095\n",
      "[11353,     1] loss: 0.0094168767\n",
      "[11354,     1] loss: 0.0094168440\n",
      "[11355,     1] loss: 0.0094168119\n",
      "[11356,     1] loss: 0.0094167776\n",
      "[11357,     1] loss: 0.0094167426\n",
      "[11358,     1] loss: 0.0094167098\n",
      "[11359,     1] loss: 0.0094166771\n",
      "[11360,     1] loss: 0.0094166435\n",
      "[11361,     1] loss: 0.0094166093\n",
      "[11362,     1] loss: 0.0094165765\n",
      "[11363,     1] loss: 0.0094165437\n",
      "[11364,     1] loss: 0.0094165094\n",
      "[11365,     1] loss: 0.0094164759\n",
      "[11366,     1] loss: 0.0094164439\n",
      "[11367,     1] loss: 0.0094164088\n",
      "[11368,     1] loss: 0.0094163746\n",
      "[11369,     1] loss: 0.0094163425\n",
      "[11370,     1] loss: 0.0094163090\n",
      "[11371,     1] loss: 0.0094162747\n",
      "[11372,     1] loss: 0.0094162434\n",
      "[11373,     1] loss: 0.0094162099\n",
      "[11374,     1] loss: 0.0094161741\n",
      "[11375,     1] loss: 0.0094161429\n",
      "[11376,     1] loss: 0.0094161093\n",
      "[11377,     1] loss: 0.0094160728\n",
      "[11378,     1] loss: 0.0094160423\n",
      "[11379,     1] loss: 0.0094160095\n",
      "[11380,     1] loss: 0.0094159752\n",
      "[11381,     1] loss: 0.0094159417\n",
      "[11382,     1] loss: 0.0094159082\n",
      "[11383,     1] loss: 0.0094158746\n",
      "[11384,     1] loss: 0.0094158404\n",
      "[11385,     1] loss: 0.0094158068\n",
      "[11386,     1] loss: 0.0094157748\n",
      "[11387,     1] loss: 0.0094157413\n",
      "[11388,     1] loss: 0.0094157070\n",
      "[11389,     1] loss: 0.0094156742\n",
      "[11390,     1] loss: 0.0094156414\n",
      "[11391,     1] loss: 0.0094156072\n",
      "[11392,     1] loss: 0.0094155721\n",
      "[11393,     1] loss: 0.0094155401\n",
      "[11394,     1] loss: 0.0094155051\n",
      "[11395,     1] loss: 0.0094154723\n",
      "[11396,     1] loss: 0.0094154395\n",
      "[11397,     1] loss: 0.0094154067\n",
      "[11398,     1] loss: 0.0094153710\n",
      "[11399,     1] loss: 0.0094153382\n",
      "[11400,     1] loss: 0.0094153039\n",
      "[11401,     1] loss: 0.0094152711\n",
      "[11402,     1] loss: 0.0094152361\n",
      "[11403,     1] loss: 0.0094152041\n",
      "[11404,     1] loss: 0.0094151691\n",
      "[11405,     1] loss: 0.0094151348\n",
      "[11406,     1] loss: 0.0094151035\n",
      "[11407,     1] loss: 0.0094150692\n",
      "[11408,     1] loss: 0.0094150349\n",
      "[11409,     1] loss: 0.0094150014\n",
      "[11410,     1] loss: 0.0094149679\n",
      "[11411,     1] loss: 0.0094149351\n",
      "[11412,     1] loss: 0.0094149001\n",
      "[11413,     1] loss: 0.0094148673\n",
      "[11414,     1] loss: 0.0094148330\n",
      "[11415,     1] loss: 0.0094148003\n",
      "[11416,     1] loss: 0.0094147667\n",
      "[11417,     1] loss: 0.0094147339\n",
      "[11418,     1] loss: 0.0094146997\n",
      "[11419,     1] loss: 0.0094146647\n",
      "[11420,     1] loss: 0.0094146319\n",
      "[11421,     1] loss: 0.0094145969\n",
      "[11422,     1] loss: 0.0094145626\n",
      "[11423,     1] loss: 0.0094145298\n",
      "[11424,     1] loss: 0.0094144970\n",
      "[11425,     1] loss: 0.0094144627\n",
      "[11426,     1] loss: 0.0094144285\n",
      "[11427,     1] loss: 0.0094143957\n",
      "[11428,     1] loss: 0.0094143622\n",
      "[11429,     1] loss: 0.0094143271\n",
      "[11430,     1] loss: 0.0094142951\n",
      "[11431,     1] loss: 0.0094142623\n",
      "[11432,     1] loss: 0.0094142281\n",
      "[11433,     1] loss: 0.0094141945\n",
      "[11434,     1] loss: 0.0094141603\n",
      "[11435,     1] loss: 0.0094141260\n",
      "[11436,     1] loss: 0.0094140910\n",
      "[11437,     1] loss: 0.0094140589\n",
      "[11438,     1] loss: 0.0094140239\n",
      "[11439,     1] loss: 0.0094139911\n",
      "[11440,     1] loss: 0.0094139583\n",
      "[11441,     1] loss: 0.0094139233\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11442,     1] loss: 0.0094138898\n",
      "[11443,     1] loss: 0.0094138563\n",
      "[11444,     1] loss: 0.0094138227\n",
      "[11445,     1] loss: 0.0094137885\n",
      "[11446,     1] loss: 0.0094137557\n",
      "[11447,     1] loss: 0.0094137207\n",
      "[11448,     1] loss: 0.0094136864\n",
      "[11449,     1] loss: 0.0094136544\n",
      "[11450,     1] loss: 0.0094136193\n",
      "[11451,     1] loss: 0.0094135858\n",
      "[11452,     1] loss: 0.0094135515\n",
      "[11453,     1] loss: 0.0094135188\n",
      "[11454,     1] loss: 0.0094134845\n",
      "[11455,     1] loss: 0.0094134495\n",
      "[11456,     1] loss: 0.0094134167\n",
      "[11457,     1] loss: 0.0094133832\n",
      "[11458,     1] loss: 0.0094133489\n",
      "[11459,     1] loss: 0.0094133154\n",
      "[11460,     1] loss: 0.0094132818\n",
      "[11461,     1] loss: 0.0094132468\n",
      "[11462,     1] loss: 0.0094132140\n",
      "[11463,     1] loss: 0.0094131805\n",
      "[11464,     1] loss: 0.0094131470\n",
      "[11465,     1] loss: 0.0094131120\n",
      "[11466,     1] loss: 0.0094130784\n",
      "[11467,     1] loss: 0.0094130449\n",
      "[11468,     1] loss: 0.0094130114\n",
      "[11469,     1] loss: 0.0094129764\n",
      "[11470,     1] loss: 0.0094129436\n",
      "[11471,     1] loss: 0.0094129093\n",
      "[11472,     1] loss: 0.0094128758\n",
      "[11473,     1] loss: 0.0094128415\n",
      "[11474,     1] loss: 0.0094128072\n",
      "[11475,     1] loss: 0.0094127737\n",
      "[11476,     1] loss: 0.0094127402\n",
      "[11477,     1] loss: 0.0094127059\n",
      "[11478,     1] loss: 0.0094126724\n",
      "[11479,     1] loss: 0.0094126388\n",
      "[11480,     1] loss: 0.0094126038\n",
      "[11481,     1] loss: 0.0094125710\n",
      "[11482,     1] loss: 0.0094125375\n",
      "[11483,     1] loss: 0.0094125032\n",
      "[11484,     1] loss: 0.0094124682\n",
      "[11485,     1] loss: 0.0094124362\n",
      "[11486,     1] loss: 0.0094124027\n",
      "[11487,     1] loss: 0.0094123669\n",
      "[11488,     1] loss: 0.0094123334\n",
      "[11489,     1] loss: 0.0094122998\n",
      "[11490,     1] loss: 0.0094122663\n",
      "[11491,     1] loss: 0.0094122320\n",
      "[11492,     1] loss: 0.0094121970\n",
      "[11493,     1] loss: 0.0094121628\n",
      "[11494,     1] loss: 0.0094121307\n",
      "[11495,     1] loss: 0.0094120957\n",
      "[11496,     1] loss: 0.0094120622\n",
      "[11497,     1] loss: 0.0094120272\n",
      "[11498,     1] loss: 0.0094119951\n",
      "[11499,     1] loss: 0.0094119608\n",
      "[11500,     1] loss: 0.0094119243\n",
      "[11501,     1] loss: 0.0094118908\n",
      "[11502,     1] loss: 0.0094118588\n",
      "[11503,     1] loss: 0.0094118237\n",
      "[11504,     1] loss: 0.0094117880\n",
      "[11505,     1] loss: 0.0094117545\n",
      "[11506,     1] loss: 0.0094117217\n",
      "[11507,     1] loss: 0.0094116867\n",
      "[11508,     1] loss: 0.0094116531\n",
      "[11509,     1] loss: 0.0094116189\n",
      "[11510,     1] loss: 0.0094115846\n",
      "[11511,     1] loss: 0.0094115503\n",
      "[11512,     1] loss: 0.0094115160\n",
      "[11513,     1] loss: 0.0094114818\n",
      "[11514,     1] loss: 0.0094114482\n",
      "[11515,     1] loss: 0.0094114140\n",
      "[11516,     1] loss: 0.0094113804\n",
      "[11517,     1] loss: 0.0094113462\n",
      "[11518,     1] loss: 0.0094113119\n",
      "[11519,     1] loss: 0.0094112776\n",
      "[11520,     1] loss: 0.0094112441\n",
      "[11521,     1] loss: 0.0094112106\n",
      "[11522,     1] loss: 0.0094111763\n",
      "[11523,     1] loss: 0.0094111405\n",
      "[11524,     1] loss: 0.0094111077\n",
      "[11525,     1] loss: 0.0094110742\n",
      "[11526,     1] loss: 0.0094110407\n",
      "[11527,     1] loss: 0.0094110057\n",
      "[11528,     1] loss: 0.0094109714\n",
      "[11529,     1] loss: 0.0094109379\n",
      "[11530,     1] loss: 0.0094109029\n",
      "[11531,     1] loss: 0.0094108686\n",
      "[11532,     1] loss: 0.0094108343\n",
      "[11533,     1] loss: 0.0094108000\n",
      "[11534,     1] loss: 0.0094107650\n",
      "[11535,     1] loss: 0.0094107300\n",
      "[11536,     1] loss: 0.0094106972\n",
      "[11537,     1] loss: 0.0094106637\n",
      "[11538,     1] loss: 0.0094106294\n",
      "[11539,     1] loss: 0.0094105951\n",
      "[11540,     1] loss: 0.0094105609\n",
      "[11541,     1] loss: 0.0094105251\n",
      "[11542,     1] loss: 0.0094104931\n",
      "[11543,     1] loss: 0.0094104588\n",
      "[11544,     1] loss: 0.0094104245\n",
      "[11545,     1] loss: 0.0094103895\n",
      "[11546,     1] loss: 0.0094103552\n",
      "[11547,     1] loss: 0.0094103210\n",
      "[11548,     1] loss: 0.0094102867\n",
      "[11549,     1] loss: 0.0094102524\n",
      "[11550,     1] loss: 0.0094102181\n",
      "[11551,     1] loss: 0.0094101839\n",
      "[11552,     1] loss: 0.0094101496\n",
      "[11553,     1] loss: 0.0094101153\n",
      "[11554,     1] loss: 0.0094100803\n",
      "[11555,     1] loss: 0.0094100468\n",
      "[11556,     1] loss: 0.0094100133\n",
      "[11557,     1] loss: 0.0094099797\n",
      "[11558,     1] loss: 0.0094099455\n",
      "[11559,     1] loss: 0.0094099104\n",
      "[11560,     1] loss: 0.0094098762\n",
      "[11561,     1] loss: 0.0094098404\n",
      "[11562,     1] loss: 0.0094098069\n",
      "[11563,     1] loss: 0.0094097726\n",
      "[11564,     1] loss: 0.0094097383\n",
      "[11565,     1] loss: 0.0094097033\n",
      "[11566,     1] loss: 0.0094096705\n",
      "[11567,     1] loss: 0.0094096363\n",
      "[11568,     1] loss: 0.0094096027\n",
      "[11569,     1] loss: 0.0094095662\n",
      "[11570,     1] loss: 0.0094095320\n",
      "[11571,     1] loss: 0.0094094992\n",
      "[11572,     1] loss: 0.0094094642\n",
      "[11573,     1] loss: 0.0094094299\n",
      "[11574,     1] loss: 0.0094093956\n",
      "[11575,     1] loss: 0.0094093606\n",
      "[11576,     1] loss: 0.0094093271\n",
      "[11577,     1] loss: 0.0094092935\n",
      "[11578,     1] loss: 0.0094092585\n",
      "[11579,     1] loss: 0.0094092235\n",
      "[11580,     1] loss: 0.0094091900\n",
      "[11581,     1] loss: 0.0094091557\n",
      "[11582,     1] loss: 0.0094091207\n",
      "[11583,     1] loss: 0.0094090864\n",
      "[11584,     1] loss: 0.0094090529\n",
      "[11585,     1] loss: 0.0094090179\n",
      "[11586,     1] loss: 0.0094089836\n",
      "[11587,     1] loss: 0.0094089486\n",
      "[11588,     1] loss: 0.0094089150\n",
      "[11589,     1] loss: 0.0094088800\n",
      "[11590,     1] loss: 0.0094088458\n",
      "[11591,     1] loss: 0.0094088122\n",
      "[11592,     1] loss: 0.0094087772\n",
      "[11593,     1] loss: 0.0094087422\n",
      "[11594,     1] loss: 0.0094087087\n",
      "[11595,     1] loss: 0.0094086759\n",
      "[11596,     1] loss: 0.0094086401\n",
      "[11597,     1] loss: 0.0094086036\n",
      "[11598,     1] loss: 0.0094085716\n",
      "[11599,     1] loss: 0.0094085366\n",
      "[11600,     1] loss: 0.0094085008\n",
      "[11601,     1] loss: 0.0094084673\n",
      "[11602,     1] loss: 0.0094084330\n",
      "[11603,     1] loss: 0.0094083995\n",
      "[11604,     1] loss: 0.0094083637\n",
      "[11605,     1] loss: 0.0094083287\n",
      "[11606,     1] loss: 0.0094082944\n",
      "[11607,     1] loss: 0.0094082601\n",
      "[11608,     1] loss: 0.0094082251\n",
      "[11609,     1] loss: 0.0094081901\n",
      "[11610,     1] loss: 0.0094081566\n",
      "[11611,     1] loss: 0.0094081216\n",
      "[11612,     1] loss: 0.0094080880\n",
      "[11613,     1] loss: 0.0094080523\n",
      "[11614,     1] loss: 0.0094080195\n",
      "[11615,     1] loss: 0.0094079852\n",
      "[11616,     1] loss: 0.0094079494\n",
      "[11617,     1] loss: 0.0094079144\n",
      "[11618,     1] loss: 0.0094078794\n",
      "[11619,     1] loss: 0.0094078451\n",
      "[11620,     1] loss: 0.0094078124\n",
      "[11621,     1] loss: 0.0094077773\n",
      "[11622,     1] loss: 0.0094077438\n",
      "[11623,     1] loss: 0.0094077103\n",
      "[11624,     1] loss: 0.0094076745\n",
      "[11625,     1] loss: 0.0094076395\n",
      "[11626,     1] loss: 0.0094076030\n",
      "[11627,     1] loss: 0.0094075702\n",
      "[11628,     1] loss: 0.0094075352\n",
      "[11629,     1] loss: 0.0094075002\n",
      "[11630,     1] loss: 0.0094074659\n",
      "[11631,     1] loss: 0.0094074316\n",
      "[11632,     1] loss: 0.0094073966\n",
      "[11633,     1] loss: 0.0094073616\n",
      "[11634,     1] loss: 0.0094073258\n",
      "[11635,     1] loss: 0.0094072916\n",
      "[11636,     1] loss: 0.0094072580\n",
      "[11637,     1] loss: 0.0094072230\n",
      "[11638,     1] loss: 0.0094071880\n",
      "[11639,     1] loss: 0.0094071545\n",
      "[11640,     1] loss: 0.0094071195\n",
      "[11641,     1] loss: 0.0094070844\n",
      "[11642,     1] loss: 0.0094070502\n",
      "[11643,     1] loss: 0.0094070174\n",
      "[11644,     1] loss: 0.0094069809\n",
      "[11645,     1] loss: 0.0094069473\n",
      "[11646,     1] loss: 0.0094069116\n",
      "[11647,     1] loss: 0.0094068781\n",
      "[11648,     1] loss: 0.0094068423\n",
      "[11649,     1] loss: 0.0094068088\n",
      "[11650,     1] loss: 0.0094067737\n",
      "[11651,     1] loss: 0.0094067387\n",
      "[11652,     1] loss: 0.0094067037\n",
      "[11653,     1] loss: 0.0094066709\n",
      "[11654,     1] loss: 0.0094066359\n",
      "[11655,     1] loss: 0.0094066001\n",
      "[11656,     1] loss: 0.0094065666\n",
      "[11657,     1] loss: 0.0094065331\n",
      "[11658,     1] loss: 0.0094064973\n",
      "[11659,     1] loss: 0.0094064631\n",
      "[11660,     1] loss: 0.0094064295\n",
      "[11661,     1] loss: 0.0094063945\n",
      "[11662,     1] loss: 0.0094063595\n",
      "[11663,     1] loss: 0.0094063245\n",
      "[11664,     1] loss: 0.0094062909\n",
      "[11665,     1] loss: 0.0094062544\n",
      "[11666,     1] loss: 0.0094062194\n",
      "[11667,     1] loss: 0.0094061852\n",
      "[11668,     1] loss: 0.0094061501\n",
      "[11669,     1] loss: 0.0094061159\n",
      "[11670,     1] loss: 0.0094060794\n",
      "[11671,     1] loss: 0.0094060451\n",
      "[11672,     1] loss: 0.0094060108\n",
      "[11673,     1] loss: 0.0094059758\n",
      "[11674,     1] loss: 0.0094059415\n",
      "[11675,     1] loss: 0.0094059065\n",
      "[11676,     1] loss: 0.0094058715\n",
      "[11677,     1] loss: 0.0094058380\n",
      "[11678,     1] loss: 0.0094058029\n",
      "[11679,     1] loss: 0.0094057687\n",
      "[11680,     1] loss: 0.0094057336\n",
      "[11681,     1] loss: 0.0094056979\n",
      "[11682,     1] loss: 0.0094056629\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11683,     1] loss: 0.0094056278\n",
      "[11684,     1] loss: 0.0094055936\n",
      "[11685,     1] loss: 0.0094055586\n",
      "[11686,     1] loss: 0.0094055250\n",
      "[11687,     1] loss: 0.0094054900\n",
      "[11688,     1] loss: 0.0094054550\n",
      "[11689,     1] loss: 0.0094054200\n",
      "[11690,     1] loss: 0.0094053857\n",
      "[11691,     1] loss: 0.0094053507\n",
      "[11692,     1] loss: 0.0094053164\n",
      "[11693,     1] loss: 0.0094052799\n",
      "[11694,     1] loss: 0.0094052456\n",
      "[11695,     1] loss: 0.0094052114\n",
      "[11696,     1] loss: 0.0094051763\n",
      "[11697,     1] loss: 0.0094051413\n",
      "[11698,     1] loss: 0.0094051078\n",
      "[11699,     1] loss: 0.0094050720\n",
      "[11700,     1] loss: 0.0094050355\n",
      "[11701,     1] loss: 0.0094050027\n",
      "[11702,     1] loss: 0.0094049685\n",
      "[11703,     1] loss: 0.0094049327\n",
      "[11704,     1] loss: 0.0094048969\n",
      "[11705,     1] loss: 0.0094048627\n",
      "[11706,     1] loss: 0.0094048269\n",
      "[11707,     1] loss: 0.0094047926\n",
      "[11708,     1] loss: 0.0094047576\n",
      "[11709,     1] loss: 0.0094047226\n",
      "[11710,     1] loss: 0.0094046876\n",
      "[11711,     1] loss: 0.0094046533\n",
      "[11712,     1] loss: 0.0094046183\n",
      "[11713,     1] loss: 0.0094045840\n",
      "[11714,     1] loss: 0.0094045475\n",
      "[11715,     1] loss: 0.0094045132\n",
      "[11716,     1] loss: 0.0094044790\n",
      "[11717,     1] loss: 0.0094044439\n",
      "[11718,     1] loss: 0.0094044074\n",
      "[11719,     1] loss: 0.0094043724\n",
      "[11720,     1] loss: 0.0094043389\n",
      "[11721,     1] loss: 0.0094043024\n",
      "[11722,     1] loss: 0.0094042681\n",
      "[11723,     1] loss: 0.0094042331\n",
      "[11724,     1] loss: 0.0094041988\n",
      "[11725,     1] loss: 0.0094041653\n",
      "[11726,     1] loss: 0.0094041288\n",
      "[11727,     1] loss: 0.0094040938\n",
      "[11728,     1] loss: 0.0094040588\n",
      "[11729,     1] loss: 0.0094040245\n",
      "[11730,     1] loss: 0.0094039895\n",
      "[11731,     1] loss: 0.0094039552\n",
      "[11732,     1] loss: 0.0094039202\n",
      "[11733,     1] loss: 0.0094038852\n",
      "[11734,     1] loss: 0.0094038501\n",
      "[11735,     1] loss: 0.0094038151\n",
      "[11736,     1] loss: 0.0094037816\n",
      "[11737,     1] loss: 0.0094037458\n",
      "[11738,     1] loss: 0.0094037093\n",
      "[11739,     1] loss: 0.0094036758\n",
      "[11740,     1] loss: 0.0094036408\n",
      "[11741,     1] loss: 0.0094036050\n",
      "[11742,     1] loss: 0.0094035693\n",
      "[11743,     1] loss: 0.0094035350\n",
      "[11744,     1] loss: 0.0094035000\n",
      "[11745,     1] loss: 0.0094034649\n",
      "[11746,     1] loss: 0.0094034307\n",
      "[11747,     1] loss: 0.0094033957\n",
      "[11748,     1] loss: 0.0094033599\n",
      "[11749,     1] loss: 0.0094033241\n",
      "[11750,     1] loss: 0.0094032899\n",
      "[11751,     1] loss: 0.0094032548\n",
      "[11752,     1] loss: 0.0094032206\n",
      "[11753,     1] loss: 0.0094031863\n",
      "[11754,     1] loss: 0.0094031498\n",
      "[11755,     1] loss: 0.0094031140\n",
      "[11756,     1] loss: 0.0094030797\n",
      "[11757,     1] loss: 0.0094030447\n",
      "[11758,     1] loss: 0.0094030097\n",
      "[11759,     1] loss: 0.0094029747\n",
      "[11760,     1] loss: 0.0094029404\n",
      "[11761,     1] loss: 0.0094029039\n",
      "[11762,     1] loss: 0.0094028704\n",
      "[11763,     1] loss: 0.0094028354\n",
      "[11764,     1] loss: 0.0094027989\n",
      "[11765,     1] loss: 0.0094027638\n",
      "[11766,     1] loss: 0.0094027303\n",
      "[11767,     1] loss: 0.0094026946\n",
      "[11768,     1] loss: 0.0094026595\n",
      "[11769,     1] loss: 0.0094026238\n",
      "[11770,     1] loss: 0.0094025895\n",
      "[11771,     1] loss: 0.0094025552\n",
      "[11772,     1] loss: 0.0094025195\n",
      "[11773,     1] loss: 0.0094024837\n",
      "[11774,     1] loss: 0.0094024494\n",
      "[11775,     1] loss: 0.0094024144\n",
      "[11776,     1] loss: 0.0094023794\n",
      "[11777,     1] loss: 0.0094023444\n",
      "[11778,     1] loss: 0.0094023079\n",
      "[11779,     1] loss: 0.0094022729\n",
      "[11780,     1] loss: 0.0094022386\n",
      "[11781,     1] loss: 0.0094022036\n",
      "[11782,     1] loss: 0.0094021685\n",
      "[11783,     1] loss: 0.0094021335\n",
      "[11784,     1] loss: 0.0094020993\n",
      "[11785,     1] loss: 0.0094020627\n",
      "[11786,     1] loss: 0.0094020292\n",
      "[11787,     1] loss: 0.0094019935\n",
      "[11788,     1] loss: 0.0094019584\n",
      "[11789,     1] loss: 0.0094019227\n",
      "[11790,     1] loss: 0.0094018877\n",
      "[11791,     1] loss: 0.0094018526\n",
      "[11792,     1] loss: 0.0094018176\n",
      "[11793,     1] loss: 0.0094017833\n",
      "[11794,     1] loss: 0.0094017483\n",
      "[11795,     1] loss: 0.0094017118\n",
      "[11796,     1] loss: 0.0094016783\n",
      "[11797,     1] loss: 0.0094016433\n",
      "[11798,     1] loss: 0.0094016068\n",
      "[11799,     1] loss: 0.0094015718\n",
      "[11800,     1] loss: 0.0094015360\n",
      "[11801,     1] loss: 0.0094015010\n",
      "[11802,     1] loss: 0.0094014667\n",
      "[11803,     1] loss: 0.0094014294\n",
      "[11804,     1] loss: 0.0094013967\n",
      "[11805,     1] loss: 0.0094013609\n",
      "[11806,     1] loss: 0.0094013266\n",
      "[11807,     1] loss: 0.0094012909\n",
      "[11808,     1] loss: 0.0094012558\n",
      "[11809,     1] loss: 0.0094012201\n",
      "[11810,     1] loss: 0.0094011851\n",
      "[11811,     1] loss: 0.0094011500\n",
      "[11812,     1] loss: 0.0094011158\n",
      "[11813,     1] loss: 0.0094010793\n",
      "[11814,     1] loss: 0.0094010442\n",
      "[11815,     1] loss: 0.0094010085\n",
      "[11816,     1] loss: 0.0094009742\n",
      "[11817,     1] loss: 0.0094009377\n",
      "[11818,     1] loss: 0.0094009034\n",
      "[11819,     1] loss: 0.0094008684\n",
      "[11820,     1] loss: 0.0094008334\n",
      "[11821,     1] loss: 0.0094007991\n",
      "[11822,     1] loss: 0.0094007641\n",
      "[11823,     1] loss: 0.0094007276\n",
      "[11824,     1] loss: 0.0094006933\n",
      "[11825,     1] loss: 0.0094006576\n",
      "[11826,     1] loss: 0.0094006218\n",
      "[11827,     1] loss: 0.0094005875\n",
      "[11828,     1] loss: 0.0094005518\n",
      "[11829,     1] loss: 0.0094005175\n",
      "[11830,     1] loss: 0.0094004817\n",
      "[11831,     1] loss: 0.0094004467\n",
      "[11832,     1] loss: 0.0094004124\n",
      "[11833,     1] loss: 0.0094003767\n",
      "[11834,     1] loss: 0.0094003417\n",
      "[11835,     1] loss: 0.0094003059\n",
      "[11836,     1] loss: 0.0094002701\n",
      "[11837,     1] loss: 0.0094002359\n",
      "[11838,     1] loss: 0.0094002008\n",
      "[11839,     1] loss: 0.0094001643\n",
      "[11840,     1] loss: 0.0094001308\n",
      "[11841,     1] loss: 0.0094000943\n",
      "[11842,     1] loss: 0.0094000585\n",
      "[11843,     1] loss: 0.0094000243\n",
      "[11844,     1] loss: 0.0093999900\n",
      "[11845,     1] loss: 0.0093999535\n",
      "[11846,     1] loss: 0.0093999177\n",
      "[11847,     1] loss: 0.0093998834\n",
      "[11848,     1] loss: 0.0093998477\n",
      "[11849,     1] loss: 0.0093998112\n",
      "[11850,     1] loss: 0.0093997769\n",
      "[11851,     1] loss: 0.0093997426\n",
      "[11852,     1] loss: 0.0093997069\n",
      "[11853,     1] loss: 0.0093996719\n",
      "[11854,     1] loss: 0.0093996376\n",
      "[11855,     1] loss: 0.0093996011\n",
      "[11856,     1] loss: 0.0093995668\n",
      "[11857,     1] loss: 0.0093995303\n",
      "[11858,     1] loss: 0.0093994945\n",
      "[11859,     1] loss: 0.0093994603\n",
      "[11860,     1] loss: 0.0093994252\n",
      "[11861,     1] loss: 0.0093993887\n",
      "[11862,     1] loss: 0.0093993537\n",
      "[11863,     1] loss: 0.0093993194\n",
      "[11864,     1] loss: 0.0093992829\n",
      "[11865,     1] loss: 0.0093992472\n",
      "[11866,     1] loss: 0.0093992136\n",
      "[11867,     1] loss: 0.0093991779\n",
      "[11868,     1] loss: 0.0093991429\n",
      "[11869,     1] loss: 0.0093991056\n",
      "[11870,     1] loss: 0.0093990713\n",
      "[11871,     1] loss: 0.0093990356\n",
      "[11872,     1] loss: 0.0093990006\n",
      "[11873,     1] loss: 0.0093989648\n",
      "[11874,     1] loss: 0.0093989305\n",
      "[11875,     1] loss: 0.0093988940\n",
      "[11876,     1] loss: 0.0093988612\n",
      "[11877,     1] loss: 0.0093988247\n",
      "[11878,     1] loss: 0.0093987890\n",
      "[11879,     1] loss: 0.0093987539\n",
      "[11880,     1] loss: 0.0093987182\n",
      "[11881,     1] loss: 0.0093986839\n",
      "[11882,     1] loss: 0.0093986489\n",
      "[11883,     1] loss: 0.0093986124\n",
      "[11884,     1] loss: 0.0093985766\n",
      "[11885,     1] loss: 0.0093985416\n",
      "[11886,     1] loss: 0.0093985066\n",
      "[11887,     1] loss: 0.0093984716\n",
      "[11888,     1] loss: 0.0093984365\n",
      "[11889,     1] loss: 0.0093984008\n",
      "[11890,     1] loss: 0.0093983665\n",
      "[11891,     1] loss: 0.0093983293\n",
      "[11892,     1] loss: 0.0093982957\n",
      "[11893,     1] loss: 0.0093982592\n",
      "[11894,     1] loss: 0.0093982242\n",
      "[11895,     1] loss: 0.0093981892\n",
      "[11896,     1] loss: 0.0093981534\n",
      "[11897,     1] loss: 0.0093981192\n",
      "[11898,     1] loss: 0.0093980826\n",
      "[11899,     1] loss: 0.0093980491\n",
      "[11900,     1] loss: 0.0093980134\n",
      "[11901,     1] loss: 0.0093979776\n",
      "[11902,     1] loss: 0.0093979426\n",
      "[11903,     1] loss: 0.0093979083\n",
      "[11904,     1] loss: 0.0093978703\n",
      "[11905,     1] loss: 0.0093978375\n",
      "[11906,     1] loss: 0.0093978025\n",
      "[11907,     1] loss: 0.0093977645\n",
      "[11908,     1] loss: 0.0093977325\n",
      "[11909,     1] loss: 0.0093976960\n",
      "[11910,     1] loss: 0.0093976602\n",
      "[11911,     1] loss: 0.0093976259\n",
      "[11912,     1] loss: 0.0093975894\n",
      "[11913,     1] loss: 0.0093975559\n",
      "[11914,     1] loss: 0.0093975194\n",
      "[11915,     1] loss: 0.0093974836\n",
      "[11916,     1] loss: 0.0093974486\n",
      "[11917,     1] loss: 0.0093974121\n",
      "[11918,     1] loss: 0.0093973778\n",
      "[11919,     1] loss: 0.0093973421\n",
      "[11920,     1] loss: 0.0093973070\n",
      "[11921,     1] loss: 0.0093972705\n",
      "[11922,     1] loss: 0.0093972355\n",
      "[11923,     1] loss: 0.0093972005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11924,     1] loss: 0.0093971647\n",
      "[11925,     1] loss: 0.0093971297\n",
      "[11926,     1] loss: 0.0093970947\n",
      "[11927,     1] loss: 0.0093970589\n",
      "[11928,     1] loss: 0.0093970232\n",
      "[11929,     1] loss: 0.0093969882\n",
      "[11930,     1] loss: 0.0093969516\n",
      "[11931,     1] loss: 0.0093969166\n",
      "[11932,     1] loss: 0.0093968809\n",
      "[11933,     1] loss: 0.0093968458\n",
      "[11934,     1] loss: 0.0093968108\n",
      "[11935,     1] loss: 0.0093967743\n",
      "[11936,     1] loss: 0.0093967408\n",
      "[11937,     1] loss: 0.0093967043\n",
      "[11938,     1] loss: 0.0093966678\n",
      "[11939,     1] loss: 0.0093966335\n",
      "[11940,     1] loss: 0.0093965977\n",
      "[11941,     1] loss: 0.0093965627\n",
      "[11942,     1] loss: 0.0093965262\n",
      "[11943,     1] loss: 0.0093964905\n",
      "[11944,     1] loss: 0.0093964562\n",
      "[11945,     1] loss: 0.0093964212\n",
      "[11946,     1] loss: 0.0093963861\n",
      "[11947,     1] loss: 0.0093963511\n",
      "[11948,     1] loss: 0.0093963154\n",
      "[11949,     1] loss: 0.0093962811\n",
      "[11950,     1] loss: 0.0093962438\n",
      "[11951,     1] loss: 0.0093962088\n",
      "[11952,     1] loss: 0.0093961746\n",
      "[11953,     1] loss: 0.0093961380\n",
      "[11954,     1] loss: 0.0093961030\n",
      "[11955,     1] loss: 0.0093960665\n",
      "[11956,     1] loss: 0.0093960315\n",
      "[11957,     1] loss: 0.0093959957\n",
      "[11958,     1] loss: 0.0093959615\n",
      "[11959,     1] loss: 0.0093959250\n",
      "[11960,     1] loss: 0.0093958892\n",
      "[11961,     1] loss: 0.0093958549\n",
      "[11962,     1] loss: 0.0093958192\n",
      "[11963,     1] loss: 0.0093957834\n",
      "[11964,     1] loss: 0.0093957491\n",
      "[11965,     1] loss: 0.0093957134\n",
      "[11966,     1] loss: 0.0093956783\n",
      "[11967,     1] loss: 0.0093956441\n",
      "[11968,     1] loss: 0.0093956076\n",
      "[11969,     1] loss: 0.0093955711\n",
      "[11970,     1] loss: 0.0093955360\n",
      "[11971,     1] loss: 0.0093955018\n",
      "[11972,     1] loss: 0.0093954667\n",
      "[11973,     1] loss: 0.0093954295\n",
      "[11974,     1] loss: 0.0093953945\n",
      "[11975,     1] loss: 0.0093953587\n",
      "[11976,     1] loss: 0.0093953237\n",
      "[11977,     1] loss: 0.0093952879\n",
      "[11978,     1] loss: 0.0093952522\n",
      "[11979,     1] loss: 0.0093952157\n",
      "[11980,     1] loss: 0.0093951814\n",
      "[11981,     1] loss: 0.0093951456\n",
      "[11982,     1] loss: 0.0093951099\n",
      "[11983,     1] loss: 0.0093950748\n",
      "[11984,     1] loss: 0.0093950398\n",
      "[11985,     1] loss: 0.0093950048\n",
      "[11986,     1] loss: 0.0093949683\n",
      "[11987,     1] loss: 0.0093949340\n",
      "[11988,     1] loss: 0.0093948990\n",
      "[11989,     1] loss: 0.0093948632\n",
      "[11990,     1] loss: 0.0093948290\n",
      "[11991,     1] loss: 0.0093947910\n",
      "[11992,     1] loss: 0.0093947574\n",
      "[11993,     1] loss: 0.0093947217\n",
      "[11994,     1] loss: 0.0093946867\n",
      "[11995,     1] loss: 0.0093946524\n",
      "[11996,     1] loss: 0.0093946159\n",
      "[11997,     1] loss: 0.0093945809\n",
      "[11998,     1] loss: 0.0093945451\n",
      "[11999,     1] loss: 0.0093945093\n",
      "[12000,     1] loss: 0.0093944743\n",
      "[12001,     1] loss: 0.0093944401\n",
      "[12002,     1] loss: 0.0093944035\n",
      "[12003,     1] loss: 0.0093943685\n",
      "[12004,     1] loss: 0.0093943313\n",
      "[12005,     1] loss: 0.0093942970\n",
      "[12006,     1] loss: 0.0093942612\n",
      "[12007,     1] loss: 0.0093942270\n",
      "[12008,     1] loss: 0.0093941905\n",
      "[12009,     1] loss: 0.0093941554\n",
      "[12010,     1] loss: 0.0093941189\n",
      "[12011,     1] loss: 0.0093940839\n",
      "[12012,     1] loss: 0.0093940482\n",
      "[12013,     1] loss: 0.0093940139\n",
      "[12014,     1] loss: 0.0093939781\n",
      "[12015,     1] loss: 0.0093939438\n",
      "[12016,     1] loss: 0.0093939066\n",
      "[12017,     1] loss: 0.0093938723\n",
      "[12018,     1] loss: 0.0093938380\n",
      "[12019,     1] loss: 0.0093938015\n",
      "[12020,     1] loss: 0.0093937665\n",
      "[12021,     1] loss: 0.0093937300\n",
      "[12022,     1] loss: 0.0093936972\n",
      "[12023,     1] loss: 0.0093936600\n",
      "[12024,     1] loss: 0.0093936250\n",
      "[12025,     1] loss: 0.0093935885\n",
      "[12026,     1] loss: 0.0093935542\n",
      "[12027,     1] loss: 0.0093935177\n",
      "[12028,     1] loss: 0.0093934819\n",
      "[12029,     1] loss: 0.0093934469\n",
      "[12030,     1] loss: 0.0093934126\n",
      "[12031,     1] loss: 0.0093933761\n",
      "[12032,     1] loss: 0.0093933411\n",
      "[12033,     1] loss: 0.0093933053\n",
      "[12034,     1] loss: 0.0093932696\n",
      "[12035,     1] loss: 0.0093932346\n",
      "[12036,     1] loss: 0.0093931995\n",
      "[12037,     1] loss: 0.0093931615\n",
      "[12038,     1] loss: 0.0093931273\n",
      "[12039,     1] loss: 0.0093930922\n",
      "[12040,     1] loss: 0.0093930572\n",
      "[12041,     1] loss: 0.0093930222\n",
      "[12042,     1] loss: 0.0093929872\n",
      "[12043,     1] loss: 0.0093929499\n",
      "[12044,     1] loss: 0.0093929164\n",
      "[12045,     1] loss: 0.0093928806\n",
      "[12046,     1] loss: 0.0093928449\n",
      "[12047,     1] loss: 0.0093928091\n",
      "[12048,     1] loss: 0.0093927749\n",
      "[12049,     1] loss: 0.0093927376\n",
      "[12050,     1] loss: 0.0093927026\n",
      "[12051,     1] loss: 0.0093926676\n",
      "[12052,     1] loss: 0.0093926325\n",
      "[12053,     1] loss: 0.0093925975\n",
      "[12054,     1] loss: 0.0093925618\n",
      "[12055,     1] loss: 0.0093925260\n",
      "[12056,     1] loss: 0.0093924910\n",
      "[12057,     1] loss: 0.0093924545\n",
      "[12058,     1] loss: 0.0093924195\n",
      "[12059,     1] loss: 0.0093923837\n",
      "[12060,     1] loss: 0.0093923487\n",
      "[12061,     1] loss: 0.0093923122\n",
      "[12062,     1] loss: 0.0093922764\n",
      "[12063,     1] loss: 0.0093922429\n",
      "[12064,     1] loss: 0.0093922071\n",
      "[12065,     1] loss: 0.0093921714\n",
      "[12066,     1] loss: 0.0093921371\n",
      "[12067,     1] loss: 0.0093921013\n",
      "[12068,     1] loss: 0.0093920648\n",
      "[12069,     1] loss: 0.0093920298\n",
      "[12070,     1] loss: 0.0093919955\n",
      "[12071,     1] loss: 0.0093919590\n",
      "[12072,     1] loss: 0.0093919240\n",
      "[12073,     1] loss: 0.0093918905\n",
      "[12074,     1] loss: 0.0093918547\n",
      "[12075,     1] loss: 0.0093918175\n",
      "[12076,     1] loss: 0.0093917824\n",
      "[12077,     1] loss: 0.0093917489\n",
      "[12078,     1] loss: 0.0093917117\n",
      "[12079,     1] loss: 0.0093916774\n",
      "[12080,     1] loss: 0.0093916424\n",
      "[12081,     1] loss: 0.0093916059\n",
      "[12082,     1] loss: 0.0093915701\n",
      "[12083,     1] loss: 0.0093915358\n",
      "[12084,     1] loss: 0.0093915008\n",
      "[12085,     1] loss: 0.0093914658\n",
      "[12086,     1] loss: 0.0093914300\n",
      "[12087,     1] loss: 0.0093913957\n",
      "[12088,     1] loss: 0.0093913600\n",
      "[12089,     1] loss: 0.0093913227\n",
      "[12090,     1] loss: 0.0093912877\n",
      "[12091,     1] loss: 0.0093912527\n",
      "[12092,     1] loss: 0.0093912169\n",
      "[12093,     1] loss: 0.0093911819\n",
      "[12094,     1] loss: 0.0093911462\n",
      "[12095,     1] loss: 0.0093911104\n",
      "[12096,     1] loss: 0.0093910754\n",
      "[12097,     1] loss: 0.0093910411\n",
      "[12098,     1] loss: 0.0093910053\n",
      "[12099,     1] loss: 0.0093909688\n",
      "[12100,     1] loss: 0.0093909331\n",
      "[12101,     1] loss: 0.0093908980\n",
      "[12102,     1] loss: 0.0093908638\n",
      "[12103,     1] loss: 0.0093908280\n",
      "[12104,     1] loss: 0.0093907915\n",
      "[12105,     1] loss: 0.0093907565\n",
      "[12106,     1] loss: 0.0093907215\n",
      "[12107,     1] loss: 0.0093906842\n",
      "[12108,     1] loss: 0.0093906492\n",
      "[12109,     1] loss: 0.0093906157\n",
      "[12110,     1] loss: 0.0093905799\n",
      "[12111,     1] loss: 0.0093905441\n",
      "[12112,     1] loss: 0.0093905084\n",
      "[12113,     1] loss: 0.0093904741\n",
      "[12114,     1] loss: 0.0093904376\n",
      "[12115,     1] loss: 0.0093904026\n",
      "[12116,     1] loss: 0.0093903668\n",
      "[12117,     1] loss: 0.0093903325\n",
      "[12118,     1] loss: 0.0093902968\n",
      "[12119,     1] loss: 0.0093902618\n",
      "[12120,     1] loss: 0.0093902260\n",
      "[12121,     1] loss: 0.0093901917\n",
      "[12122,     1] loss: 0.0093901552\n",
      "[12123,     1] loss: 0.0093901195\n",
      "[12124,     1] loss: 0.0093900852\n",
      "[12125,     1] loss: 0.0093900487\n",
      "[12126,     1] loss: 0.0093900137\n",
      "[12127,     1] loss: 0.0093899794\n",
      "[12128,     1] loss: 0.0093899444\n",
      "[12129,     1] loss: 0.0093899086\n",
      "[12130,     1] loss: 0.0093898743\n",
      "[12131,     1] loss: 0.0093898363\n",
      "[12132,     1] loss: 0.0093898013\n",
      "[12133,     1] loss: 0.0093897671\n",
      "[12134,     1] loss: 0.0093897313\n",
      "[12135,     1] loss: 0.0093896955\n",
      "[12136,     1] loss: 0.0093896598\n",
      "[12137,     1] loss: 0.0093896270\n",
      "[12138,     1] loss: 0.0093895897\n",
      "[12139,     1] loss: 0.0093895547\n",
      "[12140,     1] loss: 0.0093895204\n",
      "[12141,     1] loss: 0.0093894832\n",
      "[12142,     1] loss: 0.0093894504\n",
      "[12143,     1] loss: 0.0093894131\n",
      "[12144,     1] loss: 0.0093893766\n",
      "[12145,     1] loss: 0.0093893416\n",
      "[12146,     1] loss: 0.0093893081\n",
      "[12147,     1] loss: 0.0093892716\n",
      "[12148,     1] loss: 0.0093892381\n",
      "[12149,     1] loss: 0.0093892008\n",
      "[12150,     1] loss: 0.0093891658\n",
      "[12151,     1] loss: 0.0093891315\n",
      "[12152,     1] loss: 0.0093890965\n",
      "[12153,     1] loss: 0.0093890600\n",
      "[12154,     1] loss: 0.0093890265\n",
      "[12155,     1] loss: 0.0093889900\n",
      "[12156,     1] loss: 0.0093889542\n",
      "[12157,     1] loss: 0.0093889199\n",
      "[12158,     1] loss: 0.0093888842\n",
      "[12159,     1] loss: 0.0093888491\n",
      "[12160,     1] loss: 0.0093888134\n",
      "[12161,     1] loss: 0.0093887791\n",
      "[12162,     1] loss: 0.0093887426\n",
      "[12163,     1] loss: 0.0093887083\n",
      "[12164,     1] loss: 0.0093886718\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12165,     1] loss: 0.0093886368\n",
      "[12166,     1] loss: 0.0093886010\n",
      "[12167,     1] loss: 0.0093885660\n",
      "[12168,     1] loss: 0.0093885317\n",
      "[12169,     1] loss: 0.0093884967\n",
      "[12170,     1] loss: 0.0093884617\n",
      "[12171,     1] loss: 0.0093884252\n",
      "[12172,     1] loss: 0.0093883917\n",
      "[12173,     1] loss: 0.0093883567\n",
      "[12174,     1] loss: 0.0093883209\n",
      "[12175,     1] loss: 0.0093882851\n",
      "[12176,     1] loss: 0.0093882501\n",
      "[12177,     1] loss: 0.0093882158\n",
      "[12178,     1] loss: 0.0093881801\n",
      "[12179,     1] loss: 0.0093881458\n",
      "[12180,     1] loss: 0.0093881093\n",
      "[12181,     1] loss: 0.0093880743\n",
      "[12182,     1] loss: 0.0093880408\n",
      "[12183,     1] loss: 0.0093880035\n",
      "[12184,     1] loss: 0.0093879677\n",
      "[12185,     1] loss: 0.0093879342\n",
      "[12186,     1] loss: 0.0093878992\n",
      "[12187,     1] loss: 0.0093878634\n",
      "[12188,     1] loss: 0.0093878284\n",
      "[12189,     1] loss: 0.0093877926\n",
      "[12190,     1] loss: 0.0093877584\n",
      "[12191,     1] loss: 0.0093877226\n",
      "[12192,     1] loss: 0.0093876868\n",
      "[12193,     1] loss: 0.0093876511\n",
      "[12194,     1] loss: 0.0093876153\n",
      "[12195,     1] loss: 0.0093875818\n",
      "[12196,     1] loss: 0.0093875468\n",
      "[12197,     1] loss: 0.0093875095\n",
      "[12198,     1] loss: 0.0093874738\n",
      "[12199,     1] loss: 0.0093874402\n",
      "[12200,     1] loss: 0.0093874045\n",
      "[12201,     1] loss: 0.0093873695\n",
      "[12202,     1] loss: 0.0093873344\n",
      "[12203,     1] loss: 0.0093872987\n",
      "[12204,     1] loss: 0.0093872637\n",
      "[12205,     1] loss: 0.0093872271\n",
      "[12206,     1] loss: 0.0093871921\n",
      "[12207,     1] loss: 0.0093871571\n",
      "[12208,     1] loss: 0.0093871228\n",
      "[12209,     1] loss: 0.0093870871\n",
      "[12210,     1] loss: 0.0093870528\n",
      "[12211,     1] loss: 0.0093870178\n",
      "[12212,     1] loss: 0.0093869820\n",
      "[12213,     1] loss: 0.0093869478\n",
      "[12214,     1] loss: 0.0093869120\n",
      "[12215,     1] loss: 0.0093868770\n",
      "[12216,     1] loss: 0.0093868427\n",
      "[12217,     1] loss: 0.0093868062\n",
      "[12218,     1] loss: 0.0093867712\n",
      "[12219,     1] loss: 0.0093867354\n",
      "[12220,     1] loss: 0.0093867019\n",
      "[12221,     1] loss: 0.0093866646\n",
      "[12222,     1] loss: 0.0093866304\n",
      "[12223,     1] loss: 0.0093865946\n",
      "[12224,     1] loss: 0.0093865603\n",
      "[12225,     1] loss: 0.0093865253\n",
      "[12226,     1] loss: 0.0093864910\n",
      "[12227,     1] loss: 0.0093864553\n",
      "[12228,     1] loss: 0.0093864210\n",
      "[12229,     1] loss: 0.0093863845\n",
      "[12230,     1] loss: 0.0093863502\n",
      "[12231,     1] loss: 0.0093863152\n",
      "[12232,     1] loss: 0.0093862787\n",
      "[12233,     1] loss: 0.0093862459\n",
      "[12234,     1] loss: 0.0093862124\n",
      "[12235,     1] loss: 0.0093861744\n",
      "[12236,     1] loss: 0.0093861386\n",
      "[12237,     1] loss: 0.0093861058\n",
      "[12238,     1] loss: 0.0093860693\n",
      "[12239,     1] loss: 0.0093860343\n",
      "[12240,     1] loss: 0.0093860008\n",
      "[12241,     1] loss: 0.0093859650\n",
      "[12242,     1] loss: 0.0093859307\n",
      "[12243,     1] loss: 0.0093858957\n",
      "[12244,     1] loss: 0.0093858592\n",
      "[12245,     1] loss: 0.0093858242\n",
      "[12246,     1] loss: 0.0093857899\n",
      "[12247,     1] loss: 0.0093857542\n",
      "[12248,     1] loss: 0.0093857184\n",
      "[12249,     1] loss: 0.0093856841\n",
      "[12250,     1] loss: 0.0093856491\n",
      "[12251,     1] loss: 0.0093856141\n",
      "[12252,     1] loss: 0.0093855791\n",
      "[12253,     1] loss: 0.0093855441\n",
      "[12254,     1] loss: 0.0093855090\n",
      "[12255,     1] loss: 0.0093854733\n",
      "[12256,     1] loss: 0.0093854368\n",
      "[12257,     1] loss: 0.0093854040\n",
      "[12258,     1] loss: 0.0093853690\n",
      "[12259,     1] loss: 0.0093853332\n",
      "[12260,     1] loss: 0.0093852982\n",
      "[12261,     1] loss: 0.0093852632\n",
      "[12262,     1] loss: 0.0093852282\n",
      "[12263,     1] loss: 0.0093851931\n",
      "[12264,     1] loss: 0.0093851589\n",
      "[12265,     1] loss: 0.0093851246\n",
      "[12266,     1] loss: 0.0093850888\n",
      "[12267,     1] loss: 0.0093850531\n",
      "[12268,     1] loss: 0.0093850195\n",
      "[12269,     1] loss: 0.0093849845\n",
      "[12270,     1] loss: 0.0093849480\n",
      "[12271,     1] loss: 0.0093849130\n",
      "[12272,     1] loss: 0.0093848795\n",
      "[12273,     1] loss: 0.0093848445\n",
      "[12274,     1] loss: 0.0093848079\n",
      "[12275,     1] loss: 0.0093847729\n",
      "[12276,     1] loss: 0.0093847401\n",
      "[12277,     1] loss: 0.0093847044\n",
      "[12278,     1] loss: 0.0093846686\n",
      "[12279,     1] loss: 0.0093846343\n",
      "[12280,     1] loss: 0.0093845993\n",
      "[12281,     1] loss: 0.0093845628\n",
      "[12282,     1] loss: 0.0093845285\n",
      "[12283,     1] loss: 0.0093844935\n",
      "[12284,     1] loss: 0.0093844585\n",
      "[12285,     1] loss: 0.0093844235\n",
      "[12286,     1] loss: 0.0093843877\n",
      "[12287,     1] loss: 0.0093843527\n",
      "[12288,     1] loss: 0.0093843184\n",
      "[12289,     1] loss: 0.0093842834\n",
      "[12290,     1] loss: 0.0093842484\n",
      "[12291,     1] loss: 0.0093842126\n",
      "[12292,     1] loss: 0.0093841784\n",
      "[12293,     1] loss: 0.0093841419\n",
      "[12294,     1] loss: 0.0093841083\n",
      "[12295,     1] loss: 0.0093840733\n",
      "[12296,     1] loss: 0.0093840390\n",
      "[12297,     1] loss: 0.0093840025\n",
      "[12298,     1] loss: 0.0093839698\n",
      "[12299,     1] loss: 0.0093839340\n",
      "[12300,     1] loss: 0.0093838982\n",
      "[12301,     1] loss: 0.0093838632\n",
      "[12302,     1] loss: 0.0093838289\n",
      "[12303,     1] loss: 0.0093837939\n",
      "[12304,     1] loss: 0.0093837589\n",
      "[12305,     1] loss: 0.0093837239\n",
      "[12306,     1] loss: 0.0093836889\n",
      "[12307,     1] loss: 0.0093836531\n",
      "[12308,     1] loss: 0.0093836196\n",
      "[12309,     1] loss: 0.0093835846\n",
      "[12310,     1] loss: 0.0093835503\n",
      "[12311,     1] loss: 0.0093835145\n",
      "[12312,     1] loss: 0.0093834803\n",
      "[12313,     1] loss: 0.0093834445\n",
      "[12314,     1] loss: 0.0093834102\n",
      "[12315,     1] loss: 0.0093833745\n",
      "[12316,     1] loss: 0.0093833402\n",
      "[12317,     1] loss: 0.0093833044\n",
      "[12318,     1] loss: 0.0093832701\n",
      "[12319,     1] loss: 0.0093832359\n",
      "[12320,     1] loss: 0.0093832016\n",
      "[12321,     1] loss: 0.0093831658\n",
      "[12322,     1] loss: 0.0093831308\n",
      "[12323,     1] loss: 0.0093830958\n",
      "[12324,     1] loss: 0.0093830615\n",
      "[12325,     1] loss: 0.0093830273\n",
      "[12326,     1] loss: 0.0093829930\n",
      "[12327,     1] loss: 0.0093829587\n",
      "[12328,     1] loss: 0.0093829229\n",
      "[12329,     1] loss: 0.0093828879\n",
      "[12330,     1] loss: 0.0093828529\n",
      "[12331,     1] loss: 0.0093828194\n",
      "[12332,     1] loss: 0.0093827836\n",
      "[12333,     1] loss: 0.0093827486\n",
      "[12334,     1] loss: 0.0093827136\n",
      "[12335,     1] loss: 0.0093826801\n",
      "[12336,     1] loss: 0.0093826443\n",
      "[12337,     1] loss: 0.0093826100\n",
      "[12338,     1] loss: 0.0093825758\n",
      "[12339,     1] loss: 0.0093825407\n",
      "[12340,     1] loss: 0.0093825050\n",
      "[12341,     1] loss: 0.0093824714\n",
      "[12342,     1] loss: 0.0093824364\n",
      "[12343,     1] loss: 0.0093824014\n",
      "[12344,     1] loss: 0.0093823656\n",
      "[12345,     1] loss: 0.0093823314\n",
      "[12346,     1] loss: 0.0093822978\n",
      "[12347,     1] loss: 0.0093822621\n",
      "[12348,     1] loss: 0.0093822278\n",
      "[12349,     1] loss: 0.0093821935\n",
      "[12350,     1] loss: 0.0093821593\n",
      "[12351,     1] loss: 0.0093821235\n",
      "[12352,     1] loss: 0.0093820885\n",
      "[12353,     1] loss: 0.0093820535\n",
      "[12354,     1] loss: 0.0093820177\n",
      "[12355,     1] loss: 0.0093819834\n",
      "[12356,     1] loss: 0.0093819499\n",
      "[12357,     1] loss: 0.0093819141\n",
      "[12358,     1] loss: 0.0093818791\n",
      "[12359,     1] loss: 0.0093818456\n",
      "[12360,     1] loss: 0.0093818098\n",
      "[12361,     1] loss: 0.0093817763\n",
      "[12362,     1] loss: 0.0093817405\n",
      "[12363,     1] loss: 0.0093817063\n",
      "[12364,     1] loss: 0.0093816712\n",
      "[12365,     1] loss: 0.0093816370\n",
      "[12366,     1] loss: 0.0093816020\n",
      "[12367,     1] loss: 0.0093815677\n",
      "[12368,     1] loss: 0.0093815334\n",
      "[12369,     1] loss: 0.0093814991\n",
      "[12370,     1] loss: 0.0093814641\n",
      "[12371,     1] loss: 0.0093814291\n",
      "[12372,     1] loss: 0.0093813941\n",
      "[12373,     1] loss: 0.0093813606\n",
      "[12374,     1] loss: 0.0093813241\n",
      "[12375,     1] loss: 0.0093812905\n",
      "[12376,     1] loss: 0.0093812548\n",
      "[12377,     1] loss: 0.0093812205\n",
      "[12378,     1] loss: 0.0093811862\n",
      "[12379,     1] loss: 0.0093811512\n",
      "[12380,     1] loss: 0.0093811169\n",
      "[12381,     1] loss: 0.0093810827\n",
      "[12382,     1] loss: 0.0093810484\n",
      "[12383,     1] loss: 0.0093810149\n",
      "[12384,     1] loss: 0.0093809791\n",
      "[12385,     1] loss: 0.0093809448\n",
      "[12386,     1] loss: 0.0093809091\n",
      "[12387,     1] loss: 0.0093808763\n",
      "[12388,     1] loss: 0.0093808413\n",
      "[12389,     1] loss: 0.0093808062\n",
      "[12390,     1] loss: 0.0093807720\n",
      "[12391,     1] loss: 0.0093807369\n",
      "[12392,     1] loss: 0.0093807027\n",
      "[12393,     1] loss: 0.0093806677\n",
      "[12394,     1] loss: 0.0093806334\n",
      "[12395,     1] loss: 0.0093805991\n",
      "[12396,     1] loss: 0.0093805648\n",
      "[12397,     1] loss: 0.0093805291\n",
      "[12398,     1] loss: 0.0093804948\n",
      "[12399,     1] loss: 0.0093804605\n",
      "[12400,     1] loss: 0.0093804255\n",
      "[12401,     1] loss: 0.0093803912\n",
      "[12402,     1] loss: 0.0093803547\n",
      "[12403,     1] loss: 0.0093803227\n",
      "[12404,     1] loss: 0.0093802877\n",
      "[12405,     1] loss: 0.0093802527\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12406,     1] loss: 0.0093802191\n",
      "[12407,     1] loss: 0.0093801834\n",
      "[12408,     1] loss: 0.0093801498\n",
      "[12409,     1] loss: 0.0093801156\n",
      "[12410,     1] loss: 0.0093800813\n",
      "[12411,     1] loss: 0.0093800470\n",
      "[12412,     1] loss: 0.0093800113\n",
      "[12413,     1] loss: 0.0093799777\n",
      "[12414,     1] loss: 0.0093799435\n",
      "[12415,     1] loss: 0.0093799092\n",
      "[12416,     1] loss: 0.0093798764\n",
      "[12417,     1] loss: 0.0093798406\n",
      "[12418,     1] loss: 0.0093798064\n",
      "[12419,     1] loss: 0.0093797714\n",
      "[12420,     1] loss: 0.0093797378\n",
      "[12421,     1] loss: 0.0093797028\n",
      "[12422,     1] loss: 0.0093796685\n",
      "[12423,     1] loss: 0.0093796335\n",
      "[12424,     1] loss: 0.0093796000\n",
      "[12425,     1] loss: 0.0093795650\n",
      "[12426,     1] loss: 0.0093795300\n",
      "[12427,     1] loss: 0.0093794972\n",
      "[12428,     1] loss: 0.0093794614\n",
      "[12429,     1] loss: 0.0093794271\n",
      "[12430,     1] loss: 0.0093793929\n",
      "[12431,     1] loss: 0.0093793586\n",
      "[12432,     1] loss: 0.0093793243\n",
      "[12433,     1] loss: 0.0093792893\n",
      "[12434,     1] loss: 0.0093792550\n",
      "[12435,     1] loss: 0.0093792200\n",
      "[12436,     1] loss: 0.0093791850\n",
      "[12437,     1] loss: 0.0093791530\n",
      "[12438,     1] loss: 0.0093791179\n",
      "[12439,     1] loss: 0.0093790829\n",
      "[12440,     1] loss: 0.0093790486\n",
      "[12441,     1] loss: 0.0093790144\n",
      "[12442,     1] loss: 0.0093789816\n",
      "[12443,     1] loss: 0.0093789458\n",
      "[12444,     1] loss: 0.0093789108\n",
      "[12445,     1] loss: 0.0093788773\n",
      "[12446,     1] loss: 0.0093788430\n",
      "[12447,     1] loss: 0.0093788080\n",
      "[12448,     1] loss: 0.0093787737\n",
      "[12449,     1] loss: 0.0093787387\n",
      "[12450,     1] loss: 0.0093787044\n",
      "[12451,     1] loss: 0.0093786694\n",
      "[12452,     1] loss: 0.0093786366\n",
      "[12453,     1] loss: 0.0093786009\n",
      "[12454,     1] loss: 0.0093785666\n",
      "[12455,     1] loss: 0.0093785346\n",
      "[12456,     1] loss: 0.0093785003\n",
      "[12457,     1] loss: 0.0093784653\n",
      "[12458,     1] loss: 0.0093784310\n",
      "[12459,     1] loss: 0.0093783960\n",
      "[12460,     1] loss: 0.0093783617\n",
      "[12461,     1] loss: 0.0093783282\n",
      "[12462,     1] loss: 0.0093782924\n",
      "[12463,     1] loss: 0.0093782581\n",
      "[12464,     1] loss: 0.0093782246\n",
      "[12465,     1] loss: 0.0093781903\n",
      "[12466,     1] loss: 0.0093781576\n",
      "[12467,     1] loss: 0.0093781218\n",
      "[12468,     1] loss: 0.0093780868\n",
      "[12469,     1] loss: 0.0093780532\n",
      "[12470,     1] loss: 0.0093780182\n",
      "[12471,     1] loss: 0.0093779847\n",
      "[12472,     1] loss: 0.0093779519\n",
      "[12473,     1] loss: 0.0093779169\n",
      "[12474,     1] loss: 0.0093778826\n",
      "[12475,     1] loss: 0.0093778498\n",
      "[12476,     1] loss: 0.0093778133\n",
      "[12477,     1] loss: 0.0093777798\n",
      "[12478,     1] loss: 0.0093777463\n",
      "[12479,     1] loss: 0.0093777113\n",
      "[12480,     1] loss: 0.0093776792\n",
      "[12481,     1] loss: 0.0093776442\n",
      "[12482,     1] loss: 0.0093776107\n",
      "[12483,     1] loss: 0.0093775764\n",
      "[12484,     1] loss: 0.0093775414\n",
      "[12485,     1] loss: 0.0093775079\n",
      "[12486,     1] loss: 0.0093774728\n",
      "[12487,     1] loss: 0.0093774386\n",
      "[12488,     1] loss: 0.0093774050\n",
      "[12489,     1] loss: 0.0093773700\n",
      "[12490,     1] loss: 0.0093773365\n",
      "[12491,     1] loss: 0.0093773000\n",
      "[12492,     1] loss: 0.0093772680\n",
      "[12493,     1] loss: 0.0093772344\n",
      "[12494,     1] loss: 0.0093771987\n",
      "[12495,     1] loss: 0.0093771659\n",
      "[12496,     1] loss: 0.0093771324\n",
      "[12497,     1] loss: 0.0093770958\n",
      "[12498,     1] loss: 0.0093770623\n",
      "[12499,     1] loss: 0.0093770303\n",
      "[12500,     1] loss: 0.0093769945\n",
      "[12501,     1] loss: 0.0093769617\n",
      "[12502,     1] loss: 0.0093769267\n",
      "[12503,     1] loss: 0.0093768924\n",
      "[12504,     1] loss: 0.0093768589\n",
      "[12505,     1] loss: 0.0093768254\n",
      "[12506,     1] loss: 0.0093767904\n",
      "[12507,     1] loss: 0.0093767576\n",
      "[12508,     1] loss: 0.0093767233\n",
      "[12509,     1] loss: 0.0093766890\n",
      "[12510,     1] loss: 0.0093766563\n",
      "[12511,     1] loss: 0.0093766205\n",
      "[12512,     1] loss: 0.0093765870\n",
      "[12513,     1] loss: 0.0093765534\n",
      "[12514,     1] loss: 0.0093765207\n",
      "[12515,     1] loss: 0.0093764856\n",
      "[12516,     1] loss: 0.0093764529\n",
      "[12517,     1] loss: 0.0093764178\n",
      "[12518,     1] loss: 0.0093763836\n",
      "[12519,     1] loss: 0.0093763500\n",
      "[12520,     1] loss: 0.0093763150\n",
      "[12521,     1] loss: 0.0093762822\n",
      "[12522,     1] loss: 0.0093762495\n",
      "[12523,     1] loss: 0.0093762152\n",
      "[12524,     1] loss: 0.0093761809\n",
      "[12525,     1] loss: 0.0093761459\n",
      "[12526,     1] loss: 0.0093761116\n",
      "[12527,     1] loss: 0.0093760796\n",
      "[12528,     1] loss: 0.0093760446\n",
      "[12529,     1] loss: 0.0093760110\n",
      "[12530,     1] loss: 0.0093759775\n",
      "[12531,     1] loss: 0.0093759425\n",
      "[12532,     1] loss: 0.0093759082\n",
      "[12533,     1] loss: 0.0093758747\n",
      "[12534,     1] loss: 0.0093758412\n",
      "[12535,     1] loss: 0.0093758084\n",
      "[12536,     1] loss: 0.0093757719\n",
      "[12537,     1] loss: 0.0093757398\n",
      "[12538,     1] loss: 0.0093757056\n",
      "[12539,     1] loss: 0.0093756713\n",
      "[12540,     1] loss: 0.0093756378\n",
      "[12541,     1] loss: 0.0093756042\n",
      "[12542,     1] loss: 0.0093755700\n",
      "[12543,     1] loss: 0.0093755379\n",
      "[12544,     1] loss: 0.0093755022\n",
      "[12545,     1] loss: 0.0093754679\n",
      "[12546,     1] loss: 0.0093754351\n",
      "[12547,     1] loss: 0.0093754008\n",
      "[12548,     1] loss: 0.0093753673\n",
      "[12549,     1] loss: 0.0093753338\n",
      "[12550,     1] loss: 0.0093752995\n",
      "[12551,     1] loss: 0.0093752660\n",
      "[12552,     1] loss: 0.0093752332\n",
      "[12553,     1] loss: 0.0093751997\n",
      "[12554,     1] loss: 0.0093751654\n",
      "[12555,     1] loss: 0.0093751319\n",
      "[12556,     1] loss: 0.0093750983\n",
      "[12557,     1] loss: 0.0093750641\n",
      "[12558,     1] loss: 0.0093750305\n",
      "[12559,     1] loss: 0.0093749978\n",
      "[12560,     1] loss: 0.0093749642\n",
      "[12561,     1] loss: 0.0093749307\n",
      "[12562,     1] loss: 0.0093748972\n",
      "[12563,     1] loss: 0.0093748622\n",
      "[12564,     1] loss: 0.0093748279\n",
      "[12565,     1] loss: 0.0093747944\n",
      "[12566,     1] loss: 0.0093747631\n",
      "[12567,     1] loss: 0.0093747266\n",
      "[12568,     1] loss: 0.0093746930\n",
      "[12569,     1] loss: 0.0093746595\n",
      "[12570,     1] loss: 0.0093746260\n",
      "[12571,     1] loss: 0.0093745917\n",
      "[12572,     1] loss: 0.0093745582\n",
      "[12573,     1] loss: 0.0093745254\n",
      "[12574,     1] loss: 0.0093744904\n",
      "[12575,     1] loss: 0.0093744569\n",
      "[12576,     1] loss: 0.0093744241\n",
      "[12577,     1] loss: 0.0093743905\n",
      "[12578,     1] loss: 0.0093743563\n",
      "[12579,     1] loss: 0.0093743227\n",
      "[12580,     1] loss: 0.0093742885\n",
      "[12581,     1] loss: 0.0093742564\n",
      "[12582,     1] loss: 0.0093742222\n",
      "[12583,     1] loss: 0.0093741886\n",
      "[12584,     1] loss: 0.0093741551\n",
      "[12585,     1] loss: 0.0093741216\n",
      "[12586,     1] loss: 0.0093740873\n",
      "[12587,     1] loss: 0.0093740538\n",
      "[12588,     1] loss: 0.0093740195\n",
      "[12589,     1] loss: 0.0093739875\n",
      "[12590,     1] loss: 0.0093739532\n",
      "[12591,     1] loss: 0.0093739189\n",
      "[12592,     1] loss: 0.0093738861\n",
      "[12593,     1] loss: 0.0093738511\n",
      "[12594,     1] loss: 0.0093738198\n",
      "[12595,     1] loss: 0.0093737856\n",
      "[12596,     1] loss: 0.0093737505\n",
      "[12597,     1] loss: 0.0093737185\n",
      "[12598,     1] loss: 0.0093736865\n",
      "[12599,     1] loss: 0.0093736522\n",
      "[12600,     1] loss: 0.0093736194\n",
      "[12601,     1] loss: 0.0093735859\n",
      "[12602,     1] loss: 0.0093735509\n",
      "[12603,     1] loss: 0.0093735173\n",
      "[12604,     1] loss: 0.0093734846\n",
      "[12605,     1] loss: 0.0093734510\n",
      "[12606,     1] loss: 0.0093734175\n",
      "[12607,     1] loss: 0.0093733840\n",
      "[12608,     1] loss: 0.0093733504\n",
      "[12609,     1] loss: 0.0093733162\n",
      "[12610,     1] loss: 0.0093732834\n",
      "[12611,     1] loss: 0.0093732506\n",
      "[12612,     1] loss: 0.0093732171\n",
      "[12613,     1] loss: 0.0093731828\n",
      "[12614,     1] loss: 0.0093731515\n",
      "[12615,     1] loss: 0.0093731165\n",
      "[12616,     1] loss: 0.0093730822\n",
      "[12617,     1] loss: 0.0093730494\n",
      "[12618,     1] loss: 0.0093730167\n",
      "[12619,     1] loss: 0.0093729824\n",
      "[12620,     1] loss: 0.0093729496\n",
      "[12621,     1] loss: 0.0093729153\n",
      "[12622,     1] loss: 0.0093728818\n",
      "[12623,     1] loss: 0.0093728483\n",
      "[12624,     1] loss: 0.0093728162\n",
      "[12625,     1] loss: 0.0093727805\n",
      "[12626,     1] loss: 0.0093727492\n",
      "[12627,     1] loss: 0.0093727149\n",
      "[12628,     1] loss: 0.0093726821\n",
      "[12629,     1] loss: 0.0093726486\n",
      "[12630,     1] loss: 0.0093726158\n",
      "[12631,     1] loss: 0.0093725830\n",
      "[12632,     1] loss: 0.0093725488\n",
      "[12633,     1] loss: 0.0093725152\n",
      "[12634,     1] loss: 0.0093724824\n",
      "[12635,     1] loss: 0.0093724489\n",
      "[12636,     1] loss: 0.0093724154\n",
      "[12637,     1] loss: 0.0093723834\n",
      "[12638,     1] loss: 0.0093723483\n",
      "[12639,     1] loss: 0.0093723163\n",
      "[12640,     1] loss: 0.0093722820\n",
      "[12641,     1] loss: 0.0093722492\n",
      "[12642,     1] loss: 0.0093722157\n",
      "[12643,     1] loss: 0.0093721837\n",
      "[12644,     1] loss: 0.0093721494\n",
      "[12645,     1] loss: 0.0093721174\n",
      "[12646,     1] loss: 0.0093720831\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12647,     1] loss: 0.0093720496\n",
      "[12648,     1] loss: 0.0093720168\n",
      "[12649,     1] loss: 0.0093719840\n",
      "[12650,     1] loss: 0.0093719505\n",
      "[12651,     1] loss: 0.0093719155\n",
      "[12652,     1] loss: 0.0093718834\n",
      "[12653,     1] loss: 0.0093718506\n",
      "[12654,     1] loss: 0.0093718179\n",
      "[12655,     1] loss: 0.0093717851\n",
      "[12656,     1] loss: 0.0093717523\n",
      "[12657,     1] loss: 0.0093717173\n",
      "[12658,     1] loss: 0.0093716852\n",
      "[12659,     1] loss: 0.0093716510\n",
      "[12660,     1] loss: 0.0093716182\n",
      "[12661,     1] loss: 0.0093715847\n",
      "[12662,     1] loss: 0.0093715511\n",
      "[12663,     1] loss: 0.0093715191\n",
      "[12664,     1] loss: 0.0093714856\n",
      "[12665,     1] loss: 0.0093714513\n",
      "[12666,     1] loss: 0.0093714185\n",
      "[12667,     1] loss: 0.0093713850\n",
      "[12668,     1] loss: 0.0093713515\n",
      "[12669,     1] loss: 0.0093713172\n",
      "[12670,     1] loss: 0.0093712844\n",
      "[12671,     1] loss: 0.0093712524\n",
      "[12672,     1] loss: 0.0093712203\n",
      "[12673,     1] loss: 0.0093711838\n",
      "[12674,     1] loss: 0.0093711525\n",
      "[12675,     1] loss: 0.0093711190\n",
      "[12676,     1] loss: 0.0093710862\n",
      "[12677,     1] loss: 0.0093710534\n",
      "[12678,     1] loss: 0.0093710221\n",
      "[12679,     1] loss: 0.0093709879\n",
      "[12680,     1] loss: 0.0093709543\n",
      "[12681,     1] loss: 0.0093709208\n",
      "[12682,     1] loss: 0.0093708873\n",
      "[12683,     1] loss: 0.0093708552\n",
      "[12684,     1] loss: 0.0093708210\n",
      "[12685,     1] loss: 0.0093707889\n",
      "[12686,     1] loss: 0.0093707554\n",
      "[12687,     1] loss: 0.0093707234\n",
      "[12688,     1] loss: 0.0093706891\n",
      "[12689,     1] loss: 0.0093706571\n",
      "[12690,     1] loss: 0.0093706235\n",
      "[12691,     1] loss: 0.0093705900\n",
      "[12692,     1] loss: 0.0093705572\n",
      "[12693,     1] loss: 0.0093705244\n",
      "[12694,     1] loss: 0.0093704924\n",
      "[12695,     1] loss: 0.0093704589\n",
      "[12696,     1] loss: 0.0093704261\n",
      "[12697,     1] loss: 0.0093703926\n",
      "[12698,     1] loss: 0.0093703598\n",
      "[12699,     1] loss: 0.0093703263\n",
      "[12700,     1] loss: 0.0093702942\n",
      "[12701,     1] loss: 0.0093702599\n",
      "[12702,     1] loss: 0.0093702286\n",
      "[12703,     1] loss: 0.0093701951\n",
      "[12704,     1] loss: 0.0093701616\n",
      "[12705,     1] loss: 0.0093701288\n",
      "[12706,     1] loss: 0.0093700975\n",
      "[12707,     1] loss: 0.0093700647\n",
      "[12708,     1] loss: 0.0093700327\n",
      "[12709,     1] loss: 0.0093699984\n",
      "[12710,     1] loss: 0.0093699649\n",
      "[12711,     1] loss: 0.0093699329\n",
      "[12712,     1] loss: 0.0093698986\n",
      "[12713,     1] loss: 0.0093698665\n",
      "[12714,     1] loss: 0.0093698353\n",
      "[12715,     1] loss: 0.0093698002\n",
      "[12716,     1] loss: 0.0093697689\n",
      "[12717,     1] loss: 0.0093697369\n",
      "[12718,     1] loss: 0.0093697019\n",
      "[12719,     1] loss: 0.0093696706\n",
      "[12720,     1] loss: 0.0093696378\n",
      "[12721,     1] loss: 0.0093696035\n",
      "[12722,     1] loss: 0.0093695708\n",
      "[12723,     1] loss: 0.0093695395\n",
      "[12724,     1] loss: 0.0093695052\n",
      "[12725,     1] loss: 0.0093694739\n",
      "[12726,     1] loss: 0.0093694404\n",
      "[12727,     1] loss: 0.0093694068\n",
      "[12728,     1] loss: 0.0093693756\n",
      "[12729,     1] loss: 0.0093693428\n",
      "[12730,     1] loss: 0.0093693100\n",
      "[12731,     1] loss: 0.0093692757\n",
      "[12732,     1] loss: 0.0093692452\n",
      "[12733,     1] loss: 0.0093692131\n",
      "[12734,     1] loss: 0.0093691781\n",
      "[12735,     1] loss: 0.0093691461\n",
      "[12736,     1] loss: 0.0093691155\n",
      "[12737,     1] loss: 0.0093690805\n",
      "[12738,     1] loss: 0.0093690485\n",
      "[12739,     1] loss: 0.0093690164\n",
      "[12740,     1] loss: 0.0093689851\n",
      "[12741,     1] loss: 0.0093689509\n",
      "[12742,     1] loss: 0.0093689203\n",
      "[12743,     1] loss: 0.0093688861\n",
      "[12744,     1] loss: 0.0093688540\n",
      "[12745,     1] loss: 0.0093688220\n",
      "[12746,     1] loss: 0.0093687885\n",
      "[12747,     1] loss: 0.0093687557\n",
      "[12748,     1] loss: 0.0093687236\n",
      "[12749,     1] loss: 0.0093686908\n",
      "[12750,     1] loss: 0.0093686566\n",
      "[12751,     1] loss: 0.0093686245\n",
      "[12752,     1] loss: 0.0093685918\n",
      "[12753,     1] loss: 0.0093685590\n",
      "[12754,     1] loss: 0.0093685262\n",
      "[12755,     1] loss: 0.0093684942\n",
      "[12756,     1] loss: 0.0093684606\n",
      "[12757,     1] loss: 0.0093684278\n",
      "[12758,     1] loss: 0.0093683958\n",
      "[12759,     1] loss: 0.0093683638\n",
      "[12760,     1] loss: 0.0093683302\n",
      "[12761,     1] loss: 0.0093682989\n",
      "[12762,     1] loss: 0.0093682647\n",
      "[12763,     1] loss: 0.0093682334\n",
      "[12764,     1] loss: 0.0093682006\n",
      "[12765,     1] loss: 0.0093681678\n",
      "[12766,     1] loss: 0.0093681358\n",
      "[12767,     1] loss: 0.0093681030\n",
      "[12768,     1] loss: 0.0093680710\n",
      "[12769,     1] loss: 0.0093680389\n",
      "[12770,     1] loss: 0.0093680076\n",
      "[12771,     1] loss: 0.0093679741\n",
      "[12772,     1] loss: 0.0093679421\n",
      "[12773,     1] loss: 0.0093679100\n",
      "[12774,     1] loss: 0.0093678765\n",
      "[12775,     1] loss: 0.0093678452\n",
      "[12776,     1] loss: 0.0093678124\n",
      "[12777,     1] loss: 0.0093677804\n",
      "[12778,     1] loss: 0.0093677483\n",
      "[12779,     1] loss: 0.0093677163\n",
      "[12780,     1] loss: 0.0093676820\n",
      "[12781,     1] loss: 0.0093676515\n",
      "[12782,     1] loss: 0.0093676165\n",
      "[12783,     1] loss: 0.0093675852\n",
      "[12784,     1] loss: 0.0093675531\n",
      "[12785,     1] loss: 0.0093675204\n",
      "[12786,     1] loss: 0.0093674883\n",
      "[12787,     1] loss: 0.0093674555\n",
      "[12788,     1] loss: 0.0093674235\n",
      "[12789,     1] loss: 0.0093673915\n",
      "[12790,     1] loss: 0.0093673594\n",
      "[12791,     1] loss: 0.0093673252\n",
      "[12792,     1] loss: 0.0093672931\n",
      "[12793,     1] loss: 0.0093672611\n",
      "[12794,     1] loss: 0.0093672298\n",
      "[12795,     1] loss: 0.0093671978\n",
      "[12796,     1] loss: 0.0093671650\n",
      "[12797,     1] loss: 0.0093671322\n",
      "[12798,     1] loss: 0.0093671009\n",
      "[12799,     1] loss: 0.0093670674\n",
      "[12800,     1] loss: 0.0093670346\n",
      "[12801,     1] loss: 0.0093670033\n",
      "[12802,     1] loss: 0.0093669705\n",
      "[12803,     1] loss: 0.0093669385\n",
      "[12804,     1] loss: 0.0093669064\n",
      "[12805,     1] loss: 0.0093668744\n",
      "[12806,     1] loss: 0.0093668416\n",
      "[12807,     1] loss: 0.0093668103\n",
      "[12808,     1] loss: 0.0093667775\n",
      "[12809,     1] loss: 0.0093667455\n",
      "[12810,     1] loss: 0.0093667135\n",
      "[12811,     1] loss: 0.0093666814\n",
      "[12812,     1] loss: 0.0093666472\n",
      "[12813,     1] loss: 0.0093666159\n",
      "[12814,     1] loss: 0.0093665846\n",
      "[12815,     1] loss: 0.0093665503\n",
      "[12816,     1] loss: 0.0093665197\n",
      "[12817,     1] loss: 0.0093664885\n",
      "[12818,     1] loss: 0.0093664549\n",
      "[12819,     1] loss: 0.0093664236\n",
      "[12820,     1] loss: 0.0093663901\n",
      "[12821,     1] loss: 0.0093663581\n",
      "[12822,     1] loss: 0.0093663275\n",
      "[12823,     1] loss: 0.0093662940\n",
      "[12824,     1] loss: 0.0093662620\n",
      "[12825,     1] loss: 0.0093662299\n",
      "[12826,     1] loss: 0.0093661979\n",
      "[12827,     1] loss: 0.0093661666\n",
      "[12828,     1] loss: 0.0093661338\n",
      "[12829,     1] loss: 0.0093661033\n",
      "[12830,     1] loss: 0.0093660712\n",
      "[12831,     1] loss: 0.0093660384\n",
      "[12832,     1] loss: 0.0093660064\n",
      "[12833,     1] loss: 0.0093659751\n",
      "[12834,     1] loss: 0.0093659423\n",
      "[12835,     1] loss: 0.0093659110\n",
      "[12836,     1] loss: 0.0093658783\n",
      "[12837,     1] loss: 0.0093658462\n",
      "[12838,     1] loss: 0.0093658134\n",
      "[12839,     1] loss: 0.0093657821\n",
      "[12840,     1] loss: 0.0093657494\n",
      "[12841,     1] loss: 0.0093657181\n",
      "[12842,     1] loss: 0.0093656853\n",
      "[12843,     1] loss: 0.0093656532\n",
      "[12844,     1] loss: 0.0093656205\n",
      "[12845,     1] loss: 0.0093655899\n",
      "[12846,     1] loss: 0.0093655586\n",
      "[12847,     1] loss: 0.0093655251\n",
      "[12848,     1] loss: 0.0093654938\n",
      "[12849,     1] loss: 0.0093654618\n",
      "[12850,     1] loss: 0.0093654305\n",
      "[12851,     1] loss: 0.0093653977\n",
      "[12852,     1] loss: 0.0093653657\n",
      "[12853,     1] loss: 0.0093653344\n",
      "[12854,     1] loss: 0.0093653016\n",
      "[12855,     1] loss: 0.0093652681\n",
      "[12856,     1] loss: 0.0093652390\n",
      "[12857,     1] loss: 0.0093652055\n",
      "[12858,     1] loss: 0.0093651734\n",
      "[12859,     1] loss: 0.0093651429\n",
      "[12860,     1] loss: 0.0093651108\n",
      "[12861,     1] loss: 0.0093650788\n",
      "[12862,     1] loss: 0.0093650460\n",
      "[12863,     1] loss: 0.0093650147\n",
      "[12864,     1] loss: 0.0093649827\n",
      "[12865,     1] loss: 0.0093649499\n",
      "[12866,     1] loss: 0.0093649194\n",
      "[12867,     1] loss: 0.0093648866\n",
      "[12868,     1] loss: 0.0093648560\n",
      "[12869,     1] loss: 0.0093648240\n",
      "[12870,     1] loss: 0.0093647905\n",
      "[12871,     1] loss: 0.0093647592\n",
      "[12872,     1] loss: 0.0093647279\n",
      "[12873,     1] loss: 0.0093646951\n",
      "[12874,     1] loss: 0.0093646638\n",
      "[12875,     1] loss: 0.0093646310\n",
      "[12876,     1] loss: 0.0093645997\n",
      "[12877,     1] loss: 0.0093645692\n",
      "[12878,     1] loss: 0.0093645379\n",
      "[12879,     1] loss: 0.0093645059\n",
      "[12880,     1] loss: 0.0093644731\n",
      "[12881,     1] loss: 0.0093644418\n",
      "[12882,     1] loss: 0.0093644090\n",
      "[12883,     1] loss: 0.0093643777\n",
      "[12884,     1] loss: 0.0093643472\n",
      "[12885,     1] loss: 0.0093643159\n",
      "[12886,     1] loss: 0.0093642831\n",
      "[12887,     1] loss: 0.0093642518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12888,     1] loss: 0.0093642205\n",
      "[12889,     1] loss: 0.0093641870\n",
      "[12890,     1] loss: 0.0093641557\n",
      "[12891,     1] loss: 0.0093641229\n",
      "[12892,     1] loss: 0.0093640931\n",
      "[12893,     1] loss: 0.0093640611\n",
      "[12894,     1] loss: 0.0093640298\n",
      "[12895,     1] loss: 0.0093639985\n",
      "[12896,     1] loss: 0.0093639664\n",
      "[12897,     1] loss: 0.0093639366\n",
      "[12898,     1] loss: 0.0093639031\n",
      "[12899,     1] loss: 0.0093638733\n",
      "[12900,     1] loss: 0.0093638413\n",
      "[12901,     1] loss: 0.0093638092\n",
      "[12902,     1] loss: 0.0093637772\n",
      "[12903,     1] loss: 0.0093637452\n",
      "[12904,     1] loss: 0.0093637154\n",
      "[12905,     1] loss: 0.0093636826\n",
      "[12906,     1] loss: 0.0093636528\n",
      "[12907,     1] loss: 0.0093636200\n",
      "[12908,     1] loss: 0.0093635887\n",
      "[12909,     1] loss: 0.0093635559\n",
      "[12910,     1] loss: 0.0093635231\n",
      "[12911,     1] loss: 0.0093634933\n",
      "[12912,     1] loss: 0.0093634613\n",
      "[12913,     1] loss: 0.0093634285\n",
      "[12914,     1] loss: 0.0093633987\n",
      "[12915,     1] loss: 0.0093633659\n",
      "[12916,     1] loss: 0.0093633339\n",
      "[12917,     1] loss: 0.0093633041\n",
      "[12918,     1] loss: 0.0093632728\n",
      "[12919,     1] loss: 0.0093632407\n",
      "[12920,     1] loss: 0.0093632102\n",
      "[12921,     1] loss: 0.0093631782\n",
      "[12922,     1] loss: 0.0093631461\n",
      "[12923,     1] loss: 0.0093631156\n",
      "[12924,     1] loss: 0.0093630843\n",
      "[12925,     1] loss: 0.0093630537\n",
      "[12926,     1] loss: 0.0093630210\n",
      "[12927,     1] loss: 0.0093629904\n",
      "[12928,     1] loss: 0.0093629599\n",
      "[12929,     1] loss: 0.0093629278\n",
      "[12930,     1] loss: 0.0093628950\n",
      "[12931,     1] loss: 0.0093628645\n",
      "[12932,     1] loss: 0.0093628325\n",
      "[12933,     1] loss: 0.0093628019\n",
      "[12934,     1] loss: 0.0093627699\n",
      "[12935,     1] loss: 0.0093627386\n",
      "[12936,     1] loss: 0.0093627080\n",
      "[12937,     1] loss: 0.0093626760\n",
      "[12938,     1] loss: 0.0093626440\n",
      "[12939,     1] loss: 0.0093626149\n",
      "[12940,     1] loss: 0.0093625829\n",
      "[12941,     1] loss: 0.0093625501\n",
      "[12942,     1] loss: 0.0093625203\n",
      "[12943,     1] loss: 0.0093624897\n",
      "[12944,     1] loss: 0.0093624577\n",
      "[12945,     1] loss: 0.0093624264\n",
      "[12946,     1] loss: 0.0093623951\n",
      "[12947,     1] loss: 0.0093623616\n",
      "[12948,     1] loss: 0.0093623325\n",
      "[12949,     1] loss: 0.0093623012\n",
      "[12950,     1] loss: 0.0093622692\n",
      "[12951,     1] loss: 0.0093622394\n",
      "[12952,     1] loss: 0.0093622074\n",
      "[12953,     1] loss: 0.0093621753\n",
      "[12954,     1] loss: 0.0093621448\n",
      "[12955,     1] loss: 0.0093621135\n",
      "[12956,     1] loss: 0.0093620822\n",
      "[12957,     1] loss: 0.0093620516\n",
      "[12958,     1] loss: 0.0093620203\n",
      "[12959,     1] loss: 0.0093619891\n",
      "[12960,     1] loss: 0.0093619578\n",
      "[12961,     1] loss: 0.0093619265\n",
      "[12962,     1] loss: 0.0093618952\n",
      "[12963,     1] loss: 0.0093618631\n",
      "[12964,     1] loss: 0.0093618318\n",
      "[12965,     1] loss: 0.0093618013\n",
      "[12966,     1] loss: 0.0093617707\n",
      "[12967,     1] loss: 0.0093617387\n",
      "[12968,     1] loss: 0.0093617074\n",
      "[12969,     1] loss: 0.0093616769\n",
      "[12970,     1] loss: 0.0093616456\n",
      "[12971,     1] loss: 0.0093616143\n",
      "[12972,     1] loss: 0.0093615822\n",
      "[12973,     1] loss: 0.0093615517\n",
      "[12974,     1] loss: 0.0093615204\n",
      "[12975,     1] loss: 0.0093614899\n",
      "[12976,     1] loss: 0.0093614593\n",
      "[12977,     1] loss: 0.0093614288\n",
      "[12978,     1] loss: 0.0093613967\n",
      "[12979,     1] loss: 0.0093613647\n",
      "[12980,     1] loss: 0.0093613349\n",
      "[12981,     1] loss: 0.0093613036\n",
      "[12982,     1] loss: 0.0093612731\n",
      "[12983,     1] loss: 0.0093612418\n",
      "[12984,     1] loss: 0.0093612097\n",
      "[12985,     1] loss: 0.0093611792\n",
      "[12986,     1] loss: 0.0093611494\n",
      "[12987,     1] loss: 0.0093611173\n",
      "[12988,     1] loss: 0.0093610875\n",
      "[12989,     1] loss: 0.0093610547\n",
      "[12990,     1] loss: 0.0093610264\n",
      "[12991,     1] loss: 0.0093609944\n",
      "[12992,     1] loss: 0.0093609624\n",
      "[12993,     1] loss: 0.0093609326\n",
      "[12994,     1] loss: 0.0093609013\n",
      "[12995,     1] loss: 0.0093608692\n",
      "[12996,     1] loss: 0.0093608402\n",
      "[12997,     1] loss: 0.0093608096\n",
      "[12998,     1] loss: 0.0093607768\n",
      "[12999,     1] loss: 0.0093607485\n",
      "[13000,     1] loss: 0.0093607165\n",
      "[13001,     1] loss: 0.0093606852\n",
      "[13002,     1] loss: 0.0093606547\n",
      "[13003,     1] loss: 0.0093606234\n",
      "[13004,     1] loss: 0.0093605928\n",
      "[13005,     1] loss: 0.0093605608\n",
      "[13006,     1] loss: 0.0093605317\n",
      "[13007,     1] loss: 0.0093605004\n",
      "[13008,     1] loss: 0.0093604699\n",
      "[13009,     1] loss: 0.0093604393\n",
      "[13010,     1] loss: 0.0093604073\n",
      "[13011,     1] loss: 0.0093603767\n",
      "[13012,     1] loss: 0.0093603462\n",
      "[13013,     1] loss: 0.0093603149\n",
      "[13014,     1] loss: 0.0093602851\n",
      "[13015,     1] loss: 0.0093602538\n",
      "[13016,     1] loss: 0.0093602225\n",
      "[13017,     1] loss: 0.0093601912\n",
      "[13018,     1] loss: 0.0093601599\n",
      "[13019,     1] loss: 0.0093601301\n",
      "[13020,     1] loss: 0.0093600988\n",
      "[13021,     1] loss: 0.0093600698\n",
      "[13022,     1] loss: 0.0093600385\n",
      "[13023,     1] loss: 0.0093600079\n",
      "[13024,     1] loss: 0.0093599774\n",
      "[13025,     1] loss: 0.0093599468\n",
      "[13026,     1] loss: 0.0093599148\n",
      "[13027,     1] loss: 0.0093598843\n",
      "[13028,     1] loss: 0.0093598545\n",
      "[13029,     1] loss: 0.0093598232\n",
      "[13030,     1] loss: 0.0093597919\n",
      "[13031,     1] loss: 0.0093597613\n",
      "[13032,     1] loss: 0.0093597300\n",
      "[13033,     1] loss: 0.0093597002\n",
      "[13034,     1] loss: 0.0093596697\n",
      "[13035,     1] loss: 0.0093596384\n",
      "[13036,     1] loss: 0.0093596086\n",
      "[13037,     1] loss: 0.0093595766\n",
      "[13038,     1] loss: 0.0093595475\n",
      "[13039,     1] loss: 0.0093595155\n",
      "[13040,     1] loss: 0.0093594857\n",
      "[13041,     1] loss: 0.0093594566\n",
      "[13042,     1] loss: 0.0093594261\n",
      "[13043,     1] loss: 0.0093593925\n",
      "[13044,     1] loss: 0.0093593635\n",
      "[13045,     1] loss: 0.0093593337\n",
      "[13046,     1] loss: 0.0093593039\n",
      "[13047,     1] loss: 0.0093592718\n",
      "[13048,     1] loss: 0.0093592420\n",
      "[13049,     1] loss: 0.0093592122\n",
      "[13050,     1] loss: 0.0093591802\n",
      "[13051,     1] loss: 0.0093591511\n",
      "[13052,     1] loss: 0.0093591198\n",
      "[13053,     1] loss: 0.0093590900\n",
      "[13054,     1] loss: 0.0093590587\n",
      "[13055,     1] loss: 0.0093590297\n",
      "[13056,     1] loss: 0.0093589976\n",
      "[13057,     1] loss: 0.0093589678\n",
      "[13058,     1] loss: 0.0093589380\n",
      "[13059,     1] loss: 0.0093589060\n",
      "[13060,     1] loss: 0.0093588762\n",
      "[13061,     1] loss: 0.0093588464\n",
      "[13062,     1] loss: 0.0093588158\n",
      "[13063,     1] loss: 0.0093587853\n",
      "[13064,     1] loss: 0.0093587548\n",
      "[13065,     1] loss: 0.0093587250\n",
      "[13066,     1] loss: 0.0093586937\n",
      "[13067,     1] loss: 0.0093586624\n",
      "[13068,     1] loss: 0.0093586326\n",
      "[13069,     1] loss: 0.0093586020\n",
      "[13070,     1] loss: 0.0093585730\n",
      "[13071,     1] loss: 0.0093585424\n",
      "[13072,     1] loss: 0.0093585119\n",
      "[13073,     1] loss: 0.0093584821\n",
      "[13074,     1] loss: 0.0093584508\n",
      "[13075,     1] loss: 0.0093584210\n",
      "[13076,     1] loss: 0.0093583912\n",
      "[13077,     1] loss: 0.0093583606\n",
      "[13078,     1] loss: 0.0093583278\n",
      "[13079,     1] loss: 0.0093582995\n",
      "[13080,     1] loss: 0.0093582697\n",
      "[13081,     1] loss: 0.0093582392\n",
      "[13082,     1] loss: 0.0093582086\n",
      "[13083,     1] loss: 0.0093581788\n",
      "[13084,     1] loss: 0.0093581475\n",
      "[13085,     1] loss: 0.0093581177\n",
      "[13086,     1] loss: 0.0093580872\n",
      "[13087,     1] loss: 0.0093580566\n",
      "[13088,     1] loss: 0.0093580268\n",
      "[13089,     1] loss: 0.0093579963\n",
      "[13090,     1] loss: 0.0093579657\n",
      "[13091,     1] loss: 0.0093579359\n",
      "[13092,     1] loss: 0.0093579061\n",
      "[13093,     1] loss: 0.0093578756\n",
      "[13094,     1] loss: 0.0093578450\n",
      "[13095,     1] loss: 0.0093578145\n",
      "[13096,     1] loss: 0.0093577854\n",
      "[13097,     1] loss: 0.0093577541\n",
      "[13098,     1] loss: 0.0093577236\n",
      "[13099,     1] loss: 0.0093576953\n",
      "[13100,     1] loss: 0.0093576655\n",
      "[13101,     1] loss: 0.0093576342\n",
      "[13102,     1] loss: 0.0093576044\n",
      "[13103,     1] loss: 0.0093575731\n",
      "[13104,     1] loss: 0.0093575433\n",
      "[13105,     1] loss: 0.0093575135\n",
      "[13106,     1] loss: 0.0093574844\n",
      "[13107,     1] loss: 0.0093574531\n",
      "[13108,     1] loss: 0.0093574233\n",
      "[13109,     1] loss: 0.0093573935\n",
      "[13110,     1] loss: 0.0093573637\n",
      "[13111,     1] loss: 0.0093573332\n",
      "[13112,     1] loss: 0.0093573034\n",
      "[13113,     1] loss: 0.0093572736\n",
      "[13114,     1] loss: 0.0093572423\n",
      "[13115,     1] loss: 0.0093572147\n",
      "[13116,     1] loss: 0.0093571834\n",
      "[13117,     1] loss: 0.0093571536\n",
      "[13118,     1] loss: 0.0093571231\n",
      "[13119,     1] loss: 0.0093570940\n",
      "[13120,     1] loss: 0.0093570642\n",
      "[13121,     1] loss: 0.0093570344\n",
      "[13122,     1] loss: 0.0093570046\n",
      "[13123,     1] loss: 0.0093569733\n",
      "[13124,     1] loss: 0.0093569435\n",
      "[13125,     1] loss: 0.0093569130\n",
      "[13126,     1] loss: 0.0093568832\n",
      "[13127,     1] loss: 0.0093568534\n",
      "[13128,     1] loss: 0.0093568236\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13129,     1] loss: 0.0093567938\n",
      "[13130,     1] loss: 0.0093567647\n",
      "[13131,     1] loss: 0.0093567349\n",
      "[13132,     1] loss: 0.0093567044\n",
      "[13133,     1] loss: 0.0093566746\n",
      "[13134,     1] loss: 0.0093566447\n",
      "[13135,     1] loss: 0.0093566142\n",
      "[13136,     1] loss: 0.0093565866\n",
      "[13137,     1] loss: 0.0093565553\n",
      "[13138,     1] loss: 0.0093565241\n",
      "[13139,     1] loss: 0.0093564957\n",
      "[13140,     1] loss: 0.0093564659\n",
      "[13141,     1] loss: 0.0093564346\n",
      "[13142,     1] loss: 0.0093564056\n",
      "[13143,     1] loss: 0.0093563750\n",
      "[13144,     1] loss: 0.0093563460\n",
      "[13145,     1] loss: 0.0093563154\n",
      "[13146,     1] loss: 0.0093562864\n",
      "[13147,     1] loss: 0.0093562566\n",
      "[13148,     1] loss: 0.0093562268\n",
      "[13149,     1] loss: 0.0093561970\n",
      "[13150,     1] loss: 0.0093561657\n",
      "[13151,     1] loss: 0.0093561366\n",
      "[13152,     1] loss: 0.0093561068\n",
      "[13153,     1] loss: 0.0093560770\n",
      "[13154,     1] loss: 0.0093560457\n",
      "[13155,     1] loss: 0.0093560174\n",
      "[13156,     1] loss: 0.0093559861\n",
      "[13157,     1] loss: 0.0093559586\n",
      "[13158,     1] loss: 0.0093559273\n",
      "[13159,     1] loss: 0.0093558989\n",
      "[13160,     1] loss: 0.0093558684\n",
      "[13161,     1] loss: 0.0093558386\n",
      "[13162,     1] loss: 0.0093558095\n",
      "[13163,     1] loss: 0.0093557790\n",
      "[13164,     1] loss: 0.0093557484\n",
      "[13165,     1] loss: 0.0093557201\n",
      "[13166,     1] loss: 0.0093556903\n",
      "[13167,     1] loss: 0.0093556605\n",
      "[13168,     1] loss: 0.0093556300\n",
      "[13169,     1] loss: 0.0093556017\n",
      "[13170,     1] loss: 0.0093555734\n",
      "[13171,     1] loss: 0.0093555428\n",
      "[13172,     1] loss: 0.0093555138\n",
      "[13173,     1] loss: 0.0093554832\n",
      "[13174,     1] loss: 0.0093554549\n",
      "[13175,     1] loss: 0.0093554251\n",
      "[13176,     1] loss: 0.0093553945\n",
      "[13177,     1] loss: 0.0093553647\n",
      "[13178,     1] loss: 0.0093553357\n",
      "[13179,     1] loss: 0.0093553059\n",
      "[13180,     1] loss: 0.0093552776\n",
      "[13181,     1] loss: 0.0093552463\n",
      "[13182,     1] loss: 0.0093552172\n",
      "[13183,     1] loss: 0.0093551889\n",
      "[13184,     1] loss: 0.0093551598\n",
      "[13185,     1] loss: 0.0093551286\n",
      "[13186,     1] loss: 0.0093550995\n",
      "[13187,     1] loss: 0.0093550704\n",
      "[13188,     1] loss: 0.0093550406\n",
      "[13189,     1] loss: 0.0093550108\n",
      "[13190,     1] loss: 0.0093549818\n",
      "[13191,     1] loss: 0.0093549520\n",
      "[13192,     1] loss: 0.0093549222\n",
      "[13193,     1] loss: 0.0093548924\n",
      "[13194,     1] loss: 0.0093548626\n",
      "[13195,     1] loss: 0.0093548343\n",
      "[13196,     1] loss: 0.0093548045\n",
      "[13197,     1] loss: 0.0093547754\n",
      "[13198,     1] loss: 0.0093547434\n",
      "[13199,     1] loss: 0.0093547150\n",
      "[13200,     1] loss: 0.0093546875\n",
      "[13201,     1] loss: 0.0093546562\n",
      "[13202,     1] loss: 0.0093546279\n",
      "[13203,     1] loss: 0.0093545981\n",
      "[13204,     1] loss: 0.0093545698\n",
      "[13205,     1] loss: 0.0093545407\n",
      "[13206,     1] loss: 0.0093545102\n",
      "[13207,     1] loss: 0.0093544811\n",
      "[13208,     1] loss: 0.0093544520\n",
      "[13209,     1] loss: 0.0093544222\n",
      "[13210,     1] loss: 0.0093543939\n",
      "[13211,     1] loss: 0.0093543641\n",
      "[13212,     1] loss: 0.0093543336\n",
      "[13213,     1] loss: 0.0093543053\n",
      "[13214,     1] loss: 0.0093542762\n",
      "[13215,     1] loss: 0.0093542472\n",
      "[13216,     1] loss: 0.0093542174\n",
      "[13217,     1] loss: 0.0093541875\n",
      "[13218,     1] loss: 0.0093541585\n",
      "[13219,     1] loss: 0.0093541287\n",
      "[13220,     1] loss: 0.0093541004\n",
      "[13221,     1] loss: 0.0093540698\n",
      "[13222,     1] loss: 0.0093540415\n",
      "[13223,     1] loss: 0.0093540125\n",
      "[13224,     1] loss: 0.0093539827\n",
      "[13225,     1] loss: 0.0093539543\n",
      "[13226,     1] loss: 0.0093539238\n",
      "[13227,     1] loss: 0.0093538955\n",
      "[13228,     1] loss: 0.0093538657\n",
      "[13229,     1] loss: 0.0093538351\n",
      "[13230,     1] loss: 0.0093538083\n",
      "[13231,     1] loss: 0.0093537770\n",
      "[13232,     1] loss: 0.0093537487\n",
      "[13233,     1] loss: 0.0093537189\n",
      "[13234,     1] loss: 0.0093536906\n",
      "[13235,     1] loss: 0.0093536615\n",
      "[13236,     1] loss: 0.0093536332\n",
      "[13237,     1] loss: 0.0093536027\n",
      "[13238,     1] loss: 0.0093535751\n",
      "[13239,     1] loss: 0.0093535446\n",
      "[13240,     1] loss: 0.0093535155\n",
      "[13241,     1] loss: 0.0093534887\n",
      "[13242,     1] loss: 0.0093534581\n",
      "[13243,     1] loss: 0.0093534291\n",
      "[13244,     1] loss: 0.0093534000\n",
      "[13245,     1] loss: 0.0093533710\n",
      "[13246,     1] loss: 0.0093533412\n",
      "[13247,     1] loss: 0.0093533121\n",
      "[13248,     1] loss: 0.0093532830\n",
      "[13249,     1] loss: 0.0093532540\n",
      "[13250,     1] loss: 0.0093532264\n",
      "[13251,     1] loss: 0.0093531966\n",
      "[13252,     1] loss: 0.0093531668\n",
      "[13253,     1] loss: 0.0093531400\n",
      "[13254,     1] loss: 0.0093531087\n",
      "[13255,     1] loss: 0.0093530789\n",
      "[13256,     1] loss: 0.0093530513\n",
      "[13257,     1] loss: 0.0093530238\n",
      "[13258,     1] loss: 0.0093529932\n",
      "[13259,     1] loss: 0.0093529649\n",
      "[13260,     1] loss: 0.0093529373\n",
      "[13261,     1] loss: 0.0093529075\n",
      "[13262,     1] loss: 0.0093528785\n",
      "[13263,     1] loss: 0.0093528502\n",
      "[13264,     1] loss: 0.0093528219\n",
      "[13265,     1] loss: 0.0093527913\n",
      "[13266,     1] loss: 0.0093527630\n",
      "[13267,     1] loss: 0.0093527339\n",
      "[13268,     1] loss: 0.0093527049\n",
      "[13269,     1] loss: 0.0093526773\n",
      "[13270,     1] loss: 0.0093526475\n",
      "[13271,     1] loss: 0.0093526199\n",
      "[13272,     1] loss: 0.0093525909\n",
      "[13273,     1] loss: 0.0093525611\n",
      "[13274,     1] loss: 0.0093525298\n",
      "[13275,     1] loss: 0.0093525030\n",
      "[13276,     1] loss: 0.0093524747\n",
      "[13277,     1] loss: 0.0093524449\n",
      "[13278,     1] loss: 0.0093524158\n",
      "[13279,     1] loss: 0.0093523882\n",
      "[13280,     1] loss: 0.0093523569\n",
      "[13281,     1] loss: 0.0093523301\n",
      "[13282,     1] loss: 0.0093523011\n",
      "[13283,     1] loss: 0.0093522720\n",
      "[13284,     1] loss: 0.0093522429\n",
      "[13285,     1] loss: 0.0093522139\n",
      "[13286,     1] loss: 0.0093521856\n",
      "[13287,     1] loss: 0.0093521573\n",
      "[13288,     1] loss: 0.0093521297\n",
      "[13289,     1] loss: 0.0093520992\n",
      "[13290,     1] loss: 0.0093520708\n",
      "[13291,     1] loss: 0.0093520425\n",
      "[13292,     1] loss: 0.0093520142\n",
      "[13293,     1] loss: 0.0093519859\n",
      "[13294,     1] loss: 0.0093519561\n",
      "[13295,     1] loss: 0.0093519278\n",
      "[13296,     1] loss: 0.0093518987\n",
      "[13297,     1] loss: 0.0093518712\n",
      "[13298,     1] loss: 0.0093518414\n",
      "[13299,     1] loss: 0.0093518123\n",
      "[13300,     1] loss: 0.0093517847\n",
      "[13301,     1] loss: 0.0093517549\n",
      "[13302,     1] loss: 0.0093517274\n",
      "[13303,     1] loss: 0.0093516983\n",
      "[13304,     1] loss: 0.0093516693\n",
      "[13305,     1] loss: 0.0093516417\n",
      "[13306,     1] loss: 0.0093516126\n",
      "[13307,     1] loss: 0.0093515851\n",
      "[13308,     1] loss: 0.0093515553\n",
      "[13309,     1] loss: 0.0093515262\n",
      "[13310,     1] loss: 0.0093514986\n",
      "[13311,     1] loss: 0.0093514688\n",
      "[13312,     1] loss: 0.0093514405\n",
      "[13313,     1] loss: 0.0093514122\n",
      "[13314,     1] loss: 0.0093513854\n",
      "[13315,     1] loss: 0.0093513556\n",
      "[13316,     1] loss: 0.0093513273\n",
      "[13317,     1] loss: 0.0093512990\n",
      "[13318,     1] loss: 0.0093512684\n",
      "[13319,     1] loss: 0.0093512416\n",
      "[13320,     1] loss: 0.0093512125\n",
      "[13321,     1] loss: 0.0093511850\n",
      "[13322,     1] loss: 0.0093511559\n",
      "[13323,     1] loss: 0.0093511283\n",
      "[13324,     1] loss: 0.0093510993\n",
      "[13325,     1] loss: 0.0093510725\n",
      "[13326,     1] loss: 0.0093510412\n",
      "[13327,     1] loss: 0.0093510129\n",
      "[13328,     1] loss: 0.0093509845\n",
      "[13329,     1] loss: 0.0093509570\n",
      "[13330,     1] loss: 0.0093509287\n",
      "[13331,     1] loss: 0.0093509004\n",
      "[13332,     1] loss: 0.0093508720\n",
      "[13333,     1] loss: 0.0093508437\n",
      "[13334,     1] loss: 0.0093508154\n",
      "[13335,     1] loss: 0.0093507864\n",
      "[13336,     1] loss: 0.0093507573\n",
      "[13337,     1] loss: 0.0093507305\n",
      "[13338,     1] loss: 0.0093507007\n",
      "[13339,     1] loss: 0.0093506709\n",
      "[13340,     1] loss: 0.0093506433\n",
      "[13341,     1] loss: 0.0093506150\n",
      "[13342,     1] loss: 0.0093505874\n",
      "[13343,     1] loss: 0.0093505576\n",
      "[13344,     1] loss: 0.0093505308\n",
      "[13345,     1] loss: 0.0093505017\n",
      "[13346,     1] loss: 0.0093504734\n",
      "[13347,     1] loss: 0.0093504451\n",
      "[13348,     1] loss: 0.0093504168\n",
      "[13349,     1] loss: 0.0093503885\n",
      "[13350,     1] loss: 0.0093503609\n",
      "[13351,     1] loss: 0.0093503334\n",
      "[13352,     1] loss: 0.0093503036\n",
      "[13353,     1] loss: 0.0093502760\n",
      "[13354,     1] loss: 0.0093502484\n",
      "[13355,     1] loss: 0.0093502194\n",
      "[13356,     1] loss: 0.0093501903\n",
      "[13357,     1] loss: 0.0093501642\n",
      "[13358,     1] loss: 0.0093501352\n",
      "[13359,     1] loss: 0.0093501076\n",
      "[13360,     1] loss: 0.0093500786\n",
      "[13361,     1] loss: 0.0093500502\n",
      "[13362,     1] loss: 0.0093500234\n",
      "[13363,     1] loss: 0.0093499944\n",
      "[13364,     1] loss: 0.0093499660\n",
      "[13365,     1] loss: 0.0093499385\n",
      "[13366,     1] loss: 0.0093499102\n",
      "[13367,     1] loss: 0.0093498819\n",
      "[13368,     1] loss: 0.0093498550\n",
      "[13369,     1] loss: 0.0093498260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13370,     1] loss: 0.0093497992\n",
      "[13371,     1] loss: 0.0093497694\n",
      "[13372,     1] loss: 0.0093497418\n",
      "[13373,     1] loss: 0.0093497135\n",
      "[13374,     1] loss: 0.0093496859\n",
      "[13375,     1] loss: 0.0093496576\n",
      "[13376,     1] loss: 0.0093496300\n",
      "[13377,     1] loss: 0.0093496002\n",
      "[13378,     1] loss: 0.0093495734\n",
      "[13379,     1] loss: 0.0093495451\n",
      "[13380,     1] loss: 0.0093495160\n",
      "[13381,     1] loss: 0.0093494885\n",
      "[13382,     1] loss: 0.0093494624\n",
      "[13383,     1] loss: 0.0093494326\n",
      "[13384,     1] loss: 0.0093494050\n",
      "[13385,     1] loss: 0.0093493789\n",
      "[13386,     1] loss: 0.0093493491\n",
      "[13387,     1] loss: 0.0093493216\n",
      "[13388,     1] loss: 0.0093492955\n",
      "[13389,     1] loss: 0.0093492657\n",
      "[13390,     1] loss: 0.0093492381\n",
      "[13391,     1] loss: 0.0093492098\n",
      "[13392,     1] loss: 0.0093491800\n",
      "[13393,     1] loss: 0.0093491554\n",
      "[13394,     1] loss: 0.0093491271\n",
      "[13395,     1] loss: 0.0093490981\n",
      "[13396,     1] loss: 0.0093490705\n",
      "[13397,     1] loss: 0.0093490422\n",
      "[13398,     1] loss: 0.0093490146\n",
      "[13399,     1] loss: 0.0093489870\n",
      "[13400,     1] loss: 0.0093489587\n",
      "[13401,     1] loss: 0.0093489304\n",
      "[13402,     1] loss: 0.0093489021\n",
      "[13403,     1] loss: 0.0093488745\n",
      "[13404,     1] loss: 0.0093488470\n",
      "[13405,     1] loss: 0.0093488187\n",
      "[13406,     1] loss: 0.0093487896\n",
      "[13407,     1] loss: 0.0093487635\n",
      "[13408,     1] loss: 0.0093487352\n",
      "[13409,     1] loss: 0.0093487076\n",
      "[13410,     1] loss: 0.0093486786\n",
      "[13411,     1] loss: 0.0093486510\n",
      "[13412,     1] loss: 0.0093486242\n",
      "[13413,     1] loss: 0.0093485944\n",
      "[13414,     1] loss: 0.0093485676\n",
      "[13415,     1] loss: 0.0093485400\n",
      "[13416,     1] loss: 0.0093485117\n",
      "[13417,     1] loss: 0.0093484841\n",
      "[13418,     1] loss: 0.0093484573\n",
      "[13419,     1] loss: 0.0093484297\n",
      "[13420,     1] loss: 0.0093483999\n",
      "[13421,     1] loss: 0.0093483746\n",
      "[13422,     1] loss: 0.0093483470\n",
      "[13423,     1] loss: 0.0093483165\n",
      "[13424,     1] loss: 0.0093482904\n",
      "[13425,     1] loss: 0.0093482628\n",
      "[13426,     1] loss: 0.0093482353\n",
      "[13427,     1] loss: 0.0093482085\n",
      "[13428,     1] loss: 0.0093481809\n",
      "[13429,     1] loss: 0.0093481533\n",
      "[13430,     1] loss: 0.0093481250\n",
      "[13431,     1] loss: 0.0093480974\n",
      "[13432,     1] loss: 0.0093480714\n",
      "[13433,     1] loss: 0.0093480431\n",
      "[13434,     1] loss: 0.0093480147\n",
      "[13435,     1] loss: 0.0093479872\n",
      "[13436,     1] loss: 0.0093479596\n",
      "[13437,     1] loss: 0.0093479320\n",
      "[13438,     1] loss: 0.0093479045\n",
      "[13439,     1] loss: 0.0093478777\n",
      "[13440,     1] loss: 0.0093478493\n",
      "[13441,     1] loss: 0.0093478225\n",
      "[13442,     1] loss: 0.0093477942\n",
      "[13443,     1] loss: 0.0093477674\n",
      "[13444,     1] loss: 0.0093477413\n",
      "[13445,     1] loss: 0.0093477122\n",
      "[13446,     1] loss: 0.0093476839\n",
      "[13447,     1] loss: 0.0093476571\n",
      "[13448,     1] loss: 0.0093476295\n",
      "[13449,     1] loss: 0.0093476012\n",
      "[13450,     1] loss: 0.0093475737\n",
      "[13451,     1] loss: 0.0093475476\n",
      "[13452,     1] loss: 0.0093475185\n",
      "[13453,     1] loss: 0.0093474932\n",
      "[13454,     1] loss: 0.0093474641\n",
      "[13455,     1] loss: 0.0093474366\n",
      "[13456,     1] loss: 0.0093474083\n",
      "[13457,     1] loss: 0.0093473814\n",
      "[13458,     1] loss: 0.0093473546\n",
      "[13459,     1] loss: 0.0093473278\n",
      "[13460,     1] loss: 0.0093472995\n",
      "[13461,     1] loss: 0.0093472712\n",
      "[13462,     1] loss: 0.0093472458\n",
      "[13463,     1] loss: 0.0093472160\n",
      "[13464,     1] loss: 0.0093471892\n",
      "[13465,     1] loss: 0.0093471631\n",
      "[13466,     1] loss: 0.0093471341\n",
      "[13467,     1] loss: 0.0093471073\n",
      "[13468,     1] loss: 0.0093470812\n",
      "[13469,     1] loss: 0.0093470529\n",
      "[13470,     1] loss: 0.0093470246\n",
      "[13471,     1] loss: 0.0093469985\n",
      "[13472,     1] loss: 0.0093469724\n",
      "[13473,     1] loss: 0.0093469441\n",
      "[13474,     1] loss: 0.0093469165\n",
      "[13475,     1] loss: 0.0093468904\n",
      "[13476,     1] loss: 0.0093468621\n",
      "[13477,     1] loss: 0.0093468346\n",
      "[13478,     1] loss: 0.0093468077\n",
      "[13479,     1] loss: 0.0093467802\n",
      "[13480,     1] loss: 0.0093467519\n",
      "[13481,     1] loss: 0.0093467258\n",
      "[13482,     1] loss: 0.0093466997\n",
      "[13483,     1] loss: 0.0093466721\n",
      "[13484,     1] loss: 0.0093466438\n",
      "[13485,     1] loss: 0.0093466178\n",
      "[13486,     1] loss: 0.0093465917\n",
      "[13487,     1] loss: 0.0093465626\n",
      "[13488,     1] loss: 0.0093465343\n",
      "[13489,     1] loss: 0.0093465090\n",
      "[13490,     1] loss: 0.0093464807\n",
      "[13491,     1] loss: 0.0093464524\n",
      "[13492,     1] loss: 0.0093464263\n",
      "[13493,     1] loss: 0.0093464002\n",
      "[13494,     1] loss: 0.0093463719\n",
      "[13495,     1] loss: 0.0093463443\n",
      "[13496,     1] loss: 0.0093463197\n",
      "[13497,     1] loss: 0.0093462907\n",
      "[13498,     1] loss: 0.0093462624\n",
      "[13499,     1] loss: 0.0093462370\n",
      "[13500,     1] loss: 0.0093462102\n",
      "[13501,     1] loss: 0.0093461826\n",
      "[13502,     1] loss: 0.0093461551\n",
      "[13503,     1] loss: 0.0093461275\n",
      "[13504,     1] loss: 0.0093461007\n",
      "[13505,     1] loss: 0.0093460746\n",
      "[13506,     1] loss: 0.0093460485\n",
      "[13507,     1] loss: 0.0093460202\n",
      "[13508,     1] loss: 0.0093459941\n",
      "[13509,     1] loss: 0.0093459673\n",
      "[13510,     1] loss: 0.0093459398\n",
      "[13511,     1] loss: 0.0093459122\n",
      "[13512,     1] loss: 0.0093458846\n",
      "[13513,     1] loss: 0.0093458585\n",
      "[13514,     1] loss: 0.0093458310\n",
      "[13515,     1] loss: 0.0093458027\n",
      "[13516,     1] loss: 0.0093457781\n",
      "[13517,     1] loss: 0.0093457505\n",
      "[13518,     1] loss: 0.0093457237\n",
      "[13519,     1] loss: 0.0093456961\n",
      "[13520,     1] loss: 0.0093456693\n",
      "[13521,     1] loss: 0.0093456425\n",
      "[13522,     1] loss: 0.0093456142\n",
      "[13523,     1] loss: 0.0093455873\n",
      "[13524,     1] loss: 0.0093455620\n",
      "[13525,     1] loss: 0.0093455352\n",
      "[13526,     1] loss: 0.0093455069\n",
      "[13527,     1] loss: 0.0093454815\n",
      "[13528,     1] loss: 0.0093454555\n",
      "[13529,     1] loss: 0.0093454264\n",
      "[13530,     1] loss: 0.0093454003\n",
      "[13531,     1] loss: 0.0093453743\n",
      "[13532,     1] loss: 0.0093453467\n",
      "[13533,     1] loss: 0.0093453199\n",
      "[13534,     1] loss: 0.0093452930\n",
      "[13535,     1] loss: 0.0093452655\n",
      "[13536,     1] loss: 0.0093452387\n",
      "[13537,     1] loss: 0.0093452133\n",
      "[13538,     1] loss: 0.0093451843\n",
      "[13539,     1] loss: 0.0093451589\n",
      "[13540,     1] loss: 0.0093451321\n",
      "[13541,     1] loss: 0.0093451053\n",
      "[13542,     1] loss: 0.0093450777\n",
      "[13543,     1] loss: 0.0093450524\n",
      "[13544,     1] loss: 0.0093450248\n",
      "[13545,     1] loss: 0.0093449973\n",
      "[13546,     1] loss: 0.0093449719\n",
      "[13547,     1] loss: 0.0093449458\n",
      "[13548,     1] loss: 0.0093449190\n",
      "[13549,     1] loss: 0.0093448929\n",
      "[13550,     1] loss: 0.0093448661\n",
      "[13551,     1] loss: 0.0093448386\n",
      "[13552,     1] loss: 0.0093448132\n",
      "[13553,     1] loss: 0.0093447864\n",
      "[13554,     1] loss: 0.0093447596\n",
      "[13555,     1] loss: 0.0093447313\n",
      "[13556,     1] loss: 0.0093447059\n",
      "[13557,     1] loss: 0.0093446784\n",
      "[13558,     1] loss: 0.0093446530\n",
      "[13559,     1] loss: 0.0093446255\n",
      "[13560,     1] loss: 0.0093445987\n",
      "[13561,     1] loss: 0.0093445726\n",
      "[13562,     1] loss: 0.0093445458\n",
      "[13563,     1] loss: 0.0093445182\n",
      "[13564,     1] loss: 0.0093444929\n",
      "[13565,     1] loss: 0.0093444660\n",
      "[13566,     1] loss: 0.0093444377\n",
      "[13567,     1] loss: 0.0093444124\n",
      "[13568,     1] loss: 0.0093443856\n",
      "[13569,     1] loss: 0.0093443587\n",
      "[13570,     1] loss: 0.0093443327\n",
      "[13571,     1] loss: 0.0093443044\n",
      "[13572,     1] loss: 0.0093442805\n",
      "[13573,     1] loss: 0.0093442529\n",
      "[13574,     1] loss: 0.0093442261\n",
      "[13575,     1] loss: 0.0093441986\n",
      "[13576,     1] loss: 0.0093441725\n",
      "[13577,     1] loss: 0.0093441464\n",
      "[13578,     1] loss: 0.0093441181\n",
      "[13579,     1] loss: 0.0093440935\n",
      "[13580,     1] loss: 0.0093440667\n",
      "[13581,     1] loss: 0.0093440406\n",
      "[13582,     1] loss: 0.0093440123\n",
      "[13583,     1] loss: 0.0093439862\n",
      "[13584,     1] loss: 0.0093439601\n",
      "[13585,     1] loss: 0.0093439341\n",
      "[13586,     1] loss: 0.0093439080\n",
      "[13587,     1] loss: 0.0093438819\n",
      "[13588,     1] loss: 0.0093438543\n",
      "[13589,     1] loss: 0.0093438268\n",
      "[13590,     1] loss: 0.0093438022\n",
      "[13591,     1] loss: 0.0093437761\n",
      "[13592,     1] loss: 0.0093437493\n",
      "[13593,     1] loss: 0.0093437217\n",
      "[13594,     1] loss: 0.0093436956\n",
      "[13595,     1] loss: 0.0093436696\n",
      "[13596,     1] loss: 0.0093436442\n",
      "[13597,     1] loss: 0.0093436174\n",
      "[13598,     1] loss: 0.0093435913\n",
      "[13599,     1] loss: 0.0093435645\n",
      "[13600,     1] loss: 0.0093435392\n",
      "[13601,     1] loss: 0.0093435124\n",
      "[13602,     1] loss: 0.0093434840\n",
      "[13603,     1] loss: 0.0093434595\n",
      "[13604,     1] loss: 0.0093434334\n",
      "[13605,     1] loss: 0.0093434073\n",
      "[13606,     1] loss: 0.0093433805\n",
      "[13607,     1] loss: 0.0093433551\n",
      "[13608,     1] loss: 0.0093433283\n",
      "[13609,     1] loss: 0.0093433015\n",
      "[13610,     1] loss: 0.0093432754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13611,     1] loss: 0.0093432501\n",
      "[13612,     1] loss: 0.0093432233\n",
      "[13613,     1] loss: 0.0093431972\n",
      "[13614,     1] loss: 0.0093431711\n",
      "[13615,     1] loss: 0.0093431436\n",
      "[13616,     1] loss: 0.0093431175\n",
      "[13617,     1] loss: 0.0093430914\n",
      "[13618,     1] loss: 0.0093430653\n",
      "[13619,     1] loss: 0.0093430392\n",
      "[13620,     1] loss: 0.0093430139\n",
      "[13621,     1] loss: 0.0093429863\n",
      "[13622,     1] loss: 0.0093429603\n",
      "[13623,     1] loss: 0.0093429349\n",
      "[13624,     1] loss: 0.0093429103\n",
      "[13625,     1] loss: 0.0093428820\n",
      "[13626,     1] loss: 0.0093428560\n",
      "[13627,     1] loss: 0.0093428306\n",
      "[13628,     1] loss: 0.0093428038\n",
      "[13629,     1] loss: 0.0093427770\n",
      "[13630,     1] loss: 0.0093427509\n",
      "[13631,     1] loss: 0.0093427241\n",
      "[13632,     1] loss: 0.0093426988\n",
      "[13633,     1] loss: 0.0093426734\n",
      "[13634,     1] loss: 0.0093426459\n",
      "[13635,     1] loss: 0.0093426205\n",
      "[13636,     1] loss: 0.0093425944\n",
      "[13637,     1] loss: 0.0093425684\n",
      "[13638,     1] loss: 0.0093425423\n",
      "[13639,     1] loss: 0.0093425162\n",
      "[13640,     1] loss: 0.0093424909\n",
      "[13641,     1] loss: 0.0093424641\n",
      "[13642,     1] loss: 0.0093424380\n",
      "[13643,     1] loss: 0.0093424127\n",
      "[13644,     1] loss: 0.0093423866\n",
      "[13645,     1] loss: 0.0093423605\n",
      "[13646,     1] loss: 0.0093423337\n",
      "[13647,     1] loss: 0.0093423083\n",
      "[13648,     1] loss: 0.0093422823\n",
      "[13649,     1] loss: 0.0093422562\n",
      "[13650,     1] loss: 0.0093422309\n",
      "[13651,     1] loss: 0.0093422048\n",
      "[13652,     1] loss: 0.0093421787\n",
      "[13653,     1] loss: 0.0093421534\n",
      "[13654,     1] loss: 0.0093421273\n",
      "[13655,     1] loss: 0.0093421012\n",
      "[13656,     1] loss: 0.0093420751\n",
      "[13657,     1] loss: 0.0093420513\n",
      "[13658,     1] loss: 0.0093420237\n",
      "[13659,     1] loss: 0.0093419977\n",
      "[13660,     1] loss: 0.0093419731\n",
      "[13661,     1] loss: 0.0093419462\n",
      "[13662,     1] loss: 0.0093419194\n",
      "[13663,     1] loss: 0.0093418956\n",
      "[13664,     1] loss: 0.0093418695\n",
      "[13665,     1] loss: 0.0093418427\n",
      "[13666,     1] loss: 0.0093418173\n",
      "[13667,     1] loss: 0.0093417920\n",
      "[13668,     1] loss: 0.0093417667\n",
      "[13669,     1] loss: 0.0093417399\n",
      "[13670,     1] loss: 0.0093417145\n",
      "[13671,     1] loss: 0.0093416877\n",
      "[13672,     1] loss: 0.0093416624\n",
      "[13673,     1] loss: 0.0093416363\n",
      "[13674,     1] loss: 0.0093416102\n",
      "[13675,     1] loss: 0.0093415834\n",
      "[13676,     1] loss: 0.0093415588\n",
      "[13677,     1] loss: 0.0093415327\n",
      "[13678,     1] loss: 0.0093415067\n",
      "[13679,     1] loss: 0.0093414813\n",
      "[13680,     1] loss: 0.0093414560\n",
      "[13681,     1] loss: 0.0093414292\n",
      "[13682,     1] loss: 0.0093414031\n",
      "[13683,     1] loss: 0.0093413785\n",
      "[13684,     1] loss: 0.0093413532\n",
      "[13685,     1] loss: 0.0093413264\n",
      "[13686,     1] loss: 0.0093413025\n",
      "[13687,     1] loss: 0.0093412764\n",
      "[13688,     1] loss: 0.0093412511\n",
      "[13689,     1] loss: 0.0093412250\n",
      "[13690,     1] loss: 0.0093411990\n",
      "[13691,     1] loss: 0.0093411736\n",
      "[13692,     1] loss: 0.0093411483\n",
      "[13693,     1] loss: 0.0093411230\n",
      "[13694,     1] loss: 0.0093410976\n",
      "[13695,     1] loss: 0.0093410715\n",
      "[13696,     1] loss: 0.0093410470\n",
      "[13697,     1] loss: 0.0093410201\n",
      "[13698,     1] loss: 0.0093409948\n",
      "[13699,     1] loss: 0.0093409687\n",
      "[13700,     1] loss: 0.0093409441\n",
      "[13701,     1] loss: 0.0093409173\n",
      "[13702,     1] loss: 0.0093408912\n",
      "[13703,     1] loss: 0.0093408674\n",
      "[13704,     1] loss: 0.0093408421\n",
      "[13705,     1] loss: 0.0093408152\n",
      "[13706,     1] loss: 0.0093407914\n",
      "[13707,     1] loss: 0.0093407653\n",
      "[13708,     1] loss: 0.0093407393\n",
      "[13709,     1] loss: 0.0093407132\n",
      "[13710,     1] loss: 0.0093406878\n",
      "[13711,     1] loss: 0.0093406618\n",
      "[13712,     1] loss: 0.0093406372\n",
      "[13713,     1] loss: 0.0093406104\n",
      "[13714,     1] loss: 0.0093405858\n",
      "[13715,     1] loss: 0.0093405612\n",
      "[13716,     1] loss: 0.0093405358\n",
      "[13717,     1] loss: 0.0093405098\n",
      "[13718,     1] loss: 0.0093404837\n",
      "[13719,     1] loss: 0.0093404584\n",
      "[13720,     1] loss: 0.0093404338\n",
      "[13721,     1] loss: 0.0093404092\n",
      "[13722,     1] loss: 0.0093403831\n",
      "[13723,     1] loss: 0.0093403578\n",
      "[13724,     1] loss: 0.0093403324\n",
      "[13725,     1] loss: 0.0093403071\n",
      "[13726,     1] loss: 0.0093402825\n",
      "[13727,     1] loss: 0.0093402565\n",
      "[13728,     1] loss: 0.0093402319\n",
      "[13729,     1] loss: 0.0093402058\n",
      "[13730,     1] loss: 0.0093401819\n",
      "[13731,     1] loss: 0.0093401551\n",
      "[13732,     1] loss: 0.0093401298\n",
      "[13733,     1] loss: 0.0093401045\n",
      "[13734,     1] loss: 0.0093400806\n",
      "[13735,     1] loss: 0.0093400531\n",
      "[13736,     1] loss: 0.0093400292\n",
      "[13737,     1] loss: 0.0093400046\n",
      "[13738,     1] loss: 0.0093399785\n",
      "[13739,     1] loss: 0.0093399532\n",
      "[13740,     1] loss: 0.0093399264\n",
      "[13741,     1] loss: 0.0093399025\n",
      "[13742,     1] loss: 0.0093398757\n",
      "[13743,     1] loss: 0.0093398504\n",
      "[13744,     1] loss: 0.0093398258\n",
      "[13745,     1] loss: 0.0093398005\n",
      "[13746,     1] loss: 0.0093397766\n",
      "[13747,     1] loss: 0.0093397506\n",
      "[13748,     1] loss: 0.0093397252\n",
      "[13749,     1] loss: 0.0093396991\n",
      "[13750,     1] loss: 0.0093396746\n",
      "[13751,     1] loss: 0.0093396492\n",
      "[13752,     1] loss: 0.0093396246\n",
      "[13753,     1] loss: 0.0093396008\n",
      "[13754,     1] loss: 0.0093395732\n",
      "[13755,     1] loss: 0.0093395479\n",
      "[13756,     1] loss: 0.0093395241\n",
      "[13757,     1] loss: 0.0093394980\n",
      "[13758,     1] loss: 0.0093394727\n",
      "[13759,     1] loss: 0.0093394473\n",
      "[13760,     1] loss: 0.0093394227\n",
      "[13761,     1] loss: 0.0093393974\n",
      "[13762,     1] loss: 0.0093393721\n",
      "[13763,     1] loss: 0.0093393475\n",
      "[13764,     1] loss: 0.0093393221\n",
      "[13765,     1] loss: 0.0093392976\n",
      "[13766,     1] loss: 0.0093392722\n",
      "[13767,     1] loss: 0.0093392469\n",
      "[13768,     1] loss: 0.0093392223\n",
      "[13769,     1] loss: 0.0093391970\n",
      "[13770,     1] loss: 0.0093391716\n",
      "[13771,     1] loss: 0.0093391463\n",
      "[13772,     1] loss: 0.0093391225\n",
      "[13773,     1] loss: 0.0093390971\n",
      "[13774,     1] loss: 0.0093390726\n",
      "[13775,     1] loss: 0.0093390472\n",
      "[13776,     1] loss: 0.0093390234\n",
      "[13777,     1] loss: 0.0093389980\n",
      "[13778,     1] loss: 0.0093389720\n",
      "[13779,     1] loss: 0.0093389489\n",
      "[13780,     1] loss: 0.0093389221\n",
      "[13781,     1] loss: 0.0093388982\n",
      "[13782,     1] loss: 0.0093388736\n",
      "[13783,     1] loss: 0.0093388468\n",
      "[13784,     1] loss: 0.0093388230\n",
      "[13785,     1] loss: 0.0093387991\n",
      "[13786,     1] loss: 0.0093387738\n",
      "[13787,     1] loss: 0.0093387492\n",
      "[13788,     1] loss: 0.0093387239\n",
      "[13789,     1] loss: 0.0093387008\n",
      "[13790,     1] loss: 0.0093386747\n",
      "[13791,     1] loss: 0.0093386501\n",
      "[13792,     1] loss: 0.0093386255\n",
      "[13793,     1] loss: 0.0093386009\n",
      "[13794,     1] loss: 0.0093385763\n",
      "[13795,     1] loss: 0.0093385510\n",
      "[13796,     1] loss: 0.0093385257\n",
      "[13797,     1] loss: 0.0093385026\n",
      "[13798,     1] loss: 0.0093384765\n",
      "[13799,     1] loss: 0.0093384512\n",
      "[13800,     1] loss: 0.0093384258\n",
      "[13801,     1] loss: 0.0093384027\n",
      "[13802,     1] loss: 0.0093383767\n",
      "[13803,     1] loss: 0.0093383506\n",
      "[13804,     1] loss: 0.0093383282\n",
      "[13805,     1] loss: 0.0093383029\n",
      "[13806,     1] loss: 0.0093382768\n",
      "[13807,     1] loss: 0.0093382537\n",
      "[13808,     1] loss: 0.0093382291\n",
      "[13809,     1] loss: 0.0093382031\n",
      "[13810,     1] loss: 0.0093381777\n",
      "[13811,     1] loss: 0.0093381546\n",
      "[13812,     1] loss: 0.0093381308\n",
      "[13813,     1] loss: 0.0093381055\n",
      "[13814,     1] loss: 0.0093380801\n",
      "[13815,     1] loss: 0.0093380548\n",
      "[13816,     1] loss: 0.0093380302\n",
      "[13817,     1] loss: 0.0093380056\n",
      "[13818,     1] loss: 0.0093379810\n",
      "[13819,     1] loss: 0.0093379565\n",
      "[13820,     1] loss: 0.0093379304\n",
      "[13821,     1] loss: 0.0093379073\n",
      "[13822,     1] loss: 0.0093378820\n",
      "[13823,     1] loss: 0.0093378574\n",
      "[13824,     1] loss: 0.0093378320\n",
      "[13825,     1] loss: 0.0093378089\n",
      "[13826,     1] loss: 0.0093377836\n",
      "[13827,     1] loss: 0.0093377598\n",
      "[13828,     1] loss: 0.0093377344\n",
      "[13829,     1] loss: 0.0093377113\n",
      "[13830,     1] loss: 0.0093376853\n",
      "[13831,     1] loss: 0.0093376607\n",
      "[13832,     1] loss: 0.0093376361\n",
      "[13833,     1] loss: 0.0093376122\n",
      "[13834,     1] loss: 0.0093375869\n",
      "[13835,     1] loss: 0.0093375638\n",
      "[13836,     1] loss: 0.0093375385\n",
      "[13837,     1] loss: 0.0093375131\n",
      "[13838,     1] loss: 0.0093374908\n",
      "[13839,     1] loss: 0.0093374655\n",
      "[13840,     1] loss: 0.0093374394\n",
      "[13841,     1] loss: 0.0093374170\n",
      "[13842,     1] loss: 0.0093373924\n",
      "[13843,     1] loss: 0.0093373671\n",
      "[13844,     1] loss: 0.0093373448\n",
      "[13845,     1] loss: 0.0093373187\n",
      "[13846,     1] loss: 0.0093372956\n",
      "[13847,     1] loss: 0.0093372688\n",
      "[13848,     1] loss: 0.0093372449\n",
      "[13849,     1] loss: 0.0093372203\n",
      "[13850,     1] loss: 0.0093371965\n",
      "[13851,     1] loss: 0.0093371741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13852,     1] loss: 0.0093371488\n",
      "[13853,     1] loss: 0.0093371220\n",
      "[13854,     1] loss: 0.0093370996\n",
      "[13855,     1] loss: 0.0093370743\n",
      "[13856,     1] loss: 0.0093370490\n",
      "[13857,     1] loss: 0.0093370259\n",
      "[13858,     1] loss: 0.0093370028\n",
      "[13859,     1] loss: 0.0093369760\n",
      "[13860,     1] loss: 0.0093369506\n",
      "[13861,     1] loss: 0.0093369298\n",
      "[13862,     1] loss: 0.0093369029\n",
      "[13863,     1] loss: 0.0093368798\n",
      "[13864,     1] loss: 0.0093368553\n",
      "[13865,     1] loss: 0.0093368314\n",
      "[13866,     1] loss: 0.0093368053\n",
      "[13867,     1] loss: 0.0093367845\n",
      "[13868,     1] loss: 0.0093367577\n",
      "[13869,     1] loss: 0.0093367338\n",
      "[13870,     1] loss: 0.0093367085\n",
      "[13871,     1] loss: 0.0093366839\n",
      "[13872,     1] loss: 0.0093366608\n",
      "[13873,     1] loss: 0.0093366362\n",
      "[13874,     1] loss: 0.0093366124\n",
      "[13875,     1] loss: 0.0093365893\n",
      "[13876,     1] loss: 0.0093365647\n",
      "[13877,     1] loss: 0.0093365394\n",
      "[13878,     1] loss: 0.0093365148\n",
      "[13879,     1] loss: 0.0093364909\n",
      "[13880,     1] loss: 0.0093364671\n",
      "[13881,     1] loss: 0.0093364425\n",
      "[13882,     1] loss: 0.0093364201\n",
      "[13883,     1] loss: 0.0093363948\n",
      "[13884,     1] loss: 0.0093363702\n",
      "[13885,     1] loss: 0.0093363479\n",
      "[13886,     1] loss: 0.0093363225\n",
      "[13887,     1] loss: 0.0093362987\n",
      "[13888,     1] loss: 0.0093362734\n",
      "[13889,     1] loss: 0.0093362503\n",
      "[13890,     1] loss: 0.0093362264\n",
      "[13891,     1] loss: 0.0093362011\n",
      "[13892,     1] loss: 0.0093361773\n",
      "[13893,     1] loss: 0.0093361527\n",
      "[13894,     1] loss: 0.0093361311\n",
      "[13895,     1] loss: 0.0093361050\n",
      "[13896,     1] loss: 0.0093360826\n",
      "[13897,     1] loss: 0.0093360573\n",
      "[13898,     1] loss: 0.0093360342\n",
      "[13899,     1] loss: 0.0093360104\n",
      "[13900,     1] loss: 0.0093359865\n",
      "[13901,     1] loss: 0.0093359627\n",
      "[13902,     1] loss: 0.0093359381\n",
      "[13903,     1] loss: 0.0093359143\n",
      "[13904,     1] loss: 0.0093358897\n",
      "[13905,     1] loss: 0.0093358666\n",
      "[13906,     1] loss: 0.0093358420\n",
      "[13907,     1] loss: 0.0093358189\n",
      "[13908,     1] loss: 0.0093357943\n",
      "[13909,     1] loss: 0.0093357705\n",
      "[13910,     1] loss: 0.0093357466\n",
      "[13911,     1] loss: 0.0093357228\n",
      "[13912,     1] loss: 0.0093356982\n",
      "[13913,     1] loss: 0.0093356743\n",
      "[13914,     1] loss: 0.0093356498\n",
      "[13915,     1] loss: 0.0093356267\n",
      "[13916,     1] loss: 0.0093356021\n",
      "[13917,     1] loss: 0.0093355775\n",
      "[13918,     1] loss: 0.0093355544\n",
      "[13919,     1] loss: 0.0093355305\n",
      "[13920,     1] loss: 0.0093355067\n",
      "[13921,     1] loss: 0.0093354829\n",
      "[13922,     1] loss: 0.0093354590\n",
      "[13923,     1] loss: 0.0093354367\n",
      "[13924,     1] loss: 0.0093354113\n",
      "[13925,     1] loss: 0.0093353875\n",
      "[13926,     1] loss: 0.0093353644\n",
      "[13927,     1] loss: 0.0093353391\n",
      "[13928,     1] loss: 0.0093353160\n",
      "[13929,     1] loss: 0.0093352914\n",
      "[13930,     1] loss: 0.0093352683\n",
      "[13931,     1] loss: 0.0093352430\n",
      "[13932,     1] loss: 0.0093352206\n",
      "[13933,     1] loss: 0.0093351968\n",
      "[13934,     1] loss: 0.0093351722\n",
      "[13935,     1] loss: 0.0093351491\n",
      "[13936,     1] loss: 0.0093351245\n",
      "[13937,     1] loss: 0.0093351014\n",
      "[13938,     1] loss: 0.0093350776\n",
      "[13939,     1] loss: 0.0093350552\n",
      "[13940,     1] loss: 0.0093350284\n",
      "[13941,     1] loss: 0.0093350068\n",
      "[13942,     1] loss: 0.0093349829\n",
      "[13943,     1] loss: 0.0093349576\n",
      "[13944,     1] loss: 0.0093349345\n",
      "[13945,     1] loss: 0.0093349114\n",
      "[13946,     1] loss: 0.0093348883\n",
      "[13947,     1] loss: 0.0093348637\n",
      "[13948,     1] loss: 0.0093348399\n",
      "[13949,     1] loss: 0.0093348175\n",
      "[13950,     1] loss: 0.0093347944\n",
      "[13951,     1] loss: 0.0093347721\n",
      "[13952,     1] loss: 0.0093347460\n",
      "[13953,     1] loss: 0.0093347237\n",
      "[13954,     1] loss: 0.0093346991\n",
      "[13955,     1] loss: 0.0093346752\n",
      "[13956,     1] loss: 0.0093346536\n",
      "[13957,     1] loss: 0.0093346298\n",
      "[13958,     1] loss: 0.0093346067\n",
      "[13959,     1] loss: 0.0093345828\n",
      "[13960,     1] loss: 0.0093345605\n",
      "[13961,     1] loss: 0.0093345352\n",
      "[13962,     1] loss: 0.0093345121\n",
      "[13963,     1] loss: 0.0093344882\n",
      "[13964,     1] loss: 0.0093344644\n",
      "[13965,     1] loss: 0.0093344413\n",
      "[13966,     1] loss: 0.0093344182\n",
      "[13967,     1] loss: 0.0093343936\n",
      "[13968,     1] loss: 0.0093343690\n",
      "[13969,     1] loss: 0.0093343459\n",
      "[13970,     1] loss: 0.0093343236\n",
      "[13971,     1] loss: 0.0093342990\n",
      "[13972,     1] loss: 0.0093342759\n",
      "[13973,     1] loss: 0.0093342513\n",
      "[13974,     1] loss: 0.0093342282\n",
      "[13975,     1] loss: 0.0093342043\n",
      "[13976,     1] loss: 0.0093341820\n",
      "[13977,     1] loss: 0.0093341596\n",
      "[13978,     1] loss: 0.0093341365\n",
      "[13979,     1] loss: 0.0093341112\n",
      "[13980,     1] loss: 0.0093340874\n",
      "[13981,     1] loss: 0.0093340650\n",
      "[13982,     1] loss: 0.0093340419\n",
      "[13983,     1] loss: 0.0093340188\n",
      "[13984,     1] loss: 0.0093339942\n",
      "[13985,     1] loss: 0.0093339719\n",
      "[13986,     1] loss: 0.0093339488\n",
      "[13987,     1] loss: 0.0093339242\n",
      "[13988,     1] loss: 0.0093339011\n",
      "[13989,     1] loss: 0.0093338773\n",
      "[13990,     1] loss: 0.0093338534\n",
      "[13991,     1] loss: 0.0093338303\n",
      "[13992,     1] loss: 0.0093338057\n",
      "[13993,     1] loss: 0.0093337826\n",
      "[13994,     1] loss: 0.0093337610\n",
      "[13995,     1] loss: 0.0093337364\n",
      "[13996,     1] loss: 0.0093337141\n",
      "[13997,     1] loss: 0.0093336910\n",
      "[13998,     1] loss: 0.0093336649\n",
      "[13999,     1] loss: 0.0093336441\n",
      "[14000,     1] loss: 0.0093336210\n",
      "[14001,     1] loss: 0.0093335971\n",
      "[14002,     1] loss: 0.0093335740\n",
      "[14003,     1] loss: 0.0093335517\n",
      "[14004,     1] loss: 0.0093335263\n",
      "[14005,     1] loss: 0.0093335047\n",
      "[14006,     1] loss: 0.0093334824\n",
      "[14007,     1] loss: 0.0093334578\n",
      "[14008,     1] loss: 0.0093334340\n",
      "[14009,     1] loss: 0.0093334123\n",
      "[14010,     1] loss: 0.0093333893\n",
      "[14011,     1] loss: 0.0093333654\n",
      "[14012,     1] loss: 0.0093333423\n",
      "[14013,     1] loss: 0.0093333200\n",
      "[14014,     1] loss: 0.0093332969\n",
      "[14015,     1] loss: 0.0093332738\n",
      "[14016,     1] loss: 0.0093332507\n",
      "[14017,     1] loss: 0.0093332261\n",
      "[14018,     1] loss: 0.0093332052\n",
      "[14019,     1] loss: 0.0093331806\n",
      "[14020,     1] loss: 0.0093331568\n",
      "[14021,     1] loss: 0.0093331337\n",
      "[14022,     1] loss: 0.0093331106\n",
      "[14023,     1] loss: 0.0093330882\n",
      "[14024,     1] loss: 0.0093330644\n",
      "[14025,     1] loss: 0.0093330421\n",
      "[14026,     1] loss: 0.0093330190\n",
      "[14027,     1] loss: 0.0093329974\n",
      "[14028,     1] loss: 0.0093329743\n",
      "[14029,     1] loss: 0.0093329504\n",
      "[14030,     1] loss: 0.0093329266\n",
      "[14031,     1] loss: 0.0093329035\n",
      "[14032,     1] loss: 0.0093328804\n",
      "[14033,     1] loss: 0.0093328573\n",
      "[14034,     1] loss: 0.0093328357\n",
      "[14035,     1] loss: 0.0093328118\n",
      "[14036,     1] loss: 0.0093327880\n",
      "[14037,     1] loss: 0.0093327664\n",
      "[14038,     1] loss: 0.0093327425\n",
      "[14039,     1] loss: 0.0093327194\n",
      "[14040,     1] loss: 0.0093326949\n",
      "[14041,     1] loss: 0.0093326733\n",
      "[14042,     1] loss: 0.0093326509\n",
      "[14043,     1] loss: 0.0093326285\n",
      "[14044,     1] loss: 0.0093326047\n",
      "[14045,     1] loss: 0.0093325816\n",
      "[14046,     1] loss: 0.0093325593\n",
      "[14047,     1] loss: 0.0093325362\n",
      "[14048,     1] loss: 0.0093325131\n",
      "[14049,     1] loss: 0.0093324900\n",
      "[14050,     1] loss: 0.0093324661\n",
      "[14051,     1] loss: 0.0093324453\n",
      "[14052,     1] loss: 0.0093324207\n",
      "[14053,     1] loss: 0.0093323983\n",
      "[14054,     1] loss: 0.0093323745\n",
      "[14055,     1] loss: 0.0093323529\n",
      "[14056,     1] loss: 0.0093323283\n",
      "[14057,     1] loss: 0.0093323067\n",
      "[14058,     1] loss: 0.0093322843\n",
      "[14059,     1] loss: 0.0093322605\n",
      "[14060,     1] loss: 0.0093322359\n",
      "[14061,     1] loss: 0.0093322150\n",
      "[14062,     1] loss: 0.0093321912\n",
      "[14063,     1] loss: 0.0093321696\n",
      "[14064,     1] loss: 0.0093321465\n",
      "[14065,     1] loss: 0.0093321227\n",
      "[14066,     1] loss: 0.0093321025\n",
      "[14067,     1] loss: 0.0093320787\n",
      "[14068,     1] loss: 0.0093320556\n",
      "[14069,     1] loss: 0.0093320332\n",
      "[14070,     1] loss: 0.0093320109\n",
      "[14071,     1] loss: 0.0093319878\n",
      "[14072,     1] loss: 0.0093319654\n",
      "[14073,     1] loss: 0.0093319438\n",
      "[14074,     1] loss: 0.0093319193\n",
      "[14075,     1] loss: 0.0093318976\n",
      "[14076,     1] loss: 0.0093318753\n",
      "[14077,     1] loss: 0.0093318515\n",
      "[14078,     1] loss: 0.0093318291\n",
      "[14079,     1] loss: 0.0093318060\n",
      "[14080,     1] loss: 0.0093317837\n",
      "[14081,     1] loss: 0.0093317598\n",
      "[14082,     1] loss: 0.0093317375\n",
      "[14083,     1] loss: 0.0093317151\n",
      "[14084,     1] loss: 0.0093316935\n",
      "[14085,     1] loss: 0.0093316704\n",
      "[14086,     1] loss: 0.0093316473\n",
      "[14087,     1] loss: 0.0093316250\n",
      "[14088,     1] loss: 0.0093316011\n",
      "[14089,     1] loss: 0.0093315788\n",
      "[14090,     1] loss: 0.0093315579\n",
      "[14091,     1] loss: 0.0093315326\n",
      "[14092,     1] loss: 0.0093315110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14093,     1] loss: 0.0093314886\n",
      "[14094,     1] loss: 0.0093314663\n",
      "[14095,     1] loss: 0.0093314424\n",
      "[14096,     1] loss: 0.0093314193\n",
      "[14097,     1] loss: 0.0093313977\n",
      "[14098,     1] loss: 0.0093313761\n",
      "[14099,     1] loss: 0.0093313523\n",
      "[14100,     1] loss: 0.0093313299\n",
      "[14101,     1] loss: 0.0093313076\n",
      "[14102,     1] loss: 0.0093312852\n",
      "[14103,     1] loss: 0.0093312621\n",
      "[14104,     1] loss: 0.0093312405\n",
      "[14105,     1] loss: 0.0093312182\n",
      "[14106,     1] loss: 0.0093311951\n",
      "[14107,     1] loss: 0.0093311705\n",
      "[14108,     1] loss: 0.0093311496\n",
      "[14109,     1] loss: 0.0093311280\n",
      "[14110,     1] loss: 0.0093311049\n",
      "[14111,     1] loss: 0.0093310811\n",
      "[14112,     1] loss: 0.0093310609\n",
      "[14113,     1] loss: 0.0093310378\n",
      "[14114,     1] loss: 0.0093310162\n",
      "[14115,     1] loss: 0.0093309917\n",
      "[14116,     1] loss: 0.0093309693\n",
      "[14117,     1] loss: 0.0093309477\n",
      "[14118,     1] loss: 0.0093309253\n",
      "[14119,     1] loss: 0.0093309037\n",
      "[14120,     1] loss: 0.0093308814\n",
      "[14121,     1] loss: 0.0093308561\n",
      "[14122,     1] loss: 0.0093308359\n",
      "[14123,     1] loss: 0.0093308143\n",
      "[14124,     1] loss: 0.0093307905\n",
      "[14125,     1] loss: 0.0093307689\n",
      "[14126,     1] loss: 0.0093307465\n",
      "[14127,     1] loss: 0.0093307227\n",
      "[14128,     1] loss: 0.0093307026\n",
      "[14129,     1] loss: 0.0093306810\n",
      "[14130,     1] loss: 0.0093306564\n",
      "[14131,     1] loss: 0.0093306355\n",
      "[14132,     1] loss: 0.0093306147\n",
      "[14133,     1] loss: 0.0093305916\n",
      "[14134,     1] loss: 0.0093305685\n",
      "[14135,     1] loss: 0.0093305454\n",
      "[14136,     1] loss: 0.0093305238\n",
      "[14137,     1] loss: 0.0093305022\n",
      "[14138,     1] loss: 0.0093304783\n",
      "[14139,     1] loss: 0.0093304567\n",
      "[14140,     1] loss: 0.0093304336\n",
      "[14141,     1] loss: 0.0093304127\n",
      "[14142,     1] loss: 0.0093303882\n",
      "[14143,     1] loss: 0.0093303688\n",
      "[14144,     1] loss: 0.0093303442\n",
      "[14145,     1] loss: 0.0093303233\n",
      "[14146,     1] loss: 0.0093303010\n",
      "[14147,     1] loss: 0.0093302794\n",
      "[14148,     1] loss: 0.0093302578\n",
      "[14149,     1] loss: 0.0093302362\n",
      "[14150,     1] loss: 0.0093302138\n",
      "[14151,     1] loss: 0.0093301922\n",
      "[14152,     1] loss: 0.0093301684\n",
      "[14153,     1] loss: 0.0093301468\n",
      "[14154,     1] loss: 0.0093301259\n",
      "[14155,     1] loss: 0.0093301021\n",
      "[14156,     1] loss: 0.0093300797\n",
      "[14157,     1] loss: 0.0093300588\n",
      "[14158,     1] loss: 0.0093300365\n",
      "[14159,     1] loss: 0.0093300141\n",
      "[14160,     1] loss: 0.0093299918\n",
      "[14161,     1] loss: 0.0093299687\n",
      "[14162,     1] loss: 0.0093299471\n",
      "[14163,     1] loss: 0.0093299255\n",
      "[14164,     1] loss: 0.0093299024\n",
      "[14165,     1] loss: 0.0093298800\n",
      "[14166,     1] loss: 0.0093298584\n",
      "[14167,     1] loss: 0.0093298368\n",
      "[14168,     1] loss: 0.0093298145\n",
      "[14169,     1] loss: 0.0093297921\n",
      "[14170,     1] loss: 0.0093297690\n",
      "[14171,     1] loss: 0.0093297474\n",
      "[14172,     1] loss: 0.0093297243\n",
      "[14173,     1] loss: 0.0093297027\n",
      "[14174,     1] loss: 0.0093296804\n",
      "[14175,     1] loss: 0.0093296595\n",
      "[14176,     1] loss: 0.0093296379\n",
      "[14177,     1] loss: 0.0093296155\n",
      "[14178,     1] loss: 0.0093295939\n",
      "[14179,     1] loss: 0.0093295723\n",
      "[14180,     1] loss: 0.0093295492\n",
      "[14181,     1] loss: 0.0093295284\n",
      "[14182,     1] loss: 0.0093295053\n",
      "[14183,     1] loss: 0.0093294851\n",
      "[14184,     1] loss: 0.0093294628\n",
      "[14185,     1] loss: 0.0093294404\n",
      "[14186,     1] loss: 0.0093294188\n",
      "[14187,     1] loss: 0.0093293972\n",
      "[14188,     1] loss: 0.0093293756\n",
      "[14189,     1] loss: 0.0093293540\n",
      "[14190,     1] loss: 0.0093293324\n",
      "[14191,     1] loss: 0.0093293078\n",
      "[14192,     1] loss: 0.0093292877\n",
      "[14193,     1] loss: 0.0093292654\n",
      "[14194,     1] loss: 0.0093292423\n",
      "[14195,     1] loss: 0.0093292199\n",
      "[14196,     1] loss: 0.0093291998\n",
      "[14197,     1] loss: 0.0093291759\n",
      "[14198,     1] loss: 0.0093291566\n",
      "[14199,     1] loss: 0.0093291335\n",
      "[14200,     1] loss: 0.0093291119\n",
      "[14201,     1] loss: 0.0093290895\n",
      "[14202,     1] loss: 0.0093290679\n",
      "[14203,     1] loss: 0.0093290463\n",
      "[14204,     1] loss: 0.0093290240\n",
      "[14205,     1] loss: 0.0093290031\n",
      "[14206,     1] loss: 0.0093289815\n",
      "[14207,     1] loss: 0.0093289591\n",
      "[14208,     1] loss: 0.0093289368\n",
      "[14209,     1] loss: 0.0093289152\n",
      "[14210,     1] loss: 0.0093288943\n",
      "[14211,     1] loss: 0.0093288720\n",
      "[14212,     1] loss: 0.0093288511\n",
      "[14213,     1] loss: 0.0093288280\n",
      "[14214,     1] loss: 0.0093288086\n",
      "[14215,     1] loss: 0.0093287863\n",
      "[14216,     1] loss: 0.0093287647\n",
      "[14217,     1] loss: 0.0093287438\n",
      "[14218,     1] loss: 0.0093287207\n",
      "[14219,     1] loss: 0.0093287006\n",
      "[14220,     1] loss: 0.0093286775\n",
      "[14221,     1] loss: 0.0093286574\n",
      "[14222,     1] loss: 0.0093286343\n",
      "[14223,     1] loss: 0.0093286134\n",
      "[14224,     1] loss: 0.0093285903\n",
      "[14225,     1] loss: 0.0093285695\n",
      "[14226,     1] loss: 0.0093285479\n",
      "[14227,     1] loss: 0.0093285270\n",
      "[14228,     1] loss: 0.0093285039\n",
      "[14229,     1] loss: 0.0093284838\n",
      "[14230,     1] loss: 0.0093284622\n",
      "[14231,     1] loss: 0.0093284413\n",
      "[14232,     1] loss: 0.0093284182\n",
      "[14233,     1] loss: 0.0093283959\n",
      "[14234,     1] loss: 0.0093283750\n",
      "[14235,     1] loss: 0.0093283527\n",
      "[14236,     1] loss: 0.0093283296\n",
      "[14237,     1] loss: 0.0093283094\n",
      "[14238,     1] loss: 0.0093282878\n",
      "[14239,     1] loss: 0.0093282670\n",
      "[14240,     1] loss: 0.0093282439\n",
      "[14241,     1] loss: 0.0093282238\n",
      "[14242,     1] loss: 0.0093282014\n",
      "[14243,     1] loss: 0.0093281813\n",
      "[14244,     1] loss: 0.0093281589\n",
      "[14245,     1] loss: 0.0093281358\n",
      "[14246,     1] loss: 0.0093281150\n",
      "[14247,     1] loss: 0.0093280941\n",
      "[14248,     1] loss: 0.0093280725\n",
      "[14249,     1] loss: 0.0093280509\n",
      "[14250,     1] loss: 0.0093280308\n",
      "[14251,     1] loss: 0.0093280099\n",
      "[14252,     1] loss: 0.0093279868\n",
      "[14253,     1] loss: 0.0093279667\n",
      "[14254,     1] loss: 0.0093279459\n",
      "[14255,     1] loss: 0.0093279228\n",
      "[14256,     1] loss: 0.0093279026\n",
      "[14257,     1] loss: 0.0093278818\n",
      "[14258,     1] loss: 0.0093278594\n",
      "[14259,     1] loss: 0.0093278378\n",
      "[14260,     1] loss: 0.0093278177\n",
      "[14261,     1] loss: 0.0093277954\n",
      "[14262,     1] loss: 0.0093277745\n",
      "[14263,     1] loss: 0.0093277536\n",
      "[14264,     1] loss: 0.0093277305\n",
      "[14265,     1] loss: 0.0093277104\n",
      "[14266,     1] loss: 0.0093276896\n",
      "[14267,     1] loss: 0.0093276687\n",
      "[14268,     1] loss: 0.0093276456\n",
      "[14269,     1] loss: 0.0093276255\n",
      "[14270,     1] loss: 0.0093276046\n",
      "[14271,     1] loss: 0.0093275815\n",
      "[14272,     1] loss: 0.0093275592\n",
      "[14273,     1] loss: 0.0093275398\n",
      "[14274,     1] loss: 0.0093275182\n",
      "[14275,     1] loss: 0.0093274958\n",
      "[14276,     1] loss: 0.0093274750\n",
      "[14277,     1] loss: 0.0093274541\n",
      "[14278,     1] loss: 0.0093274310\n",
      "[14279,     1] loss: 0.0093274109\n",
      "[14280,     1] loss: 0.0093273900\n",
      "[14281,     1] loss: 0.0093273684\n",
      "[14282,     1] loss: 0.0093273468\n",
      "[14283,     1] loss: 0.0093273260\n",
      "[14284,     1] loss: 0.0093273051\n",
      "[14285,     1] loss: 0.0093272835\n",
      "[14286,     1] loss: 0.0093272626\n",
      "[14287,     1] loss: 0.0093272418\n",
      "[14288,     1] loss: 0.0093272202\n",
      "[14289,     1] loss: 0.0093271993\n",
      "[14290,     1] loss: 0.0093271792\n",
      "[14291,     1] loss: 0.0093271568\n",
      "[14292,     1] loss: 0.0093271367\n",
      "[14293,     1] loss: 0.0093271151\n",
      "[14294,     1] loss: 0.0093270928\n",
      "[14295,     1] loss: 0.0093270719\n",
      "[14296,     1] loss: 0.0093270518\n",
      "[14297,     1] loss: 0.0093270302\n",
      "[14298,     1] loss: 0.0093270086\n",
      "[14299,     1] loss: 0.0093269885\n",
      "[14300,     1] loss: 0.0093269676\n",
      "[14301,     1] loss: 0.0093269467\n",
      "[14302,     1] loss: 0.0093269251\n",
      "[14303,     1] loss: 0.0093269043\n",
      "[14304,     1] loss: 0.0093268819\n",
      "[14305,     1] loss: 0.0093268618\n",
      "[14306,     1] loss: 0.0093268402\n",
      "[14307,     1] loss: 0.0093268186\n",
      "[14308,     1] loss: 0.0093267985\n",
      "[14309,     1] loss: 0.0093267769\n",
      "[14310,     1] loss: 0.0093267553\n",
      "[14311,     1] loss: 0.0093267344\n",
      "[14312,     1] loss: 0.0093267120\n",
      "[14313,     1] loss: 0.0093266942\n",
      "[14314,     1] loss: 0.0093266726\n",
      "[14315,     1] loss: 0.0093266502\n",
      "[14316,     1] loss: 0.0093266286\n",
      "[14317,     1] loss: 0.0093266085\n",
      "[14318,     1] loss: 0.0093265869\n",
      "[14319,     1] loss: 0.0093265675\n",
      "[14320,     1] loss: 0.0093265459\n",
      "[14321,     1] loss: 0.0093265250\n",
      "[14322,     1] loss: 0.0093265042\n",
      "[14323,     1] loss: 0.0093264833\n",
      "[14324,     1] loss: 0.0093264632\n",
      "[14325,     1] loss: 0.0093264431\n",
      "[14326,     1] loss: 0.0093264215\n",
      "[14327,     1] loss: 0.0093263999\n",
      "[14328,     1] loss: 0.0093263783\n",
      "[14329,     1] loss: 0.0093263589\n",
      "[14330,     1] loss: 0.0093263388\n",
      "[14331,     1] loss: 0.0093263172\n",
      "[14332,     1] loss: 0.0093262956\n",
      "[14333,     1] loss: 0.0093262754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14334,     1] loss: 0.0093262538\n",
      "[14335,     1] loss: 0.0093262345\n",
      "[14336,     1] loss: 0.0093262129\n",
      "[14337,     1] loss: 0.0093261920\n",
      "[14338,     1] loss: 0.0093261704\n",
      "[14339,     1] loss: 0.0093261503\n",
      "[14340,     1] loss: 0.0093261302\n",
      "[14341,     1] loss: 0.0093261078\n",
      "[14342,     1] loss: 0.0093260877\n",
      "[14343,     1] loss: 0.0093260661\n",
      "[14344,     1] loss: 0.0093260437\n",
      "[14345,     1] loss: 0.0093260251\n",
      "[14346,     1] loss: 0.0093260050\n",
      "[14347,     1] loss: 0.0093259819\n",
      "[14348,     1] loss: 0.0093259610\n",
      "[14349,     1] loss: 0.0093259424\n",
      "[14350,     1] loss: 0.0093259200\n",
      "[14351,     1] loss: 0.0093258984\n",
      "[14352,     1] loss: 0.0093258783\n",
      "[14353,     1] loss: 0.0093258575\n",
      "[14354,     1] loss: 0.0093258359\n",
      "[14355,     1] loss: 0.0093258172\n",
      "[14356,     1] loss: 0.0093257971\n",
      "[14357,     1] loss: 0.0093257755\n",
      "[14358,     1] loss: 0.0093257539\n",
      "[14359,     1] loss: 0.0093257360\n",
      "[14360,     1] loss: 0.0093257144\n",
      "[14361,     1] loss: 0.0093256921\n",
      "[14362,     1] loss: 0.0093256719\n",
      "[14363,     1] loss: 0.0093256518\n",
      "[14364,     1] loss: 0.0093256317\n",
      "[14365,     1] loss: 0.0093256101\n",
      "[14366,     1] loss: 0.0093255907\n",
      "[14367,     1] loss: 0.0093255684\n",
      "[14368,     1] loss: 0.0093255468\n",
      "[14369,     1] loss: 0.0093255289\n",
      "[14370,     1] loss: 0.0093255065\n",
      "[14371,     1] loss: 0.0093254872\n",
      "[14372,     1] loss: 0.0093254663\n",
      "[14373,     1] loss: 0.0093254454\n",
      "[14374,     1] loss: 0.0093254246\n",
      "[14375,     1] loss: 0.0093254037\n",
      "[14376,     1] loss: 0.0093253843\n",
      "[14377,     1] loss: 0.0093253642\n",
      "[14378,     1] loss: 0.0093253426\n",
      "[14379,     1] loss: 0.0093253225\n",
      "[14380,     1] loss: 0.0093253016\n",
      "[14381,     1] loss: 0.0093252808\n",
      "[14382,     1] loss: 0.0093252599\n",
      "[14383,     1] loss: 0.0093252398\n",
      "[14384,     1] loss: 0.0093252182\n",
      "[14385,     1] loss: 0.0093251988\n",
      "[14386,     1] loss: 0.0093251780\n",
      "[14387,     1] loss: 0.0093251571\n",
      "[14388,     1] loss: 0.0093251370\n",
      "[14389,     1] loss: 0.0093251161\n",
      "[14390,     1] loss: 0.0093250960\n",
      "[14391,     1] loss: 0.0093250751\n",
      "[14392,     1] loss: 0.0093250558\n",
      "[14393,     1] loss: 0.0093250342\n",
      "[14394,     1] loss: 0.0093250148\n",
      "[14395,     1] loss: 0.0093249939\n",
      "[14396,     1] loss: 0.0093249731\n",
      "[14397,     1] loss: 0.0093249530\n",
      "[14398,     1] loss: 0.0093249328\n",
      "[14399,     1] loss: 0.0093249112\n",
      "[14400,     1] loss: 0.0093248911\n",
      "[14401,     1] loss: 0.0093248717\n",
      "[14402,     1] loss: 0.0093248509\n",
      "[14403,     1] loss: 0.0093248308\n",
      "[14404,     1] loss: 0.0093248092\n",
      "[14405,     1] loss: 0.0093247913\n",
      "[14406,     1] loss: 0.0093247704\n",
      "[14407,     1] loss: 0.0093247488\n",
      "[14408,     1] loss: 0.0093247302\n",
      "[14409,     1] loss: 0.0093247086\n",
      "[14410,     1] loss: 0.0093246877\n",
      "[14411,     1] loss: 0.0093246676\n",
      "[14412,     1] loss: 0.0093246482\n",
      "[14413,     1] loss: 0.0093246274\n",
      "[14414,     1] loss: 0.0093246065\n",
      "[14415,     1] loss: 0.0093245864\n",
      "[14416,     1] loss: 0.0093245655\n",
      "[14417,     1] loss: 0.0093245454\n",
      "[14418,     1] loss: 0.0093245253\n",
      "[14419,     1] loss: 0.0093245052\n",
      "[14420,     1] loss: 0.0093244843\n",
      "[14421,     1] loss: 0.0093244649\n",
      "[14422,     1] loss: 0.0093244441\n",
      "[14423,     1] loss: 0.0093244255\n",
      "[14424,     1] loss: 0.0093244031\n",
      "[14425,     1] loss: 0.0093243837\n",
      "[14426,     1] loss: 0.0093243636\n",
      "[14427,     1] loss: 0.0093243435\n",
      "[14428,     1] loss: 0.0093243226\n",
      "[14429,     1] loss: 0.0093243018\n",
      "[14430,     1] loss: 0.0093242832\n",
      "[14431,     1] loss: 0.0093242623\n",
      "[14432,     1] loss: 0.0093242414\n",
      "[14433,     1] loss: 0.0093242213\n",
      "[14434,     1] loss: 0.0093242019\n",
      "[14435,     1] loss: 0.0093241818\n",
      "[14436,     1] loss: 0.0093241610\n",
      "[14437,     1] loss: 0.0093241416\n",
      "[14438,     1] loss: 0.0093241215\n",
      "[14439,     1] loss: 0.0093241021\n",
      "[14440,     1] loss: 0.0093240805\n",
      "[14441,     1] loss: 0.0093240596\n",
      "[14442,     1] loss: 0.0093240410\n",
      "[14443,     1] loss: 0.0093240209\n",
      "[14444,     1] loss: 0.0093240000\n",
      "[14445,     1] loss: 0.0093239799\n",
      "[14446,     1] loss: 0.0093239598\n",
      "[14447,     1] loss: 0.0093239389\n",
      "[14448,     1] loss: 0.0093239188\n",
      "[14449,     1] loss: 0.0093238994\n",
      "[14450,     1] loss: 0.0093238778\n",
      "[14451,     1] loss: 0.0093238570\n",
      "[14452,     1] loss: 0.0093238398\n",
      "[14453,     1] loss: 0.0093238197\n",
      "[14454,     1] loss: 0.0093237989\n",
      "[14455,     1] loss: 0.0093237795\n",
      "[14456,     1] loss: 0.0093237594\n",
      "[14457,     1] loss: 0.0093237378\n",
      "[14458,     1] loss: 0.0093237206\n",
      "[14459,     1] loss: 0.0093236990\n",
      "[14460,     1] loss: 0.0093236782\n",
      "[14461,     1] loss: 0.0093236588\n",
      "[14462,     1] loss: 0.0093236402\n",
      "[14463,     1] loss: 0.0093236186\n",
      "[14464,     1] loss: 0.0093235984\n",
      "[14465,     1] loss: 0.0093235776\n",
      "[14466,     1] loss: 0.0093235590\n",
      "[14467,     1] loss: 0.0093235381\n",
      "[14468,     1] loss: 0.0093235187\n",
      "[14469,     1] loss: 0.0093234979\n",
      "[14470,     1] loss: 0.0093234800\n",
      "[14471,     1] loss: 0.0093234606\n",
      "[14472,     1] loss: 0.0093234397\n",
      "[14473,     1] loss: 0.0093234196\n",
      "[14474,     1] loss: 0.0093234010\n",
      "[14475,     1] loss: 0.0093233816\n",
      "[14476,     1] loss: 0.0093233615\n",
      "[14477,     1] loss: 0.0093233414\n",
      "[14478,     1] loss: 0.0093233213\n",
      "[14479,     1] loss: 0.0093233012\n",
      "[14480,     1] loss: 0.0093232818\n",
      "[14481,     1] loss: 0.0093232609\n",
      "[14482,     1] loss: 0.0093232408\n",
      "[14483,     1] loss: 0.0093232207\n",
      "[14484,     1] loss: 0.0093232021\n",
      "[14485,     1] loss: 0.0093231805\n",
      "[14486,     1] loss: 0.0093231618\n",
      "[14487,     1] loss: 0.0093231432\n",
      "[14488,     1] loss: 0.0093231224\n",
      "[14489,     1] loss: 0.0093231022\n",
      "[14490,     1] loss: 0.0093230844\n",
      "[14491,     1] loss: 0.0093230627\n",
      "[14492,     1] loss: 0.0093230441\n",
      "[14493,     1] loss: 0.0093230255\n",
      "[14494,     1] loss: 0.0093230054\n",
      "[14495,     1] loss: 0.0093229845\n",
      "[14496,     1] loss: 0.0093229659\n",
      "[14497,     1] loss: 0.0093229458\n",
      "[14498,     1] loss: 0.0093229257\n",
      "[14499,     1] loss: 0.0093229063\n",
      "[14500,     1] loss: 0.0093228854\n",
      "[14501,     1] loss: 0.0093228661\n",
      "[14502,     1] loss: 0.0093228467\n",
      "[14503,     1] loss: 0.0093228273\n",
      "[14504,     1] loss: 0.0093228072\n",
      "[14505,     1] loss: 0.0093227871\n",
      "[14506,     1] loss: 0.0093227677\n",
      "[14507,     1] loss: 0.0093227483\n",
      "[14508,     1] loss: 0.0093227282\n",
      "[14509,     1] loss: 0.0093227088\n",
      "[14510,     1] loss: 0.0093226895\n",
      "[14511,     1] loss: 0.0093226701\n",
      "[14512,     1] loss: 0.0093226507\n",
      "[14513,     1] loss: 0.0093226291\n",
      "[14514,     1] loss: 0.0093226105\n",
      "[14515,     1] loss: 0.0093225911\n",
      "[14516,     1] loss: 0.0093225718\n",
      "[14517,     1] loss: 0.0093225516\n",
      "[14518,     1] loss: 0.0093225315\n",
      "[14519,     1] loss: 0.0093225114\n",
      "[14520,     1] loss: 0.0093224920\n",
      "[14521,     1] loss: 0.0093224719\n",
      "[14522,     1] loss: 0.0093224518\n",
      "[14523,     1] loss: 0.0093224332\n",
      "[14524,     1] loss: 0.0093224138\n",
      "[14525,     1] loss: 0.0093223937\n",
      "[14526,     1] loss: 0.0093223736\n",
      "[14527,     1] loss: 0.0093223557\n",
      "[14528,     1] loss: 0.0093223341\n",
      "[14529,     1] loss: 0.0093223155\n",
      "[14530,     1] loss: 0.0093222961\n",
      "[14531,     1] loss: 0.0093222767\n",
      "[14532,     1] loss: 0.0093222566\n",
      "[14533,     1] loss: 0.0093222380\n",
      "[14534,     1] loss: 0.0093222179\n",
      "[14535,     1] loss: 0.0093221985\n",
      "[14536,     1] loss: 0.0093221784\n",
      "[14537,     1] loss: 0.0093221597\n",
      "[14538,     1] loss: 0.0093221396\n",
      "[14539,     1] loss: 0.0093221195\n",
      "[14540,     1] loss: 0.0093221001\n",
      "[14541,     1] loss: 0.0093220800\n",
      "[14542,     1] loss: 0.0093220614\n",
      "[14543,     1] loss: 0.0093220428\n",
      "[14544,     1] loss: 0.0093220226\n",
      "[14545,     1] loss: 0.0093220040\n",
      "[14546,     1] loss: 0.0093219846\n",
      "[14547,     1] loss: 0.0093219645\n",
      "[14548,     1] loss: 0.0093219444\n",
      "[14549,     1] loss: 0.0093219258\n",
      "[14550,     1] loss: 0.0093219064\n",
      "[14551,     1] loss: 0.0093218870\n",
      "[14552,     1] loss: 0.0093218677\n",
      "[14553,     1] loss: 0.0093218483\n",
      "[14554,     1] loss: 0.0093218297\n",
      "[14555,     1] loss: 0.0093218103\n",
      "[14556,     1] loss: 0.0093217902\n",
      "[14557,     1] loss: 0.0093217708\n",
      "[14558,     1] loss: 0.0093217522\n",
      "[14559,     1] loss: 0.0093217336\n",
      "[14560,     1] loss: 0.0093217127\n",
      "[14561,     1] loss: 0.0093216948\n",
      "[14562,     1] loss: 0.0093216747\n",
      "[14563,     1] loss: 0.0093216561\n",
      "[14564,     1] loss: 0.0093216352\n",
      "[14565,     1] loss: 0.0093216173\n",
      "[14566,     1] loss: 0.0093215987\n",
      "[14567,     1] loss: 0.0093215778\n",
      "[14568,     1] loss: 0.0093215600\n",
      "[14569,     1] loss: 0.0093215384\n",
      "[14570,     1] loss: 0.0093215205\n",
      "[14571,     1] loss: 0.0093215004\n",
      "[14572,     1] loss: 0.0093214840\n",
      "[14573,     1] loss: 0.0093214624\n",
      "[14574,     1] loss: 0.0093214430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14575,     1] loss: 0.0093214236\n",
      "[14576,     1] loss: 0.0093214050\n",
      "[14577,     1] loss: 0.0093213864\n",
      "[14578,     1] loss: 0.0093213663\n",
      "[14579,     1] loss: 0.0093213476\n",
      "[14580,     1] loss: 0.0093213275\n",
      "[14581,     1] loss: 0.0093213104\n",
      "[14582,     1] loss: 0.0093212895\n",
      "[14583,     1] loss: 0.0093212701\n",
      "[14584,     1] loss: 0.0093212508\n",
      "[14585,     1] loss: 0.0093212321\n",
      "[14586,     1] loss: 0.0093212120\n",
      "[14587,     1] loss: 0.0093211941\n",
      "[14588,     1] loss: 0.0093211748\n",
      "[14589,     1] loss: 0.0093211554\n",
      "[14590,     1] loss: 0.0093211368\n",
      "[14591,     1] loss: 0.0093211167\n",
      "[14592,     1] loss: 0.0093210980\n",
      "[14593,     1] loss: 0.0093210794\n",
      "[14594,     1] loss: 0.0093210585\n",
      "[14595,     1] loss: 0.0093210414\n",
      "[14596,     1] loss: 0.0093210205\n",
      "[14597,     1] loss: 0.0093210027\n",
      "[14598,     1] loss: 0.0093209833\n",
      "[14599,     1] loss: 0.0093209639\n",
      "[14600,     1] loss: 0.0093209460\n",
      "[14601,     1] loss: 0.0093209259\n",
      "[14602,     1] loss: 0.0093209080\n",
      "[14603,     1] loss: 0.0093208894\n",
      "[14604,     1] loss: 0.0093208708\n",
      "[14605,     1] loss: 0.0093208507\n",
      "[14606,     1] loss: 0.0093208313\n",
      "[14607,     1] loss: 0.0093208127\n",
      "[14608,     1] loss: 0.0093207918\n",
      "[14609,     1] loss: 0.0093207739\n",
      "[14610,     1] loss: 0.0093207560\n",
      "[14611,     1] loss: 0.0093207374\n",
      "[14612,     1] loss: 0.0093207166\n",
      "[14613,     1] loss: 0.0093206987\n",
      "[14614,     1] loss: 0.0093206793\n",
      "[14615,     1] loss: 0.0093206599\n",
      "[14616,     1] loss: 0.0093206435\n",
      "[14617,     1] loss: 0.0093206227\n",
      "[14618,     1] loss: 0.0093206048\n",
      "[14619,     1] loss: 0.0093205854\n",
      "[14620,     1] loss: 0.0093205653\n",
      "[14621,     1] loss: 0.0093205482\n",
      "[14622,     1] loss: 0.0093205288\n",
      "[14623,     1] loss: 0.0093205094\n",
      "[14624,     1] loss: 0.0093204908\n",
      "[14625,     1] loss: 0.0093204729\n",
      "[14626,     1] loss: 0.0093204528\n",
      "[14627,     1] loss: 0.0093204349\n",
      "[14628,     1] loss: 0.0093204156\n",
      "[14629,     1] loss: 0.0093203962\n",
      "[14630,     1] loss: 0.0093203783\n",
      "[14631,     1] loss: 0.0093203597\n",
      "[14632,     1] loss: 0.0093203381\n",
      "[14633,     1] loss: 0.0093203209\n",
      "[14634,     1] loss: 0.0093203016\n",
      "[14635,     1] loss: 0.0093202829\n",
      "[14636,     1] loss: 0.0093202651\n",
      "[14637,     1] loss: 0.0093202449\n",
      "[14638,     1] loss: 0.0093202263\n",
      "[14639,     1] loss: 0.0093202077\n",
      "[14640,     1] loss: 0.0093201876\n",
      "[14641,     1] loss: 0.0093201697\n",
      "[14642,     1] loss: 0.0093201526\n",
      "[14643,     1] loss: 0.0093201317\n",
      "[14644,     1] loss: 0.0093201138\n",
      "[14645,     1] loss: 0.0093200929\n",
      "[14646,     1] loss: 0.0093200751\n",
      "[14647,     1] loss: 0.0093200572\n",
      "[14648,     1] loss: 0.0093200378\n",
      "[14649,     1] loss: 0.0093200184\n",
      "[14650,     1] loss: 0.0093200013\n",
      "[14651,     1] loss: 0.0093199804\n",
      "[14652,     1] loss: 0.0093199618\n",
      "[14653,     1] loss: 0.0093199439\n",
      "[14654,     1] loss: 0.0093199246\n",
      "[14655,     1] loss: 0.0093199059\n",
      "[14656,     1] loss: 0.0093198888\n",
      "[14657,     1] loss: 0.0093198687\n",
      "[14658,     1] loss: 0.0093198508\n",
      "[14659,     1] loss: 0.0093198322\n",
      "[14660,     1] loss: 0.0093198113\n",
      "[14661,     1] loss: 0.0093197934\n",
      "[14662,     1] loss: 0.0093197756\n",
      "[14663,     1] loss: 0.0093197554\n",
      "[14664,     1] loss: 0.0093197376\n",
      "[14665,     1] loss: 0.0093197197\n",
      "[14666,     1] loss: 0.0093196996\n",
      "[14667,     1] loss: 0.0093196802\n",
      "[14668,     1] loss: 0.0093196630\n",
      "[14669,     1] loss: 0.0093196422\n",
      "[14670,     1] loss: 0.0093196250\n",
      "[14671,     1] loss: 0.0093196064\n",
      "[14672,     1] loss: 0.0093195878\n",
      "[14673,     1] loss: 0.0093195699\n",
      "[14674,     1] loss: 0.0093195505\n",
      "[14675,     1] loss: 0.0093195334\n",
      "[14676,     1] loss: 0.0093195125\n",
      "[14677,     1] loss: 0.0093194962\n",
      "[14678,     1] loss: 0.0093194768\n",
      "[14679,     1] loss: 0.0093194582\n",
      "[14680,     1] loss: 0.0093194395\n",
      "[14681,     1] loss: 0.0093194216\n",
      "[14682,     1] loss: 0.0093194023\n",
      "[14683,     1] loss: 0.0093193829\n",
      "[14684,     1] loss: 0.0093193665\n",
      "[14685,     1] loss: 0.0093193471\n",
      "[14686,     1] loss: 0.0093193278\n",
      "[14687,     1] loss: 0.0093193106\n",
      "[14688,     1] loss: 0.0093192905\n",
      "[14689,     1] loss: 0.0093192719\n",
      "[14690,     1] loss: 0.0093192548\n",
      "[14691,     1] loss: 0.0093192354\n",
      "[14692,     1] loss: 0.0093192175\n",
      "[14693,     1] loss: 0.0093191989\n",
      "[14694,     1] loss: 0.0093191817\n",
      "[14695,     1] loss: 0.0093191624\n",
      "[14696,     1] loss: 0.0093191437\n",
      "[14697,     1] loss: 0.0093191259\n",
      "[14698,     1] loss: 0.0093191080\n",
      "[14699,     1] loss: 0.0093190894\n",
      "[14700,     1] loss: 0.0093190692\n",
      "[14701,     1] loss: 0.0093190521\n",
      "[14702,     1] loss: 0.0093190342\n",
      "[14703,     1] loss: 0.0093190134\n",
      "[14704,     1] loss: 0.0093189970\n",
      "[14705,     1] loss: 0.0093189791\n",
      "[14706,     1] loss: 0.0093189597\n",
      "[14707,     1] loss: 0.0093189411\n",
      "[14708,     1] loss: 0.0093189240\n",
      "[14709,     1] loss: 0.0093189053\n",
      "[14710,     1] loss: 0.0093188874\n",
      "[14711,     1] loss: 0.0093188688\n",
      "[14712,     1] loss: 0.0093188502\n",
      "[14713,     1] loss: 0.0093188308\n",
      "[14714,     1] loss: 0.0093188114\n",
      "[14715,     1] loss: 0.0093187936\n",
      "[14716,     1] loss: 0.0093187742\n",
      "[14717,     1] loss: 0.0093187571\n",
      "[14718,     1] loss: 0.0093187377\n",
      "[14719,     1] loss: 0.0093187205\n",
      "[14720,     1] loss: 0.0093187019\n",
      "[14721,     1] loss: 0.0093186826\n",
      "[14722,     1] loss: 0.0093186662\n",
      "[14723,     1] loss: 0.0093186475\n",
      "[14724,     1] loss: 0.0093186297\n",
      "[14725,     1] loss: 0.0093186103\n",
      "[14726,     1] loss: 0.0093185924\n",
      "[14727,     1] loss: 0.0093185730\n",
      "[14728,     1] loss: 0.0093185566\n",
      "[14729,     1] loss: 0.0093185365\n",
      "[14730,     1] loss: 0.0093185179\n",
      "[14731,     1] loss: 0.0093185030\n",
      "[14732,     1] loss: 0.0093184821\n",
      "[14733,     1] loss: 0.0093184665\n",
      "[14734,     1] loss: 0.0093184471\n",
      "[14735,     1] loss: 0.0093184277\n",
      "[14736,     1] loss: 0.0093184106\n",
      "[14737,     1] loss: 0.0093183912\n",
      "[14738,     1] loss: 0.0093183741\n",
      "[14739,     1] loss: 0.0093183547\n",
      "[14740,     1] loss: 0.0093183376\n",
      "[14741,     1] loss: 0.0093183190\n",
      "[14742,     1] loss: 0.0093183018\n",
      "[14743,     1] loss: 0.0093182825\n",
      "[14744,     1] loss: 0.0093182646\n",
      "[14745,     1] loss: 0.0093182474\n",
      "[14746,     1] loss: 0.0093182281\n",
      "[14747,     1] loss: 0.0093182109\n",
      "[14748,     1] loss: 0.0093181930\n",
      "[14749,     1] loss: 0.0093181737\n",
      "[14750,     1] loss: 0.0093181573\n",
      "[14751,     1] loss: 0.0093181372\n",
      "[14752,     1] loss: 0.0093181200\n",
      "[14753,     1] loss: 0.0093181022\n",
      "[14754,     1] loss: 0.0093180835\n",
      "[14755,     1] loss: 0.0093180656\n",
      "[14756,     1] loss: 0.0093180478\n",
      "[14757,     1] loss: 0.0093180276\n",
      "[14758,     1] loss: 0.0093180113\n",
      "[14759,     1] loss: 0.0093179919\n",
      "[14760,     1] loss: 0.0093179747\n",
      "[14761,     1] loss: 0.0093179569\n",
      "[14762,     1] loss: 0.0093179382\n",
      "[14763,     1] loss: 0.0093179204\n",
      "[14764,     1] loss: 0.0093179032\n",
      "[14765,     1] loss: 0.0093178838\n",
      "[14766,     1] loss: 0.0093178667\n",
      "[14767,     1] loss: 0.0093178473\n",
      "[14768,     1] loss: 0.0093178302\n",
      "[14769,     1] loss: 0.0093178123\n",
      "[14770,     1] loss: 0.0093177944\n",
      "[14771,     1] loss: 0.0093177758\n",
      "[14772,     1] loss: 0.0093177594\n",
      "[14773,     1] loss: 0.0093177408\n",
      "[14774,     1] loss: 0.0093177229\n",
      "[14775,     1] loss: 0.0093177050\n",
      "[14776,     1] loss: 0.0093176864\n",
      "[14777,     1] loss: 0.0093176693\n",
      "[14778,     1] loss: 0.0093176514\n",
      "[14779,     1] loss: 0.0093176343\n",
      "[14780,     1] loss: 0.0093176149\n",
      "[14781,     1] loss: 0.0093175970\n",
      "[14782,     1] loss: 0.0093175791\n",
      "[14783,     1] loss: 0.0093175620\n",
      "[14784,     1] loss: 0.0093175426\n",
      "[14785,     1] loss: 0.0093175247\n",
      "[14786,     1] loss: 0.0093175083\n",
      "[14787,     1] loss: 0.0093174897\n",
      "[14788,     1] loss: 0.0093174703\n",
      "[14789,     1] loss: 0.0093174540\n",
      "[14790,     1] loss: 0.0093174353\n",
      "[14791,     1] loss: 0.0093174174\n",
      "[14792,     1] loss: 0.0093174003\n",
      "[14793,     1] loss: 0.0093173839\n",
      "[14794,     1] loss: 0.0093173653\n",
      "[14795,     1] loss: 0.0093173459\n",
      "[14796,     1] loss: 0.0093173288\n",
      "[14797,     1] loss: 0.0093173116\n",
      "[14798,     1] loss: 0.0093172923\n",
      "[14799,     1] loss: 0.0093172744\n",
      "[14800,     1] loss: 0.0093172573\n",
      "[14801,     1] loss: 0.0093172401\n",
      "[14802,     1] loss: 0.0093172215\n",
      "[14803,     1] loss: 0.0093172044\n",
      "[14804,     1] loss: 0.0093171872\n",
      "[14805,     1] loss: 0.0093171678\n",
      "[14806,     1] loss: 0.0093171492\n",
      "[14807,     1] loss: 0.0093171313\n",
      "[14808,     1] loss: 0.0093171142\n",
      "[14809,     1] loss: 0.0093170971\n",
      "[14810,     1] loss: 0.0093170799\n",
      "[14811,     1] loss: 0.0093170613\n",
      "[14812,     1] loss: 0.0093170434\n",
      "[14813,     1] loss: 0.0093170255\n",
      "[14814,     1] loss: 0.0093170092\n",
      "[14815,     1] loss: 0.0093169898\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14816,     1] loss: 0.0093169719\n",
      "[14817,     1] loss: 0.0093169563\n",
      "[14818,     1] loss: 0.0093169384\n",
      "[14819,     1] loss: 0.0093169197\n",
      "[14820,     1] loss: 0.0093169019\n",
      "[14821,     1] loss: 0.0093168847\n",
      "[14822,     1] loss: 0.0093168676\n",
      "[14823,     1] loss: 0.0093168497\n",
      "[14824,     1] loss: 0.0093168326\n",
      "[14825,     1] loss: 0.0093168132\n",
      "[14826,     1] loss: 0.0093167961\n",
      "[14827,     1] loss: 0.0093167789\n",
      "[14828,     1] loss: 0.0093167596\n",
      "[14829,     1] loss: 0.0093167424\n",
      "[14830,     1] loss: 0.0093167245\n",
      "[14831,     1] loss: 0.0093167081\n",
      "[14832,     1] loss: 0.0093166895\n",
      "[14833,     1] loss: 0.0093166716\n",
      "[14834,     1] loss: 0.0093166545\n",
      "[14835,     1] loss: 0.0093166366\n",
      "[14836,     1] loss: 0.0093166180\n",
      "[14837,     1] loss: 0.0093166016\n",
      "[14838,     1] loss: 0.0093165837\n",
      "[14839,     1] loss: 0.0093165658\n",
      "[14840,     1] loss: 0.0093165502\n",
      "[14841,     1] loss: 0.0093165331\n",
      "[14842,     1] loss: 0.0093165137\n",
      "[14843,     1] loss: 0.0093164980\n",
      "[14844,     1] loss: 0.0093164787\n",
      "[14845,     1] loss: 0.0093164608\n",
      "[14846,     1] loss: 0.0093164437\n",
      "[14847,     1] loss: 0.0093164265\n",
      "[14848,     1] loss: 0.0093164086\n",
      "[14849,     1] loss: 0.0093163908\n",
      "[14850,     1] loss: 0.0093163736\n",
      "[14851,     1] loss: 0.0093163565\n",
      "[14852,     1] loss: 0.0093163379\n",
      "[14853,     1] loss: 0.0093163215\n",
      "[14854,     1] loss: 0.0093163036\n",
      "[14855,     1] loss: 0.0093162864\n",
      "[14856,     1] loss: 0.0093162686\n",
      "[14857,     1] loss: 0.0093162522\n",
      "[14858,     1] loss: 0.0093162350\n",
      "[14859,     1] loss: 0.0093162164\n",
      "[14860,     1] loss: 0.0093161993\n",
      "[14861,     1] loss: 0.0093161814\n",
      "[14862,     1] loss: 0.0093161650\n",
      "[14863,     1] loss: 0.0093161479\n",
      "[14864,     1] loss: 0.0093161300\n",
      "[14865,     1] loss: 0.0093161128\n",
      "[14866,     1] loss: 0.0093160965\n",
      "[14867,     1] loss: 0.0093160778\n",
      "[14868,     1] loss: 0.0093160599\n",
      "[14869,     1] loss: 0.0093160436\n",
      "[14870,     1] loss: 0.0093160272\n",
      "[14871,     1] loss: 0.0093160085\n",
      "[14872,     1] loss: 0.0093159907\n",
      "[14873,     1] loss: 0.0093159735\n",
      "[14874,     1] loss: 0.0093159564\n",
      "[14875,     1] loss: 0.0093159378\n",
      "[14876,     1] loss: 0.0093159206\n",
      "[14877,     1] loss: 0.0093159035\n",
      "[14878,     1] loss: 0.0093158856\n",
      "[14879,     1] loss: 0.0093158685\n",
      "[14880,     1] loss: 0.0093158513\n",
      "[14881,     1] loss: 0.0093158342\n",
      "[14882,     1] loss: 0.0093158156\n",
      "[14883,     1] loss: 0.0093157984\n",
      "[14884,     1] loss: 0.0093157813\n",
      "[14885,     1] loss: 0.0093157656\n",
      "[14886,     1] loss: 0.0093157485\n",
      "[14887,     1] loss: 0.0093157291\n",
      "[14888,     1] loss: 0.0093157142\n",
      "[14889,     1] loss: 0.0093156934\n",
      "[14890,     1] loss: 0.0093156792\n",
      "[14891,     1] loss: 0.0093156613\n",
      "[14892,     1] loss: 0.0093156435\n",
      "[14893,     1] loss: 0.0093156256\n",
      "[14894,     1] loss: 0.0093156099\n",
      "[14895,     1] loss: 0.0093155928\n",
      "[14896,     1] loss: 0.0093155749\n",
      "[14897,     1] loss: 0.0093155585\n",
      "[14898,     1] loss: 0.0093155406\n",
      "[14899,     1] loss: 0.0093155243\n",
      "[14900,     1] loss: 0.0093155064\n",
      "[14901,     1] loss: 0.0093154885\n",
      "[14902,     1] loss: 0.0093154728\n",
      "[14903,     1] loss: 0.0093154542\n",
      "[14904,     1] loss: 0.0093154371\n",
      "[14905,     1] loss: 0.0093154199\n",
      "[14906,     1] loss: 0.0093154028\n",
      "[14907,     1] loss: 0.0093153857\n",
      "[14908,     1] loss: 0.0093153670\n",
      "[14909,     1] loss: 0.0093153521\n",
      "[14910,     1] loss: 0.0093153335\n",
      "[14911,     1] loss: 0.0093153164\n",
      "[14912,     1] loss: 0.0093153000\n",
      "[14913,     1] loss: 0.0093152806\n",
      "[14914,     1] loss: 0.0093152650\n",
      "[14915,     1] loss: 0.0093152456\n",
      "[14916,     1] loss: 0.0093152300\n",
      "[14917,     1] loss: 0.0093152151\n",
      "[14918,     1] loss: 0.0093151964\n",
      "[14919,     1] loss: 0.0093151800\n",
      "[14920,     1] loss: 0.0093151622\n",
      "[14921,     1] loss: 0.0093151450\n",
      "[14922,     1] loss: 0.0093151286\n",
      "[14923,     1] loss: 0.0093151115\n",
      "[14924,     1] loss: 0.0093150944\n",
      "[14925,     1] loss: 0.0093150772\n",
      "[14926,     1] loss: 0.0093150616\n",
      "[14927,     1] loss: 0.0093150422\n",
      "[14928,     1] loss: 0.0093150266\n",
      "[14929,     1] loss: 0.0093150094\n",
      "[14930,     1] loss: 0.0093149915\n",
      "[14931,     1] loss: 0.0093149751\n",
      "[14932,     1] loss: 0.0093149573\n",
      "[14933,     1] loss: 0.0093149409\n",
      "[14934,     1] loss: 0.0093149230\n",
      "[14935,     1] loss: 0.0093149073\n",
      "[14936,     1] loss: 0.0093148902\n",
      "[14937,     1] loss: 0.0093148731\n",
      "[14938,     1] loss: 0.0093148559\n",
      "[14939,     1] loss: 0.0093148381\n",
      "[14940,     1] loss: 0.0093148217\n",
      "[14941,     1] loss: 0.0093148053\n",
      "[14942,     1] loss: 0.0093147881\n",
      "[14943,     1] loss: 0.0093147710\n",
      "[14944,     1] loss: 0.0093147539\n",
      "[14945,     1] loss: 0.0093147367\n",
      "[14946,     1] loss: 0.0093147196\n",
      "[14947,     1] loss: 0.0093147039\n",
      "[14948,     1] loss: 0.0093146868\n",
      "[14949,     1] loss: 0.0093146697\n",
      "[14950,     1] loss: 0.0093146533\n",
      "[14951,     1] loss: 0.0093146347\n",
      "[14952,     1] loss: 0.0093146183\n",
      "[14953,     1] loss: 0.0093146011\n",
      "[14954,     1] loss: 0.0093145840\n",
      "[14955,     1] loss: 0.0093145683\n",
      "[14956,     1] loss: 0.0093145519\n",
      "[14957,     1] loss: 0.0093145348\n",
      "[14958,     1] loss: 0.0093145169\n",
      "[14959,     1] loss: 0.0093145005\n",
      "[14960,     1] loss: 0.0093144827\n",
      "[14961,     1] loss: 0.0093144655\n",
      "[14962,     1] loss: 0.0093144484\n",
      "[14963,     1] loss: 0.0093144320\n",
      "[14964,     1] loss: 0.0093144149\n",
      "[14965,     1] loss: 0.0093143977\n",
      "[14966,     1] loss: 0.0093143813\n",
      "[14967,     1] loss: 0.0093143642\n",
      "[14968,     1] loss: 0.0093143478\n",
      "[14969,     1] loss: 0.0093143307\n",
      "[14970,     1] loss: 0.0093143120\n",
      "[14971,     1] loss: 0.0093142979\n",
      "[14972,     1] loss: 0.0093142815\n",
      "[14973,     1] loss: 0.0093142621\n",
      "[14974,     1] loss: 0.0093142480\n",
      "[14975,     1] loss: 0.0093142301\n",
      "[14976,     1] loss: 0.0093142144\n",
      "[14977,     1] loss: 0.0093141966\n",
      "[14978,     1] loss: 0.0093141794\n",
      "[14979,     1] loss: 0.0093141630\n",
      "[14980,     1] loss: 0.0093141459\n",
      "[14981,     1] loss: 0.0093141295\n",
      "[14982,     1] loss: 0.0093141116\n",
      "[14983,     1] loss: 0.0093140960\n",
      "[14984,     1] loss: 0.0093140796\n",
      "[14985,     1] loss: 0.0093140624\n",
      "[14986,     1] loss: 0.0093140461\n",
      "[14987,     1] loss: 0.0093140289\n",
      "[14988,     1] loss: 0.0093140133\n",
      "[14989,     1] loss: 0.0093139961\n",
      "[14990,     1] loss: 0.0093139790\n",
      "[14991,     1] loss: 0.0093139626\n",
      "[14992,     1] loss: 0.0093139455\n",
      "[14993,     1] loss: 0.0093139291\n",
      "[14994,     1] loss: 0.0093139142\n",
      "[14995,     1] loss: 0.0093138970\n",
      "[14996,     1] loss: 0.0093138792\n",
      "[14997,     1] loss: 0.0093138643\n",
      "[14998,     1] loss: 0.0093138464\n",
      "[14999,     1] loss: 0.0093138285\n",
      "[15000,     1] loss: 0.0093138121\n",
      "[15001,     1] loss: 0.0093137957\n",
      "[15002,     1] loss: 0.0093137808\n",
      "[15003,     1] loss: 0.0093137622\n",
      "[15004,     1] loss: 0.0093137458\n",
      "[15005,     1] loss: 0.0093137294\n",
      "[15006,     1] loss: 0.0093137115\n",
      "[15007,     1] loss: 0.0093136966\n",
      "[15008,     1] loss: 0.0093136787\n",
      "[15009,     1] loss: 0.0093136638\n",
      "[15010,     1] loss: 0.0093136474\n",
      "[15011,     1] loss: 0.0093136303\n",
      "[15012,     1] loss: 0.0093136124\n",
      "[15013,     1] loss: 0.0093135983\n",
      "[15014,     1] loss: 0.0093135811\n",
      "[15015,     1] loss: 0.0093135640\n",
      "[15016,     1] loss: 0.0093135469\n",
      "[15017,     1] loss: 0.0093135297\n",
      "[15018,     1] loss: 0.0093135141\n",
      "[15019,     1] loss: 0.0093134992\n",
      "[15020,     1] loss: 0.0093134828\n",
      "[15021,     1] loss: 0.0093134642\n",
      "[15022,     1] loss: 0.0093134478\n",
      "[15023,     1] loss: 0.0093134321\n",
      "[15024,     1] loss: 0.0093134157\n",
      "[15025,     1] loss: 0.0093133993\n",
      "[15026,     1] loss: 0.0093133822\n",
      "[15027,     1] loss: 0.0093133681\n",
      "[15028,     1] loss: 0.0093133487\n",
      "[15029,     1] loss: 0.0093133338\n",
      "[15030,     1] loss: 0.0093133166\n",
      "[15031,     1] loss: 0.0093132995\n",
      "[15032,     1] loss: 0.0093132831\n",
      "[15033,     1] loss: 0.0093132652\n",
      "[15034,     1] loss: 0.0093132503\n",
      "[15035,     1] loss: 0.0093132347\n",
      "[15036,     1] loss: 0.0093132176\n",
      "[15037,     1] loss: 0.0093131997\n",
      "[15038,     1] loss: 0.0093131848\n",
      "[15039,     1] loss: 0.0093131691\n",
      "[15040,     1] loss: 0.0093131512\n",
      "[15041,     1] loss: 0.0093131348\n",
      "[15042,     1] loss: 0.0093131192\n",
      "[15043,     1] loss: 0.0093131013\n",
      "[15044,     1] loss: 0.0093130857\n",
      "[15045,     1] loss: 0.0093130700\n",
      "[15046,     1] loss: 0.0093130521\n",
      "[15047,     1] loss: 0.0093130365\n",
      "[15048,     1] loss: 0.0093130201\n",
      "[15049,     1] loss: 0.0093130030\n",
      "[15050,     1] loss: 0.0093129873\n",
      "[15051,     1] loss: 0.0093129702\n",
      "[15052,     1] loss: 0.0093129531\n",
      "[15053,     1] loss: 0.0093129389\n",
      "[15054,     1] loss: 0.0093129210\n",
      "[15055,     1] loss: 0.0093129054\n",
      "[15056,     1] loss: 0.0093128890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15057,     1] loss: 0.0093128741\n",
      "[15058,     1] loss: 0.0093128569\n",
      "[15059,     1] loss: 0.0093128383\n",
      "[15060,     1] loss: 0.0093128234\n",
      "[15061,     1] loss: 0.0093128078\n",
      "[15062,     1] loss: 0.0093127914\n",
      "[15063,     1] loss: 0.0093127750\n",
      "[15064,     1] loss: 0.0093127593\n",
      "[15065,     1] loss: 0.0093127429\n",
      "[15066,     1] loss: 0.0093127258\n",
      "[15067,     1] loss: 0.0093127102\n",
      "[15068,     1] loss: 0.0093126945\n",
      "[15069,     1] loss: 0.0093126774\n",
      "[15070,     1] loss: 0.0093126617\n",
      "[15071,     1] loss: 0.0093126453\n",
      "[15072,     1] loss: 0.0093126304\n",
      "[15073,     1] loss: 0.0093126126\n",
      "[15074,     1] loss: 0.0093125969\n",
      "[15075,     1] loss: 0.0093125813\n",
      "[15076,     1] loss: 0.0093125641\n",
      "[15077,     1] loss: 0.0093125492\n",
      "[15078,     1] loss: 0.0093125336\n",
      "[15079,     1] loss: 0.0093125157\n",
      "[15080,     1] loss: 0.0093125008\n",
      "[15081,     1] loss: 0.0093124837\n",
      "[15082,     1] loss: 0.0093124673\n",
      "[15083,     1] loss: 0.0093124509\n",
      "[15084,     1] loss: 0.0093124337\n",
      "[15085,     1] loss: 0.0093124188\n",
      "[15086,     1] loss: 0.0093124025\n",
      "[15087,     1] loss: 0.0093123853\n",
      "[15088,     1] loss: 0.0093123712\n",
      "[15089,     1] loss: 0.0093123548\n",
      "[15090,     1] loss: 0.0093123376\n",
      "[15091,     1] loss: 0.0093123235\n",
      "[15092,     1] loss: 0.0093123071\n",
      "[15093,     1] loss: 0.0093122892\n",
      "[15094,     1] loss: 0.0093122743\n",
      "[15095,     1] loss: 0.0093122579\n",
      "[15096,     1] loss: 0.0093122415\n",
      "[15097,     1] loss: 0.0093122251\n",
      "[15098,     1] loss: 0.0093122087\n",
      "[15099,     1] loss: 0.0093121916\n",
      "[15100,     1] loss: 0.0093121767\n",
      "[15101,     1] loss: 0.0093121603\n",
      "[15102,     1] loss: 0.0093121447\n",
      "[15103,     1] loss: 0.0093121275\n",
      "[15104,     1] loss: 0.0093121119\n",
      "[15105,     1] loss: 0.0093120962\n",
      "[15106,     1] loss: 0.0093120798\n",
      "[15107,     1] loss: 0.0093120635\n",
      "[15108,     1] loss: 0.0093120471\n",
      "[15109,     1] loss: 0.0093120307\n",
      "[15110,     1] loss: 0.0093120150\n",
      "[15111,     1] loss: 0.0093119986\n",
      "[15112,     1] loss: 0.0093119837\n",
      "[15113,     1] loss: 0.0093119681\n",
      "[15114,     1] loss: 0.0093119510\n",
      "[15115,     1] loss: 0.0093119346\n",
      "[15116,     1] loss: 0.0093119204\n",
      "[15117,     1] loss: 0.0093119018\n",
      "[15118,     1] loss: 0.0093118884\n",
      "[15119,     1] loss: 0.0093118697\n",
      "[15120,     1] loss: 0.0093118563\n",
      "[15121,     1] loss: 0.0093118384\n",
      "[15122,     1] loss: 0.0093118221\n",
      "[15123,     1] loss: 0.0093118079\n",
      "[15124,     1] loss: 0.0093117923\n",
      "[15125,     1] loss: 0.0093117751\n",
      "[15126,     1] loss: 0.0093117580\n",
      "[15127,     1] loss: 0.0093117423\n",
      "[15128,     1] loss: 0.0093117282\n",
      "[15129,     1] loss: 0.0093117103\n",
      "[15130,     1] loss: 0.0093116939\n",
      "[15131,     1] loss: 0.0093116783\n",
      "[15132,     1] loss: 0.0093116626\n",
      "[15133,     1] loss: 0.0093116477\n",
      "[15134,     1] loss: 0.0093116306\n",
      "[15135,     1] loss: 0.0093116157\n",
      "[15136,     1] loss: 0.0093115985\n",
      "[15137,     1] loss: 0.0093115829\n",
      "[15138,     1] loss: 0.0093115665\n",
      "[15139,     1] loss: 0.0093115509\n",
      "[15140,     1] loss: 0.0093115352\n",
      "[15141,     1] loss: 0.0093115203\n",
      "[15142,     1] loss: 0.0093115024\n",
      "[15143,     1] loss: 0.0093114875\n",
      "[15144,     1] loss: 0.0093114726\n",
      "[15145,     1] loss: 0.0093114570\n",
      "[15146,     1] loss: 0.0093114406\n",
      "[15147,     1] loss: 0.0093114242\n",
      "[15148,     1] loss: 0.0093114093\n",
      "[15149,     1] loss: 0.0093113944\n",
      "[15150,     1] loss: 0.0093113765\n",
      "[15151,     1] loss: 0.0093113616\n",
      "[15152,     1] loss: 0.0093113475\n",
      "[15153,     1] loss: 0.0093113311\n",
      "[15154,     1] loss: 0.0093113154\n",
      "[15155,     1] loss: 0.0093112983\n",
      "[15156,     1] loss: 0.0093112834\n",
      "[15157,     1] loss: 0.0093112677\n",
      "[15158,     1] loss: 0.0093112513\n",
      "[15159,     1] loss: 0.0093112357\n",
      "[15160,     1] loss: 0.0093112208\n",
      "[15161,     1] loss: 0.0093112051\n",
      "[15162,     1] loss: 0.0093111880\n",
      "[15163,     1] loss: 0.0093111739\n",
      "[15164,     1] loss: 0.0093111560\n",
      "[15165,     1] loss: 0.0093111418\n",
      "[15166,     1] loss: 0.0093111254\n",
      "[15167,     1] loss: 0.0093111083\n",
      "[15168,     1] loss: 0.0093110934\n",
      "[15169,     1] loss: 0.0093110770\n",
      "[15170,     1] loss: 0.0093110614\n",
      "[15171,     1] loss: 0.0093110457\n",
      "[15172,     1] loss: 0.0093110301\n",
      "[15173,     1] loss: 0.0093110152\n",
      "[15174,     1] loss: 0.0093110003\n",
      "[15175,     1] loss: 0.0093109831\n",
      "[15176,     1] loss: 0.0093109675\n",
      "[15177,     1] loss: 0.0093109526\n",
      "[15178,     1] loss: 0.0093109354\n",
      "[15179,     1] loss: 0.0093109205\n",
      "[15180,     1] loss: 0.0093109064\n",
      "[15181,     1] loss: 0.0093108892\n",
      "[15182,     1] loss: 0.0093108721\n",
      "[15183,     1] loss: 0.0093108587\n",
      "[15184,     1] loss: 0.0093108423\n",
      "[15185,     1] loss: 0.0093108274\n",
      "[15186,     1] loss: 0.0093108110\n",
      "[15187,     1] loss: 0.0093107954\n",
      "[15188,     1] loss: 0.0093107812\n",
      "[15189,     1] loss: 0.0093107641\n",
      "[15190,     1] loss: 0.0093107492\n",
      "[15191,     1] loss: 0.0093107335\n",
      "[15192,     1] loss: 0.0093107171\n",
      "[15193,     1] loss: 0.0093107030\n",
      "[15194,     1] loss: 0.0093106866\n",
      "[15195,     1] loss: 0.0093106724\n",
      "[15196,     1] loss: 0.0093106568\n",
      "[15197,     1] loss: 0.0093106404\n",
      "[15198,     1] loss: 0.0093106255\n",
      "[15199,     1] loss: 0.0093106106\n",
      "[15200,     1] loss: 0.0093105942\n",
      "[15201,     1] loss: 0.0093105786\n",
      "[15202,     1] loss: 0.0093105637\n",
      "[15203,     1] loss: 0.0093105480\n",
      "[15204,     1] loss: 0.0093105324\n",
      "[15205,     1] loss: 0.0093105160\n",
      "[15206,     1] loss: 0.0093104996\n",
      "[15207,     1] loss: 0.0093104847\n",
      "[15208,     1] loss: 0.0093104690\n",
      "[15209,     1] loss: 0.0093104541\n",
      "[15210,     1] loss: 0.0093104377\n",
      "[15211,     1] loss: 0.0093104228\n",
      "[15212,     1] loss: 0.0093104064\n",
      "[15213,     1] loss: 0.0093103923\n",
      "[15214,     1] loss: 0.0093103766\n",
      "[15215,     1] loss: 0.0093103595\n",
      "[15216,     1] loss: 0.0093103446\n",
      "[15217,     1] loss: 0.0093103297\n",
      "[15218,     1] loss: 0.0093103155\n",
      "[15219,     1] loss: 0.0093102977\n",
      "[15220,     1] loss: 0.0093102828\n",
      "[15221,     1] loss: 0.0093102679\n",
      "[15222,     1] loss: 0.0093102522\n",
      "[15223,     1] loss: 0.0093102366\n",
      "[15224,     1] loss: 0.0093102217\n",
      "[15225,     1] loss: 0.0093102060\n",
      "[15226,     1] loss: 0.0093101904\n",
      "[15227,     1] loss: 0.0093101762\n",
      "[15228,     1] loss: 0.0093101598\n",
      "[15229,     1] loss: 0.0093101434\n",
      "[15230,     1] loss: 0.0093101285\n",
      "[15231,     1] loss: 0.0093101136\n",
      "[15232,     1] loss: 0.0093100987\n",
      "[15233,     1] loss: 0.0093100846\n",
      "[15234,     1] loss: 0.0093100689\n",
      "[15235,     1] loss: 0.0093100518\n",
      "[15236,     1] loss: 0.0093100362\n",
      "[15237,     1] loss: 0.0093100220\n",
      "[15238,     1] loss: 0.0093100049\n",
      "[15239,     1] loss: 0.0093099914\n",
      "[15240,     1] loss: 0.0093099773\n",
      "[15241,     1] loss: 0.0093099602\n",
      "[15242,     1] loss: 0.0093099460\n",
      "[15243,     1] loss: 0.0093099311\n",
      "[15244,     1] loss: 0.0093099162\n",
      "[15245,     1] loss: 0.0093098998\n",
      "[15246,     1] loss: 0.0093098834\n",
      "[15247,     1] loss: 0.0093098693\n",
      "[15248,     1] loss: 0.0093098536\n",
      "[15249,     1] loss: 0.0093098380\n",
      "[15250,     1] loss: 0.0093098223\n",
      "[15251,     1] loss: 0.0093098074\n",
      "[15252,     1] loss: 0.0093097933\n",
      "[15253,     1] loss: 0.0093097776\n",
      "[15254,     1] loss: 0.0093097620\n",
      "[15255,     1] loss: 0.0093097471\n",
      "[15256,     1] loss: 0.0093097292\n",
      "[15257,     1] loss: 0.0093097158\n",
      "[15258,     1] loss: 0.0093097001\n",
      "[15259,     1] loss: 0.0093096845\n",
      "[15260,     1] loss: 0.0093096673\n",
      "[15261,     1] loss: 0.0093096539\n",
      "[15262,     1] loss: 0.0093096390\n",
      "[15263,     1] loss: 0.0093096234\n",
      "[15264,     1] loss: 0.0093096085\n",
      "[15265,     1] loss: 0.0093095928\n",
      "[15266,     1] loss: 0.0093095779\n",
      "[15267,     1] loss: 0.0093095630\n",
      "[15268,     1] loss: 0.0093095481\n",
      "[15269,     1] loss: 0.0093095325\n",
      "[15270,     1] loss: 0.0093095161\n",
      "[15271,     1] loss: 0.0093095042\n",
      "[15272,     1] loss: 0.0093094870\n",
      "[15273,     1] loss: 0.0093094699\n",
      "[15274,     1] loss: 0.0093094572\n",
      "[15275,     1] loss: 0.0093094416\n",
      "[15276,     1] loss: 0.0093094260\n",
      "[15277,     1] loss: 0.0093094110\n",
      "[15278,     1] loss: 0.0093093961\n",
      "[15279,     1] loss: 0.0093093812\n",
      "[15280,     1] loss: 0.0093093663\n",
      "[15281,     1] loss: 0.0093093507\n",
      "[15282,     1] loss: 0.0093093365\n",
      "[15283,     1] loss: 0.0093093202\n",
      "[15284,     1] loss: 0.0093093038\n",
      "[15285,     1] loss: 0.0093092903\n",
      "[15286,     1] loss: 0.0093092754\n",
      "[15287,     1] loss: 0.0093092598\n",
      "[15288,     1] loss: 0.0093092434\n",
      "[15289,     1] loss: 0.0093092293\n",
      "[15290,     1] loss: 0.0093092151\n",
      "[15291,     1] loss: 0.0093091995\n",
      "[15292,     1] loss: 0.0093091838\n",
      "[15293,     1] loss: 0.0093091697\n",
      "[15294,     1] loss: 0.0093091540\n",
      "[15295,     1] loss: 0.0093091391\n",
      "[15296,     1] loss: 0.0093091242\n",
      "[15297,     1] loss: 0.0093091078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15298,     1] loss: 0.0093090937\n",
      "[15299,     1] loss: 0.0093090780\n",
      "[15300,     1] loss: 0.0093090631\n",
      "[15301,     1] loss: 0.0093090482\n",
      "[15302,     1] loss: 0.0093090340\n",
      "[15303,     1] loss: 0.0093090177\n",
      "[15304,     1] loss: 0.0093090035\n",
      "[15305,     1] loss: 0.0093089879\n",
      "[15306,     1] loss: 0.0093089722\n",
      "[15307,     1] loss: 0.0093089573\n",
      "[15308,     1] loss: 0.0093089432\n",
      "[15309,     1] loss: 0.0093089253\n",
      "[15310,     1] loss: 0.0093089126\n",
      "[15311,     1] loss: 0.0093088977\n",
      "[15312,     1] loss: 0.0093088821\n",
      "[15313,     1] loss: 0.0093088679\n",
      "[15314,     1] loss: 0.0093088530\n",
      "[15315,     1] loss: 0.0093088388\n",
      "[15316,     1] loss: 0.0093088217\n",
      "[15317,     1] loss: 0.0093088076\n",
      "[15318,     1] loss: 0.0093087919\n",
      "[15319,     1] loss: 0.0093087770\n",
      "[15320,     1] loss: 0.0093087628\n",
      "[15321,     1] loss: 0.0093087479\n",
      "[15322,     1] loss: 0.0093087345\n",
      "[15323,     1] loss: 0.0093087174\n",
      "[15324,     1] loss: 0.0093087047\n",
      "[15325,     1] loss: 0.0093086891\n",
      "[15326,     1] loss: 0.0093086742\n",
      "[15327,     1] loss: 0.0093086600\n",
      "[15328,     1] loss: 0.0093086436\n",
      "[15329,     1] loss: 0.0093086295\n",
      "[15330,     1] loss: 0.0093086138\n",
      "[15331,     1] loss: 0.0093086004\n",
      "[15332,     1] loss: 0.0093085855\n",
      "[15333,     1] loss: 0.0093085706\n",
      "[15334,     1] loss: 0.0093085550\n",
      "[15335,     1] loss: 0.0093085416\n",
      "[15336,     1] loss: 0.0093085252\n",
      "[15337,     1] loss: 0.0093085095\n",
      "[15338,     1] loss: 0.0093084961\n",
      "[15339,     1] loss: 0.0093084827\n",
      "[15340,     1] loss: 0.0093084663\n",
      "[15341,     1] loss: 0.0093084529\n",
      "[15342,     1] loss: 0.0093084358\n",
      "[15343,     1] loss: 0.0093084209\n",
      "[15344,     1] loss: 0.0093084067\n",
      "[15345,     1] loss: 0.0093083918\n",
      "[15346,     1] loss: 0.0093083777\n",
      "[15347,     1] loss: 0.0093083628\n",
      "[15348,     1] loss: 0.0093083486\n",
      "[15349,     1] loss: 0.0093083344\n",
      "[15350,     1] loss: 0.0093083188\n",
      "[15351,     1] loss: 0.0093083039\n",
      "[15352,     1] loss: 0.0093082897\n",
      "[15353,     1] loss: 0.0093082726\n",
      "[15354,     1] loss: 0.0093082584\n",
      "[15355,     1] loss: 0.0093082450\n",
      "[15356,     1] loss: 0.0093082279\n",
      "[15357,     1] loss: 0.0093082145\n",
      "[15358,     1] loss: 0.0093081996\n",
      "[15359,     1] loss: 0.0093081854\n",
      "[15360,     1] loss: 0.0093081698\n",
      "[15361,     1] loss: 0.0093081549\n",
      "[15362,     1] loss: 0.0093081407\n",
      "[15363,     1] loss: 0.0093081251\n",
      "[15364,     1] loss: 0.0093081117\n",
      "[15365,     1] loss: 0.0093080968\n",
      "[15366,     1] loss: 0.0093080826\n",
      "[15367,     1] loss: 0.0093080670\n",
      "[15368,     1] loss: 0.0093080521\n",
      "[15369,     1] loss: 0.0093080372\n",
      "[15370,     1] loss: 0.0093080238\n",
      "[15371,     1] loss: 0.0093080103\n",
      "[15372,     1] loss: 0.0093079962\n",
      "[15373,     1] loss: 0.0093079805\n",
      "[15374,     1] loss: 0.0093079656\n",
      "[15375,     1] loss: 0.0093079500\n",
      "[15376,     1] loss: 0.0093079366\n",
      "[15377,     1] loss: 0.0093079209\n",
      "[15378,     1] loss: 0.0093079068\n",
      "[15379,     1] loss: 0.0093078941\n",
      "[15380,     1] loss: 0.0093078770\n",
      "[15381,     1] loss: 0.0093078636\n",
      "[15382,     1] loss: 0.0093078487\n",
      "[15383,     1] loss: 0.0093078330\n",
      "[15384,     1] loss: 0.0093078203\n",
      "[15385,     1] loss: 0.0093078032\n",
      "[15386,     1] loss: 0.0093077898\n",
      "[15387,     1] loss: 0.0093077756\n",
      "[15388,     1] loss: 0.0093077607\n",
      "[15389,     1] loss: 0.0093077458\n",
      "[15390,     1] loss: 0.0093077302\n",
      "[15391,     1] loss: 0.0093077168\n",
      "[15392,     1] loss: 0.0093077019\n",
      "[15393,     1] loss: 0.0093076862\n",
      "[15394,     1] loss: 0.0093076721\n",
      "[15395,     1] loss: 0.0093076587\n",
      "[15396,     1] loss: 0.0093076423\n",
      "[15397,     1] loss: 0.0093076274\n",
      "[15398,     1] loss: 0.0093076132\n",
      "[15399,     1] loss: 0.0093075991\n",
      "[15400,     1] loss: 0.0093075849\n",
      "[15401,     1] loss: 0.0093075685\n",
      "[15402,     1] loss: 0.0093075566\n",
      "[15403,     1] loss: 0.0093075410\n",
      "[15404,     1] loss: 0.0093075253\n",
      "[15405,     1] loss: 0.0093075126\n",
      "[15406,     1] loss: 0.0093074977\n",
      "[15407,     1] loss: 0.0093074821\n",
      "[15408,     1] loss: 0.0093074687\n",
      "[15409,     1] loss: 0.0093074545\n",
      "[15410,     1] loss: 0.0093074389\n",
      "[15411,     1] loss: 0.0093074255\n",
      "[15412,     1] loss: 0.0093074121\n",
      "[15413,     1] loss: 0.0093073964\n",
      "[15414,     1] loss: 0.0093073823\n",
      "[15415,     1] loss: 0.0093073681\n",
      "[15416,     1] loss: 0.0093073525\n",
      "[15417,     1] loss: 0.0093073390\n",
      "[15418,     1] loss: 0.0093073249\n",
      "[15419,     1] loss: 0.0093073100\n",
      "[15420,     1] loss: 0.0093072966\n",
      "[15421,     1] loss: 0.0093072817\n",
      "[15422,     1] loss: 0.0093072668\n",
      "[15423,     1] loss: 0.0093072526\n",
      "[15424,     1] loss: 0.0093072385\n",
      "[15425,     1] loss: 0.0093072243\n",
      "[15426,     1] loss: 0.0093072087\n",
      "[15427,     1] loss: 0.0093071952\n",
      "[15428,     1] loss: 0.0093071789\n",
      "[15429,     1] loss: 0.0093071654\n",
      "[15430,     1] loss: 0.0093071513\n",
      "[15431,     1] loss: 0.0093071371\n",
      "[15432,     1] loss: 0.0093071222\n",
      "[15433,     1] loss: 0.0093071088\n",
      "[15434,     1] loss: 0.0093070939\n",
      "[15435,     1] loss: 0.0093070790\n",
      "[15436,     1] loss: 0.0093070649\n",
      "[15437,     1] loss: 0.0093070485\n",
      "[15438,     1] loss: 0.0093070365\n",
      "[15439,     1] loss: 0.0093070224\n",
      "[15440,     1] loss: 0.0093070060\n",
      "[15441,     1] loss: 0.0093069941\n",
      "[15442,     1] loss: 0.0093069792\n",
      "[15443,     1] loss: 0.0093069650\n",
      "[15444,     1] loss: 0.0093069509\n",
      "[15445,     1] loss: 0.0093069367\n",
      "[15446,     1] loss: 0.0093069203\n",
      "[15447,     1] loss: 0.0093069069\n",
      "[15448,     1] loss: 0.0093068928\n",
      "[15449,     1] loss: 0.0093068764\n",
      "[15450,     1] loss: 0.0093068637\n",
      "[15451,     1] loss: 0.0093068488\n",
      "[15452,     1] loss: 0.0093068346\n",
      "[15453,     1] loss: 0.0093068197\n",
      "[15454,     1] loss: 0.0093068056\n",
      "[15455,     1] loss: 0.0093067929\n",
      "[15456,     1] loss: 0.0093067765\n",
      "[15457,     1] loss: 0.0093067631\n",
      "[15458,     1] loss: 0.0093067497\n",
      "[15459,     1] loss: 0.0093067348\n",
      "[15460,     1] loss: 0.0093067206\n",
      "[15461,     1] loss: 0.0093067065\n",
      "[15462,     1] loss: 0.0093066916\n",
      "[15463,     1] loss: 0.0093066789\n",
      "[15464,     1] loss: 0.0093066648\n",
      "[15465,     1] loss: 0.0093066499\n",
      "[15466,     1] loss: 0.0093066350\n",
      "[15467,     1] loss: 0.0093066208\n",
      "[15468,     1] loss: 0.0093066081\n",
      "[15469,     1] loss: 0.0093065932\n",
      "[15470,     1] loss: 0.0093065783\n",
      "[15471,     1] loss: 0.0093065649\n",
      "[15472,     1] loss: 0.0093065508\n",
      "[15473,     1] loss: 0.0093065374\n",
      "[15474,     1] loss: 0.0093065217\n",
      "[15475,     1] loss: 0.0093065090\n",
      "[15476,     1] loss: 0.0093064934\n",
      "[15477,     1] loss: 0.0093064800\n",
      "[15478,     1] loss: 0.0093064651\n",
      "[15479,     1] loss: 0.0093064502\n",
      "[15480,     1] loss: 0.0093064368\n",
      "[15481,     1] loss: 0.0093064226\n",
      "[15482,     1] loss: 0.0093064077\n",
      "[15483,     1] loss: 0.0093063943\n",
      "[15484,     1] loss: 0.0093063787\n",
      "[15485,     1] loss: 0.0093063660\n",
      "[15486,     1] loss: 0.0093063518\n",
      "[15487,     1] loss: 0.0093063362\n",
      "[15488,     1] loss: 0.0093063228\n",
      "[15489,     1] loss: 0.0093063079\n",
      "[15490,     1] loss: 0.0093062945\n",
      "[15491,     1] loss: 0.0093062803\n",
      "[15492,     1] loss: 0.0093062654\n",
      "[15493,     1] loss: 0.0093062520\n",
      "[15494,     1] loss: 0.0093062378\n",
      "[15495,     1] loss: 0.0093062229\n",
      "[15496,     1] loss: 0.0093062095\n",
      "[15497,     1] loss: 0.0093061961\n",
      "[15498,     1] loss: 0.0093061812\n",
      "[15499,     1] loss: 0.0093061671\n",
      "[15500,     1] loss: 0.0093061537\n",
      "[15501,     1] loss: 0.0093061380\n",
      "[15502,     1] loss: 0.0093061253\n",
      "[15503,     1] loss: 0.0093061119\n",
      "[15504,     1] loss: 0.0093060978\n",
      "[15505,     1] loss: 0.0093060844\n",
      "[15506,     1] loss: 0.0093060695\n",
      "[15507,     1] loss: 0.0093060561\n",
      "[15508,     1] loss: 0.0093060419\n",
      "[15509,     1] loss: 0.0093060285\n",
      "[15510,     1] loss: 0.0093060128\n",
      "[15511,     1] loss: 0.0093060009\n",
      "[15512,     1] loss: 0.0093059868\n",
      "[15513,     1] loss: 0.0093059719\n",
      "[15514,     1] loss: 0.0093059584\n",
      "[15515,     1] loss: 0.0093059435\n",
      "[15516,     1] loss: 0.0093059301\n",
      "[15517,     1] loss: 0.0093059145\n",
      "[15518,     1] loss: 0.0093059018\n",
      "[15519,     1] loss: 0.0093058892\n",
      "[15520,     1] loss: 0.0093058735\n",
      "[15521,     1] loss: 0.0093058594\n",
      "[15522,     1] loss: 0.0093058445\n",
      "[15523,     1] loss: 0.0093058318\n",
      "[15524,     1] loss: 0.0093058176\n",
      "[15525,     1] loss: 0.0093058050\n",
      "[15526,     1] loss: 0.0093057901\n",
      "[15527,     1] loss: 0.0093057752\n",
      "[15528,     1] loss: 0.0093057610\n",
      "[15529,     1] loss: 0.0093057476\n",
      "[15530,     1] loss: 0.0093057327\n",
      "[15531,     1] loss: 0.0093057185\n",
      "[15532,     1] loss: 0.0093057044\n",
      "[15533,     1] loss: 0.0093056910\n",
      "[15534,     1] loss: 0.0093056783\n",
      "[15535,     1] loss: 0.0093056656\n",
      "[15536,     1] loss: 0.0093056515\n",
      "[15537,     1] loss: 0.0093056366\n",
      "[15538,     1] loss: 0.0093056224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15539,     1] loss: 0.0093056090\n",
      "[15540,     1] loss: 0.0093055949\n",
      "[15541,     1] loss: 0.0093055807\n",
      "[15542,     1] loss: 0.0093055665\n",
      "[15543,     1] loss: 0.0093055531\n",
      "[15544,     1] loss: 0.0093055397\n",
      "[15545,     1] loss: 0.0093055248\n",
      "[15546,     1] loss: 0.0093055107\n",
      "[15547,     1] loss: 0.0093054973\n",
      "[15548,     1] loss: 0.0093054838\n",
      "[15549,     1] loss: 0.0093054704\n",
      "[15550,     1] loss: 0.0093054563\n",
      "[15551,     1] loss: 0.0093054429\n",
      "[15552,     1] loss: 0.0093054287\n",
      "[15553,     1] loss: 0.0093054146\n",
      "[15554,     1] loss: 0.0093054004\n",
      "[15555,     1] loss: 0.0093053855\n",
      "[15556,     1] loss: 0.0093053736\n",
      "[15557,     1] loss: 0.0093053594\n",
      "[15558,     1] loss: 0.0093053453\n",
      "[15559,     1] loss: 0.0093053319\n",
      "[15560,     1] loss: 0.0093053170\n",
      "[15561,     1] loss: 0.0093053043\n",
      "[15562,     1] loss: 0.0093052901\n",
      "[15563,     1] loss: 0.0093052767\n",
      "[15564,     1] loss: 0.0093052648\n",
      "[15565,     1] loss: 0.0093052499\n",
      "[15566,     1] loss: 0.0093052365\n",
      "[15567,     1] loss: 0.0093052208\n",
      "[15568,     1] loss: 0.0093052074\n",
      "[15569,     1] loss: 0.0093051940\n",
      "[15570,     1] loss: 0.0093051806\n",
      "[15571,     1] loss: 0.0093051672\n",
      "[15572,     1] loss: 0.0093051530\n",
      "[15573,     1] loss: 0.0093051396\n",
      "[15574,     1] loss: 0.0093051262\n",
      "[15575,     1] loss: 0.0093051106\n",
      "[15576,     1] loss: 0.0093050972\n",
      "[15577,     1] loss: 0.0093050823\n",
      "[15578,     1] loss: 0.0093050696\n",
      "[15579,     1] loss: 0.0093050569\n",
      "[15580,     1] loss: 0.0093050428\n",
      "[15581,     1] loss: 0.0093050279\n",
      "[15582,     1] loss: 0.0093050152\n",
      "[15583,     1] loss: 0.0093050025\n",
      "[15584,     1] loss: 0.0093049876\n",
      "[15585,     1] loss: 0.0093049720\n",
      "[15586,     1] loss: 0.0093049601\n",
      "[15587,     1] loss: 0.0093049459\n",
      "[15588,     1] loss: 0.0093049332\n",
      "[15589,     1] loss: 0.0093049191\n",
      "[15590,     1] loss: 0.0093049042\n",
      "[15591,     1] loss: 0.0093048923\n",
      "[15592,     1] loss: 0.0093048774\n",
      "[15593,     1] loss: 0.0093048632\n",
      "[15594,     1] loss: 0.0093048528\n",
      "[15595,     1] loss: 0.0093048379\n",
      "[15596,     1] loss: 0.0093048215\n",
      "[15597,     1] loss: 0.0093048103\n",
      "[15598,     1] loss: 0.0093047984\n",
      "[15599,     1] loss: 0.0093047827\n",
      "[15600,     1] loss: 0.0093047701\n",
      "[15601,     1] loss: 0.0093047559\n",
      "[15602,     1] loss: 0.0093047433\n",
      "[15603,     1] loss: 0.0093047291\n",
      "[15604,     1] loss: 0.0093047149\n",
      "[15605,     1] loss: 0.0093047030\n",
      "[15606,     1] loss: 0.0093046896\n",
      "[15607,     1] loss: 0.0093046747\n",
      "[15608,     1] loss: 0.0093046606\n",
      "[15609,     1] loss: 0.0093046471\n",
      "[15610,     1] loss: 0.0093046330\n",
      "[15611,     1] loss: 0.0093046211\n",
      "[15612,     1] loss: 0.0093046077\n",
      "[15613,     1] loss: 0.0093045920\n",
      "[15614,     1] loss: 0.0093045808\n",
      "[15615,     1] loss: 0.0093045652\n",
      "[15616,     1] loss: 0.0093045518\n",
      "[15617,     1] loss: 0.0093045391\n",
      "[15618,     1] loss: 0.0093045257\n",
      "[15619,     1] loss: 0.0093045130\n",
      "[15620,     1] loss: 0.0093044974\n",
      "[15621,     1] loss: 0.0093044840\n",
      "[15622,     1] loss: 0.0093044713\n",
      "[15623,     1] loss: 0.0093044586\n",
      "[15624,     1] loss: 0.0093044430\n",
      "[15625,     1] loss: 0.0093044303\n",
      "[15626,     1] loss: 0.0093044169\n",
      "[15627,     1] loss: 0.0093044035\n",
      "[15628,     1] loss: 0.0093043901\n",
      "[15629,     1] loss: 0.0093043759\n",
      "[15630,     1] loss: 0.0093043633\n",
      "[15631,     1] loss: 0.0093043484\n",
      "[15632,     1] loss: 0.0093043350\n",
      "[15633,     1] loss: 0.0093043223\n",
      "[15634,     1] loss: 0.0093043074\n",
      "[15635,     1] loss: 0.0093042970\n",
      "[15636,     1] loss: 0.0093042821\n",
      "[15637,     1] loss: 0.0093042694\n",
      "[15638,     1] loss: 0.0093042552\n",
      "[15639,     1] loss: 0.0093042426\n",
      "[15640,     1] loss: 0.0093042284\n",
      "[15641,     1] loss: 0.0093042143\n",
      "[15642,     1] loss: 0.0093042016\n",
      "[15643,     1] loss: 0.0093041874\n",
      "[15644,     1] loss: 0.0093041755\n",
      "[15645,     1] loss: 0.0093041614\n",
      "[15646,     1] loss: 0.0093041465\n",
      "[15647,     1] loss: 0.0093041353\n",
      "[15648,     1] loss: 0.0093041226\n",
      "[15649,     1] loss: 0.0093041085\n",
      "[15650,     1] loss: 0.0093040943\n",
      "[15651,     1] loss: 0.0093040809\n",
      "[15652,     1] loss: 0.0093040682\n",
      "[15653,     1] loss: 0.0093040541\n",
      "[15654,     1] loss: 0.0093040414\n",
      "[15655,     1] loss: 0.0093040273\n",
      "[15656,     1] loss: 0.0093040138\n",
      "[15657,     1] loss: 0.0093040004\n",
      "[15658,     1] loss: 0.0093039870\n",
      "[15659,     1] loss: 0.0093039736\n",
      "[15660,     1] loss: 0.0093039617\n",
      "[15661,     1] loss: 0.0093039460\n",
      "[15662,     1] loss: 0.0093039341\n",
      "[15663,     1] loss: 0.0093039207\n",
      "[15664,     1] loss: 0.0093039073\n",
      "[15665,     1] loss: 0.0093038939\n",
      "[15666,     1] loss: 0.0093038797\n",
      "[15667,     1] loss: 0.0093038671\n",
      "[15668,     1] loss: 0.0093038537\n",
      "[15669,     1] loss: 0.0093038410\n",
      "[15670,     1] loss: 0.0093038268\n",
      "[15671,     1] loss: 0.0093038127\n",
      "[15672,     1] loss: 0.0093038000\n",
      "[15673,     1] loss: 0.0093037859\n",
      "[15674,     1] loss: 0.0093037724\n",
      "[15675,     1] loss: 0.0093037590\n",
      "[15676,     1] loss: 0.0093037464\n",
      "[15677,     1] loss: 0.0093037330\n",
      "[15678,     1] loss: 0.0093037203\n",
      "[15679,     1] loss: 0.0093037061\n",
      "[15680,     1] loss: 0.0093036935\n",
      "[15681,     1] loss: 0.0093036801\n",
      "[15682,     1] loss: 0.0093036674\n",
      "[15683,     1] loss: 0.0093036540\n",
      "[15684,     1] loss: 0.0093036421\n",
      "[15685,     1] loss: 0.0093036264\n",
      "[15686,     1] loss: 0.0093036152\n",
      "[15687,     1] loss: 0.0093036018\n",
      "[15688,     1] loss: 0.0093035877\n",
      "[15689,     1] loss: 0.0093035750\n",
      "[15690,     1] loss: 0.0093035631\n",
      "[15691,     1] loss: 0.0093035467\n",
      "[15692,     1] loss: 0.0093035355\n",
      "[15693,     1] loss: 0.0093035221\n",
      "[15694,     1] loss: 0.0093035080\n",
      "[15695,     1] loss: 0.0093034953\n",
      "[15696,     1] loss: 0.0093034834\n",
      "[15697,     1] loss: 0.0093034707\n",
      "[15698,     1] loss: 0.0093034565\n",
      "[15699,     1] loss: 0.0093034431\n",
      "[15700,     1] loss: 0.0093034312\n",
      "[15701,     1] loss: 0.0093034185\n",
      "[15702,     1] loss: 0.0093034059\n",
      "[15703,     1] loss: 0.0093033917\n",
      "[15704,     1] loss: 0.0093033791\n",
      "[15705,     1] loss: 0.0093033664\n",
      "[15706,     1] loss: 0.0093033522\n",
      "[15707,     1] loss: 0.0093033388\n",
      "[15708,     1] loss: 0.0093033254\n",
      "[15709,     1] loss: 0.0093033135\n",
      "[15710,     1] loss: 0.0093033001\n",
      "[15711,     1] loss: 0.0093032867\n",
      "[15712,     1] loss: 0.0093032733\n",
      "[15713,     1] loss: 0.0093032606\n",
      "[15714,     1] loss: 0.0093032479\n",
      "[15715,     1] loss: 0.0093032345\n",
      "[15716,     1] loss: 0.0093032204\n",
      "[15717,     1] loss: 0.0093032077\n",
      "[15718,     1] loss: 0.0093031958\n",
      "[15719,     1] loss: 0.0093031816\n",
      "[15720,     1] loss: 0.0093031682\n",
      "[15721,     1] loss: 0.0093031555\n",
      "[15722,     1] loss: 0.0093031436\n",
      "[15723,     1] loss: 0.0093031310\n",
      "[15724,     1] loss: 0.0093031168\n",
      "[15725,     1] loss: 0.0093031034\n",
      "[15726,     1] loss: 0.0093030907\n",
      "[15727,     1] loss: 0.0093030781\n",
      "[15728,     1] loss: 0.0093030646\n",
      "[15729,     1] loss: 0.0093030520\n",
      "[15730,     1] loss: 0.0093030386\n",
      "[15731,     1] loss: 0.0093030259\n",
      "[15732,     1] loss: 0.0093030110\n",
      "[15733,     1] loss: 0.0093029991\n",
      "[15734,     1] loss: 0.0093029849\n",
      "[15735,     1] loss: 0.0093029730\n",
      "[15736,     1] loss: 0.0093029611\n",
      "[15737,     1] loss: 0.0093029469\n",
      "[15738,     1] loss: 0.0093029343\n",
      "[15739,     1] loss: 0.0093029216\n",
      "[15740,     1] loss: 0.0093029089\n",
      "[15741,     1] loss: 0.0093028940\n",
      "[15742,     1] loss: 0.0093028829\n",
      "[15743,     1] loss: 0.0093028694\n",
      "[15744,     1] loss: 0.0093028560\n",
      "[15745,     1] loss: 0.0093028434\n",
      "[15746,     1] loss: 0.0093028307\n",
      "[15747,     1] loss: 0.0093028173\n",
      "[15748,     1] loss: 0.0093028046\n",
      "[15749,     1] loss: 0.0093027920\n",
      "[15750,     1] loss: 0.0093027785\n",
      "[15751,     1] loss: 0.0093027651\n",
      "[15752,     1] loss: 0.0093027532\n",
      "[15753,     1] loss: 0.0093027391\n",
      "[15754,     1] loss: 0.0093027271\n",
      "[15755,     1] loss: 0.0093027137\n",
      "[15756,     1] loss: 0.0093027003\n",
      "[15757,     1] loss: 0.0093026869\n",
      "[15758,     1] loss: 0.0093026757\n",
      "[15759,     1] loss: 0.0093026623\n",
      "[15760,     1] loss: 0.0093026489\n",
      "[15761,     1] loss: 0.0093026362\n",
      "[15762,     1] loss: 0.0093026251\n",
      "[15763,     1] loss: 0.0093026116\n",
      "[15764,     1] loss: 0.0093025975\n",
      "[15765,     1] loss: 0.0093025856\n",
      "[15766,     1] loss: 0.0093025707\n",
      "[15767,     1] loss: 0.0093025580\n",
      "[15768,     1] loss: 0.0093025476\n",
      "[15769,     1] loss: 0.0093025327\n",
      "[15770,     1] loss: 0.0093025208\n",
      "[15771,     1] loss: 0.0093025073\n",
      "[15772,     1] loss: 0.0093024932\n",
      "[15773,     1] loss: 0.0093024805\n",
      "[15774,     1] loss: 0.0093024686\n",
      "[15775,     1] loss: 0.0093024552\n",
      "[15776,     1] loss: 0.0093024418\n",
      "[15777,     1] loss: 0.0093024299\n",
      "[15778,     1] loss: 0.0093024164\n",
      "[15779,     1] loss: 0.0093024030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15780,     1] loss: 0.0093023911\n",
      "[15781,     1] loss: 0.0093023784\n",
      "[15782,     1] loss: 0.0093023650\n",
      "[15783,     1] loss: 0.0093023524\n",
      "[15784,     1] loss: 0.0093023397\n",
      "[15785,     1] loss: 0.0093023263\n",
      "[15786,     1] loss: 0.0093023151\n",
      "[15787,     1] loss: 0.0093023017\n",
      "[15788,     1] loss: 0.0093022883\n",
      "[15789,     1] loss: 0.0093022756\n",
      "[15790,     1] loss: 0.0093022637\n",
      "[15791,     1] loss: 0.0093022503\n",
      "[15792,     1] loss: 0.0093022384\n",
      "[15793,     1] loss: 0.0093022250\n",
      "[15794,     1] loss: 0.0093022116\n",
      "[15795,     1] loss: 0.0093021989\n",
      "[15796,     1] loss: 0.0093021885\n",
      "[15797,     1] loss: 0.0093021743\n",
      "[15798,     1] loss: 0.0093021624\n",
      "[15799,     1] loss: 0.0093021497\n",
      "[15800,     1] loss: 0.0093021370\n",
      "[15801,     1] loss: 0.0093021229\n",
      "[15802,     1] loss: 0.0093021102\n",
      "[15803,     1] loss: 0.0093020983\n",
      "[15804,     1] loss: 0.0093020849\n",
      "[15805,     1] loss: 0.0093020722\n",
      "[15806,     1] loss: 0.0093020588\n",
      "[15807,     1] loss: 0.0093020461\n",
      "[15808,     1] loss: 0.0093020327\n",
      "[15809,     1] loss: 0.0093020201\n",
      "[15810,     1] loss: 0.0093020089\n",
      "[15811,     1] loss: 0.0093019940\n",
      "[15812,     1] loss: 0.0093019828\n",
      "[15813,     1] loss: 0.0093019687\n",
      "[15814,     1] loss: 0.0093019590\n",
      "[15815,     1] loss: 0.0093019426\n",
      "[15816,     1] loss: 0.0093019322\n",
      "[15817,     1] loss: 0.0093019180\n",
      "[15818,     1] loss: 0.0093019061\n",
      "[15819,     1] loss: 0.0093018942\n",
      "[15820,     1] loss: 0.0093018815\n",
      "[15821,     1] loss: 0.0093018681\n",
      "[15822,     1] loss: 0.0093018554\n",
      "[15823,     1] loss: 0.0093018420\n",
      "[15824,     1] loss: 0.0093018293\n",
      "[15825,     1] loss: 0.0093018182\n",
      "[15826,     1] loss: 0.0093018048\n",
      "[15827,     1] loss: 0.0093017906\n",
      "[15828,     1] loss: 0.0093017794\n",
      "[15829,     1] loss: 0.0093017653\n",
      "[15830,     1] loss: 0.0093017548\n",
      "[15831,     1] loss: 0.0093017414\n",
      "[15832,     1] loss: 0.0093017273\n",
      "[15833,     1] loss: 0.0093017168\n",
      "[15834,     1] loss: 0.0093017034\n",
      "[15835,     1] loss: 0.0093016908\n",
      "[15836,     1] loss: 0.0093016803\n",
      "[15837,     1] loss: 0.0093016654\n",
      "[15838,     1] loss: 0.0093016535\n",
      "[15839,     1] loss: 0.0093016408\n",
      "[15840,     1] loss: 0.0093016289\n",
      "[15841,     1] loss: 0.0093016170\n",
      "[15842,     1] loss: 0.0093016043\n",
      "[15843,     1] loss: 0.0093015909\n",
      "[15844,     1] loss: 0.0093015790\n",
      "[15845,     1] loss: 0.0093015656\n",
      "[15846,     1] loss: 0.0093015522\n",
      "[15847,     1] loss: 0.0093015425\n",
      "[15848,     1] loss: 0.0093015298\n",
      "[15849,     1] loss: 0.0093015164\n",
      "[15850,     1] loss: 0.0093015037\n",
      "[15851,     1] loss: 0.0093014911\n",
      "[15852,     1] loss: 0.0093014792\n",
      "[15853,     1] loss: 0.0093014665\n",
      "[15854,     1] loss: 0.0093014523\n",
      "[15855,     1] loss: 0.0093014397\n",
      "[15856,     1] loss: 0.0093014278\n",
      "[15857,     1] loss: 0.0093014151\n",
      "[15858,     1] loss: 0.0093014032\n",
      "[15859,     1] loss: 0.0093013905\n",
      "[15860,     1] loss: 0.0093013786\n",
      "[15861,     1] loss: 0.0093013652\n",
      "[15862,     1] loss: 0.0093013525\n",
      "[15863,     1] loss: 0.0093013391\n",
      "[15864,     1] loss: 0.0093013264\n",
      "[15865,     1] loss: 0.0093013152\n",
      "[15866,     1] loss: 0.0093013011\n",
      "[15867,     1] loss: 0.0093012892\n",
      "[15868,     1] loss: 0.0093012765\n",
      "[15869,     1] loss: 0.0093012646\n",
      "[15870,     1] loss: 0.0093012519\n",
      "[15871,     1] loss: 0.0093012407\n",
      "[15872,     1] loss: 0.0093012266\n",
      "[15873,     1] loss: 0.0093012147\n",
      "[15874,     1] loss: 0.0093012020\n",
      "[15875,     1] loss: 0.0093011886\n",
      "[15876,     1] loss: 0.0093011774\n",
      "[15877,     1] loss: 0.0093011655\n",
      "[15878,     1] loss: 0.0093011536\n",
      "[15879,     1] loss: 0.0093011387\n",
      "[15880,     1] loss: 0.0093011282\n",
      "[15881,     1] loss: 0.0093011141\n",
      "[15882,     1] loss: 0.0093011029\n",
      "[15883,     1] loss: 0.0093010910\n",
      "[15884,     1] loss: 0.0093010791\n",
      "[15885,     1] loss: 0.0093010657\n",
      "[15886,     1] loss: 0.0093010537\n",
      "[15887,     1] loss: 0.0093010418\n",
      "[15888,     1] loss: 0.0093010277\n",
      "[15889,     1] loss: 0.0093010165\n",
      "[15890,     1] loss: 0.0093010038\n",
      "[15891,     1] loss: 0.0093009911\n",
      "[15892,     1] loss: 0.0093009785\n",
      "[15893,     1] loss: 0.0093009673\n",
      "[15894,     1] loss: 0.0093009539\n",
      "[15895,     1] loss: 0.0093009420\n",
      "[15896,     1] loss: 0.0093009308\n",
      "[15897,     1] loss: 0.0093009166\n",
      "[15898,     1] loss: 0.0093009040\n",
      "[15899,     1] loss: 0.0093008928\n",
      "[15900,     1] loss: 0.0093008794\n",
      "[15901,     1] loss: 0.0093008682\n",
      "[15902,     1] loss: 0.0093008548\n",
      "[15903,     1] loss: 0.0093008436\n",
      "[15904,     1] loss: 0.0093008325\n",
      "[15905,     1] loss: 0.0093008183\n",
      "[15906,     1] loss: 0.0093008064\n",
      "[15907,     1] loss: 0.0093007930\n",
      "[15908,     1] loss: 0.0093007810\n",
      "[15909,     1] loss: 0.0093007691\n",
      "[15910,     1] loss: 0.0093007579\n",
      "[15911,     1] loss: 0.0093007438\n",
      "[15912,     1] loss: 0.0093007326\n",
      "[15913,     1] loss: 0.0093007199\n",
      "[15914,     1] loss: 0.0093007088\n",
      "[15915,     1] loss: 0.0093006954\n",
      "[15916,     1] loss: 0.0093006842\n",
      "[15917,     1] loss: 0.0093006708\n",
      "[15918,     1] loss: 0.0093006603\n",
      "[15919,     1] loss: 0.0093006462\n",
      "[15920,     1] loss: 0.0093006358\n",
      "[15921,     1] loss: 0.0093006231\n",
      "[15922,     1] loss: 0.0093006097\n",
      "[15923,     1] loss: 0.0093005978\n",
      "[15924,     1] loss: 0.0093005851\n",
      "[15925,     1] loss: 0.0093005732\n",
      "[15926,     1] loss: 0.0093005620\n",
      "[15927,     1] loss: 0.0093005486\n",
      "[15928,     1] loss: 0.0093005367\n",
      "[15929,     1] loss: 0.0093005247\n",
      "[15930,     1] loss: 0.0093005113\n",
      "[15931,     1] loss: 0.0093004994\n",
      "[15932,     1] loss: 0.0093004875\n",
      "[15933,     1] loss: 0.0093004741\n",
      "[15934,     1] loss: 0.0093004622\n",
      "[15935,     1] loss: 0.0093004510\n",
      "[15936,     1] loss: 0.0093004376\n",
      "[15937,     1] loss: 0.0093004249\n",
      "[15938,     1] loss: 0.0093004152\n",
      "[15939,     1] loss: 0.0093004026\n",
      "[15940,     1] loss: 0.0093003899\n",
      "[15941,     1] loss: 0.0093003772\n",
      "[15942,     1] loss: 0.0093003653\n",
      "[15943,     1] loss: 0.0093003526\n",
      "[15944,     1] loss: 0.0093003407\n",
      "[15945,     1] loss: 0.0093003295\n",
      "[15946,     1] loss: 0.0093003169\n",
      "[15947,     1] loss: 0.0093003057\n",
      "[15948,     1] loss: 0.0093002923\n",
      "[15949,     1] loss: 0.0093002804\n",
      "[15950,     1] loss: 0.0093002677\n",
      "[15951,     1] loss: 0.0093002550\n",
      "[15952,     1] loss: 0.0093002439\n",
      "[15953,     1] loss: 0.0093002304\n",
      "[15954,     1] loss: 0.0093002185\n",
      "[15955,     1] loss: 0.0093002059\n",
      "[15956,     1] loss: 0.0093001947\n",
      "[15957,     1] loss: 0.0093001820\n",
      "[15958,     1] loss: 0.0093001701\n",
      "[15959,     1] loss: 0.0093001574\n",
      "[15960,     1] loss: 0.0093001477\n",
      "[15961,     1] loss: 0.0093001343\n",
      "[15962,     1] loss: 0.0093001224\n",
      "[15963,     1] loss: 0.0093001097\n",
      "[15964,     1] loss: 0.0093000956\n",
      "[15965,     1] loss: 0.0093000859\n",
      "[15966,     1] loss: 0.0093000732\n",
      "[15967,     1] loss: 0.0093000606\n",
      "[15968,     1] loss: 0.0093000486\n",
      "[15969,     1] loss: 0.0093000375\n",
      "[15970,     1] loss: 0.0093000226\n",
      "[15971,     1] loss: 0.0093000129\n",
      "[15972,     1] loss: 0.0093000002\n",
      "[15973,     1] loss: 0.0092999868\n",
      "[15974,     1] loss: 0.0092999764\n",
      "[15975,     1] loss: 0.0092999637\n",
      "[15976,     1] loss: 0.0092999525\n",
      "[15977,     1] loss: 0.0092999384\n",
      "[15978,     1] loss: 0.0092999287\n",
      "[15979,     1] loss: 0.0092999153\n",
      "[15980,     1] loss: 0.0092999041\n",
      "[15981,     1] loss: 0.0092998914\n",
      "[15982,     1] loss: 0.0092998795\n",
      "[15983,     1] loss: 0.0092998683\n",
      "[15984,     1] loss: 0.0092998542\n",
      "[15985,     1] loss: 0.0092998438\n",
      "[15986,     1] loss: 0.0092998311\n",
      "[15987,     1] loss: 0.0092998184\n",
      "[15988,     1] loss: 0.0092998087\n",
      "[15989,     1] loss: 0.0092997961\n",
      "[15990,     1] loss: 0.0092997849\n",
      "[15991,     1] loss: 0.0092997730\n",
      "[15992,     1] loss: 0.0092997603\n",
      "[15993,     1] loss: 0.0092997484\n",
      "[15994,     1] loss: 0.0092997372\n",
      "[15995,     1] loss: 0.0092997238\n",
      "[15996,     1] loss: 0.0092997119\n",
      "[15997,     1] loss: 0.0092997022\n",
      "[15998,     1] loss: 0.0092996895\n",
      "[15999,     1] loss: 0.0092996769\n",
      "[16000,     1] loss: 0.0092996657\n",
      "[16001,     1] loss: 0.0092996530\n",
      "[16002,     1] loss: 0.0092996418\n",
      "[16003,     1] loss: 0.0092996299\n",
      "[16004,     1] loss: 0.0092996188\n",
      "[16005,     1] loss: 0.0092996068\n",
      "[16006,     1] loss: 0.0092995934\n",
      "[16007,     1] loss: 0.0092995815\n",
      "[16008,     1] loss: 0.0092995696\n",
      "[16009,     1] loss: 0.0092995577\n",
      "[16010,     1] loss: 0.0092995457\n",
      "[16011,     1] loss: 0.0092995353\n",
      "[16012,     1] loss: 0.0092995234\n",
      "[16013,     1] loss: 0.0092995107\n",
      "[16014,     1] loss: 0.0092994981\n",
      "[16015,     1] loss: 0.0092994876\n",
      "[16016,     1] loss: 0.0092994742\n",
      "[16017,     1] loss: 0.0092994623\n",
      "[16018,     1] loss: 0.0092994511\n",
      "[16019,     1] loss: 0.0092994384\n",
      "[16020,     1] loss: 0.0092994273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16021,     1] loss: 0.0092994161\n",
      "[16022,     1] loss: 0.0092994042\n",
      "[16023,     1] loss: 0.0092993930\n",
      "[16024,     1] loss: 0.0092993803\n",
      "[16025,     1] loss: 0.0092993684\n",
      "[16026,     1] loss: 0.0092993557\n",
      "[16027,     1] loss: 0.0092993453\n",
      "[16028,     1] loss: 0.0092993326\n",
      "[16029,     1] loss: 0.0092993215\n",
      "[16030,     1] loss: 0.0092993096\n",
      "[16031,     1] loss: 0.0092992961\n",
      "[16032,     1] loss: 0.0092992835\n",
      "[16033,     1] loss: 0.0092992723\n",
      "[16034,     1] loss: 0.0092992619\n",
      "[16035,     1] loss: 0.0092992477\n",
      "[16036,     1] loss: 0.0092992373\n",
      "[16037,     1] loss: 0.0092992254\n",
      "[16038,     1] loss: 0.0092992149\n",
      "[16039,     1] loss: 0.0092992038\n",
      "[16040,     1] loss: 0.0092991911\n",
      "[16041,     1] loss: 0.0092991807\n",
      "[16042,     1] loss: 0.0092991680\n",
      "[16043,     1] loss: 0.0092991553\n",
      "[16044,     1] loss: 0.0092991434\n",
      "[16045,     1] loss: 0.0092991322\n",
      "[16046,     1] loss: 0.0092991203\n",
      "[16047,     1] loss: 0.0092991084\n",
      "[16048,     1] loss: 0.0092990965\n",
      "[16049,     1] loss: 0.0092990845\n",
      "[16050,     1] loss: 0.0092990734\n",
      "[16051,     1] loss: 0.0092990614\n",
      "[16052,     1] loss: 0.0092990495\n",
      "[16053,     1] loss: 0.0092990376\n",
      "[16054,     1] loss: 0.0092990249\n",
      "[16055,     1] loss: 0.0092990138\n",
      "[16056,     1] loss: 0.0092990018\n",
      "[16057,     1] loss: 0.0092989907\n",
      "[16058,     1] loss: 0.0092989780\n",
      "[16059,     1] loss: 0.0092989661\n",
      "[16060,     1] loss: 0.0092989542\n",
      "[16061,     1] loss: 0.0092989430\n",
      "[16062,     1] loss: 0.0092989311\n",
      "[16063,     1] loss: 0.0092989184\n",
      "[16064,     1] loss: 0.0092989087\n",
      "[16065,     1] loss: 0.0092988983\n",
      "[16066,     1] loss: 0.0092988841\n",
      "[16067,     1] loss: 0.0092988737\n",
      "[16068,     1] loss: 0.0092988618\n",
      "[16069,     1] loss: 0.0092988491\n",
      "[16070,     1] loss: 0.0092988379\n",
      "[16071,     1] loss: 0.0092988253\n",
      "[16072,     1] loss: 0.0092988141\n",
      "[16073,     1] loss: 0.0092988029\n",
      "[16074,     1] loss: 0.0092987902\n",
      "[16075,     1] loss: 0.0092987798\n",
      "[16076,     1] loss: 0.0092987686\n",
      "[16077,     1] loss: 0.0092987560\n",
      "[16078,     1] loss: 0.0092987441\n",
      "[16079,     1] loss: 0.0092987329\n",
      "[16080,     1] loss: 0.0092987210\n",
      "[16081,     1] loss: 0.0092987090\n",
      "[16082,     1] loss: 0.0092986979\n",
      "[16083,     1] loss: 0.0092986867\n",
      "[16084,     1] loss: 0.0092986733\n",
      "[16085,     1] loss: 0.0092986636\n",
      "[16086,     1] loss: 0.0092986524\n",
      "[16087,     1] loss: 0.0092986390\n",
      "[16088,     1] loss: 0.0092986286\n",
      "[16089,     1] loss: 0.0092986174\n",
      "[16090,     1] loss: 0.0092986040\n",
      "[16091,     1] loss: 0.0092985943\n",
      "[16092,     1] loss: 0.0092985824\n",
      "[16093,     1] loss: 0.0092985697\n",
      "[16094,     1] loss: 0.0092985585\n",
      "[16095,     1] loss: 0.0092985474\n",
      "[16096,     1] loss: 0.0092985354\n",
      "[16097,     1] loss: 0.0092985243\n",
      "[16098,     1] loss: 0.0092985131\n",
      "[16099,     1] loss: 0.0092985027\n",
      "[16100,     1] loss: 0.0092984892\n",
      "[16101,     1] loss: 0.0092984788\n",
      "[16102,     1] loss: 0.0092984654\n",
      "[16103,     1] loss: 0.0092984542\n",
      "[16104,     1] loss: 0.0092984430\n",
      "[16105,     1] loss: 0.0092984311\n",
      "[16106,     1] loss: 0.0092984185\n",
      "[16107,     1] loss: 0.0092984080\n",
      "[16108,     1] loss: 0.0092983961\n",
      "[16109,     1] loss: 0.0092983864\n",
      "[16110,     1] loss: 0.0092983730\n",
      "[16111,     1] loss: 0.0092983611\n",
      "[16112,     1] loss: 0.0092983492\n",
      "[16113,     1] loss: 0.0092983395\n",
      "[16114,     1] loss: 0.0092983283\n",
      "[16115,     1] loss: 0.0092983156\n",
      "[16116,     1] loss: 0.0092983045\n",
      "[16117,     1] loss: 0.0092982933\n",
      "[16118,     1] loss: 0.0092982806\n",
      "[16119,     1] loss: 0.0092982702\n",
      "[16120,     1] loss: 0.0092982575\n",
      "[16121,     1] loss: 0.0092982456\n",
      "[16122,     1] loss: 0.0092982352\n",
      "[16123,     1] loss: 0.0092982225\n",
      "[16124,     1] loss: 0.0092982121\n",
      "[16125,     1] loss: 0.0092982009\n",
      "[16126,     1] loss: 0.0092981897\n",
      "[16127,     1] loss: 0.0092981778\n",
      "[16128,     1] loss: 0.0092981659\n",
      "[16129,     1] loss: 0.0092981540\n",
      "[16130,     1] loss: 0.0092981420\n",
      "[16131,     1] loss: 0.0092981309\n",
      "[16132,     1] loss: 0.0092981197\n",
      "[16133,     1] loss: 0.0092981085\n",
      "[16134,     1] loss: 0.0092980973\n",
      "[16135,     1] loss: 0.0092980862\n",
      "[16136,     1] loss: 0.0092980735\n",
      "[16137,     1] loss: 0.0092980623\n",
      "[16138,     1] loss: 0.0092980511\n",
      "[16139,     1] loss: 0.0092980400\n",
      "[16140,     1] loss: 0.0092980273\n",
      "[16141,     1] loss: 0.0092980169\n",
      "[16142,     1] loss: 0.0092980057\n",
      "[16143,     1] loss: 0.0092979930\n",
      "[16144,     1] loss: 0.0092979833\n",
      "[16145,     1] loss: 0.0092979692\n",
      "[16146,     1] loss: 0.0092979595\n",
      "[16147,     1] loss: 0.0092979483\n",
      "[16148,     1] loss: 0.0092979364\n",
      "[16149,     1] loss: 0.0092979252\n",
      "[16150,     1] loss: 0.0092979141\n",
      "[16151,     1] loss: 0.0092979021\n",
      "[16152,     1] loss: 0.0092978902\n",
      "[16153,     1] loss: 0.0092978805\n",
      "[16154,     1] loss: 0.0092978694\n",
      "[16155,     1] loss: 0.0092978574\n",
      "[16156,     1] loss: 0.0092978463\n",
      "[16157,     1] loss: 0.0092978351\n",
      "[16158,     1] loss: 0.0092978224\n",
      "[16159,     1] loss: 0.0092978120\n",
      "[16160,     1] loss: 0.0092978016\n",
      "[16161,     1] loss: 0.0092977889\n",
      "[16162,     1] loss: 0.0092977777\n",
      "[16163,     1] loss: 0.0092977665\n",
      "[16164,     1] loss: 0.0092977546\n",
      "[16165,     1] loss: 0.0092977434\n",
      "[16166,     1] loss: 0.0092977330\n",
      "[16167,     1] loss: 0.0092977203\n",
      "[16168,     1] loss: 0.0092977092\n",
      "[16169,     1] loss: 0.0092976995\n",
      "[16170,     1] loss: 0.0092976876\n",
      "[16171,     1] loss: 0.0092976764\n",
      "[16172,     1] loss: 0.0092976637\n",
      "[16173,     1] loss: 0.0092976540\n",
      "[16174,     1] loss: 0.0092976406\n",
      "[16175,     1] loss: 0.0092976294\n",
      "[16176,     1] loss: 0.0092976183\n",
      "[16177,     1] loss: 0.0092976063\n",
      "[16178,     1] loss: 0.0092975959\n",
      "[16179,     1] loss: 0.0092975855\n",
      "[16180,     1] loss: 0.0092975728\n",
      "[16181,     1] loss: 0.0092975631\n",
      "[16182,     1] loss: 0.0092975527\n",
      "[16183,     1] loss: 0.0092975385\n",
      "[16184,     1] loss: 0.0092975289\n",
      "[16185,     1] loss: 0.0092975177\n",
      "[16186,     1] loss: 0.0092975058\n",
      "[16187,     1] loss: 0.0092974946\n",
      "[16188,     1] loss: 0.0092974842\n",
      "[16189,     1] loss: 0.0092974730\n",
      "[16190,     1] loss: 0.0092974603\n",
      "[16191,     1] loss: 0.0092974499\n",
      "[16192,     1] loss: 0.0092974380\n",
      "[16193,     1] loss: 0.0092974268\n",
      "[16194,     1] loss: 0.0092974156\n",
      "[16195,     1] loss: 0.0092974044\n",
      "[16196,     1] loss: 0.0092973933\n",
      "[16197,     1] loss: 0.0092973821\n",
      "[16198,     1] loss: 0.0092973694\n",
      "[16199,     1] loss: 0.0092973597\n",
      "[16200,     1] loss: 0.0092973471\n",
      "[16201,     1] loss: 0.0092973374\n",
      "[16202,     1] loss: 0.0092973255\n",
      "[16203,     1] loss: 0.0092973135\n",
      "[16204,     1] loss: 0.0092973016\n",
      "[16205,     1] loss: 0.0092972919\n",
      "[16206,     1] loss: 0.0092972808\n",
      "[16207,     1] loss: 0.0092972688\n",
      "[16208,     1] loss: 0.0092972577\n",
      "[16209,     1] loss: 0.0092972472\n",
      "[16210,     1] loss: 0.0092972353\n",
      "[16211,     1] loss: 0.0092972241\n",
      "[16212,     1] loss: 0.0092972130\n",
      "[16213,     1] loss: 0.0092972033\n",
      "[16214,     1] loss: 0.0092971914\n",
      "[16215,     1] loss: 0.0092971779\n",
      "[16216,     1] loss: 0.0092971675\n",
      "[16217,     1] loss: 0.0092971571\n",
      "[16218,     1] loss: 0.0092971459\n",
      "[16219,     1] loss: 0.0092971340\n",
      "[16220,     1] loss: 0.0092971228\n",
      "[16221,     1] loss: 0.0092971116\n",
      "[16222,     1] loss: 0.0092971012\n",
      "[16223,     1] loss: 0.0092970900\n",
      "[16224,     1] loss: 0.0092970788\n",
      "[16225,     1] loss: 0.0092970684\n",
      "[16226,     1] loss: 0.0092970565\n",
      "[16227,     1] loss: 0.0092970461\n",
      "[16228,     1] loss: 0.0092970341\n",
      "[16229,     1] loss: 0.0092970230\n",
      "[16230,     1] loss: 0.0092970125\n",
      "[16231,     1] loss: 0.0092970006\n",
      "[16232,     1] loss: 0.0092969894\n",
      "[16233,     1] loss: 0.0092969775\n",
      "[16234,     1] loss: 0.0092969671\n",
      "[16235,     1] loss: 0.0092969567\n",
      "[16236,     1] loss: 0.0092969447\n",
      "[16237,     1] loss: 0.0092969336\n",
      "[16238,     1] loss: 0.0092969224\n",
      "[16239,     1] loss: 0.0092969105\n",
      "[16240,     1] loss: 0.0092969008\n",
      "[16241,     1] loss: 0.0092968889\n",
      "[16242,     1] loss: 0.0092968784\n",
      "[16243,     1] loss: 0.0092968658\n",
      "[16244,     1] loss: 0.0092968568\n",
      "[16245,     1] loss: 0.0092968449\n",
      "[16246,     1] loss: 0.0092968337\n",
      "[16247,     1] loss: 0.0092968225\n",
      "[16248,     1] loss: 0.0092968129\n",
      "[16249,     1] loss: 0.0092968002\n",
      "[16250,     1] loss: 0.0092967883\n",
      "[16251,     1] loss: 0.0092967786\n",
      "[16252,     1] loss: 0.0092967674\n",
      "[16253,     1] loss: 0.0092967570\n",
      "[16254,     1] loss: 0.0092967458\n",
      "[16255,     1] loss: 0.0092967346\n",
      "[16256,     1] loss: 0.0092967235\n",
      "[16257,     1] loss: 0.0092967138\n",
      "[16258,     1] loss: 0.0092967011\n",
      "[16259,     1] loss: 0.0092966899\n",
      "[16260,     1] loss: 0.0092966795\n",
      "[16261,     1] loss: 0.0092966698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16262,     1] loss: 0.0092966571\n",
      "[16263,     1] loss: 0.0092966460\n",
      "[16264,     1] loss: 0.0092966363\n",
      "[16265,     1] loss: 0.0092966244\n",
      "[16266,     1] loss: 0.0092966117\n",
      "[16267,     1] loss: 0.0092966020\n",
      "[16268,     1] loss: 0.0092965916\n",
      "[16269,     1] loss: 0.0092965789\n",
      "[16270,     1] loss: 0.0092965700\n",
      "[16271,     1] loss: 0.0092965581\n",
      "[16272,     1] loss: 0.0092965469\n",
      "[16273,     1] loss: 0.0092965364\n",
      "[16274,     1] loss: 0.0092965253\n",
      "[16275,     1] loss: 0.0092965141\n",
      "[16276,     1] loss: 0.0092965022\n",
      "[16277,     1] loss: 0.0092964925\n",
      "[16278,     1] loss: 0.0092964821\n",
      "[16279,     1] loss: 0.0092964701\n",
      "[16280,     1] loss: 0.0092964590\n",
      "[16281,     1] loss: 0.0092964493\n",
      "[16282,     1] loss: 0.0092964374\n",
      "[16283,     1] loss: 0.0092964269\n",
      "[16284,     1] loss: 0.0092964157\n",
      "[16285,     1] loss: 0.0092964038\n",
      "[16286,     1] loss: 0.0092963941\n",
      "[16287,     1] loss: 0.0092963845\n",
      "[16288,     1] loss: 0.0092963718\n",
      "[16289,     1] loss: 0.0092963606\n",
      "[16290,     1] loss: 0.0092963502\n",
      "[16291,     1] loss: 0.0092963398\n",
      "[16292,     1] loss: 0.0092963286\n",
      "[16293,     1] loss: 0.0092963174\n",
      "[16294,     1] loss: 0.0092963070\n",
      "[16295,     1] loss: 0.0092962958\n",
      "[16296,     1] loss: 0.0092962846\n",
      "[16297,     1] loss: 0.0092962749\n",
      "[16298,     1] loss: 0.0092962630\n",
      "[16299,     1] loss: 0.0092962533\n",
      "[16300,     1] loss: 0.0092962407\n",
      "[16301,     1] loss: 0.0092962332\n",
      "[16302,     1] loss: 0.0092962198\n",
      "[16303,     1] loss: 0.0092962086\n",
      "[16304,     1] loss: 0.0092961982\n",
      "[16305,     1] loss: 0.0092961878\n",
      "[16306,     1] loss: 0.0092961766\n",
      "[16307,     1] loss: 0.0092961654\n",
      "[16308,     1] loss: 0.0092961550\n",
      "[16309,     1] loss: 0.0092961438\n",
      "[16310,     1] loss: 0.0092961334\n",
      "[16311,     1] loss: 0.0092961237\n",
      "[16312,     1] loss: 0.0092961118\n",
      "[16313,     1] loss: 0.0092961006\n",
      "[16314,     1] loss: 0.0092960894\n",
      "[16315,     1] loss: 0.0092960797\n",
      "[16316,     1] loss: 0.0092960678\n",
      "[16317,     1] loss: 0.0092960574\n",
      "[16318,     1] loss: 0.0092960469\n",
      "[16319,     1] loss: 0.0092960365\n",
      "[16320,     1] loss: 0.0092960246\n",
      "[16321,     1] loss: 0.0092960157\n",
      "[16322,     1] loss: 0.0092960045\n",
      "[16323,     1] loss: 0.0092959926\n",
      "[16324,     1] loss: 0.0092959814\n",
      "[16325,     1] loss: 0.0092959724\n",
      "[16326,     1] loss: 0.0092959598\n",
      "[16327,     1] loss: 0.0092959493\n",
      "[16328,     1] loss: 0.0092959389\n",
      "[16329,     1] loss: 0.0092959277\n",
      "[16330,     1] loss: 0.0092959180\n",
      "[16331,     1] loss: 0.0092959054\n",
      "[16332,     1] loss: 0.0092958964\n",
      "[16333,     1] loss: 0.0092958845\n",
      "[16334,     1] loss: 0.0092958748\n",
      "[16335,     1] loss: 0.0092958614\n",
      "[16336,     1] loss: 0.0092958532\n",
      "[16337,     1] loss: 0.0092958413\n",
      "[16338,     1] loss: 0.0092958309\n",
      "[16339,     1] loss: 0.0092958204\n",
      "[16340,     1] loss: 0.0092958100\n",
      "[16341,     1] loss: 0.0092957988\n",
      "[16342,     1] loss: 0.0092957884\n",
      "[16343,     1] loss: 0.0092957780\n",
      "[16344,     1] loss: 0.0092957675\n",
      "[16345,     1] loss: 0.0092957564\n",
      "[16346,     1] loss: 0.0092957459\n",
      "[16347,     1] loss: 0.0092957348\n",
      "[16348,     1] loss: 0.0092957243\n",
      "[16349,     1] loss: 0.0092957132\n",
      "[16350,     1] loss: 0.0092957020\n",
      "[16351,     1] loss: 0.0092956908\n",
      "[16352,     1] loss: 0.0092956804\n",
      "[16353,     1] loss: 0.0092956692\n",
      "[16354,     1] loss: 0.0092956588\n",
      "[16355,     1] loss: 0.0092956498\n",
      "[16356,     1] loss: 0.0092956387\n",
      "[16357,     1] loss: 0.0092956290\n",
      "[16358,     1] loss: 0.0092956178\n",
      "[16359,     1] loss: 0.0092956066\n",
      "[16360,     1] loss: 0.0092955969\n",
      "[16361,     1] loss: 0.0092955872\n",
      "[16362,     1] loss: 0.0092955738\n",
      "[16363,     1] loss: 0.0092955634\n",
      "[16364,     1] loss: 0.0092955522\n",
      "[16365,     1] loss: 0.0092955433\n",
      "[16366,     1] loss: 0.0092955306\n",
      "[16367,     1] loss: 0.0092955209\n",
      "[16368,     1] loss: 0.0092955098\n",
      "[16369,     1] loss: 0.0092954986\n",
      "[16370,     1] loss: 0.0092954889\n",
      "[16371,     1] loss: 0.0092954777\n",
      "[16372,     1] loss: 0.0092954673\n",
      "[16373,     1] loss: 0.0092954554\n",
      "[16374,     1] loss: 0.0092954464\n",
      "[16375,     1] loss: 0.0092954352\n",
      "[16376,     1] loss: 0.0092954248\n",
      "[16377,     1] loss: 0.0092954151\n",
      "[16378,     1] loss: 0.0092954032\n",
      "[16379,     1] loss: 0.0092953920\n",
      "[16380,     1] loss: 0.0092953831\n",
      "[16381,     1] loss: 0.0092953712\n",
      "[16382,     1] loss: 0.0092953615\n",
      "[16383,     1] loss: 0.0092953503\n",
      "[16384,     1] loss: 0.0092953399\n",
      "[16385,     1] loss: 0.0092953295\n",
      "[16386,     1] loss: 0.0092953183\n",
      "[16387,     1] loss: 0.0092953086\n",
      "[16388,     1] loss: 0.0092952967\n",
      "[16389,     1] loss: 0.0092952855\n",
      "[16390,     1] loss: 0.0092952758\n",
      "[16391,     1] loss: 0.0092952654\n",
      "[16392,     1] loss: 0.0092952549\n",
      "[16393,     1] loss: 0.0092952445\n",
      "[16394,     1] loss: 0.0092952341\n",
      "[16395,     1] loss: 0.0092952229\n",
      "[16396,     1] loss: 0.0092952132\n",
      "[16397,     1] loss: 0.0092952013\n",
      "[16398,     1] loss: 0.0092951916\n",
      "[16399,     1] loss: 0.0092951812\n",
      "[16400,     1] loss: 0.0092951693\n",
      "[16401,     1] loss: 0.0092951603\n",
      "[16402,     1] loss: 0.0092951491\n",
      "[16403,     1] loss: 0.0092951380\n",
      "[16404,     1] loss: 0.0092951275\n",
      "[16405,     1] loss: 0.0092951179\n",
      "[16406,     1] loss: 0.0092951074\n",
      "[16407,     1] loss: 0.0092950970\n",
      "[16408,     1] loss: 0.0092950858\n",
      "[16409,     1] loss: 0.0092950754\n",
      "[16410,     1] loss: 0.0092950650\n",
      "[16411,     1] loss: 0.0092950560\n",
      "[16412,     1] loss: 0.0092950433\n",
      "[16413,     1] loss: 0.0092950337\n",
      "[16414,     1] loss: 0.0092950232\n",
      "[16415,     1] loss: 0.0092950135\n",
      "[16416,     1] loss: 0.0092950016\n",
      "[16417,     1] loss: 0.0092949919\n",
      "[16418,     1] loss: 0.0092949808\n",
      "[16419,     1] loss: 0.0092949718\n",
      "[16420,     1] loss: 0.0092949592\n",
      "[16421,     1] loss: 0.0092949495\n",
      "[16422,     1] loss: 0.0092949383\n",
      "[16423,     1] loss: 0.0092949294\n",
      "[16424,     1] loss: 0.0092949182\n",
      "[16425,     1] loss: 0.0092949092\n",
      "[16426,     1] loss: 0.0092948981\n",
      "[16427,     1] loss: 0.0092948884\n",
      "[16428,     1] loss: 0.0092948772\n",
      "[16429,     1] loss: 0.0092948668\n",
      "[16430,     1] loss: 0.0092948571\n",
      "[16431,     1] loss: 0.0092948467\n",
      "[16432,     1] loss: 0.0092948362\n",
      "[16433,     1] loss: 0.0092948258\n",
      "[16434,     1] loss: 0.0092948139\n",
      "[16435,     1] loss: 0.0092948049\n",
      "[16436,     1] loss: 0.0092947938\n",
      "[16437,     1] loss: 0.0092947826\n",
      "[16438,     1] loss: 0.0092947729\n",
      "[16439,     1] loss: 0.0092947632\n",
      "[16440,     1] loss: 0.0092947520\n",
      "[16441,     1] loss: 0.0092947416\n",
      "[16442,     1] loss: 0.0092947304\n",
      "[16443,     1] loss: 0.0092947200\n",
      "[16444,     1] loss: 0.0092947096\n",
      "[16445,     1] loss: 0.0092947006\n",
      "[16446,     1] loss: 0.0092946894\n",
      "[16447,     1] loss: 0.0092946805\n",
      "[16448,     1] loss: 0.0092946693\n",
      "[16449,     1] loss: 0.0092946589\n",
      "[16450,     1] loss: 0.0092946492\n",
      "[16451,     1] loss: 0.0092946380\n",
      "[16452,     1] loss: 0.0092946291\n",
      "[16453,     1] loss: 0.0092946179\n",
      "[16454,     1] loss: 0.0092946075\n",
      "[16455,     1] loss: 0.0092945971\n",
      "[16456,     1] loss: 0.0092945859\n",
      "[16457,     1] loss: 0.0092945762\n",
      "[16458,     1] loss: 0.0092945650\n",
      "[16459,     1] loss: 0.0092945553\n",
      "[16460,     1] loss: 0.0092945464\n",
      "[16461,     1] loss: 0.0092945337\n",
      "[16462,     1] loss: 0.0092945240\n",
      "[16463,     1] loss: 0.0092945136\n",
      "[16464,     1] loss: 0.0092945032\n",
      "[16465,     1] loss: 0.0092944928\n",
      "[16466,     1] loss: 0.0092944816\n",
      "[16467,     1] loss: 0.0092944704\n",
      "[16468,     1] loss: 0.0092944615\n",
      "[16469,     1] loss: 0.0092944510\n",
      "[16470,     1] loss: 0.0092944391\n",
      "[16471,     1] loss: 0.0092944294\n",
      "[16472,     1] loss: 0.0092944190\n",
      "[16473,     1] loss: 0.0092944093\n",
      "[16474,     1] loss: 0.0092943996\n",
      "[16475,     1] loss: 0.0092943884\n",
      "[16476,     1] loss: 0.0092943780\n",
      "[16477,     1] loss: 0.0092943668\n",
      "[16478,     1] loss: 0.0092943557\n",
      "[16479,     1] loss: 0.0092943467\n",
      "[16480,     1] loss: 0.0092943363\n",
      "[16481,     1] loss: 0.0092943259\n",
      "[16482,     1] loss: 0.0092943169\n",
      "[16483,     1] loss: 0.0092943065\n",
      "[16484,     1] loss: 0.0092942953\n",
      "[16485,     1] loss: 0.0092942841\n",
      "[16486,     1] loss: 0.0092942744\n",
      "[16487,     1] loss: 0.0092942633\n",
      "[16488,     1] loss: 0.0092942543\n",
      "[16489,     1] loss: 0.0092942424\n",
      "[16490,     1] loss: 0.0092942327\n",
      "[16491,     1] loss: 0.0092942238\n",
      "[16492,     1] loss: 0.0092942119\n",
      "[16493,     1] loss: 0.0092942037\n",
      "[16494,     1] loss: 0.0092941917\n",
      "[16495,     1] loss: 0.0092941821\n",
      "[16496,     1] loss: 0.0092941701\n",
      "[16497,     1] loss: 0.0092941612\n",
      "[16498,     1] loss: 0.0092941523\n",
      "[16499,     1] loss: 0.0092941411\n",
      "[16500,     1] loss: 0.0092941307\n",
      "[16501,     1] loss: 0.0092941217\n",
      "[16502,     1] loss: 0.0092941090\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16503,     1] loss: 0.0092940994\n",
      "[16504,     1] loss: 0.0092940897\n",
      "[16505,     1] loss: 0.0092940792\n",
      "[16506,     1] loss: 0.0092940696\n",
      "[16507,     1] loss: 0.0092940599\n",
      "[16508,     1] loss: 0.0092940487\n",
      "[16509,     1] loss: 0.0092940390\n",
      "[16510,     1] loss: 0.0092940293\n",
      "[16511,     1] loss: 0.0092940189\n",
      "[16512,     1] loss: 0.0092940070\n",
      "[16513,     1] loss: 0.0092940010\n",
      "[16514,     1] loss: 0.0092939883\n",
      "[16515,     1] loss: 0.0092939764\n",
      "[16516,     1] loss: 0.0092939697\n",
      "[16517,     1] loss: 0.0092939578\n",
      "[16518,     1] loss: 0.0092939459\n",
      "[16519,     1] loss: 0.0092939369\n",
      "[16520,     1] loss: 0.0092939273\n",
      "[16521,     1] loss: 0.0092939153\n",
      "[16522,     1] loss: 0.0092939056\n",
      "[16523,     1] loss: 0.0092938967\n",
      "[16524,     1] loss: 0.0092938848\n",
      "[16525,     1] loss: 0.0092938751\n",
      "[16526,     1] loss: 0.0092938662\n",
      "[16527,     1] loss: 0.0092938565\n",
      "[16528,     1] loss: 0.0092938446\n",
      "[16529,     1] loss: 0.0092938349\n",
      "[16530,     1] loss: 0.0092938244\n",
      "[16531,     1] loss: 0.0092938140\n",
      "[16532,     1] loss: 0.0092938051\n",
      "[16533,     1] loss: 0.0092937954\n",
      "[16534,     1] loss: 0.0092937849\n",
      "[16535,     1] loss: 0.0092937745\n",
      "[16536,     1] loss: 0.0092937626\n",
      "[16537,     1] loss: 0.0092937537\n",
      "[16538,     1] loss: 0.0092937417\n",
      "[16539,     1] loss: 0.0092937328\n",
      "[16540,     1] loss: 0.0092937231\n",
      "[16541,     1] loss: 0.0092937119\n",
      "[16542,     1] loss: 0.0092937022\n",
      "[16543,     1] loss: 0.0092936940\n",
      "[16544,     1] loss: 0.0092936829\n",
      "[16545,     1] loss: 0.0092936710\n",
      "[16546,     1] loss: 0.0092936628\n",
      "[16547,     1] loss: 0.0092936508\n",
      "[16548,     1] loss: 0.0092936404\n",
      "[16549,     1] loss: 0.0092936322\n",
      "[16550,     1] loss: 0.0092936218\n",
      "[16551,     1] loss: 0.0092936121\n",
      "[16552,     1] loss: 0.0092936017\n",
      "[16553,     1] loss: 0.0092935905\n",
      "[16554,     1] loss: 0.0092935801\n",
      "[16555,     1] loss: 0.0092935719\n",
      "[16556,     1] loss: 0.0092935614\n",
      "[16557,     1] loss: 0.0092935503\n",
      "[16558,     1] loss: 0.0092935398\n",
      "[16559,     1] loss: 0.0092935309\n",
      "[16560,     1] loss: 0.0092935197\n",
      "[16561,     1] loss: 0.0092935108\n",
      "[16562,     1] loss: 0.0092935011\n",
      "[16563,     1] loss: 0.0092934892\n",
      "[16564,     1] loss: 0.0092934795\n",
      "[16565,     1] loss: 0.0092934698\n",
      "[16566,     1] loss: 0.0092934608\n",
      "[16567,     1] loss: 0.0092934497\n",
      "[16568,     1] loss: 0.0092934400\n",
      "[16569,     1] loss: 0.0092934296\n",
      "[16570,     1] loss: 0.0092934214\n",
      "[16571,     1] loss: 0.0092934087\n",
      "[16572,     1] loss: 0.0092933983\n",
      "[16573,     1] loss: 0.0092933893\n",
      "[16574,     1] loss: 0.0092933804\n",
      "[16575,     1] loss: 0.0092933677\n",
      "[16576,     1] loss: 0.0092933588\n",
      "[16577,     1] loss: 0.0092933483\n",
      "[16578,     1] loss: 0.0092933387\n",
      "[16579,     1] loss: 0.0092933290\n",
      "[16580,     1] loss: 0.0092933170\n",
      "[16581,     1] loss: 0.0092933096\n",
      "[16582,     1] loss: 0.0092932984\n",
      "[16583,     1] loss: 0.0092932895\n",
      "[16584,     1] loss: 0.0092932791\n",
      "[16585,     1] loss: 0.0092932694\n",
      "[16586,     1] loss: 0.0092932589\n",
      "[16587,     1] loss: 0.0092932500\n",
      "[16588,     1] loss: 0.0092932411\n",
      "[16589,     1] loss: 0.0092932291\n",
      "[16590,     1] loss: 0.0092932202\n",
      "[16591,     1] loss: 0.0092932105\n",
      "[16592,     1] loss: 0.0092931993\n",
      "[16593,     1] loss: 0.0092931904\n",
      "[16594,     1] loss: 0.0092931807\n",
      "[16595,     1] loss: 0.0092931710\n",
      "[16596,     1] loss: 0.0092931598\n",
      "[16597,     1] loss: 0.0092931509\n",
      "[16598,     1] loss: 0.0092931405\n",
      "[16599,     1] loss: 0.0092931293\n",
      "[16600,     1] loss: 0.0092931204\n",
      "[16601,     1] loss: 0.0092931114\n",
      "[16602,     1] loss: 0.0092931010\n",
      "[16603,     1] loss: 0.0092930906\n",
      "[16604,     1] loss: 0.0092930801\n",
      "[16605,     1] loss: 0.0092930704\n",
      "[16606,     1] loss: 0.0092930607\n",
      "[16607,     1] loss: 0.0092930511\n",
      "[16608,     1] loss: 0.0092930399\n",
      "[16609,     1] loss: 0.0092930332\n",
      "[16610,     1] loss: 0.0092930205\n",
      "[16611,     1] loss: 0.0092930131\n",
      "[16612,     1] loss: 0.0092930034\n",
      "[16613,     1] loss: 0.0092929922\n",
      "[16614,     1] loss: 0.0092929825\n",
      "[16615,     1] loss: 0.0092929743\n",
      "[16616,     1] loss: 0.0092929624\n",
      "[16617,     1] loss: 0.0092929535\n",
      "[16618,     1] loss: 0.0092929430\n",
      "[16619,     1] loss: 0.0092929333\n",
      "[16620,     1] loss: 0.0092929229\n",
      "[16621,     1] loss: 0.0092929140\n",
      "[16622,     1] loss: 0.0092929035\n",
      "[16623,     1] loss: 0.0092928924\n",
      "[16624,     1] loss: 0.0092928842\n",
      "[16625,     1] loss: 0.0092928737\n",
      "[16626,     1] loss: 0.0092928633\n",
      "[16627,     1] loss: 0.0092928536\n",
      "[16628,     1] loss: 0.0092928432\n",
      "[16629,     1] loss: 0.0092928335\n",
      "[16630,     1] loss: 0.0092928238\n",
      "[16631,     1] loss: 0.0092928149\n",
      "[16632,     1] loss: 0.0092928022\n",
      "[16633,     1] loss: 0.0092927948\n",
      "[16634,     1] loss: 0.0092927843\n",
      "[16635,     1] loss: 0.0092927746\n",
      "[16636,     1] loss: 0.0092927642\n",
      "[16637,     1] loss: 0.0092927553\n",
      "[16638,     1] loss: 0.0092927463\n",
      "[16639,     1] loss: 0.0092927352\n",
      "[16640,     1] loss: 0.0092927262\n",
      "[16641,     1] loss: 0.0092927150\n",
      "[16642,     1] loss: 0.0092927061\n",
      "[16643,     1] loss: 0.0092926957\n",
      "[16644,     1] loss: 0.0092926860\n",
      "[16645,     1] loss: 0.0092926770\n",
      "[16646,     1] loss: 0.0092926674\n",
      "[16647,     1] loss: 0.0092926569\n",
      "[16648,     1] loss: 0.0092926480\n",
      "[16649,     1] loss: 0.0092926383\n",
      "[16650,     1] loss: 0.0092926271\n",
      "[16651,     1] loss: 0.0092926174\n",
      "[16652,     1] loss: 0.0092926092\n",
      "[16653,     1] loss: 0.0092925988\n",
      "[16654,     1] loss: 0.0092925891\n",
      "[16655,     1] loss: 0.0092925794\n",
      "[16656,     1] loss: 0.0092925683\n",
      "[16657,     1] loss: 0.0092925593\n",
      "[16658,     1] loss: 0.0092925504\n",
      "[16659,     1] loss: 0.0092925407\n",
      "[16660,     1] loss: 0.0092925303\n",
      "[16661,     1] loss: 0.0092925213\n",
      "[16662,     1] loss: 0.0092925109\n",
      "[16663,     1] loss: 0.0092925005\n",
      "[16664,     1] loss: 0.0092924915\n",
      "[16665,     1] loss: 0.0092924818\n",
      "[16666,     1] loss: 0.0092924707\n",
      "[16667,     1] loss: 0.0092924625\n",
      "[16668,     1] loss: 0.0092924520\n",
      "[16669,     1] loss: 0.0092924424\n",
      "[16670,     1] loss: 0.0092924349\n",
      "[16671,     1] loss: 0.0092924237\n",
      "[16672,     1] loss: 0.0092924118\n",
      "[16673,     1] loss: 0.0092924036\n",
      "[16674,     1] loss: 0.0092923932\n",
      "[16675,     1] loss: 0.0092923842\n",
      "[16676,     1] loss: 0.0092923753\n",
      "[16677,     1] loss: 0.0092923656\n",
      "[16678,     1] loss: 0.0092923559\n",
      "[16679,     1] loss: 0.0092923470\n",
      "[16680,     1] loss: 0.0092923358\n",
      "[16681,     1] loss: 0.0092923254\n",
      "[16682,     1] loss: 0.0092923172\n",
      "[16683,     1] loss: 0.0092923060\n",
      "[16684,     1] loss: 0.0092922963\n",
      "[16685,     1] loss: 0.0092922881\n",
      "[16686,     1] loss: 0.0092922769\n",
      "[16687,     1] loss: 0.0092922665\n",
      "[16688,     1] loss: 0.0092922591\n",
      "[16689,     1] loss: 0.0092922471\n",
      "[16690,     1] loss: 0.0092922375\n",
      "[16691,     1] loss: 0.0092922285\n",
      "[16692,     1] loss: 0.0092922188\n",
      "[16693,     1] loss: 0.0092922099\n",
      "[16694,     1] loss: 0.0092921987\n",
      "[16695,     1] loss: 0.0092921905\n",
      "[16696,     1] loss: 0.0092921801\n",
      "[16697,     1] loss: 0.0092921712\n",
      "[16698,     1] loss: 0.0092921615\n",
      "[16699,     1] loss: 0.0092921518\n",
      "[16700,     1] loss: 0.0092921413\n",
      "[16701,     1] loss: 0.0092921302\n",
      "[16702,     1] loss: 0.0092921235\n",
      "[16703,     1] loss: 0.0092921115\n",
      "[16704,     1] loss: 0.0092921026\n",
      "[16705,     1] loss: 0.0092920929\n",
      "[16706,     1] loss: 0.0092920832\n",
      "[16707,     1] loss: 0.0092920728\n",
      "[16708,     1] loss: 0.0092920631\n",
      "[16709,     1] loss: 0.0092920542\n",
      "[16710,     1] loss: 0.0092920452\n",
      "[16711,     1] loss: 0.0092920341\n",
      "[16712,     1] loss: 0.0092920266\n",
      "[16713,     1] loss: 0.0092920169\n",
      "[16714,     1] loss: 0.0092920065\n",
      "[16715,     1] loss: 0.0092919976\n",
      "[16716,     1] loss: 0.0092919879\n",
      "[16717,     1] loss: 0.0092919759\n",
      "[16718,     1] loss: 0.0092919677\n",
      "[16719,     1] loss: 0.0092919618\n",
      "[16720,     1] loss: 0.0092919491\n",
      "[16721,     1] loss: 0.0092919394\n",
      "[16722,     1] loss: 0.0092919298\n",
      "[16723,     1] loss: 0.0092919201\n",
      "[16724,     1] loss: 0.0092919104\n",
      "[16725,     1] loss: 0.0092919007\n",
      "[16726,     1] loss: 0.0092918910\n",
      "[16727,     1] loss: 0.0092918821\n",
      "[16728,     1] loss: 0.0092918724\n",
      "[16729,     1] loss: 0.0092918634\n",
      "[16730,     1] loss: 0.0092918523\n",
      "[16731,     1] loss: 0.0092918441\n",
      "[16732,     1] loss: 0.0092918344\n",
      "[16733,     1] loss: 0.0092918232\n",
      "[16734,     1] loss: 0.0092918150\n",
      "[16735,     1] loss: 0.0092918053\n",
      "[16736,     1] loss: 0.0092917949\n",
      "[16737,     1] loss: 0.0092917860\n",
      "[16738,     1] loss: 0.0092917755\n",
      "[16739,     1] loss: 0.0092917666\n",
      "[16740,     1] loss: 0.0092917584\n",
      "[16741,     1] loss: 0.0092917480\n",
      "[16742,     1] loss: 0.0092917383\n",
      "[16743,     1] loss: 0.0092917293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16744,     1] loss: 0.0092917189\n",
      "[16745,     1] loss: 0.0092917100\n",
      "[16746,     1] loss: 0.0092916988\n",
      "[16747,     1] loss: 0.0092916898\n",
      "[16748,     1] loss: 0.0092916816\n",
      "[16749,     1] loss: 0.0092916720\n",
      "[16750,     1] loss: 0.0092916623\n",
      "[16751,     1] loss: 0.0092916526\n",
      "[16752,     1] loss: 0.0092916422\n",
      "[16753,     1] loss: 0.0092916340\n",
      "[16754,     1] loss: 0.0092916250\n",
      "[16755,     1] loss: 0.0092916153\n",
      "[16756,     1] loss: 0.0092916057\n",
      "[16757,     1] loss: 0.0092915982\n",
      "[16758,     1] loss: 0.0092915870\n",
      "[16759,     1] loss: 0.0092915781\n",
      "[16760,     1] loss: 0.0092915699\n",
      "[16761,     1] loss: 0.0092915602\n",
      "[16762,     1] loss: 0.0092915498\n",
      "[16763,     1] loss: 0.0092915408\n",
      "[16764,     1] loss: 0.0092915326\n",
      "[16765,     1] loss: 0.0092915229\n",
      "[16766,     1] loss: 0.0092915133\n",
      "[16767,     1] loss: 0.0092915036\n",
      "[16768,     1] loss: 0.0092914946\n",
      "[16769,     1] loss: 0.0092914842\n",
      "[16770,     1] loss: 0.0092914745\n",
      "[16771,     1] loss: 0.0092914656\n",
      "[16772,     1] loss: 0.0092914566\n",
      "[16773,     1] loss: 0.0092914470\n",
      "[16774,     1] loss: 0.0092914373\n",
      "[16775,     1] loss: 0.0092914298\n",
      "[16776,     1] loss: 0.0092914194\n",
      "[16777,     1] loss: 0.0092914097\n",
      "[16778,     1] loss: 0.0092914000\n",
      "[16779,     1] loss: 0.0092913911\n",
      "[16780,     1] loss: 0.0092913821\n",
      "[16781,     1] loss: 0.0092913717\n",
      "[16782,     1] loss: 0.0092913628\n",
      "[16783,     1] loss: 0.0092913516\n",
      "[16784,     1] loss: 0.0092913434\n",
      "[16785,     1] loss: 0.0092913345\n",
      "[16786,     1] loss: 0.0092913240\n",
      "[16787,     1] loss: 0.0092913151\n",
      "[16788,     1] loss: 0.0092913061\n",
      "[16789,     1] loss: 0.0092912957\n",
      "[16790,     1] loss: 0.0092912868\n",
      "[16791,     1] loss: 0.0092912763\n",
      "[16792,     1] loss: 0.0092912666\n",
      "[16793,     1] loss: 0.0092912577\n",
      "[16794,     1] loss: 0.0092912480\n",
      "[16795,     1] loss: 0.0092912376\n",
      "[16796,     1] loss: 0.0092912287\n",
      "[16797,     1] loss: 0.0092912182\n",
      "[16798,     1] loss: 0.0092912085\n",
      "[16799,     1] loss: 0.0092912011\n",
      "[16800,     1] loss: 0.0092911914\n",
      "[16801,     1] loss: 0.0092911810\n",
      "[16802,     1] loss: 0.0092911720\n",
      "[16803,     1] loss: 0.0092911609\n",
      "[16804,     1] loss: 0.0092911527\n",
      "[16805,     1] loss: 0.0092911437\n",
      "[16806,     1] loss: 0.0092911340\n",
      "[16807,     1] loss: 0.0092911243\n",
      "[16808,     1] loss: 0.0092911169\n",
      "[16809,     1] loss: 0.0092911057\n",
      "[16810,     1] loss: 0.0092910960\n",
      "[16811,     1] loss: 0.0092910871\n",
      "[16812,     1] loss: 0.0092910782\n",
      "[16813,     1] loss: 0.0092910685\n",
      "[16814,     1] loss: 0.0092910588\n",
      "[16815,     1] loss: 0.0092910498\n",
      "[16816,     1] loss: 0.0092910409\n",
      "[16817,     1] loss: 0.0092910312\n",
      "[16818,     1] loss: 0.0092910223\n",
      "[16819,     1] loss: 0.0092910133\n",
      "[16820,     1] loss: 0.0092910029\n",
      "[16821,     1] loss: 0.0092909940\n",
      "[16822,     1] loss: 0.0092909843\n",
      "[16823,     1] loss: 0.0092909746\n",
      "[16824,     1] loss: 0.0092909649\n",
      "[16825,     1] loss: 0.0092909560\n",
      "[16826,     1] loss: 0.0092909485\n",
      "[16827,     1] loss: 0.0092909388\n",
      "[16828,     1] loss: 0.0092909276\n",
      "[16829,     1] loss: 0.0092909187\n",
      "[16830,     1] loss: 0.0092909098\n",
      "[16831,     1] loss: 0.0092909008\n",
      "[16832,     1] loss: 0.0092908911\n",
      "[16833,     1] loss: 0.0092908837\n",
      "[16834,     1] loss: 0.0092908733\n",
      "[16835,     1] loss: 0.0092908628\n",
      "[16836,     1] loss: 0.0092908546\n",
      "[16837,     1] loss: 0.0092908449\n",
      "[16838,     1] loss: 0.0092908368\n",
      "[16839,     1] loss: 0.0092908271\n",
      "[16840,     1] loss: 0.0092908181\n",
      "[16841,     1] loss: 0.0092908092\n",
      "[16842,     1] loss: 0.0092907995\n",
      "[16843,     1] loss: 0.0092907906\n",
      "[16844,     1] loss: 0.0092907794\n",
      "[16845,     1] loss: 0.0092907719\n",
      "[16846,     1] loss: 0.0092907622\n",
      "[16847,     1] loss: 0.0092907533\n",
      "[16848,     1] loss: 0.0092907451\n",
      "[16849,     1] loss: 0.0092907347\n",
      "[16850,     1] loss: 0.0092907257\n",
      "[16851,     1] loss: 0.0092907175\n",
      "[16852,     1] loss: 0.0092907079\n",
      "[16853,     1] loss: 0.0092906974\n",
      "[16854,     1] loss: 0.0092906885\n",
      "[16855,     1] loss: 0.0092906788\n",
      "[16856,     1] loss: 0.0092906699\n",
      "[16857,     1] loss: 0.0092906609\n",
      "[16858,     1] loss: 0.0092906512\n",
      "[16859,     1] loss: 0.0092906430\n",
      "[16860,     1] loss: 0.0092906326\n",
      "[16861,     1] loss: 0.0092906252\n",
      "[16862,     1] loss: 0.0092906147\n",
      "[16863,     1] loss: 0.0092906050\n",
      "[16864,     1] loss: 0.0092905968\n",
      "[16865,     1] loss: 0.0092905864\n",
      "[16866,     1] loss: 0.0092905782\n",
      "[16867,     1] loss: 0.0092905693\n",
      "[16868,     1] loss: 0.0092905596\n",
      "[16869,     1] loss: 0.0092905499\n",
      "[16870,     1] loss: 0.0092905417\n",
      "[16871,     1] loss: 0.0092905320\n",
      "[16872,     1] loss: 0.0092905231\n",
      "[16873,     1] loss: 0.0092905134\n",
      "[16874,     1] loss: 0.0092905037\n",
      "[16875,     1] loss: 0.0092904955\n",
      "[16876,     1] loss: 0.0092904858\n",
      "[16877,     1] loss: 0.0092904769\n",
      "[16878,     1] loss: 0.0092904672\n",
      "[16879,     1] loss: 0.0092904590\n",
      "[16880,     1] loss: 0.0092904501\n",
      "[16881,     1] loss: 0.0092904411\n",
      "[16882,     1] loss: 0.0092904307\n",
      "[16883,     1] loss: 0.0092904218\n",
      "[16884,     1] loss: 0.0092904128\n",
      "[16885,     1] loss: 0.0092904039\n",
      "[16886,     1] loss: 0.0092903942\n",
      "[16887,     1] loss: 0.0092903860\n",
      "[16888,     1] loss: 0.0092903771\n",
      "[16889,     1] loss: 0.0092903689\n",
      "[16890,     1] loss: 0.0092903592\n",
      "[16891,     1] loss: 0.0092903502\n",
      "[16892,     1] loss: 0.0092903405\n",
      "[16893,     1] loss: 0.0092903331\n",
      "[16894,     1] loss: 0.0092903212\n",
      "[16895,     1] loss: 0.0092903115\n",
      "[16896,     1] loss: 0.0092903040\n",
      "[16897,     1] loss: 0.0092902958\n",
      "[16898,     1] loss: 0.0092902854\n",
      "[16899,     1] loss: 0.0092902757\n",
      "[16900,     1] loss: 0.0092902683\n",
      "[16901,     1] loss: 0.0092902593\n",
      "[16902,     1] loss: 0.0092902496\n",
      "[16903,     1] loss: 0.0092902407\n",
      "[16904,     1] loss: 0.0092902318\n",
      "[16905,     1] loss: 0.0092902213\n",
      "[16906,     1] loss: 0.0092902116\n",
      "[16907,     1] loss: 0.0092902042\n",
      "[16908,     1] loss: 0.0092901930\n",
      "[16909,     1] loss: 0.0092901841\n",
      "[16910,     1] loss: 0.0092901759\n",
      "[16911,     1] loss: 0.0092901669\n",
      "[16912,     1] loss: 0.0092901565\n",
      "[16913,     1] loss: 0.0092901468\n",
      "[16914,     1] loss: 0.0092901394\n",
      "[16915,     1] loss: 0.0092901297\n",
      "[16916,     1] loss: 0.0092901208\n",
      "[16917,     1] loss: 0.0092901111\n",
      "[16918,     1] loss: 0.0092901029\n",
      "[16919,     1] loss: 0.0092900924\n",
      "[16920,     1] loss: 0.0092900857\n",
      "[16921,     1] loss: 0.0092900753\n",
      "[16922,     1] loss: 0.0092900656\n",
      "[16923,     1] loss: 0.0092900574\n",
      "[16924,     1] loss: 0.0092900477\n",
      "[16925,     1] loss: 0.0092900388\n",
      "[16926,     1] loss: 0.0092900299\n",
      "[16927,     1] loss: 0.0092900209\n",
      "[16928,     1] loss: 0.0092900120\n",
      "[16929,     1] loss: 0.0092900023\n",
      "[16930,     1] loss: 0.0092899948\n",
      "[16931,     1] loss: 0.0092899837\n",
      "[16932,     1] loss: 0.0092899747\n",
      "[16933,     1] loss: 0.0092899673\n",
      "[16934,     1] loss: 0.0092899583\n",
      "[16935,     1] loss: 0.0092899486\n",
      "[16936,     1] loss: 0.0092899404\n",
      "[16937,     1] loss: 0.0092899308\n",
      "[16938,     1] loss: 0.0092899218\n",
      "[16939,     1] loss: 0.0092899136\n",
      "[16940,     1] loss: 0.0092899039\n",
      "[16941,     1] loss: 0.0092898957\n",
      "[16942,     1] loss: 0.0092898875\n",
      "[16943,     1] loss: 0.0092898779\n",
      "[16944,     1] loss: 0.0092898697\n",
      "[16945,     1] loss: 0.0092898600\n",
      "[16946,     1] loss: 0.0092898518\n",
      "[16947,     1] loss: 0.0092898414\n",
      "[16948,     1] loss: 0.0092898339\n",
      "[16949,     1] loss: 0.0092898242\n",
      "[16950,     1] loss: 0.0092898153\n",
      "[16951,     1] loss: 0.0092898063\n",
      "[16952,     1] loss: 0.0092897967\n",
      "[16953,     1] loss: 0.0092897885\n",
      "[16954,     1] loss: 0.0092897788\n",
      "[16955,     1] loss: 0.0092897698\n",
      "[16956,     1] loss: 0.0092897609\n",
      "[16957,     1] loss: 0.0092897527\n",
      "[16958,     1] loss: 0.0092897415\n",
      "[16959,     1] loss: 0.0092897348\n",
      "[16960,     1] loss: 0.0092897259\n",
      "[16961,     1] loss: 0.0092897162\n",
      "[16962,     1] loss: 0.0092897072\n",
      "[16963,     1] loss: 0.0092896983\n",
      "[16964,     1] loss: 0.0092896901\n",
      "[16965,     1] loss: 0.0092896797\n",
      "[16966,     1] loss: 0.0092896715\n",
      "[16967,     1] loss: 0.0092896618\n",
      "[16968,     1] loss: 0.0092896529\n",
      "[16969,     1] loss: 0.0092896454\n",
      "[16970,     1] loss: 0.0092896350\n",
      "[16971,     1] loss: 0.0092896260\n",
      "[16972,     1] loss: 0.0092896178\n",
      "[16973,     1] loss: 0.0092896082\n",
      "[16974,     1] loss: 0.0092896000\n",
      "[16975,     1] loss: 0.0092895910\n",
      "[16976,     1] loss: 0.0092895821\n",
      "[16977,     1] loss: 0.0092895739\n",
      "[16978,     1] loss: 0.0092895634\n",
      "[16979,     1] loss: 0.0092895553\n",
      "[16980,     1] loss: 0.0092895463\n",
      "[16981,     1] loss: 0.0092895366\n",
      "[16982,     1] loss: 0.0092895284\n",
      "[16983,     1] loss: 0.0092895187\n",
      "[16984,     1] loss: 0.0092895098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16985,     1] loss: 0.0092895024\n",
      "[16986,     1] loss: 0.0092894919\n",
      "[16987,     1] loss: 0.0092894837\n",
      "[16988,     1] loss: 0.0092894755\n",
      "[16989,     1] loss: 0.0092894666\n",
      "[16990,     1] loss: 0.0092894576\n",
      "[16991,     1] loss: 0.0092894487\n",
      "[16992,     1] loss: 0.0092894398\n",
      "[16993,     1] loss: 0.0092894301\n",
      "[16994,     1] loss: 0.0092894226\n",
      "[16995,     1] loss: 0.0092894122\n",
      "[16996,     1] loss: 0.0092894033\n",
      "[16997,     1] loss: 0.0092893951\n",
      "[16998,     1] loss: 0.0092893869\n",
      "[16999,     1] loss: 0.0092893794\n",
      "[17000,     1] loss: 0.0092893668\n",
      "[17001,     1] loss: 0.0092893600\n",
      "[17002,     1] loss: 0.0092893496\n",
      "[17003,     1] loss: 0.0092893422\n",
      "[17004,     1] loss: 0.0092893332\n",
      "[17005,     1] loss: 0.0092893235\n",
      "[17006,     1] loss: 0.0092893161\n",
      "[17007,     1] loss: 0.0092893064\n",
      "[17008,     1] loss: 0.0092892982\n",
      "[17009,     1] loss: 0.0092892885\n",
      "[17010,     1] loss: 0.0092892796\n",
      "[17011,     1] loss: 0.0092892714\n",
      "[17012,     1] loss: 0.0092892624\n",
      "[17013,     1] loss: 0.0092892535\n",
      "[17014,     1] loss: 0.0092892446\n",
      "[17015,     1] loss: 0.0092892356\n",
      "[17016,     1] loss: 0.0092892267\n",
      "[17017,     1] loss: 0.0092892192\n",
      "[17018,     1] loss: 0.0092892103\n",
      "[17019,     1] loss: 0.0092892013\n",
      "[17020,     1] loss: 0.0092891917\n",
      "[17021,     1] loss: 0.0092891835\n",
      "[17022,     1] loss: 0.0092891760\n",
      "[17023,     1] loss: 0.0092891641\n",
      "[17024,     1] loss: 0.0092891574\n",
      "[17025,     1] loss: 0.0092891484\n",
      "[17026,     1] loss: 0.0092891388\n",
      "[17027,     1] loss: 0.0092891306\n",
      "[17028,     1] loss: 0.0092891209\n",
      "[17029,     1] loss: 0.0092891119\n",
      "[17030,     1] loss: 0.0092891037\n",
      "[17031,     1] loss: 0.0092890956\n",
      "[17032,     1] loss: 0.0092890851\n",
      "[17033,     1] loss: 0.0092890762\n",
      "[17034,     1] loss: 0.0092890672\n",
      "[17035,     1] loss: 0.0092890590\n",
      "[17036,     1] loss: 0.0092890501\n",
      "[17037,     1] loss: 0.0092890412\n",
      "[17038,     1] loss: 0.0092890330\n",
      "[17039,     1] loss: 0.0092890240\n",
      "[17040,     1] loss: 0.0092890151\n",
      "[17041,     1] loss: 0.0092890069\n",
      "[17042,     1] loss: 0.0092889979\n",
      "[17043,     1] loss: 0.0092889890\n",
      "[17044,     1] loss: 0.0092889808\n",
      "[17045,     1] loss: 0.0092889726\n",
      "[17046,     1] loss: 0.0092889637\n",
      "[17047,     1] loss: 0.0092889540\n",
      "[17048,     1] loss: 0.0092889458\n",
      "[17049,     1] loss: 0.0092889369\n",
      "[17050,     1] loss: 0.0092889279\n",
      "[17051,     1] loss: 0.0092889212\n",
      "[17052,     1] loss: 0.0092889115\n",
      "[17053,     1] loss: 0.0092889026\n",
      "[17054,     1] loss: 0.0092888959\n",
      "[17055,     1] loss: 0.0092888854\n",
      "[17056,     1] loss: 0.0092888750\n",
      "[17057,     1] loss: 0.0092888691\n",
      "[17058,     1] loss: 0.0092888594\n",
      "[17059,     1] loss: 0.0092888489\n",
      "[17060,     1] loss: 0.0092888422\n",
      "[17061,     1] loss: 0.0092888333\n",
      "[17062,     1] loss: 0.0092888243\n",
      "[17063,     1] loss: 0.0092888169\n",
      "[17064,     1] loss: 0.0092888050\n",
      "[17065,     1] loss: 0.0092887990\n",
      "[17066,     1] loss: 0.0092887901\n",
      "[17067,     1] loss: 0.0092887804\n",
      "[17068,     1] loss: 0.0092887729\n",
      "[17069,     1] loss: 0.0092887640\n",
      "[17070,     1] loss: 0.0092887543\n",
      "[17071,     1] loss: 0.0092887469\n",
      "[17072,     1] loss: 0.0092887387\n",
      "[17073,     1] loss: 0.0092887290\n",
      "[17074,     1] loss: 0.0092887208\n",
      "[17075,     1] loss: 0.0092887118\n",
      "[17076,     1] loss: 0.0092887037\n",
      "[17077,     1] loss: 0.0092886932\n",
      "[17078,     1] loss: 0.0092886858\n",
      "[17079,     1] loss: 0.0092886776\n",
      "[17080,     1] loss: 0.0092886664\n",
      "[17081,     1] loss: 0.0092886597\n",
      "[17082,     1] loss: 0.0092886508\n",
      "[17083,     1] loss: 0.0092886411\n",
      "[17084,     1] loss: 0.0092886329\n",
      "[17085,     1] loss: 0.0092886247\n",
      "[17086,     1] loss: 0.0092886165\n",
      "[17087,     1] loss: 0.0092886075\n",
      "[17088,     1] loss: 0.0092885979\n",
      "[17089,     1] loss: 0.0092885911\n",
      "[17090,     1] loss: 0.0092885822\n",
      "[17091,     1] loss: 0.0092885733\n",
      "[17092,     1] loss: 0.0092885643\n",
      "[17093,     1] loss: 0.0092885576\n",
      "[17094,     1] loss: 0.0092885472\n",
      "[17095,     1] loss: 0.0092885390\n",
      "[17096,     1] loss: 0.0092885301\n",
      "[17097,     1] loss: 0.0092885219\n",
      "[17098,     1] loss: 0.0092885122\n",
      "[17099,     1] loss: 0.0092885040\n",
      "[17100,     1] loss: 0.0092884950\n",
      "[17101,     1] loss: 0.0092884868\n",
      "[17102,     1] loss: 0.0092884786\n",
      "[17103,     1] loss: 0.0092884690\n",
      "[17104,     1] loss: 0.0092884608\n",
      "[17105,     1] loss: 0.0092884526\n",
      "[17106,     1] loss: 0.0092884429\n",
      "[17107,     1] loss: 0.0092884347\n",
      "[17108,     1] loss: 0.0092884265\n",
      "[17109,     1] loss: 0.0092884190\n",
      "[17110,     1] loss: 0.0092884086\n",
      "[17111,     1] loss: 0.0092883997\n",
      "[17112,     1] loss: 0.0092883922\n",
      "[17113,     1] loss: 0.0092883840\n",
      "[17114,     1] loss: 0.0092883751\n",
      "[17115,     1] loss: 0.0092883654\n",
      "[17116,     1] loss: 0.0092883579\n",
      "[17117,     1] loss: 0.0092883490\n",
      "[17118,     1] loss: 0.0092883416\n",
      "[17119,     1] loss: 0.0092883319\n",
      "[17120,     1] loss: 0.0092883244\n",
      "[17121,     1] loss: 0.0092883147\n",
      "[17122,     1] loss: 0.0092883050\n",
      "[17123,     1] loss: 0.0092882968\n",
      "[17124,     1] loss: 0.0092882894\n",
      "[17125,     1] loss: 0.0092882797\n",
      "[17126,     1] loss: 0.0092882715\n",
      "[17127,     1] loss: 0.0092882626\n",
      "[17128,     1] loss: 0.0092882551\n",
      "[17129,     1] loss: 0.0092882454\n",
      "[17130,     1] loss: 0.0092882380\n",
      "[17131,     1] loss: 0.0092882290\n",
      "[17132,     1] loss: 0.0092882209\n",
      "[17133,     1] loss: 0.0092882134\n",
      "[17134,     1] loss: 0.0092882045\n",
      "[17135,     1] loss: 0.0092881948\n",
      "[17136,     1] loss: 0.0092881873\n",
      "[17137,     1] loss: 0.0092881799\n",
      "[17138,     1] loss: 0.0092881702\n",
      "[17139,     1] loss: 0.0092881612\n",
      "[17140,     1] loss: 0.0092881531\n",
      "[17141,     1] loss: 0.0092881441\n",
      "[17142,     1] loss: 0.0092881352\n",
      "[17143,     1] loss: 0.0092881277\n",
      "[17144,     1] loss: 0.0092881180\n",
      "[17145,     1] loss: 0.0092881098\n",
      "[17146,     1] loss: 0.0092881009\n",
      "[17147,     1] loss: 0.0092880920\n",
      "[17148,     1] loss: 0.0092880860\n",
      "[17149,     1] loss: 0.0092880756\n",
      "[17150,     1] loss: 0.0092880674\n",
      "[17151,     1] loss: 0.0092880577\n",
      "[17152,     1] loss: 0.0092880495\n",
      "[17153,     1] loss: 0.0092880413\n",
      "[17154,     1] loss: 0.0092880331\n",
      "[17155,     1] loss: 0.0092880242\n",
      "[17156,     1] loss: 0.0092880152\n",
      "[17157,     1] loss: 0.0092880078\n",
      "[17158,     1] loss: 0.0092879988\n",
      "[17159,     1] loss: 0.0092879899\n",
      "[17160,     1] loss: 0.0092879832\n",
      "[17161,     1] loss: 0.0092879720\n",
      "[17162,     1] loss: 0.0092879631\n",
      "[17163,     1] loss: 0.0092879564\n",
      "[17164,     1] loss: 0.0092879467\n",
      "[17165,     1] loss: 0.0092879400\n",
      "[17166,     1] loss: 0.0092879303\n",
      "[17167,     1] loss: 0.0092879221\n",
      "[17168,     1] loss: 0.0092879154\n",
      "[17169,     1] loss: 0.0092879057\n",
      "[17170,     1] loss: 0.0092878968\n",
      "[17171,     1] loss: 0.0092878886\n",
      "[17172,     1] loss: 0.0092878811\n",
      "[17173,     1] loss: 0.0092878722\n",
      "[17174,     1] loss: 0.0092878647\n",
      "[17175,     1] loss: 0.0092878565\n",
      "[17176,     1] loss: 0.0092878468\n",
      "[17177,     1] loss: 0.0092878386\n",
      "[17178,     1] loss: 0.0092878312\n",
      "[17179,     1] loss: 0.0092878215\n",
      "[17180,     1] loss: 0.0092878141\n",
      "[17181,     1] loss: 0.0092878059\n",
      "[17182,     1] loss: 0.0092877977\n",
      "[17183,     1] loss: 0.0092877887\n",
      "[17184,     1] loss: 0.0092877798\n",
      "[17185,     1] loss: 0.0092877716\n",
      "[17186,     1] loss: 0.0092877619\n",
      "[17187,     1] loss: 0.0092877544\n",
      "[17188,     1] loss: 0.0092877463\n",
      "[17189,     1] loss: 0.0092877381\n",
      "[17190,     1] loss: 0.0092877291\n",
      "[17191,     1] loss: 0.0092877209\n",
      "[17192,     1] loss: 0.0092877120\n",
      "[17193,     1] loss: 0.0092877045\n",
      "[17194,     1] loss: 0.0092876948\n",
      "[17195,     1] loss: 0.0092876852\n",
      "[17196,     1] loss: 0.0092876777\n",
      "[17197,     1] loss: 0.0092876710\n",
      "[17198,     1] loss: 0.0092876628\n",
      "[17199,     1] loss: 0.0092876524\n",
      "[17200,     1] loss: 0.0092876434\n",
      "[17201,     1] loss: 0.0092876345\n",
      "[17202,     1] loss: 0.0092876263\n",
      "[17203,     1] loss: 0.0092876188\n",
      "[17204,     1] loss: 0.0092876092\n",
      "[17205,     1] loss: 0.0092876025\n",
      "[17206,     1] loss: 0.0092875928\n",
      "[17207,     1] loss: 0.0092875838\n",
      "[17208,     1] loss: 0.0092875764\n",
      "[17209,     1] loss: 0.0092875674\n",
      "[17210,     1] loss: 0.0092875585\n",
      "[17211,     1] loss: 0.0092875533\n",
      "[17212,     1] loss: 0.0092875443\n",
      "[17213,     1] loss: 0.0092875347\n",
      "[17214,     1] loss: 0.0092875265\n",
      "[17215,     1] loss: 0.0092875190\n",
      "[17216,     1] loss: 0.0092875101\n",
      "[17217,     1] loss: 0.0092875019\n",
      "[17218,     1] loss: 0.0092874937\n",
      "[17219,     1] loss: 0.0092874832\n",
      "[17220,     1] loss: 0.0092874765\n",
      "[17221,     1] loss: 0.0092874683\n",
      "[17222,     1] loss: 0.0092874594\n",
      "[17223,     1] loss: 0.0092874512\n",
      "[17224,     1] loss: 0.0092874438\n",
      "[17225,     1] loss: 0.0092874363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17226,     1] loss: 0.0092874259\n",
      "[17227,     1] loss: 0.0092874177\n",
      "[17228,     1] loss: 0.0092874110\n",
      "[17229,     1] loss: 0.0092874020\n",
      "[17230,     1] loss: 0.0092873923\n",
      "[17231,     1] loss: 0.0092873856\n",
      "[17232,     1] loss: 0.0092873767\n",
      "[17233,     1] loss: 0.0092873685\n",
      "[17234,     1] loss: 0.0092873611\n",
      "[17235,     1] loss: 0.0092873521\n",
      "[17236,     1] loss: 0.0092873439\n",
      "[17237,     1] loss: 0.0092873350\n",
      "[17238,     1] loss: 0.0092873260\n",
      "[17239,     1] loss: 0.0092873201\n",
      "[17240,     1] loss: 0.0092873119\n",
      "[17241,     1] loss: 0.0092873044\n",
      "[17242,     1] loss: 0.0092872947\n",
      "[17243,     1] loss: 0.0092872851\n",
      "[17244,     1] loss: 0.0092872784\n",
      "[17245,     1] loss: 0.0092872709\n",
      "[17246,     1] loss: 0.0092872620\n",
      "[17247,     1] loss: 0.0092872530\n",
      "[17248,     1] loss: 0.0092872441\n",
      "[17249,     1] loss: 0.0092872366\n",
      "[17250,     1] loss: 0.0092872292\n",
      "[17251,     1] loss: 0.0092872195\n",
      "[17252,     1] loss: 0.0092872120\n",
      "[17253,     1] loss: 0.0092872031\n",
      "[17254,     1] loss: 0.0092871949\n",
      "[17255,     1] loss: 0.0092871875\n",
      "[17256,     1] loss: 0.0092871785\n",
      "[17257,     1] loss: 0.0092871696\n",
      "[17258,     1] loss: 0.0092871614\n",
      "[17259,     1] loss: 0.0092871539\n",
      "[17260,     1] loss: 0.0092871457\n",
      "[17261,     1] loss: 0.0092871368\n",
      "[17262,     1] loss: 0.0092871286\n",
      "[17263,     1] loss: 0.0092871211\n",
      "[17264,     1] loss: 0.0092871122\n",
      "[17265,     1] loss: 0.0092871033\n",
      "[17266,     1] loss: 0.0092870951\n",
      "[17267,     1] loss: 0.0092870869\n",
      "[17268,     1] loss: 0.0092870794\n",
      "[17269,     1] loss: 0.0092870712\n",
      "[17270,     1] loss: 0.0092870630\n",
      "[17271,     1] loss: 0.0092870548\n",
      "[17272,     1] loss: 0.0092870452\n",
      "[17273,     1] loss: 0.0092870384\n",
      "[17274,     1] loss: 0.0092870302\n",
      "[17275,     1] loss: 0.0092870213\n",
      "[17276,     1] loss: 0.0092870124\n",
      "[17277,     1] loss: 0.0092870034\n",
      "[17278,     1] loss: 0.0092869960\n",
      "[17279,     1] loss: 0.0092869878\n",
      "[17280,     1] loss: 0.0092869796\n",
      "[17281,     1] loss: 0.0092869706\n",
      "[17282,     1] loss: 0.0092869647\n",
      "[17283,     1] loss: 0.0092869557\n",
      "[17284,     1] loss: 0.0092869475\n",
      "[17285,     1] loss: 0.0092869394\n",
      "[17286,     1] loss: 0.0092869304\n",
      "[17287,     1] loss: 0.0092869222\n",
      "[17288,     1] loss: 0.0092869155\n",
      "[17289,     1] loss: 0.0092869051\n",
      "[17290,     1] loss: 0.0092868976\n",
      "[17291,     1] loss: 0.0092868902\n",
      "[17292,     1] loss: 0.0092868805\n",
      "[17293,     1] loss: 0.0092868738\n",
      "[17294,     1] loss: 0.0092868663\n",
      "[17295,     1] loss: 0.0092868567\n",
      "[17296,     1] loss: 0.0092868499\n",
      "[17297,     1] loss: 0.0092868418\n",
      "[17298,     1] loss: 0.0092868321\n",
      "[17299,     1] loss: 0.0092868246\n",
      "[17300,     1] loss: 0.0092868164\n",
      "[17301,     1] loss: 0.0092868067\n",
      "[17302,     1] loss: 0.0092868008\n",
      "[17303,     1] loss: 0.0092867918\n",
      "[17304,     1] loss: 0.0092867844\n",
      "[17305,     1] loss: 0.0092867777\n",
      "[17306,     1] loss: 0.0092867665\n",
      "[17307,     1] loss: 0.0092867605\n",
      "[17308,     1] loss: 0.0092867509\n",
      "[17309,     1] loss: 0.0092867441\n",
      "[17310,     1] loss: 0.0092867352\n",
      "[17311,     1] loss: 0.0092867270\n",
      "[17312,     1] loss: 0.0092867188\n",
      "[17313,     1] loss: 0.0092867121\n",
      "[17314,     1] loss: 0.0092867032\n",
      "[17315,     1] loss: 0.0092866950\n",
      "[17316,     1] loss: 0.0092866868\n",
      "[17317,     1] loss: 0.0092866793\n",
      "[17318,     1] loss: 0.0092866704\n",
      "[17319,     1] loss: 0.0092866614\n",
      "[17320,     1] loss: 0.0092866547\n",
      "[17321,     1] loss: 0.0092866465\n",
      "[17322,     1] loss: 0.0092866398\n",
      "[17323,     1] loss: 0.0092866309\n",
      "[17324,     1] loss: 0.0092866220\n",
      "[17325,     1] loss: 0.0092866138\n",
      "[17326,     1] loss: 0.0092866056\n",
      "[17327,     1] loss: 0.0092865966\n",
      "[17328,     1] loss: 0.0092865899\n",
      "[17329,     1] loss: 0.0092865832\n",
      "[17330,     1] loss: 0.0092865743\n",
      "[17331,     1] loss: 0.0092865661\n",
      "[17332,     1] loss: 0.0092865579\n",
      "[17333,     1] loss: 0.0092865489\n",
      "[17334,     1] loss: 0.0092865422\n",
      "[17335,     1] loss: 0.0092865326\n",
      "[17336,     1] loss: 0.0092865258\n",
      "[17337,     1] loss: 0.0092865184\n",
      "[17338,     1] loss: 0.0092865095\n",
      "[17339,     1] loss: 0.0092865013\n",
      "[17340,     1] loss: 0.0092864923\n",
      "[17341,     1] loss: 0.0092864856\n",
      "[17342,     1] loss: 0.0092864774\n",
      "[17343,     1] loss: 0.0092864685\n",
      "[17344,     1] loss: 0.0092864610\n",
      "[17345,     1] loss: 0.0092864536\n",
      "[17346,     1] loss: 0.0092864454\n",
      "[17347,     1] loss: 0.0092864364\n",
      "[17348,     1] loss: 0.0092864290\n",
      "[17349,     1] loss: 0.0092864208\n",
      "[17350,     1] loss: 0.0092864133\n",
      "[17351,     1] loss: 0.0092864044\n",
      "[17352,     1] loss: 0.0092863962\n",
      "[17353,     1] loss: 0.0092863888\n",
      "[17354,     1] loss: 0.0092863798\n",
      "[17355,     1] loss: 0.0092863731\n",
      "[17356,     1] loss: 0.0092863657\n",
      "[17357,     1] loss: 0.0092863567\n",
      "[17358,     1] loss: 0.0092863493\n",
      "[17359,     1] loss: 0.0092863411\n",
      "[17360,     1] loss: 0.0092863321\n",
      "[17361,     1] loss: 0.0092863262\n",
      "[17362,     1] loss: 0.0092863180\n",
      "[17363,     1] loss: 0.0092863090\n",
      "[17364,     1] loss: 0.0092863016\n",
      "[17365,     1] loss: 0.0092862919\n",
      "[17366,     1] loss: 0.0092862844\n",
      "[17367,     1] loss: 0.0092862763\n",
      "[17368,     1] loss: 0.0092862688\n",
      "[17369,     1] loss: 0.0092862606\n",
      "[17370,     1] loss: 0.0092862539\n",
      "[17371,     1] loss: 0.0092862450\n",
      "[17372,     1] loss: 0.0092862368\n",
      "[17373,     1] loss: 0.0092862271\n",
      "[17374,     1] loss: 0.0092862211\n",
      "[17375,     1] loss: 0.0092862114\n",
      "[17376,     1] loss: 0.0092862017\n",
      "[17377,     1] loss: 0.0092861965\n",
      "[17378,     1] loss: 0.0092861876\n",
      "[17379,     1] loss: 0.0092861794\n",
      "[17380,     1] loss: 0.0092861719\n",
      "[17381,     1] loss: 0.0092861630\n",
      "[17382,     1] loss: 0.0092861548\n",
      "[17383,     1] loss: 0.0092861474\n",
      "[17384,     1] loss: 0.0092861377\n",
      "[17385,     1] loss: 0.0092861317\n",
      "[17386,     1] loss: 0.0092861243\n",
      "[17387,     1] loss: 0.0092861146\n",
      "[17388,     1] loss: 0.0092861064\n",
      "[17389,     1] loss: 0.0092860989\n",
      "[17390,     1] loss: 0.0092860930\n",
      "[17391,     1] loss: 0.0092860848\n",
      "[17392,     1] loss: 0.0092860758\n",
      "[17393,     1] loss: 0.0092860676\n",
      "[17394,     1] loss: 0.0092860594\n",
      "[17395,     1] loss: 0.0092860512\n",
      "[17396,     1] loss: 0.0092860430\n",
      "[17397,     1] loss: 0.0092860349\n",
      "[17398,     1] loss: 0.0092860267\n",
      "[17399,     1] loss: 0.0092860207\n",
      "[17400,     1] loss: 0.0092860110\n",
      "[17401,     1] loss: 0.0092860050\n",
      "[17402,     1] loss: 0.0092859983\n",
      "[17403,     1] loss: 0.0092859872\n",
      "[17404,     1] loss: 0.0092859805\n",
      "[17405,     1] loss: 0.0092859723\n",
      "[17406,     1] loss: 0.0092859633\n",
      "[17407,     1] loss: 0.0092859559\n",
      "[17408,     1] loss: 0.0092859492\n",
      "[17409,     1] loss: 0.0092859410\n",
      "[17410,     1] loss: 0.0092859335\n",
      "[17411,     1] loss: 0.0092859246\n",
      "[17412,     1] loss: 0.0092859164\n",
      "[17413,     1] loss: 0.0092859089\n",
      "[17414,     1] loss: 0.0092859022\n",
      "[17415,     1] loss: 0.0092858925\n",
      "[17416,     1] loss: 0.0092858844\n",
      "[17417,     1] loss: 0.0092858784\n",
      "[17418,     1] loss: 0.0092858702\n",
      "[17419,     1] loss: 0.0092858620\n",
      "[17420,     1] loss: 0.0092858553\n",
      "[17421,     1] loss: 0.0092858456\n",
      "[17422,     1] loss: 0.0092858382\n",
      "[17423,     1] loss: 0.0092858300\n",
      "[17424,     1] loss: 0.0092858233\n",
      "[17425,     1] loss: 0.0092858151\n",
      "[17426,     1] loss: 0.0092858076\n",
      "[17427,     1] loss: 0.0092857979\n",
      "[17428,     1] loss: 0.0092857905\n",
      "[17429,     1] loss: 0.0092857823\n",
      "[17430,     1] loss: 0.0092857756\n",
      "[17431,     1] loss: 0.0092857681\n",
      "[17432,     1] loss: 0.0092857599\n",
      "[17433,     1] loss: 0.0092857525\n",
      "[17434,     1] loss: 0.0092857435\n",
      "[17435,     1] loss: 0.0092857368\n",
      "[17436,     1] loss: 0.0092857286\n",
      "[17437,     1] loss: 0.0092857212\n",
      "[17438,     1] loss: 0.0092857130\n",
      "[17439,     1] loss: 0.0092857048\n",
      "[17440,     1] loss: 0.0092856981\n",
      "[17441,     1] loss: 0.0092856899\n",
      "[17442,     1] loss: 0.0092856824\n",
      "[17443,     1] loss: 0.0092856742\n",
      "[17444,     1] loss: 0.0092856675\n",
      "[17445,     1] loss: 0.0092856586\n",
      "[17446,     1] loss: 0.0092856519\n",
      "[17447,     1] loss: 0.0092856437\n",
      "[17448,     1] loss: 0.0092856348\n",
      "[17449,     1] loss: 0.0092856288\n",
      "[17450,     1] loss: 0.0092856191\n",
      "[17451,     1] loss: 0.0092856124\n",
      "[17452,     1] loss: 0.0092856042\n",
      "[17453,     1] loss: 0.0092855960\n",
      "[17454,     1] loss: 0.0092855871\n",
      "[17455,     1] loss: 0.0092855796\n",
      "[17456,     1] loss: 0.0092855722\n",
      "[17457,     1] loss: 0.0092855647\n",
      "[17458,     1] loss: 0.0092855565\n",
      "[17459,     1] loss: 0.0092855491\n",
      "[17460,     1] loss: 0.0092855394\n",
      "[17461,     1] loss: 0.0092855327\n",
      "[17462,     1] loss: 0.0092855252\n",
      "[17463,     1] loss: 0.0092855170\n",
      "[17464,     1] loss: 0.0092855096\n",
      "[17465,     1] loss: 0.0092855029\n",
      "[17466,     1] loss: 0.0092854939\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17467,     1] loss: 0.0092854865\n",
      "[17468,     1] loss: 0.0092854783\n",
      "[17469,     1] loss: 0.0092854701\n",
      "[17470,     1] loss: 0.0092854619\n",
      "[17471,     1] loss: 0.0092854530\n",
      "[17472,     1] loss: 0.0092854463\n",
      "[17473,     1] loss: 0.0092854388\n",
      "[17474,     1] loss: 0.0092854299\n",
      "[17475,     1] loss: 0.0092854217\n",
      "[17476,     1] loss: 0.0092854142\n",
      "[17477,     1] loss: 0.0092854075\n",
      "[17478,     1] loss: 0.0092853986\n",
      "[17479,     1] loss: 0.0092853911\n",
      "[17480,     1] loss: 0.0092853844\n",
      "[17481,     1] loss: 0.0092853740\n",
      "[17482,     1] loss: 0.0092853680\n",
      "[17483,     1] loss: 0.0092853591\n",
      "[17484,     1] loss: 0.0092853524\n",
      "[17485,     1] loss: 0.0092853442\n",
      "[17486,     1] loss: 0.0092853360\n",
      "[17487,     1] loss: 0.0092853278\n",
      "[17488,     1] loss: 0.0092853218\n",
      "[17489,     1] loss: 0.0092853136\n",
      "[17490,     1] loss: 0.0092853047\n",
      "[17491,     1] loss: 0.0092852987\n",
      "[17492,     1] loss: 0.0092852883\n",
      "[17493,     1] loss: 0.0092852823\n",
      "[17494,     1] loss: 0.0092852741\n",
      "[17495,     1] loss: 0.0092852667\n",
      "[17496,     1] loss: 0.0092852585\n",
      "[17497,     1] loss: 0.0092852496\n",
      "[17498,     1] loss: 0.0092852443\n",
      "[17499,     1] loss: 0.0092852354\n",
      "[17500,     1] loss: 0.0092852265\n",
      "[17501,     1] loss: 0.0092852198\n",
      "[17502,     1] loss: 0.0092852116\n",
      "[17503,     1] loss: 0.0092852041\n",
      "[17504,     1] loss: 0.0092851967\n",
      "[17505,     1] loss: 0.0092851892\n",
      "[17506,     1] loss: 0.0092851810\n",
      "[17507,     1] loss: 0.0092851728\n",
      "[17508,     1] loss: 0.0092851654\n",
      "[17509,     1] loss: 0.0092851594\n",
      "[17510,     1] loss: 0.0092851497\n",
      "[17511,     1] loss: 0.0092851408\n",
      "[17512,     1] loss: 0.0092851341\n",
      "[17513,     1] loss: 0.0092851274\n",
      "[17514,     1] loss: 0.0092851192\n",
      "[17515,     1] loss: 0.0092851125\n",
      "[17516,     1] loss: 0.0092851050\n",
      "[17517,     1] loss: 0.0092850983\n",
      "[17518,     1] loss: 0.0092850886\n",
      "[17519,     1] loss: 0.0092850804\n",
      "[17520,     1] loss: 0.0092850737\n",
      "[17521,     1] loss: 0.0092850663\n",
      "[17522,     1] loss: 0.0092850581\n",
      "[17523,     1] loss: 0.0092850506\n",
      "[17524,     1] loss: 0.0092850424\n",
      "[17525,     1] loss: 0.0092850350\n",
      "[17526,     1] loss: 0.0092850275\n",
      "[17527,     1] loss: 0.0092850193\n",
      "[17528,     1] loss: 0.0092850104\n",
      "[17529,     1] loss: 0.0092850052\n",
      "[17530,     1] loss: 0.0092849962\n",
      "[17531,     1] loss: 0.0092849880\n",
      "[17532,     1] loss: 0.0092849813\n",
      "[17533,     1] loss: 0.0092849739\n",
      "[17534,     1] loss: 0.0092849657\n",
      "[17535,     1] loss: 0.0092849575\n",
      "[17536,     1] loss: 0.0092849493\n",
      "[17537,     1] loss: 0.0092849426\n",
      "[17538,     1] loss: 0.0092849366\n",
      "[17539,     1] loss: 0.0092849277\n",
      "[17540,     1] loss: 0.0092849210\n",
      "[17541,     1] loss: 0.0092849135\n",
      "[17542,     1] loss: 0.0092849039\n",
      "[17543,     1] loss: 0.0092848971\n",
      "[17544,     1] loss: 0.0092848897\n",
      "[17545,     1] loss: 0.0092848815\n",
      "[17546,     1] loss: 0.0092848741\n",
      "[17547,     1] loss: 0.0092848651\n",
      "[17548,     1] loss: 0.0092848584\n",
      "[17549,     1] loss: 0.0092848510\n",
      "[17550,     1] loss: 0.0092848428\n",
      "[17551,     1] loss: 0.0092848346\n",
      "[17552,     1] loss: 0.0092848271\n",
      "[17553,     1] loss: 0.0092848189\n",
      "[17554,     1] loss: 0.0092848115\n",
      "[17555,     1] loss: 0.0092848055\n",
      "[17556,     1] loss: 0.0092847966\n",
      "[17557,     1] loss: 0.0092847891\n",
      "[17558,     1] loss: 0.0092847824\n",
      "[17559,     1] loss: 0.0092847735\n",
      "[17560,     1] loss: 0.0092847675\n",
      "[17561,     1] loss: 0.0092847593\n",
      "[17562,     1] loss: 0.0092847511\n",
      "[17563,     1] loss: 0.0092847452\n",
      "[17564,     1] loss: 0.0092847370\n",
      "[17565,     1] loss: 0.0092847295\n",
      "[17566,     1] loss: 0.0092847206\n",
      "[17567,     1] loss: 0.0092847139\n",
      "[17568,     1] loss: 0.0092847072\n",
      "[17569,     1] loss: 0.0092846982\n",
      "[17570,     1] loss: 0.0092846908\n",
      "[17571,     1] loss: 0.0092846841\n",
      "[17572,     1] loss: 0.0092846766\n",
      "[17573,     1] loss: 0.0092846684\n",
      "[17574,     1] loss: 0.0092846610\n",
      "[17575,     1] loss: 0.0092846543\n",
      "[17576,     1] loss: 0.0092846461\n",
      "[17577,     1] loss: 0.0092846386\n",
      "[17578,     1] loss: 0.0092846312\n",
      "[17579,     1] loss: 0.0092846237\n",
      "[17580,     1] loss: 0.0092846155\n",
      "[17581,     1] loss: 0.0092846081\n",
      "[17582,     1] loss: 0.0092845984\n",
      "[17583,     1] loss: 0.0092845917\n",
      "[17584,     1] loss: 0.0092845850\n",
      "[17585,     1] loss: 0.0092845783\n",
      "[17586,     1] loss: 0.0092845693\n",
      "[17587,     1] loss: 0.0092845604\n",
      "[17588,     1] loss: 0.0092845552\n",
      "[17589,     1] loss: 0.0092845470\n",
      "[17590,     1] loss: 0.0092845395\n",
      "[17591,     1] loss: 0.0092845321\n",
      "[17592,     1] loss: 0.0092845254\n",
      "[17593,     1] loss: 0.0092845157\n",
      "[17594,     1] loss: 0.0092845097\n",
      "[17595,     1] loss: 0.0092845008\n",
      "[17596,     1] loss: 0.0092844941\n",
      "[17597,     1] loss: 0.0092844874\n",
      "[17598,     1] loss: 0.0092844792\n",
      "[17599,     1] loss: 0.0092844695\n",
      "[17600,     1] loss: 0.0092844635\n",
      "[17601,     1] loss: 0.0092844561\n",
      "[17602,     1] loss: 0.0092844494\n",
      "[17603,     1] loss: 0.0092844419\n",
      "[17604,     1] loss: 0.0092844330\n",
      "[17605,     1] loss: 0.0092844248\n",
      "[17606,     1] loss: 0.0092844188\n",
      "[17607,     1] loss: 0.0092844099\n",
      "[17608,     1] loss: 0.0092844032\n",
      "[17609,     1] loss: 0.0092843957\n",
      "[17610,     1] loss: 0.0092843875\n",
      "[17611,     1] loss: 0.0092843808\n",
      "[17612,     1] loss: 0.0092843741\n",
      "[17613,     1] loss: 0.0092843659\n",
      "[17614,     1] loss: 0.0092843570\n",
      "[17615,     1] loss: 0.0092843503\n",
      "[17616,     1] loss: 0.0092843436\n",
      "[17617,     1] loss: 0.0092843354\n",
      "[17618,     1] loss: 0.0092843294\n",
      "[17619,     1] loss: 0.0092843197\n",
      "[17620,     1] loss: 0.0092843123\n",
      "[17621,     1] loss: 0.0092843048\n",
      "[17622,     1] loss: 0.0092842996\n",
      "[17623,     1] loss: 0.0092842907\n",
      "[17624,     1] loss: 0.0092842840\n",
      "[17625,     1] loss: 0.0092842750\n",
      "[17626,     1] loss: 0.0092842691\n",
      "[17627,     1] loss: 0.0092842601\n",
      "[17628,     1] loss: 0.0092842519\n",
      "[17629,     1] loss: 0.0092842460\n",
      "[17630,     1] loss: 0.0092842385\n",
      "[17631,     1] loss: 0.0092842303\n",
      "[17632,     1] loss: 0.0092842244\n",
      "[17633,     1] loss: 0.0092842154\n",
      "[17634,     1] loss: 0.0092842080\n",
      "[17635,     1] loss: 0.0092842020\n",
      "[17636,     1] loss: 0.0092841931\n",
      "[17637,     1] loss: 0.0092841864\n",
      "[17638,     1] loss: 0.0092841782\n",
      "[17639,     1] loss: 0.0092841722\n",
      "[17640,     1] loss: 0.0092841633\n",
      "[17641,     1] loss: 0.0092841566\n",
      "[17642,     1] loss: 0.0092841476\n",
      "[17643,     1] loss: 0.0092841402\n",
      "[17644,     1] loss: 0.0092841342\n",
      "[17645,     1] loss: 0.0092841260\n",
      "[17646,     1] loss: 0.0092841178\n",
      "[17647,     1] loss: 0.0092841111\n",
      "[17648,     1] loss: 0.0092841052\n",
      "[17649,     1] loss: 0.0092840955\n",
      "[17650,     1] loss: 0.0092840880\n",
      "[17651,     1] loss: 0.0092840806\n",
      "[17652,     1] loss: 0.0092840739\n",
      "[17653,     1] loss: 0.0092840679\n",
      "[17654,     1] loss: 0.0092840582\n",
      "[17655,     1] loss: 0.0092840515\n",
      "[17656,     1] loss: 0.0092840455\n",
      "[17657,     1] loss: 0.0092840359\n",
      "[17658,     1] loss: 0.0092840292\n",
      "[17659,     1] loss: 0.0092840217\n",
      "[17660,     1] loss: 0.0092840128\n",
      "[17661,     1] loss: 0.0092840061\n",
      "[17662,     1] loss: 0.0092840001\n",
      "[17663,     1] loss: 0.0092839904\n",
      "[17664,     1] loss: 0.0092839852\n",
      "[17665,     1] loss: 0.0092839763\n",
      "[17666,     1] loss: 0.0092839688\n",
      "[17667,     1] loss: 0.0092839621\n",
      "[17668,     1] loss: 0.0092839524\n",
      "[17669,     1] loss: 0.0092839472\n",
      "[17670,     1] loss: 0.0092839390\n",
      "[17671,     1] loss: 0.0092839338\n",
      "[17672,     1] loss: 0.0092839234\n",
      "[17673,     1] loss: 0.0092839167\n",
      "[17674,     1] loss: 0.0092839107\n",
      "[17675,     1] loss: 0.0092839003\n",
      "[17676,     1] loss: 0.0092838943\n",
      "[17677,     1] loss: 0.0092838883\n",
      "[17678,     1] loss: 0.0092838794\n",
      "[17679,     1] loss: 0.0092838712\n",
      "[17680,     1] loss: 0.0092838652\n",
      "[17681,     1] loss: 0.0092838570\n",
      "[17682,     1] loss: 0.0092838489\n",
      "[17683,     1] loss: 0.0092838421\n",
      "[17684,     1] loss: 0.0092838354\n",
      "[17685,     1] loss: 0.0092838272\n",
      "[17686,     1] loss: 0.0092838190\n",
      "[17687,     1] loss: 0.0092838138\n",
      "[17688,     1] loss: 0.0092838049\n",
      "[17689,     1] loss: 0.0092837982\n",
      "[17690,     1] loss: 0.0092837915\n",
      "[17691,     1] loss: 0.0092837833\n",
      "[17692,     1] loss: 0.0092837766\n",
      "[17693,     1] loss: 0.0092837684\n",
      "[17694,     1] loss: 0.0092837602\n",
      "[17695,     1] loss: 0.0092837535\n",
      "[17696,     1] loss: 0.0092837475\n",
      "[17697,     1] loss: 0.0092837401\n",
      "[17698,     1] loss: 0.0092837311\n",
      "[17699,     1] loss: 0.0092837244\n",
      "[17700,     1] loss: 0.0092837170\n",
      "[17701,     1] loss: 0.0092837095\n",
      "[17702,     1] loss: 0.0092837028\n",
      "[17703,     1] loss: 0.0092836969\n",
      "[17704,     1] loss: 0.0092836879\n",
      "[17705,     1] loss: 0.0092836805\n",
      "[17706,     1] loss: 0.0092836730\n",
      "[17707,     1] loss: 0.0092836671\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17708,     1] loss: 0.0092836596\n",
      "[17709,     1] loss: 0.0092836507\n",
      "[17710,     1] loss: 0.0092836455\n",
      "[17711,     1] loss: 0.0092836387\n",
      "[17712,     1] loss: 0.0092836298\n",
      "[17713,     1] loss: 0.0092836231\n",
      "[17714,     1] loss: 0.0092836164\n",
      "[17715,     1] loss: 0.0092836075\n",
      "[17716,     1] loss: 0.0092836007\n",
      "[17717,     1] loss: 0.0092835926\n",
      "[17718,     1] loss: 0.0092835858\n",
      "[17719,     1] loss: 0.0092835791\n",
      "[17720,     1] loss: 0.0092835709\n",
      "[17721,     1] loss: 0.0092835650\n",
      "[17722,     1] loss: 0.0092835575\n",
      "[17723,     1] loss: 0.0092835493\n",
      "[17724,     1] loss: 0.0092835441\n",
      "[17725,     1] loss: 0.0092835352\n",
      "[17726,     1] loss: 0.0092835285\n",
      "[17727,     1] loss: 0.0092835195\n",
      "[17728,     1] loss: 0.0092835128\n",
      "[17729,     1] loss: 0.0092835061\n",
      "[17730,     1] loss: 0.0092834979\n",
      "[17731,     1] loss: 0.0092834912\n",
      "[17732,     1] loss: 0.0092834830\n",
      "[17733,     1] loss: 0.0092834756\n",
      "[17734,     1] loss: 0.0092834696\n",
      "[17735,     1] loss: 0.0092834614\n",
      "[17736,     1] loss: 0.0092834547\n",
      "[17737,     1] loss: 0.0092834465\n",
      "[17738,     1] loss: 0.0092834406\n",
      "[17739,     1] loss: 0.0092834324\n",
      "[17740,     1] loss: 0.0092834242\n",
      "[17741,     1] loss: 0.0092834182\n",
      "[17742,     1] loss: 0.0092834108\n",
      "[17743,     1] loss: 0.0092834033\n",
      "[17744,     1] loss: 0.0092833973\n",
      "[17745,     1] loss: 0.0092833884\n",
      "[17746,     1] loss: 0.0092833810\n",
      "[17747,     1] loss: 0.0092833750\n",
      "[17748,     1] loss: 0.0092833668\n",
      "[17749,     1] loss: 0.0092833593\n",
      "[17750,     1] loss: 0.0092833534\n",
      "[17751,     1] loss: 0.0092833459\n",
      "[17752,     1] loss: 0.0092833385\n",
      "[17753,     1] loss: 0.0092833318\n",
      "[17754,     1] loss: 0.0092833228\n",
      "[17755,     1] loss: 0.0092833161\n",
      "[17756,     1] loss: 0.0092833102\n",
      "[17757,     1] loss: 0.0092833027\n",
      "[17758,     1] loss: 0.0092832945\n",
      "[17759,     1] loss: 0.0092832878\n",
      "[17760,     1] loss: 0.0092832804\n",
      "[17761,     1] loss: 0.0092832737\n",
      "[17762,     1] loss: 0.0092832670\n",
      "[17763,     1] loss: 0.0092832603\n",
      "[17764,     1] loss: 0.0092832528\n",
      "[17765,     1] loss: 0.0092832454\n",
      "[17766,     1] loss: 0.0092832372\n",
      "[17767,     1] loss: 0.0092832305\n",
      "[17768,     1] loss: 0.0092832237\n",
      "[17769,     1] loss: 0.0092832156\n",
      "[17770,     1] loss: 0.0092832096\n",
      "[17771,     1] loss: 0.0092832014\n",
      "[17772,     1] loss: 0.0092831947\n",
      "[17773,     1] loss: 0.0092831880\n",
      "[17774,     1] loss: 0.0092831790\n",
      "[17775,     1] loss: 0.0092831723\n",
      "[17776,     1] loss: 0.0092831664\n",
      "[17777,     1] loss: 0.0092831582\n",
      "[17778,     1] loss: 0.0092831515\n",
      "[17779,     1] loss: 0.0092831440\n",
      "[17780,     1] loss: 0.0092831373\n",
      "[17781,     1] loss: 0.0092831291\n",
      "[17782,     1] loss: 0.0092831232\n",
      "[17783,     1] loss: 0.0092831150\n",
      "[17784,     1] loss: 0.0092831075\n",
      "[17785,     1] loss: 0.0092831016\n",
      "[17786,     1] loss: 0.0092830941\n",
      "[17787,     1] loss: 0.0092830859\n",
      "[17788,     1] loss: 0.0092830800\n",
      "[17789,     1] loss: 0.0092830725\n",
      "[17790,     1] loss: 0.0092830658\n",
      "[17791,     1] loss: 0.0092830583\n",
      "[17792,     1] loss: 0.0092830509\n",
      "[17793,     1] loss: 0.0092830442\n",
      "[17794,     1] loss: 0.0092830367\n",
      "[17795,     1] loss: 0.0092830293\n",
      "[17796,     1] loss: 0.0092830233\n",
      "[17797,     1] loss: 0.0092830144\n",
      "[17798,     1] loss: 0.0092830084\n",
      "[17799,     1] loss: 0.0092830002\n",
      "[17800,     1] loss: 0.0092829935\n",
      "[17801,     1] loss: 0.0092829853\n",
      "[17802,     1] loss: 0.0092829786\n",
      "[17803,     1] loss: 0.0092829719\n",
      "[17804,     1] loss: 0.0092829645\n",
      "[17805,     1] loss: 0.0092829570\n",
      "[17806,     1] loss: 0.0092829503\n",
      "[17807,     1] loss: 0.0092829436\n",
      "[17808,     1] loss: 0.0092829369\n",
      "[17809,     1] loss: 0.0092829280\n",
      "[17810,     1] loss: 0.0092829220\n",
      "[17811,     1] loss: 0.0092829160\n",
      "[17812,     1] loss: 0.0092829071\n",
      "[17813,     1] loss: 0.0092829004\n",
      "[17814,     1] loss: 0.0092828937\n",
      "[17815,     1] loss: 0.0092828855\n",
      "[17816,     1] loss: 0.0092828773\n",
      "[17817,     1] loss: 0.0092828713\n",
      "[17818,     1] loss: 0.0092828639\n",
      "[17819,     1] loss: 0.0092828557\n",
      "[17820,     1] loss: 0.0092828490\n",
      "[17821,     1] loss: 0.0092828423\n",
      "[17822,     1] loss: 0.0092828356\n",
      "[17823,     1] loss: 0.0092828281\n",
      "[17824,     1] loss: 0.0092828199\n",
      "[17825,     1] loss: 0.0092828140\n",
      "[17826,     1] loss: 0.0092828073\n",
      "[17827,     1] loss: 0.0092828013\n",
      "[17828,     1] loss: 0.0092827938\n",
      "[17829,     1] loss: 0.0092827864\n",
      "[17830,     1] loss: 0.0092827782\n",
      "[17831,     1] loss: 0.0092827722\n",
      "[17832,     1] loss: 0.0092827655\n",
      "[17833,     1] loss: 0.0092827581\n",
      "[17834,     1] loss: 0.0092827514\n",
      "[17835,     1] loss: 0.0092827432\n",
      "[17836,     1] loss: 0.0092827380\n",
      "[17837,     1] loss: 0.0092827298\n",
      "[17838,     1] loss: 0.0092827238\n",
      "[17839,     1] loss: 0.0092827156\n",
      "[17840,     1] loss: 0.0092827097\n",
      "[17841,     1] loss: 0.0092827015\n",
      "[17842,     1] loss: 0.0092826948\n",
      "[17843,     1] loss: 0.0092826881\n",
      "[17844,     1] loss: 0.0092826806\n",
      "[17845,     1] loss: 0.0092826732\n",
      "[17846,     1] loss: 0.0092826664\n",
      "[17847,     1] loss: 0.0092826612\n",
      "[17848,     1] loss: 0.0092826515\n",
      "[17849,     1] loss: 0.0092826463\n",
      "[17850,     1] loss: 0.0092826389\n",
      "[17851,     1] loss: 0.0092826307\n",
      "[17852,     1] loss: 0.0092826232\n",
      "[17853,     1] loss: 0.0092826165\n",
      "[17854,     1] loss: 0.0092826098\n",
      "[17855,     1] loss: 0.0092826016\n",
      "[17856,     1] loss: 0.0092825949\n",
      "[17857,     1] loss: 0.0092825890\n",
      "[17858,     1] loss: 0.0092825800\n",
      "[17859,     1] loss: 0.0092825726\n",
      "[17860,     1] loss: 0.0092825681\n",
      "[17861,     1] loss: 0.0092825599\n",
      "[17862,     1] loss: 0.0092825517\n",
      "[17863,     1] loss: 0.0092825450\n",
      "[17864,     1] loss: 0.0092825375\n",
      "[17865,     1] loss: 0.0092825308\n",
      "[17866,     1] loss: 0.0092825226\n",
      "[17867,     1] loss: 0.0092825159\n",
      "[17868,     1] loss: 0.0092825085\n",
      "[17869,     1] loss: 0.0092825025\n",
      "[17870,     1] loss: 0.0092824951\n",
      "[17871,     1] loss: 0.0092824869\n",
      "[17872,     1] loss: 0.0092824817\n",
      "[17873,     1] loss: 0.0092824742\n",
      "[17874,     1] loss: 0.0092824660\n",
      "[17875,     1] loss: 0.0092824586\n",
      "[17876,     1] loss: 0.0092824519\n",
      "[17877,     1] loss: 0.0092824437\n",
      "[17878,     1] loss: 0.0092824370\n",
      "[17879,     1] loss: 0.0092824303\n",
      "[17880,     1] loss: 0.0092824243\n",
      "[17881,     1] loss: 0.0092824154\n",
      "[17882,     1] loss: 0.0092824094\n",
      "[17883,     1] loss: 0.0092824019\n",
      "[17884,     1] loss: 0.0092823960\n",
      "[17885,     1] loss: 0.0092823893\n",
      "[17886,     1] loss: 0.0092823826\n",
      "[17887,     1] loss: 0.0092823736\n",
      "[17888,     1] loss: 0.0092823677\n",
      "[17889,     1] loss: 0.0092823610\n",
      "[17890,     1] loss: 0.0092823528\n",
      "[17891,     1] loss: 0.0092823476\n",
      "[17892,     1] loss: 0.0092823394\n",
      "[17893,     1] loss: 0.0092823334\n",
      "[17894,     1] loss: 0.0092823260\n",
      "[17895,     1] loss: 0.0092823178\n",
      "[17896,     1] loss: 0.0092823118\n",
      "[17897,     1] loss: 0.0092823043\n",
      "[17898,     1] loss: 0.0092822969\n",
      "[17899,     1] loss: 0.0092822902\n",
      "[17900,     1] loss: 0.0092822827\n",
      "[17901,     1] loss: 0.0092822745\n",
      "[17902,     1] loss: 0.0092822701\n",
      "[17903,     1] loss: 0.0092822626\n",
      "[17904,     1] loss: 0.0092822544\n",
      "[17905,     1] loss: 0.0092822485\n",
      "[17906,     1] loss: 0.0092822418\n",
      "[17907,     1] loss: 0.0092822343\n",
      "[17908,     1] loss: 0.0092822276\n",
      "[17909,     1] loss: 0.0092822202\n",
      "[17910,     1] loss: 0.0092822142\n",
      "[17911,     1] loss: 0.0092822090\n",
      "[17912,     1] loss: 0.0092822008\n",
      "[17913,     1] loss: 0.0092821933\n",
      "[17914,     1] loss: 0.0092821859\n",
      "[17915,     1] loss: 0.0092821792\n",
      "[17916,     1] loss: 0.0092821710\n",
      "[17917,     1] loss: 0.0092821650\n",
      "[17918,     1] loss: 0.0092821591\n",
      "[17919,     1] loss: 0.0092821494\n",
      "[17920,     1] loss: 0.0092821442\n",
      "[17921,     1] loss: 0.0092821360\n",
      "[17922,     1] loss: 0.0092821285\n",
      "[17923,     1] loss: 0.0092821233\n",
      "[17924,     1] loss: 0.0092821158\n",
      "[17925,     1] loss: 0.0092821084\n",
      "[17926,     1] loss: 0.0092821017\n",
      "[17927,     1] loss: 0.0092820950\n",
      "[17928,     1] loss: 0.0092820875\n",
      "[17929,     1] loss: 0.0092820808\n",
      "[17930,     1] loss: 0.0092820741\n",
      "[17931,     1] loss: 0.0092820667\n",
      "[17932,     1] loss: 0.0092820600\n",
      "[17933,     1] loss: 0.0092820533\n",
      "[17934,     1] loss: 0.0092820473\n",
      "[17935,     1] loss: 0.0092820384\n",
      "[17936,     1] loss: 0.0092820317\n",
      "[17937,     1] loss: 0.0092820242\n",
      "[17938,     1] loss: 0.0092820182\n",
      "[17939,     1] loss: 0.0092820100\n",
      "[17940,     1] loss: 0.0092820048\n",
      "[17941,     1] loss: 0.0092819981\n",
      "[17942,     1] loss: 0.0092819892\n",
      "[17943,     1] loss: 0.0092819847\n",
      "[17944,     1] loss: 0.0092819788\n",
      "[17945,     1] loss: 0.0092819698\n",
      "[17946,     1] loss: 0.0092819624\n",
      "[17947,     1] loss: 0.0092819564\n",
      "[17948,     1] loss: 0.0092819497\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17949,     1] loss: 0.0092819430\n",
      "[17950,     1] loss: 0.0092819363\n",
      "[17951,     1] loss: 0.0092819296\n",
      "[17952,     1] loss: 0.0092819229\n",
      "[17953,     1] loss: 0.0092819169\n",
      "[17954,     1] loss: 0.0092819095\n",
      "[17955,     1] loss: 0.0092819028\n",
      "[17956,     1] loss: 0.0092818975\n",
      "[17957,     1] loss: 0.0092818886\n",
      "[17958,     1] loss: 0.0092818834\n",
      "[17959,     1] loss: 0.0092818752\n",
      "[17960,     1] loss: 0.0092818677\n",
      "[17961,     1] loss: 0.0092818625\n",
      "[17962,     1] loss: 0.0092818543\n",
      "[17963,     1] loss: 0.0092818469\n",
      "[17964,     1] loss: 0.0092818409\n",
      "[17965,     1] loss: 0.0092818327\n",
      "[17966,     1] loss: 0.0092818268\n",
      "[17967,     1] loss: 0.0092818215\n",
      "[17968,     1] loss: 0.0092818126\n",
      "[17969,     1] loss: 0.0092818066\n",
      "[17970,     1] loss: 0.0092817992\n",
      "[17971,     1] loss: 0.0092817925\n",
      "[17972,     1] loss: 0.0092817865\n",
      "[17973,     1] loss: 0.0092817791\n",
      "[17974,     1] loss: 0.0092817731\n",
      "[17975,     1] loss: 0.0092817657\n",
      "[17976,     1] loss: 0.0092817590\n",
      "[17977,     1] loss: 0.0092817523\n",
      "[17978,     1] loss: 0.0092817448\n",
      "[17979,     1] loss: 0.0092817374\n",
      "[17980,     1] loss: 0.0092817307\n",
      "[17981,     1] loss: 0.0092817239\n",
      "[17982,     1] loss: 0.0092817165\n",
      "[17983,     1] loss: 0.0092817098\n",
      "[17984,     1] loss: 0.0092817023\n",
      "[17985,     1] loss: 0.0092816941\n",
      "[17986,     1] loss: 0.0092816882\n",
      "[17987,     1] loss: 0.0092816822\n",
      "[17988,     1] loss: 0.0092816755\n",
      "[17989,     1] loss: 0.0092816688\n",
      "[17990,     1] loss: 0.0092816614\n",
      "[17991,     1] loss: 0.0092816547\n",
      "[17992,     1] loss: 0.0092816472\n",
      "[17993,     1] loss: 0.0092816405\n",
      "[17994,     1] loss: 0.0092816338\n",
      "[17995,     1] loss: 0.0092816271\n",
      "[17996,     1] loss: 0.0092816204\n",
      "[17997,     1] loss: 0.0092816129\n",
      "[17998,     1] loss: 0.0092816062\n",
      "[17999,     1] loss: 0.0092815988\n",
      "[18000,     1] loss: 0.0092815936\n",
      "[18001,     1] loss: 0.0092815854\n",
      "[18002,     1] loss: 0.0092815779\n",
      "[18003,     1] loss: 0.0092815727\n",
      "[18004,     1] loss: 0.0092815638\n",
      "[18005,     1] loss: 0.0092815578\n",
      "[18006,     1] loss: 0.0092815503\n",
      "[18007,     1] loss: 0.0092815444\n",
      "[18008,     1] loss: 0.0092815377\n",
      "[18009,     1] loss: 0.0092815310\n",
      "[18010,     1] loss: 0.0092815243\n",
      "[18011,     1] loss: 0.0092815161\n",
      "[18012,     1] loss: 0.0092815101\n",
      "[18013,     1] loss: 0.0092815027\n",
      "[18014,     1] loss: 0.0092814960\n",
      "[18015,     1] loss: 0.0092814893\n",
      "[18016,     1] loss: 0.0092814811\n",
      "[18017,     1] loss: 0.0092814758\n",
      "[18018,     1] loss: 0.0092814691\n",
      "[18019,     1] loss: 0.0092814624\n",
      "[18020,     1] loss: 0.0092814550\n",
      "[18021,     1] loss: 0.0092814490\n",
      "[18022,     1] loss: 0.0092814408\n",
      "[18023,     1] loss: 0.0092814341\n",
      "[18024,     1] loss: 0.0092814282\n",
      "[18025,     1] loss: 0.0092814215\n",
      "[18026,     1] loss: 0.0092814140\n",
      "[18027,     1] loss: 0.0092814080\n",
      "[18028,     1] loss: 0.0092814013\n",
      "[18029,     1] loss: 0.0092813939\n",
      "[18030,     1] loss: 0.0092813864\n",
      "[18031,     1] loss: 0.0092813782\n",
      "[18032,     1] loss: 0.0092813745\n",
      "[18033,     1] loss: 0.0092813686\n",
      "[18034,     1] loss: 0.0092813604\n",
      "[18035,     1] loss: 0.0092813551\n",
      "[18036,     1] loss: 0.0092813469\n",
      "[18037,     1] loss: 0.0092813410\n",
      "[18038,     1] loss: 0.0092813343\n",
      "[18039,     1] loss: 0.0092813283\n",
      "[18040,     1] loss: 0.0092813201\n",
      "[18041,     1] loss: 0.0092813134\n",
      "[18042,     1] loss: 0.0092813082\n",
      "[18043,     1] loss: 0.0092813008\n",
      "[18044,     1] loss: 0.0092812926\n",
      "[18045,     1] loss: 0.0092812866\n",
      "[18046,     1] loss: 0.0092812806\n",
      "[18047,     1] loss: 0.0092812732\n",
      "[18048,     1] loss: 0.0092812672\n",
      "[18049,     1] loss: 0.0092812605\n",
      "[18050,     1] loss: 0.0092812531\n",
      "[18051,     1] loss: 0.0092812456\n",
      "[18052,     1] loss: 0.0092812389\n",
      "[18053,     1] loss: 0.0092812322\n",
      "[18054,     1] loss: 0.0092812248\n",
      "[18055,     1] loss: 0.0092812195\n",
      "[18056,     1] loss: 0.0092812128\n",
      "[18057,     1] loss: 0.0092812046\n",
      "[18058,     1] loss: 0.0092811994\n",
      "[18059,     1] loss: 0.0092811920\n",
      "[18060,     1] loss: 0.0092811845\n",
      "[18061,     1] loss: 0.0092811793\n",
      "[18062,     1] loss: 0.0092811726\n",
      "[18063,     1] loss: 0.0092811637\n",
      "[18064,     1] loss: 0.0092811577\n",
      "[18065,     1] loss: 0.0092811510\n",
      "[18066,     1] loss: 0.0092811443\n",
      "[18067,     1] loss: 0.0092811383\n",
      "[18068,     1] loss: 0.0092811309\n",
      "[18069,     1] loss: 0.0092811249\n",
      "[18070,     1] loss: 0.0092811182\n",
      "[18071,     1] loss: 0.0092811108\n",
      "[18072,     1] loss: 0.0092811041\n",
      "[18073,     1] loss: 0.0092810974\n",
      "[18074,     1] loss: 0.0092810906\n",
      "[18075,     1] loss: 0.0092810847\n",
      "[18076,     1] loss: 0.0092810780\n",
      "[18077,     1] loss: 0.0092810698\n",
      "[18078,     1] loss: 0.0092810631\n",
      "[18079,     1] loss: 0.0092810579\n",
      "[18080,     1] loss: 0.0092810504\n",
      "[18081,     1] loss: 0.0092810437\n",
      "[18082,     1] loss: 0.0092810377\n",
      "[18083,     1] loss: 0.0092810303\n",
      "[18084,     1] loss: 0.0092810243\n",
      "[18085,     1] loss: 0.0092810191\n",
      "[18086,     1] loss: 0.0092810102\n",
      "[18087,     1] loss: 0.0092810050\n",
      "[18088,     1] loss: 0.0092809975\n",
      "[18089,     1] loss: 0.0092809916\n",
      "[18090,     1] loss: 0.0092809841\n",
      "[18091,     1] loss: 0.0092809767\n",
      "[18092,     1] loss: 0.0092809707\n",
      "[18093,     1] loss: 0.0092809640\n",
      "[18094,     1] loss: 0.0092809573\n",
      "[18095,     1] loss: 0.0092809521\n",
      "[18096,     1] loss: 0.0092809439\n",
      "[18097,     1] loss: 0.0092809364\n",
      "[18098,     1] loss: 0.0092809312\n",
      "[18099,     1] loss: 0.0092809245\n",
      "[18100,     1] loss: 0.0092809170\n",
      "[18101,     1] loss: 0.0092809118\n",
      "[18102,     1] loss: 0.0092809044\n",
      "[18103,     1] loss: 0.0092808962\n",
      "[18104,     1] loss: 0.0092808910\n",
      "[18105,     1] loss: 0.0092808850\n",
      "[18106,     1] loss: 0.0092808776\n",
      "[18107,     1] loss: 0.0092808709\n",
      "[18108,     1] loss: 0.0092808634\n",
      "[18109,     1] loss: 0.0092808574\n",
      "[18110,     1] loss: 0.0092808515\n",
      "[18111,     1] loss: 0.0092808440\n",
      "[18112,     1] loss: 0.0092808388\n",
      "[18113,     1] loss: 0.0092808314\n",
      "[18114,     1] loss: 0.0092808239\n",
      "[18115,     1] loss: 0.0092808180\n",
      "[18116,     1] loss: 0.0092808113\n",
      "[18117,     1] loss: 0.0092808045\n",
      "[18118,     1] loss: 0.0092807971\n",
      "[18119,     1] loss: 0.0092807919\n",
      "[18120,     1] loss: 0.0092807844\n",
      "[18121,     1] loss: 0.0092807785\n",
      "[18122,     1] loss: 0.0092807710\n",
      "[18123,     1] loss: 0.0092807643\n",
      "[18124,     1] loss: 0.0092807576\n",
      "[18125,     1] loss: 0.0092807502\n",
      "[18126,     1] loss: 0.0092807442\n",
      "[18127,     1] loss: 0.0092807367\n",
      "[18128,     1] loss: 0.0092807300\n",
      "[18129,     1] loss: 0.0092807233\n",
      "[18130,     1] loss: 0.0092807166\n",
      "[18131,     1] loss: 0.0092807099\n",
      "[18132,     1] loss: 0.0092807047\n",
      "[18133,     1] loss: 0.0092806973\n",
      "[18134,     1] loss: 0.0092806913\n",
      "[18135,     1] loss: 0.0092806838\n",
      "[18136,     1] loss: 0.0092806756\n",
      "[18137,     1] loss: 0.0092806712\n",
      "[18138,     1] loss: 0.0092806652\n",
      "[18139,     1] loss: 0.0092806578\n",
      "[18140,     1] loss: 0.0092806511\n",
      "[18141,     1] loss: 0.0092806451\n",
      "[18142,     1] loss: 0.0092806377\n",
      "[18143,     1] loss: 0.0092806317\n",
      "[18144,     1] loss: 0.0092806242\n",
      "[18145,     1] loss: 0.0092806190\n",
      "[18146,     1] loss: 0.0092806123\n",
      "[18147,     1] loss: 0.0092806056\n",
      "[18148,     1] loss: 0.0092805989\n",
      "[18149,     1] loss: 0.0092805922\n",
      "[18150,     1] loss: 0.0092805862\n",
      "[18151,     1] loss: 0.0092805803\n",
      "[18152,     1] loss: 0.0092805736\n",
      "[18153,     1] loss: 0.0092805661\n",
      "[18154,     1] loss: 0.0092805602\n",
      "[18155,     1] loss: 0.0092805535\n",
      "[18156,     1] loss: 0.0092805482\n",
      "[18157,     1] loss: 0.0092805408\n",
      "[18158,     1] loss: 0.0092805363\n",
      "[18159,     1] loss: 0.0092805296\n",
      "[18160,     1] loss: 0.0092805229\n",
      "[18161,     1] loss: 0.0092805162\n",
      "[18162,     1] loss: 0.0092805102\n",
      "[18163,     1] loss: 0.0092805013\n",
      "[18164,     1] loss: 0.0092804961\n",
      "[18165,     1] loss: 0.0092804894\n",
      "[18166,     1] loss: 0.0092804827\n",
      "[18167,     1] loss: 0.0092804760\n",
      "[18168,     1] loss: 0.0092804708\n",
      "[18169,     1] loss: 0.0092804626\n",
      "[18170,     1] loss: 0.0092804566\n",
      "[18171,     1] loss: 0.0092804499\n",
      "[18172,     1] loss: 0.0092804439\n",
      "[18173,     1] loss: 0.0092804365\n",
      "[18174,     1] loss: 0.0092804298\n",
      "[18175,     1] loss: 0.0092804238\n",
      "[18176,     1] loss: 0.0092804156\n",
      "[18177,     1] loss: 0.0092804097\n",
      "[18178,     1] loss: 0.0092804059\n",
      "[18179,     1] loss: 0.0092803970\n",
      "[18180,     1] loss: 0.0092803910\n",
      "[18181,     1] loss: 0.0092803836\n",
      "[18182,     1] loss: 0.0092803761\n",
      "[18183,     1] loss: 0.0092803709\n",
      "[18184,     1] loss: 0.0092803635\n",
      "[18185,     1] loss: 0.0092803575\n",
      "[18186,     1] loss: 0.0092803501\n",
      "[18187,     1] loss: 0.0092803434\n",
      "[18188,     1] loss: 0.0092803374\n",
      "[18189,     1] loss: 0.0092803314\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18190,     1] loss: 0.0092803247\n",
      "[18191,     1] loss: 0.0092803180\n",
      "[18192,     1] loss: 0.0092803121\n",
      "[18193,     1] loss: 0.0092803046\n",
      "[18194,     1] loss: 0.0092802987\n",
      "[18195,     1] loss: 0.0092802919\n",
      "[18196,     1] loss: 0.0092802860\n",
      "[18197,     1] loss: 0.0092802808\n",
      "[18198,     1] loss: 0.0092802733\n",
      "[18199,     1] loss: 0.0092802666\n",
      "[18200,     1] loss: 0.0092802614\n",
      "[18201,     1] loss: 0.0092802525\n",
      "[18202,     1] loss: 0.0092802472\n",
      "[18203,     1] loss: 0.0092802413\n",
      "[18204,     1] loss: 0.0092802338\n",
      "[18205,     1] loss: 0.0092802286\n",
      "[18206,     1] loss: 0.0092802227\n",
      "[18207,     1] loss: 0.0092802145\n",
      "[18208,     1] loss: 0.0092802078\n",
      "[18209,     1] loss: 0.0092802003\n",
      "[18210,     1] loss: 0.0092801958\n",
      "[18211,     1] loss: 0.0092801876\n",
      "[18212,     1] loss: 0.0092801817\n",
      "[18213,     1] loss: 0.0092801757\n",
      "[18214,     1] loss: 0.0092801698\n",
      "[18215,     1] loss: 0.0092801616\n",
      "[18216,     1] loss: 0.0092801556\n",
      "[18217,     1] loss: 0.0092801504\n",
      "[18218,     1] loss: 0.0092801422\n",
      "[18219,     1] loss: 0.0092801355\n",
      "[18220,     1] loss: 0.0092801303\n",
      "[18221,     1] loss: 0.0092801221\n",
      "[18222,     1] loss: 0.0092801161\n",
      "[18223,     1] loss: 0.0092801109\n",
      "[18224,     1] loss: 0.0092801042\n",
      "[18225,     1] loss: 0.0092800975\n",
      "[18226,     1] loss: 0.0092800908\n",
      "[18227,     1] loss: 0.0092800856\n",
      "[18228,     1] loss: 0.0092800774\n",
      "[18229,     1] loss: 0.0092800722\n",
      "[18230,     1] loss: 0.0092800654\n",
      "[18231,     1] loss: 0.0092800573\n",
      "[18232,     1] loss: 0.0092800520\n",
      "[18233,     1] loss: 0.0092800468\n",
      "[18234,     1] loss: 0.0092800386\n",
      "[18235,     1] loss: 0.0092800334\n",
      "[18236,     1] loss: 0.0092800260\n",
      "[18237,     1] loss: 0.0092800207\n",
      "[18238,     1] loss: 0.0092800140\n",
      "[18239,     1] loss: 0.0092800066\n",
      "[18240,     1] loss: 0.0092800014\n",
      "[18241,     1] loss: 0.0092799947\n",
      "[18242,     1] loss: 0.0092799887\n",
      "[18243,     1] loss: 0.0092799820\n",
      "[18244,     1] loss: 0.0092799746\n",
      "[18245,     1] loss: 0.0092799693\n",
      "[18246,     1] loss: 0.0092799634\n",
      "[18247,     1] loss: 0.0092799552\n",
      "[18248,     1] loss: 0.0092799492\n",
      "[18249,     1] loss: 0.0092799440\n",
      "[18250,     1] loss: 0.0092799358\n",
      "[18251,     1] loss: 0.0092799284\n",
      "[18252,     1] loss: 0.0092799239\n",
      "[18253,     1] loss: 0.0092799172\n",
      "[18254,     1] loss: 0.0092799097\n",
      "[18255,     1] loss: 0.0092799038\n",
      "[18256,     1] loss: 0.0092798971\n",
      "[18257,     1] loss: 0.0092798889\n",
      "[18258,     1] loss: 0.0092798851\n",
      "[18259,     1] loss: 0.0092798777\n",
      "[18260,     1] loss: 0.0092798710\n",
      "[18261,     1] loss: 0.0092798665\n",
      "[18262,     1] loss: 0.0092798591\n",
      "[18263,     1] loss: 0.0092798516\n",
      "[18264,     1] loss: 0.0092798471\n",
      "[18265,     1] loss: 0.0092798412\n",
      "[18266,     1] loss: 0.0092798322\n",
      "[18267,     1] loss: 0.0092798255\n",
      "[18268,     1] loss: 0.0092798211\n",
      "[18269,     1] loss: 0.0092798129\n",
      "[18270,     1] loss: 0.0092798069\n",
      "[18271,     1] loss: 0.0092798002\n",
      "[18272,     1] loss: 0.0092797950\n",
      "[18273,     1] loss: 0.0092797875\n",
      "[18274,     1] loss: 0.0092797823\n",
      "[18275,     1] loss: 0.0092797756\n",
      "[18276,     1] loss: 0.0092797689\n",
      "[18277,     1] loss: 0.0092797630\n",
      "[18278,     1] loss: 0.0092797570\n",
      "[18279,     1] loss: 0.0092797488\n",
      "[18280,     1] loss: 0.0092797443\n",
      "[18281,     1] loss: 0.0092797369\n",
      "[18282,     1] loss: 0.0092797294\n",
      "[18283,     1] loss: 0.0092797250\n",
      "[18284,     1] loss: 0.0092797175\n",
      "[18285,     1] loss: 0.0092797086\n",
      "[18286,     1] loss: 0.0092797041\n",
      "[18287,     1] loss: 0.0092796989\n",
      "[18288,     1] loss: 0.0092796899\n",
      "[18289,     1] loss: 0.0092796870\n",
      "[18290,     1] loss: 0.0092796810\n",
      "[18291,     1] loss: 0.0092796706\n",
      "[18292,     1] loss: 0.0092796661\n",
      "[18293,     1] loss: 0.0092796609\n",
      "[18294,     1] loss: 0.0092796534\n",
      "[18295,     1] loss: 0.0092796467\n",
      "[18296,     1] loss: 0.0092796430\n",
      "[18297,     1] loss: 0.0092796348\n",
      "[18298,     1] loss: 0.0092796288\n",
      "[18299,     1] loss: 0.0092796229\n",
      "[18300,     1] loss: 0.0092796154\n",
      "[18301,     1] loss: 0.0092796095\n",
      "[18302,     1] loss: 0.0092796035\n",
      "[18303,     1] loss: 0.0092795968\n",
      "[18304,     1] loss: 0.0092795901\n",
      "[18305,     1] loss: 0.0092795856\n",
      "[18306,     1] loss: 0.0092795789\n",
      "[18307,     1] loss: 0.0092795715\n",
      "[18308,     1] loss: 0.0092795663\n",
      "[18309,     1] loss: 0.0092795581\n",
      "[18310,     1] loss: 0.0092795521\n",
      "[18311,     1] loss: 0.0092795461\n",
      "[18312,     1] loss: 0.0092795394\n",
      "[18313,     1] loss: 0.0092795335\n",
      "[18314,     1] loss: 0.0092795268\n",
      "[18315,     1] loss: 0.0092795223\n",
      "[18316,     1] loss: 0.0092795141\n",
      "[18317,     1] loss: 0.0092795089\n",
      "[18318,     1] loss: 0.0092795014\n",
      "[18319,     1] loss: 0.0092794955\n",
      "[18320,     1] loss: 0.0092794880\n",
      "[18321,     1] loss: 0.0092794828\n",
      "[18322,     1] loss: 0.0092794761\n",
      "[18323,     1] loss: 0.0092794694\n",
      "[18324,     1] loss: 0.0092794634\n",
      "[18325,     1] loss: 0.0092794575\n",
      "[18326,     1] loss: 0.0092794508\n",
      "[18327,     1] loss: 0.0092794441\n",
      "[18328,     1] loss: 0.0092794381\n",
      "[18329,     1] loss: 0.0092794314\n",
      "[18330,     1] loss: 0.0092794247\n",
      "[18331,     1] loss: 0.0092794195\n",
      "[18332,     1] loss: 0.0092794120\n",
      "[18333,     1] loss: 0.0092794053\n",
      "[18334,     1] loss: 0.0092794001\n",
      "[18335,     1] loss: 0.0092793919\n",
      "[18336,     1] loss: 0.0092793852\n",
      "[18337,     1] loss: 0.0092793792\n",
      "[18338,     1] loss: 0.0092793748\n",
      "[18339,     1] loss: 0.0092793666\n",
      "[18340,     1] loss: 0.0092793621\n",
      "[18341,     1] loss: 0.0092793554\n",
      "[18342,     1] loss: 0.0092793487\n",
      "[18343,     1] loss: 0.0092793435\n",
      "[18344,     1] loss: 0.0092793360\n",
      "[18345,     1] loss: 0.0092793316\n",
      "[18346,     1] loss: 0.0092793241\n",
      "[18347,     1] loss: 0.0092793182\n",
      "[18348,     1] loss: 0.0092793122\n",
      "[18349,     1] loss: 0.0092793055\n",
      "[18350,     1] loss: 0.0092792995\n",
      "[18351,     1] loss: 0.0092792921\n",
      "[18352,     1] loss: 0.0092792869\n",
      "[18353,     1] loss: 0.0092792802\n",
      "[18354,     1] loss: 0.0092792749\n",
      "[18355,     1] loss: 0.0092792690\n",
      "[18356,     1] loss: 0.0092792623\n",
      "[18357,     1] loss: 0.0092792556\n",
      "[18358,     1] loss: 0.0092792496\n",
      "[18359,     1] loss: 0.0092792436\n",
      "[18360,     1] loss: 0.0092792369\n",
      "[18361,     1] loss: 0.0092792310\n",
      "[18362,     1] loss: 0.0092792235\n",
      "[18363,     1] loss: 0.0092792183\n",
      "[18364,     1] loss: 0.0092792109\n",
      "[18365,     1] loss: 0.0092792042\n",
      "[18366,     1] loss: 0.0092792004\n",
      "[18367,     1] loss: 0.0092791922\n",
      "[18368,     1] loss: 0.0092791878\n",
      "[18369,     1] loss: 0.0092791803\n",
      "[18370,     1] loss: 0.0092791736\n",
      "[18371,     1] loss: 0.0092791684\n",
      "[18372,     1] loss: 0.0092791624\n",
      "[18373,     1] loss: 0.0092791550\n",
      "[18374,     1] loss: 0.0092791498\n",
      "[18375,     1] loss: 0.0092791431\n",
      "[18376,     1] loss: 0.0092791364\n",
      "[18377,     1] loss: 0.0092791319\n",
      "[18378,     1] loss: 0.0092791229\n",
      "[18379,     1] loss: 0.0092791170\n",
      "[18380,     1] loss: 0.0092791103\n",
      "[18381,     1] loss: 0.0092791051\n",
      "[18382,     1] loss: 0.0092790999\n",
      "[18383,     1] loss: 0.0092790917\n",
      "[18384,     1] loss: 0.0092790872\n",
      "[18385,     1] loss: 0.0092790797\n",
      "[18386,     1] loss: 0.0092790738\n",
      "[18387,     1] loss: 0.0092790671\n",
      "[18388,     1] loss: 0.0092790619\n",
      "[18389,     1] loss: 0.0092790537\n",
      "[18390,     1] loss: 0.0092790484\n",
      "[18391,     1] loss: 0.0092790410\n",
      "[18392,     1] loss: 0.0092790350\n",
      "[18393,     1] loss: 0.0092790283\n",
      "[18394,     1] loss: 0.0092790239\n",
      "[18395,     1] loss: 0.0092790172\n",
      "[18396,     1] loss: 0.0092790104\n",
      "[18397,     1] loss: 0.0092790045\n",
      "[18398,     1] loss: 0.0092789978\n",
      "[18399,     1] loss: 0.0092789926\n",
      "[18400,     1] loss: 0.0092789859\n",
      "[18401,     1] loss: 0.0092789799\n",
      "[18402,     1] loss: 0.0092789739\n",
      "[18403,     1] loss: 0.0092789680\n",
      "[18404,     1] loss: 0.0092789628\n",
      "[18405,     1] loss: 0.0092789553\n",
      "[18406,     1] loss: 0.0092789494\n",
      "[18407,     1] loss: 0.0092789426\n",
      "[18408,     1] loss: 0.0092789359\n",
      "[18409,     1] loss: 0.0092789300\n",
      "[18410,     1] loss: 0.0092789248\n",
      "[18411,     1] loss: 0.0092789188\n",
      "[18412,     1] loss: 0.0092789106\n",
      "[18413,     1] loss: 0.0092789054\n",
      "[18414,     1] loss: 0.0092788994\n",
      "[18415,     1] loss: 0.0092788935\n",
      "[18416,     1] loss: 0.0092788868\n",
      "[18417,     1] loss: 0.0092788808\n",
      "[18418,     1] loss: 0.0092788748\n",
      "[18419,     1] loss: 0.0092788674\n",
      "[18420,     1] loss: 0.0092788622\n",
      "[18421,     1] loss: 0.0092788570\n",
      "[18422,     1] loss: 0.0092788495\n",
      "[18423,     1] loss: 0.0092788443\n",
      "[18424,     1] loss: 0.0092788383\n",
      "[18425,     1] loss: 0.0092788309\n",
      "[18426,     1] loss: 0.0092788257\n",
      "[18427,     1] loss: 0.0092788190\n",
      "[18428,     1] loss: 0.0092788130\n",
      "[18429,     1] loss: 0.0092788070\n",
      "[18430,     1] loss: 0.0092788011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18431,     1] loss: 0.0092787944\n",
      "[18432,     1] loss: 0.0092787884\n",
      "[18433,     1] loss: 0.0092787825\n",
      "[18434,     1] loss: 0.0092787750\n",
      "[18435,     1] loss: 0.0092787698\n",
      "[18436,     1] loss: 0.0092787631\n",
      "[18437,     1] loss: 0.0092787579\n",
      "[18438,     1] loss: 0.0092787512\n",
      "[18439,     1] loss: 0.0092787452\n",
      "[18440,     1] loss: 0.0092787400\n",
      "[18441,     1] loss: 0.0092787325\n",
      "[18442,     1] loss: 0.0092787243\n",
      "[18443,     1] loss: 0.0092787206\n",
      "[18444,     1] loss: 0.0092787147\n",
      "[18445,     1] loss: 0.0092787065\n",
      "[18446,     1] loss: 0.0092787012\n",
      "[18447,     1] loss: 0.0092786960\n",
      "[18448,     1] loss: 0.0092786901\n",
      "[18449,     1] loss: 0.0092786834\n",
      "[18450,     1] loss: 0.0092786781\n",
      "[18451,     1] loss: 0.0092786707\n",
      "[18452,     1] loss: 0.0092786647\n",
      "[18453,     1] loss: 0.0092786588\n",
      "[18454,     1] loss: 0.0092786521\n",
      "[18455,     1] loss: 0.0092786461\n",
      "[18456,     1] loss: 0.0092786402\n",
      "[18457,     1] loss: 0.0092786342\n",
      "[18458,     1] loss: 0.0092786290\n",
      "[18459,     1] loss: 0.0092786230\n",
      "[18460,     1] loss: 0.0092786163\n",
      "[18461,     1] loss: 0.0092786096\n",
      "[18462,     1] loss: 0.0092786036\n",
      "[18463,     1] loss: 0.0092785962\n",
      "[18464,     1] loss: 0.0092785917\n",
      "[18465,     1] loss: 0.0092785850\n",
      "[18466,     1] loss: 0.0092785768\n",
      "[18467,     1] loss: 0.0092785724\n",
      "[18468,     1] loss: 0.0092785649\n",
      "[18469,     1] loss: 0.0092785597\n",
      "[18470,     1] loss: 0.0092785537\n",
      "[18471,     1] loss: 0.0092785470\n",
      "[18472,     1] loss: 0.0092785418\n",
      "[18473,     1] loss: 0.0092785351\n",
      "[18474,     1] loss: 0.0092785299\n",
      "[18475,     1] loss: 0.0092785224\n",
      "[18476,     1] loss: 0.0092785172\n",
      "[18477,     1] loss: 0.0092785120\n",
      "[18478,     1] loss: 0.0092785038\n",
      "[18479,     1] loss: 0.0092784993\n",
      "[18480,     1] loss: 0.0092784934\n",
      "[18481,     1] loss: 0.0092784867\n",
      "[18482,     1] loss: 0.0092784807\n",
      "[18483,     1] loss: 0.0092784740\n",
      "[18484,     1] loss: 0.0092784688\n",
      "[18485,     1] loss: 0.0092784636\n",
      "[18486,     1] loss: 0.0092784554\n",
      "[18487,     1] loss: 0.0092784494\n",
      "[18488,     1] loss: 0.0092784435\n",
      "[18489,     1] loss: 0.0092784375\n",
      "[18490,     1] loss: 0.0092784323\n",
      "[18491,     1] loss: 0.0092784263\n",
      "[18492,     1] loss: 0.0092784189\n",
      "[18493,     1] loss: 0.0092784129\n",
      "[18494,     1] loss: 0.0092784077\n",
      "[18495,     1] loss: 0.0092784002\n",
      "[18496,     1] loss: 0.0092783950\n",
      "[18497,     1] loss: 0.0092783883\n",
      "[18498,     1] loss: 0.0092783839\n",
      "[18499,     1] loss: 0.0092783779\n",
      "[18500,     1] loss: 0.0092783712\n",
      "[18501,     1] loss: 0.0092783667\n",
      "[18502,     1] loss: 0.0092783585\n",
      "[18503,     1] loss: 0.0092783540\n",
      "[18504,     1] loss: 0.0092783466\n",
      "[18505,     1] loss: 0.0092783421\n",
      "[18506,     1] loss: 0.0092783354\n",
      "[18507,     1] loss: 0.0092783280\n",
      "[18508,     1] loss: 0.0092783235\n",
      "[18509,     1] loss: 0.0092783161\n",
      "[18510,     1] loss: 0.0092783101\n",
      "[18511,     1] loss: 0.0092783056\n",
      "[18512,     1] loss: 0.0092782982\n",
      "[18513,     1] loss: 0.0092782922\n",
      "[18514,     1] loss: 0.0092782870\n",
      "[18515,     1] loss: 0.0092782795\n",
      "[18516,     1] loss: 0.0092782736\n",
      "[18517,     1] loss: 0.0092782691\n",
      "[18518,     1] loss: 0.0092782617\n",
      "[18519,     1] loss: 0.0092782557\n",
      "[18520,     1] loss: 0.0092782490\n",
      "[18521,     1] loss: 0.0092782438\n",
      "[18522,     1] loss: 0.0092782378\n",
      "[18523,     1] loss: 0.0092782311\n",
      "[18524,     1] loss: 0.0092782259\n",
      "[18525,     1] loss: 0.0092782199\n",
      "[18526,     1] loss: 0.0092782132\n",
      "[18527,     1] loss: 0.0092782080\n",
      "[18528,     1] loss: 0.0092782021\n",
      "[18529,     1] loss: 0.0092781954\n",
      "[18530,     1] loss: 0.0092781901\n",
      "[18531,     1] loss: 0.0092781834\n",
      "[18532,     1] loss: 0.0092781782\n",
      "[18533,     1] loss: 0.0092781715\n",
      "[18534,     1] loss: 0.0092781655\n",
      "[18535,     1] loss: 0.0092781588\n",
      "[18536,     1] loss: 0.0092781529\n",
      "[18537,     1] loss: 0.0092781477\n",
      "[18538,     1] loss: 0.0092781425\n",
      "[18539,     1] loss: 0.0092781357\n",
      "[18540,     1] loss: 0.0092781298\n",
      "[18541,     1] loss: 0.0092781238\n",
      "[18542,     1] loss: 0.0092781179\n",
      "[18543,     1] loss: 0.0092781112\n",
      "[18544,     1] loss: 0.0092781045\n",
      "[18545,     1] loss: 0.0092781007\n",
      "[18546,     1] loss: 0.0092780933\n",
      "[18547,     1] loss: 0.0092780881\n",
      "[18548,     1] loss: 0.0092780828\n",
      "[18549,     1] loss: 0.0092780747\n",
      "[18550,     1] loss: 0.0092780702\n",
      "[18551,     1] loss: 0.0092780650\n",
      "[18552,     1] loss: 0.0092780575\n",
      "[18553,     1] loss: 0.0092780508\n",
      "[18554,     1] loss: 0.0092780463\n",
      "[18555,     1] loss: 0.0092780396\n",
      "[18556,     1] loss: 0.0092780337\n",
      "[18557,     1] loss: 0.0092780277\n",
      "[18558,     1] loss: 0.0092780225\n",
      "[18559,     1] loss: 0.0092780158\n",
      "[18560,     1] loss: 0.0092780098\n",
      "[18561,     1] loss: 0.0092780031\n",
      "[18562,     1] loss: 0.0092779979\n",
      "[18563,     1] loss: 0.0092779934\n",
      "[18564,     1] loss: 0.0092779875\n",
      "[18565,     1] loss: 0.0092779793\n",
      "[18566,     1] loss: 0.0092779733\n",
      "[18567,     1] loss: 0.0092779689\n",
      "[18568,     1] loss: 0.0092779614\n",
      "[18569,     1] loss: 0.0092779562\n",
      "[18570,     1] loss: 0.0092779502\n",
      "[18571,     1] loss: 0.0092779443\n",
      "[18572,     1] loss: 0.0092779398\n",
      "[18573,     1] loss: 0.0092779323\n",
      "[18574,     1] loss: 0.0092779271\n",
      "[18575,     1] loss: 0.0092779197\n",
      "[18576,     1] loss: 0.0092779145\n",
      "[18577,     1] loss: 0.0092779092\n",
      "[18578,     1] loss: 0.0092779025\n",
      "[18579,     1] loss: 0.0092778966\n",
      "[18580,     1] loss: 0.0092778921\n",
      "[18581,     1] loss: 0.0092778847\n",
      "[18582,     1] loss: 0.0092778794\n",
      "[18583,     1] loss: 0.0092778735\n",
      "[18584,     1] loss: 0.0092778660\n",
      "[18585,     1] loss: 0.0092778586\n",
      "[18586,     1] loss: 0.0092778549\n",
      "[18587,     1] loss: 0.0092778489\n",
      "[18588,     1] loss: 0.0092778422\n",
      "[18589,     1] loss: 0.0092778370\n",
      "[18590,     1] loss: 0.0092778310\n",
      "[18591,     1] loss: 0.0092778251\n",
      "[18592,     1] loss: 0.0092778191\n",
      "[18593,     1] loss: 0.0092778116\n",
      "[18594,     1] loss: 0.0092778057\n",
      "[18595,     1] loss: 0.0092778005\n",
      "[18596,     1] loss: 0.0092777945\n",
      "[18597,     1] loss: 0.0092777885\n",
      "[18598,     1] loss: 0.0092777833\n",
      "[18599,     1] loss: 0.0092777759\n",
      "[18600,     1] loss: 0.0092777707\n",
      "[18601,     1] loss: 0.0092777655\n",
      "[18602,     1] loss: 0.0092777595\n",
      "[18603,     1] loss: 0.0092777535\n",
      "[18604,     1] loss: 0.0092777468\n",
      "[18605,     1] loss: 0.0092777401\n",
      "[18606,     1] loss: 0.0092777364\n",
      "[18607,     1] loss: 0.0092777297\n",
      "[18608,     1] loss: 0.0092777230\n",
      "[18609,     1] loss: 0.0092777185\n",
      "[18610,     1] loss: 0.0092777133\n",
      "[18611,     1] loss: 0.0092777058\n",
      "[18612,     1] loss: 0.0092776999\n",
      "[18613,     1] loss: 0.0092776947\n",
      "[18614,     1] loss: 0.0092776895\n",
      "[18615,     1] loss: 0.0092776813\n",
      "[18616,     1] loss: 0.0092776768\n",
      "[18617,     1] loss: 0.0092776708\n",
      "[18618,     1] loss: 0.0092776641\n",
      "[18619,     1] loss: 0.0092776582\n",
      "[18620,     1] loss: 0.0092776537\n",
      "[18621,     1] loss: 0.0092776470\n",
      "[18622,     1] loss: 0.0092776418\n",
      "[18623,     1] loss: 0.0092776351\n",
      "[18624,     1] loss: 0.0092776306\n",
      "[18625,     1] loss: 0.0092776231\n",
      "[18626,     1] loss: 0.0092776179\n",
      "[18627,     1] loss: 0.0092776120\n",
      "[18628,     1] loss: 0.0092776068\n",
      "[18629,     1] loss: 0.0092775993\n",
      "[18630,     1] loss: 0.0092775941\n",
      "[18631,     1] loss: 0.0092775889\n",
      "[18632,     1] loss: 0.0092775814\n",
      "[18633,     1] loss: 0.0092775762\n",
      "[18634,     1] loss: 0.0092775710\n",
      "[18635,     1] loss: 0.0092775635\n",
      "[18636,     1] loss: 0.0092775576\n",
      "[18637,     1] loss: 0.0092775509\n",
      "[18638,     1] loss: 0.0092775464\n",
      "[18639,     1] loss: 0.0092775412\n",
      "[18640,     1] loss: 0.0092775352\n",
      "[18641,     1] loss: 0.0092775285\n",
      "[18642,     1] loss: 0.0092775218\n",
      "[18643,     1] loss: 0.0092775166\n",
      "[18644,     1] loss: 0.0092775114\n",
      "[18645,     1] loss: 0.0092775054\n",
      "[18646,     1] loss: 0.0092774995\n",
      "[18647,     1] loss: 0.0092774935\n",
      "[18648,     1] loss: 0.0092774875\n",
      "[18649,     1] loss: 0.0092774808\n",
      "[18650,     1] loss: 0.0092774756\n",
      "[18651,     1] loss: 0.0092774704\n",
      "[18652,     1] loss: 0.0092774644\n",
      "[18653,     1] loss: 0.0092774570\n",
      "[18654,     1] loss: 0.0092774533\n",
      "[18655,     1] loss: 0.0092774473\n",
      "[18656,     1] loss: 0.0092774406\n",
      "[18657,     1] loss: 0.0092774346\n",
      "[18658,     1] loss: 0.0092774294\n",
      "[18659,     1] loss: 0.0092774235\n",
      "[18660,     1] loss: 0.0092774183\n",
      "[18661,     1] loss: 0.0092774116\n",
      "[18662,     1] loss: 0.0092774063\n",
      "[18663,     1] loss: 0.0092774011\n",
      "[18664,     1] loss: 0.0092773944\n",
      "[18665,     1] loss: 0.0092773877\n",
      "[18666,     1] loss: 0.0092773810\n",
      "[18667,     1] loss: 0.0092773758\n",
      "[18668,     1] loss: 0.0092773691\n",
      "[18669,     1] loss: 0.0092773654\n",
      "[18670,     1] loss: 0.0092773587\n",
      "[18671,     1] loss: 0.0092773519\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18672,     1] loss: 0.0092773467\n",
      "[18673,     1] loss: 0.0092773385\n",
      "[18674,     1] loss: 0.0092773341\n",
      "[18675,     1] loss: 0.0092773296\n",
      "[18676,     1] loss: 0.0092773214\n",
      "[18677,     1] loss: 0.0092773162\n",
      "[18678,     1] loss: 0.0092773110\n",
      "[18679,     1] loss: 0.0092773050\n",
      "[18680,     1] loss: 0.0092772990\n",
      "[18681,     1] loss: 0.0092772931\n",
      "[18682,     1] loss: 0.0092772879\n",
      "[18683,     1] loss: 0.0092772827\n",
      "[18684,     1] loss: 0.0092772767\n",
      "[18685,     1] loss: 0.0092772700\n",
      "[18686,     1] loss: 0.0092772655\n",
      "[18687,     1] loss: 0.0092772588\n",
      "[18688,     1] loss: 0.0092772536\n",
      "[18689,     1] loss: 0.0092772491\n",
      "[18690,     1] loss: 0.0092772417\n",
      "[18691,     1] loss: 0.0092772357\n",
      "[18692,     1] loss: 0.0092772298\n",
      "[18693,     1] loss: 0.0092772245\n",
      "[18694,     1] loss: 0.0092772178\n",
      "[18695,     1] loss: 0.0092772126\n",
      "[18696,     1] loss: 0.0092772074\n",
      "[18697,     1] loss: 0.0092772000\n",
      "[18698,     1] loss: 0.0092771947\n",
      "[18699,     1] loss: 0.0092771888\n",
      "[18700,     1] loss: 0.0092771836\n",
      "[18701,     1] loss: 0.0092771769\n",
      "[18702,     1] loss: 0.0092771716\n",
      "[18703,     1] loss: 0.0092771657\n",
      "[18704,     1] loss: 0.0092771597\n",
      "[18705,     1] loss: 0.0092771545\n",
      "[18706,     1] loss: 0.0092771485\n",
      "[18707,     1] loss: 0.0092771426\n",
      "[18708,     1] loss: 0.0092771381\n",
      "[18709,     1] loss: 0.0092771307\n",
      "[18710,     1] loss: 0.0092771247\n",
      "[18711,     1] loss: 0.0092771210\n",
      "[18712,     1] loss: 0.0092771135\n",
      "[18713,     1] loss: 0.0092771076\n",
      "[18714,     1] loss: 0.0092771031\n",
      "[18715,     1] loss: 0.0092770949\n",
      "[18716,     1] loss: 0.0092770904\n",
      "[18717,     1] loss: 0.0092770837\n",
      "[18718,     1] loss: 0.0092770785\n",
      "[18719,     1] loss: 0.0092770740\n",
      "[18720,     1] loss: 0.0092770681\n",
      "[18721,     1] loss: 0.0092770614\n",
      "[18722,     1] loss: 0.0092770569\n",
      "[18723,     1] loss: 0.0092770502\n",
      "[18724,     1] loss: 0.0092770442\n",
      "[18725,     1] loss: 0.0092770383\n",
      "[18726,     1] loss: 0.0092770338\n",
      "[18727,     1] loss: 0.0092770256\n",
      "[18728,     1] loss: 0.0092770211\n",
      "[18729,     1] loss: 0.0092770159\n",
      "[18730,     1] loss: 0.0092770100\n",
      "[18731,     1] loss: 0.0092770040\n",
      "[18732,     1] loss: 0.0092769988\n",
      "[18733,     1] loss: 0.0092769943\n",
      "[18734,     1] loss: 0.0092769876\n",
      "[18735,     1] loss: 0.0092769817\n",
      "[18736,     1] loss: 0.0092769764\n",
      "[18737,     1] loss: 0.0092769690\n",
      "[18738,     1] loss: 0.0092769645\n",
      "[18739,     1] loss: 0.0092769586\n",
      "[18740,     1] loss: 0.0092769533\n",
      "[18741,     1] loss: 0.0092769474\n",
      "[18742,     1] loss: 0.0092769429\n",
      "[18743,     1] loss: 0.0092769369\n",
      "[18744,     1] loss: 0.0092769317\n",
      "[18745,     1] loss: 0.0092769258\n",
      "[18746,     1] loss: 0.0092769191\n",
      "[18747,     1] loss: 0.0092769139\n",
      "[18748,     1] loss: 0.0092769071\n",
      "[18749,     1] loss: 0.0092769027\n",
      "[18750,     1] loss: 0.0092768960\n",
      "[18751,     1] loss: 0.0092768908\n",
      "[18752,     1] loss: 0.0092768848\n",
      "[18753,     1] loss: 0.0092768781\n",
      "[18754,     1] loss: 0.0092768736\n",
      "[18755,     1] loss: 0.0092768677\n",
      "[18756,     1] loss: 0.0092768624\n",
      "[18757,     1] loss: 0.0092768557\n",
      "[18758,     1] loss: 0.0092768513\n",
      "[18759,     1] loss: 0.0092768438\n",
      "[18760,     1] loss: 0.0092768386\n",
      "[18761,     1] loss: 0.0092768341\n",
      "[18762,     1] loss: 0.0092768282\n",
      "[18763,     1] loss: 0.0092768215\n",
      "[18764,     1] loss: 0.0092768162\n",
      "[18765,     1] loss: 0.0092768118\n",
      "[18766,     1] loss: 0.0092768051\n",
      "[18767,     1] loss: 0.0092767984\n",
      "[18768,     1] loss: 0.0092767939\n",
      "[18769,     1] loss: 0.0092767879\n",
      "[18770,     1] loss: 0.0092767820\n",
      "[18771,     1] loss: 0.0092767775\n",
      "[18772,     1] loss: 0.0092767693\n",
      "[18773,     1] loss: 0.0092767648\n",
      "[18774,     1] loss: 0.0092767589\n",
      "[18775,     1] loss: 0.0092767522\n",
      "[18776,     1] loss: 0.0092767477\n",
      "[18777,     1] loss: 0.0092767417\n",
      "[18778,     1] loss: 0.0092767358\n",
      "[18779,     1] loss: 0.0092767306\n",
      "[18780,     1] loss: 0.0092767246\n",
      "[18781,     1] loss: 0.0092767194\n",
      "[18782,     1] loss: 0.0092767127\n",
      "[18783,     1] loss: 0.0092767060\n",
      "[18784,     1] loss: 0.0092767008\n",
      "[18785,     1] loss: 0.0092766955\n",
      "[18786,     1] loss: 0.0092766888\n",
      "[18787,     1] loss: 0.0092766829\n",
      "[18788,     1] loss: 0.0092766769\n",
      "[18789,     1] loss: 0.0092766710\n",
      "[18790,     1] loss: 0.0092766657\n",
      "[18791,     1] loss: 0.0092766598\n",
      "[18792,     1] loss: 0.0092766538\n",
      "[18793,     1] loss: 0.0092766486\n",
      "[18794,     1] loss: 0.0092766434\n",
      "[18795,     1] loss: 0.0092766382\n",
      "[18796,     1] loss: 0.0092766307\n",
      "[18797,     1] loss: 0.0092766270\n",
      "[18798,     1] loss: 0.0092766203\n",
      "[18799,     1] loss: 0.0092766173\n",
      "[18800,     1] loss: 0.0092766084\n",
      "[18801,     1] loss: 0.0092766032\n",
      "[18802,     1] loss: 0.0092765972\n",
      "[18803,     1] loss: 0.0092765927\n",
      "[18804,     1] loss: 0.0092765860\n",
      "[18805,     1] loss: 0.0092765801\n",
      "[18806,     1] loss: 0.0092765756\n",
      "[18807,     1] loss: 0.0092765696\n",
      "[18808,     1] loss: 0.0092765629\n",
      "[18809,     1] loss: 0.0092765570\n",
      "[18810,     1] loss: 0.0092765518\n",
      "[18811,     1] loss: 0.0092765458\n",
      "[18812,     1] loss: 0.0092765398\n",
      "[18813,     1] loss: 0.0092765331\n",
      "[18814,     1] loss: 0.0092765287\n",
      "[18815,     1] loss: 0.0092765227\n",
      "[18816,     1] loss: 0.0092765175\n",
      "[18817,     1] loss: 0.0092765123\n",
      "[18818,     1] loss: 0.0092765063\n",
      "[18819,     1] loss: 0.0092765003\n",
      "[18820,     1] loss: 0.0092764966\n",
      "[18821,     1] loss: 0.0092764884\n",
      "[18822,     1] loss: 0.0092764832\n",
      "[18823,     1] loss: 0.0092764787\n",
      "[18824,     1] loss: 0.0092764720\n",
      "[18825,     1] loss: 0.0092764668\n",
      "[18826,     1] loss: 0.0092764616\n",
      "[18827,     1] loss: 0.0092764549\n",
      "[18828,     1] loss: 0.0092764497\n",
      "[18829,     1] loss: 0.0092764452\n",
      "[18830,     1] loss: 0.0092764385\n",
      "[18831,     1] loss: 0.0092764318\n",
      "[18832,     1] loss: 0.0092764281\n",
      "[18833,     1] loss: 0.0092764221\n",
      "[18834,     1] loss: 0.0092764169\n",
      "[18835,     1] loss: 0.0092764109\n",
      "[18836,     1] loss: 0.0092764042\n",
      "[18837,     1] loss: 0.0092764005\n",
      "[18838,     1] loss: 0.0092763953\n",
      "[18839,     1] loss: 0.0092763878\n",
      "[18840,     1] loss: 0.0092763834\n",
      "[18841,     1] loss: 0.0092763774\n",
      "[18842,     1] loss: 0.0092763714\n",
      "[18843,     1] loss: 0.0092763662\n",
      "[18844,     1] loss: 0.0092763603\n",
      "[18845,     1] loss: 0.0092763536\n",
      "[18846,     1] loss: 0.0092763498\n",
      "[18847,     1] loss: 0.0092763431\n",
      "[18848,     1] loss: 0.0092763372\n",
      "[18849,     1] loss: 0.0092763312\n",
      "[18850,     1] loss: 0.0092763275\n",
      "[18851,     1] loss: 0.0092763208\n",
      "[18852,     1] loss: 0.0092763141\n",
      "[18853,     1] loss: 0.0092763096\n",
      "[18854,     1] loss: 0.0092763036\n",
      "[18855,     1] loss: 0.0092762969\n",
      "[18856,     1] loss: 0.0092762925\n",
      "[18857,     1] loss: 0.0092762850\n",
      "[18858,     1] loss: 0.0092762813\n",
      "[18859,     1] loss: 0.0092762753\n",
      "[18860,     1] loss: 0.0092762694\n",
      "[18861,     1] loss: 0.0092762642\n",
      "[18862,     1] loss: 0.0092762582\n",
      "[18863,     1] loss: 0.0092762522\n",
      "[18864,     1] loss: 0.0092762478\n",
      "[18865,     1] loss: 0.0092762418\n",
      "[18866,     1] loss: 0.0092762358\n",
      "[18867,     1] loss: 0.0092762306\n",
      "[18868,     1] loss: 0.0092762247\n",
      "[18869,     1] loss: 0.0092762195\n",
      "[18870,     1] loss: 0.0092762135\n",
      "[18871,     1] loss: 0.0092762098\n",
      "[18872,     1] loss: 0.0092762031\n",
      "[18873,     1] loss: 0.0092761964\n",
      "[18874,     1] loss: 0.0092761926\n",
      "[18875,     1] loss: 0.0092761852\n",
      "[18876,     1] loss: 0.0092761800\n",
      "[18877,     1] loss: 0.0092761755\n",
      "[18878,     1] loss: 0.0092761695\n",
      "[18879,     1] loss: 0.0092761636\n",
      "[18880,     1] loss: 0.0092761584\n",
      "[18881,     1] loss: 0.0092761524\n",
      "[18882,     1] loss: 0.0092761464\n",
      "[18883,     1] loss: 0.0092761412\n",
      "[18884,     1] loss: 0.0092761368\n",
      "[18885,     1] loss: 0.0092761308\n",
      "[18886,     1] loss: 0.0092761248\n",
      "[18887,     1] loss: 0.0092761189\n",
      "[18888,     1] loss: 0.0092761129\n",
      "[18889,     1] loss: 0.0092761077\n",
      "[18890,     1] loss: 0.0092761025\n",
      "[18891,     1] loss: 0.0092760965\n",
      "[18892,     1] loss: 0.0092760928\n",
      "[18893,     1] loss: 0.0092760868\n",
      "[18894,     1] loss: 0.0092760786\n",
      "[18895,     1] loss: 0.0092760749\n",
      "[18896,     1] loss: 0.0092760682\n",
      "[18897,     1] loss: 0.0092760630\n",
      "[18898,     1] loss: 0.0092760585\n",
      "[18899,     1] loss: 0.0092760541\n",
      "[18900,     1] loss: 0.0092760473\n",
      "[18901,     1] loss: 0.0092760406\n",
      "[18902,     1] loss: 0.0092760369\n",
      "[18903,     1] loss: 0.0092760302\n",
      "[18904,     1] loss: 0.0092760228\n",
      "[18905,     1] loss: 0.0092760205\n",
      "[18906,     1] loss: 0.0092760123\n",
      "[18907,     1] loss: 0.0092760079\n",
      "[18908,     1] loss: 0.0092760019\n",
      "[18909,     1] loss: 0.0092759974\n",
      "[18910,     1] loss: 0.0092759907\n",
      "[18911,     1] loss: 0.0092759848\n",
      "[18912,     1] loss: 0.0092759818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18913,     1] loss: 0.0092759751\n",
      "[18914,     1] loss: 0.0092759691\n",
      "[18915,     1] loss: 0.0092759639\n",
      "[18916,     1] loss: 0.0092759587\n",
      "[18917,     1] loss: 0.0092759527\n",
      "[18918,     1] loss: 0.0092759475\n",
      "[18919,     1] loss: 0.0092759416\n",
      "[18920,     1] loss: 0.0092759341\n",
      "[18921,     1] loss: 0.0092759304\n",
      "[18922,     1] loss: 0.0092759237\n",
      "[18923,     1] loss: 0.0092759185\n",
      "[18924,     1] loss: 0.0092759132\n",
      "[18925,     1] loss: 0.0092759080\n",
      "[18926,     1] loss: 0.0092759021\n",
      "[18927,     1] loss: 0.0092758961\n",
      "[18928,     1] loss: 0.0092758901\n",
      "[18929,     1] loss: 0.0092758857\n",
      "[18930,     1] loss: 0.0092758797\n",
      "[18931,     1] loss: 0.0092758745\n",
      "[18932,     1] loss: 0.0092758685\n",
      "[18933,     1] loss: 0.0092758626\n",
      "[18934,     1] loss: 0.0092758566\n",
      "[18935,     1] loss: 0.0092758529\n",
      "[18936,     1] loss: 0.0092758462\n",
      "[18937,     1] loss: 0.0092758417\n",
      "[18938,     1] loss: 0.0092758365\n",
      "[18939,     1] loss: 0.0092758320\n",
      "[18940,     1] loss: 0.0092758246\n",
      "[18941,     1] loss: 0.0092758194\n",
      "[18942,     1] loss: 0.0092758149\n",
      "[18943,     1] loss: 0.0092758082\n",
      "[18944,     1] loss: 0.0092758030\n",
      "[18945,     1] loss: 0.0092757978\n",
      "[18946,     1] loss: 0.0092757910\n",
      "[18947,     1] loss: 0.0092757858\n",
      "[18948,     1] loss: 0.0092757814\n",
      "[18949,     1] loss: 0.0092757761\n",
      "[18950,     1] loss: 0.0092757694\n",
      "[18951,     1] loss: 0.0092757642\n",
      "[18952,     1] loss: 0.0092757598\n",
      "[18953,     1] loss: 0.0092757553\n",
      "[18954,     1] loss: 0.0092757478\n",
      "[18955,     1] loss: 0.0092757426\n",
      "[18956,     1] loss: 0.0092757389\n",
      "[18957,     1] loss: 0.0092757329\n",
      "[18958,     1] loss: 0.0092757262\n",
      "[18959,     1] loss: 0.0092757232\n",
      "[18960,     1] loss: 0.0092757158\n",
      "[18961,     1] loss: 0.0092757121\n",
      "[18962,     1] loss: 0.0092757076\n",
      "[18963,     1] loss: 0.0092757009\n",
      "[18964,     1] loss: 0.0092756957\n",
      "[18965,     1] loss: 0.0092756897\n",
      "[18966,     1] loss: 0.0092756845\n",
      "[18967,     1] loss: 0.0092756793\n",
      "[18968,     1] loss: 0.0092756741\n",
      "[18969,     1] loss: 0.0092756689\n",
      "[18970,     1] loss: 0.0092756644\n",
      "[18971,     1] loss: 0.0092756569\n",
      "[18972,     1] loss: 0.0092756517\n",
      "[18973,     1] loss: 0.0092756465\n",
      "[18974,     1] loss: 0.0092756413\n",
      "[18975,     1] loss: 0.0092756353\n",
      "[18976,     1] loss: 0.0092756309\n",
      "[18977,     1] loss: 0.0092756249\n",
      "[18978,     1] loss: 0.0092756197\n",
      "[18979,     1] loss: 0.0092756137\n",
      "[18980,     1] loss: 0.0092756085\n",
      "[18981,     1] loss: 0.0092756033\n",
      "[18982,     1] loss: 0.0092755981\n",
      "[18983,     1] loss: 0.0092755936\n",
      "[18984,     1] loss: 0.0092755862\n",
      "[18985,     1] loss: 0.0092755824\n",
      "[18986,     1] loss: 0.0092755765\n",
      "[18987,     1] loss: 0.0092755698\n",
      "[18988,     1] loss: 0.0092755646\n",
      "[18989,     1] loss: 0.0092755586\n",
      "[18990,     1] loss: 0.0092755549\n",
      "[18991,     1] loss: 0.0092755489\n",
      "[18992,     1] loss: 0.0092755429\n",
      "[18993,     1] loss: 0.0092755385\n",
      "[18994,     1] loss: 0.0092755318\n",
      "[18995,     1] loss: 0.0092755266\n",
      "[18996,     1] loss: 0.0092755213\n",
      "[18997,     1] loss: 0.0092755154\n",
      "[18998,     1] loss: 0.0092755109\n",
      "[18999,     1] loss: 0.0092755049\n",
      "[19000,     1] loss: 0.0092754990\n",
      "[19001,     1] loss: 0.0092754938\n",
      "[19002,     1] loss: 0.0092754886\n",
      "[19003,     1] loss: 0.0092754818\n",
      "[19004,     1] loss: 0.0092754774\n",
      "[19005,     1] loss: 0.0092754722\n",
      "[19006,     1] loss: 0.0092754662\n",
      "[19007,     1] loss: 0.0092754602\n",
      "[19008,     1] loss: 0.0092754550\n",
      "[19009,     1] loss: 0.0092754506\n",
      "[19010,     1] loss: 0.0092754431\n",
      "[19011,     1] loss: 0.0092754386\n",
      "[19012,     1] loss: 0.0092754327\n",
      "[19013,     1] loss: 0.0092754275\n",
      "[19014,     1] loss: 0.0092754208\n",
      "[19015,     1] loss: 0.0092754170\n",
      "[19016,     1] loss: 0.0092754118\n",
      "[19017,     1] loss: 0.0092754044\n",
      "[19018,     1] loss: 0.0092753999\n",
      "[19019,     1] loss: 0.0092753947\n",
      "[19020,     1] loss: 0.0092753895\n",
      "[19021,     1] loss: 0.0092753828\n",
      "[19022,     1] loss: 0.0092753775\n",
      "[19023,     1] loss: 0.0092753731\n",
      "[19024,     1] loss: 0.0092753686\n",
      "[19025,     1] loss: 0.0092753626\n",
      "[19026,     1] loss: 0.0092753567\n",
      "[19027,     1] loss: 0.0092753500\n",
      "[19028,     1] loss: 0.0092753462\n",
      "[19029,     1] loss: 0.0092753410\n",
      "[19030,     1] loss: 0.0092753343\n",
      "[19031,     1] loss: 0.0092753299\n",
      "[19032,     1] loss: 0.0092753224\n",
      "[19033,     1] loss: 0.0092753179\n",
      "[19034,     1] loss: 0.0092753127\n",
      "[19035,     1] loss: 0.0092753083\n",
      "[19036,     1] loss: 0.0092753023\n",
      "[19037,     1] loss: 0.0092752963\n",
      "[19038,     1] loss: 0.0092752926\n",
      "[19039,     1] loss: 0.0092752852\n",
      "[19040,     1] loss: 0.0092752807\n",
      "[19041,     1] loss: 0.0092752747\n",
      "[19042,     1] loss: 0.0092752710\n",
      "[19043,     1] loss: 0.0092752658\n",
      "[19044,     1] loss: 0.0092752598\n",
      "[19045,     1] loss: 0.0092752524\n",
      "[19046,     1] loss: 0.0092752479\n",
      "[19047,     1] loss: 0.0092752434\n",
      "[19048,     1] loss: 0.0092752382\n",
      "[19049,     1] loss: 0.0092752315\n",
      "[19050,     1] loss: 0.0092752285\n",
      "[19051,     1] loss: 0.0092752203\n",
      "[19052,     1] loss: 0.0092752144\n",
      "[19053,     1] loss: 0.0092752114\n",
      "[19054,     1] loss: 0.0092752054\n",
      "[19055,     1] loss: 0.0092751987\n",
      "[19056,     1] loss: 0.0092751950\n",
      "[19057,     1] loss: 0.0092751898\n",
      "[19058,     1] loss: 0.0092751838\n",
      "[19059,     1] loss: 0.0092751786\n",
      "[19060,     1] loss: 0.0092751734\n",
      "[19061,     1] loss: 0.0092751667\n",
      "[19062,     1] loss: 0.0092751630\n",
      "[19063,     1] loss: 0.0092751570\n",
      "[19064,     1] loss: 0.0092751510\n",
      "[19065,     1] loss: 0.0092751466\n",
      "[19066,     1] loss: 0.0092751391\n",
      "[19067,     1] loss: 0.0092751354\n",
      "[19068,     1] loss: 0.0092751294\n",
      "[19069,     1] loss: 0.0092751242\n",
      "[19070,     1] loss: 0.0092751183\n",
      "[19071,     1] loss: 0.0092751138\n",
      "[19072,     1] loss: 0.0092751086\n",
      "[19073,     1] loss: 0.0092751034\n",
      "[19074,     1] loss: 0.0092750967\n",
      "[19075,     1] loss: 0.0092750914\n",
      "[19076,     1] loss: 0.0092750840\n",
      "[19077,     1] loss: 0.0092750818\n",
      "[19078,     1] loss: 0.0092750750\n",
      "[19079,     1] loss: 0.0092750698\n",
      "[19080,     1] loss: 0.0092750661\n",
      "[19081,     1] loss: 0.0092750579\n",
      "[19082,     1] loss: 0.0092750542\n",
      "[19083,     1] loss: 0.0092750482\n",
      "[19084,     1] loss: 0.0092750423\n",
      "[19085,     1] loss: 0.0092750378\n",
      "[19086,     1] loss: 0.0092750318\n",
      "[19087,     1] loss: 0.0092750281\n",
      "[19088,     1] loss: 0.0092750207\n",
      "[19089,     1] loss: 0.0092750169\n",
      "[19090,     1] loss: 0.0092750140\n",
      "[19091,     1] loss: 0.0092750050\n",
      "[19092,     1] loss: 0.0092750005\n",
      "[19093,     1] loss: 0.0092749938\n",
      "[19094,     1] loss: 0.0092749909\n",
      "[19095,     1] loss: 0.0092749834\n",
      "[19096,     1] loss: 0.0092749789\n",
      "[19097,     1] loss: 0.0092749737\n",
      "[19098,     1] loss: 0.0092749670\n",
      "[19099,     1] loss: 0.0092749633\n",
      "[19100,     1] loss: 0.0092749581\n",
      "[19101,     1] loss: 0.0092749521\n",
      "[19102,     1] loss: 0.0092749476\n",
      "[19103,     1] loss: 0.0092749439\n",
      "[19104,     1] loss: 0.0092749372\n",
      "[19105,     1] loss: 0.0092749313\n",
      "[19106,     1] loss: 0.0092749268\n",
      "[19107,     1] loss: 0.0092749208\n",
      "[19108,     1] loss: 0.0092749171\n",
      "[19109,     1] loss: 0.0092749104\n",
      "[19110,     1] loss: 0.0092749052\n",
      "[19111,     1] loss: 0.0092749007\n",
      "[19112,     1] loss: 0.0092748955\n",
      "[19113,     1] loss: 0.0092748888\n",
      "[19114,     1] loss: 0.0092748851\n",
      "[19115,     1] loss: 0.0092748784\n",
      "[19116,     1] loss: 0.0092748754\n",
      "[19117,     1] loss: 0.0092748702\n",
      "[19118,     1] loss: 0.0092748635\n",
      "[19119,     1] loss: 0.0092748582\n",
      "[19120,     1] loss: 0.0092748530\n",
      "[19121,     1] loss: 0.0092748493\n",
      "[19122,     1] loss: 0.0092748411\n",
      "[19123,     1] loss: 0.0092748366\n",
      "[19124,     1] loss: 0.0092748329\n",
      "[19125,     1] loss: 0.0092748262\n",
      "[19126,     1] loss: 0.0092748210\n",
      "[19127,     1] loss: 0.0092748158\n",
      "[19128,     1] loss: 0.0092748076\n",
      "[19129,     1] loss: 0.0092748031\n",
      "[19130,     1] loss: 0.0092747994\n",
      "[19131,     1] loss: 0.0092747942\n",
      "[19132,     1] loss: 0.0092747882\n",
      "[19133,     1] loss: 0.0092747830\n",
      "[19134,     1] loss: 0.0092747778\n",
      "[19135,     1] loss: 0.0092747726\n",
      "[19136,     1] loss: 0.0092747673\n",
      "[19137,     1] loss: 0.0092747606\n",
      "[19138,     1] loss: 0.0092747569\n",
      "[19139,     1] loss: 0.0092747509\n",
      "[19140,     1] loss: 0.0092747457\n",
      "[19141,     1] loss: 0.0092747405\n",
      "[19142,     1] loss: 0.0092747360\n",
      "[19143,     1] loss: 0.0092747316\n",
      "[19144,     1] loss: 0.0092747249\n",
      "[19145,     1] loss: 0.0092747197\n",
      "[19146,     1] loss: 0.0092747152\n",
      "[19147,     1] loss: 0.0092747100\n",
      "[19148,     1] loss: 0.0092747040\n",
      "[19149,     1] loss: 0.0092746980\n",
      "[19150,     1] loss: 0.0092746928\n",
      "[19151,     1] loss: 0.0092746884\n",
      "[19152,     1] loss: 0.0092746831\n",
      "[19153,     1] loss: 0.0092746757\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19154,     1] loss: 0.0092746712\n",
      "[19155,     1] loss: 0.0092746668\n",
      "[19156,     1] loss: 0.0092746623\n",
      "[19157,     1] loss: 0.0092746563\n",
      "[19158,     1] loss: 0.0092746504\n",
      "[19159,     1] loss: 0.0092746459\n",
      "[19160,     1] loss: 0.0092746407\n",
      "[19161,     1] loss: 0.0092746340\n",
      "[19162,     1] loss: 0.0092746288\n",
      "[19163,     1] loss: 0.0092746250\n",
      "[19164,     1] loss: 0.0092746191\n",
      "[19165,     1] loss: 0.0092746146\n",
      "[19166,     1] loss: 0.0092746086\n",
      "[19167,     1] loss: 0.0092746042\n",
      "[19168,     1] loss: 0.0092745975\n",
      "[19169,     1] loss: 0.0092745937\n",
      "[19170,     1] loss: 0.0092745878\n",
      "[19171,     1] loss: 0.0092745833\n",
      "[19172,     1] loss: 0.0092745781\n",
      "[19173,     1] loss: 0.0092745729\n",
      "[19174,     1] loss: 0.0092745669\n",
      "[19175,     1] loss: 0.0092745617\n",
      "[19176,     1] loss: 0.0092745572\n",
      "[19177,     1] loss: 0.0092745513\n",
      "[19178,     1] loss: 0.0092745468\n",
      "[19179,     1] loss: 0.0092745416\n",
      "[19180,     1] loss: 0.0092745356\n",
      "[19181,     1] loss: 0.0092745312\n",
      "[19182,     1] loss: 0.0092745252\n",
      "[19183,     1] loss: 0.0092745207\n",
      "[19184,     1] loss: 0.0092745155\n",
      "[19185,     1] loss: 0.0092745088\n",
      "[19186,     1] loss: 0.0092745051\n",
      "[19187,     1] loss: 0.0092745014\n",
      "[19188,     1] loss: 0.0092744946\n",
      "[19189,     1] loss: 0.0092744902\n",
      "[19190,     1] loss: 0.0092744835\n",
      "[19191,     1] loss: 0.0092744797\n",
      "[19192,     1] loss: 0.0092744745\n",
      "[19193,     1] loss: 0.0092744701\n",
      "[19194,     1] loss: 0.0092744641\n",
      "[19195,     1] loss: 0.0092744589\n",
      "[19196,     1] loss: 0.0092744544\n",
      "[19197,     1] loss: 0.0092744492\n",
      "[19198,     1] loss: 0.0092744425\n",
      "[19199,     1] loss: 0.0092744395\n",
      "[19200,     1] loss: 0.0092744328\n",
      "[19201,     1] loss: 0.0092744276\n",
      "[19202,     1] loss: 0.0092744239\n",
      "[19203,     1] loss: 0.0092744164\n",
      "[19204,     1] loss: 0.0092744119\n",
      "[19205,     1] loss: 0.0092744075\n",
      "[19206,     1] loss: 0.0092744015\n",
      "[19207,     1] loss: 0.0092743963\n",
      "[19208,     1] loss: 0.0092743911\n",
      "[19209,     1] loss: 0.0092743859\n",
      "[19210,     1] loss: 0.0092743799\n",
      "[19211,     1] loss: 0.0092743762\n",
      "[19212,     1] loss: 0.0092743710\n",
      "[19213,     1] loss: 0.0092743658\n",
      "[19214,     1] loss: 0.0092743598\n",
      "[19215,     1] loss: 0.0092743546\n",
      "[19216,     1] loss: 0.0092743494\n",
      "[19217,     1] loss: 0.0092743441\n",
      "[19218,     1] loss: 0.0092743374\n",
      "[19219,     1] loss: 0.0092743330\n",
      "[19220,     1] loss: 0.0092743285\n",
      "[19221,     1] loss: 0.0092743225\n",
      "[19222,     1] loss: 0.0092743188\n",
      "[19223,     1] loss: 0.0092743121\n",
      "[19224,     1] loss: 0.0092743069\n",
      "[19225,     1] loss: 0.0092743009\n",
      "[19226,     1] loss: 0.0092742972\n",
      "[19227,     1] loss: 0.0092742912\n",
      "[19228,     1] loss: 0.0092742853\n",
      "[19229,     1] loss: 0.0092742816\n",
      "[19230,     1] loss: 0.0092742756\n",
      "[19231,     1] loss: 0.0092742719\n",
      "[19232,     1] loss: 0.0092742659\n",
      "[19233,     1] loss: 0.0092742600\n",
      "[19234,     1] loss: 0.0092742540\n",
      "[19235,     1] loss: 0.0092742488\n",
      "[19236,     1] loss: 0.0092742436\n",
      "[19237,     1] loss: 0.0092742391\n",
      "[19238,     1] loss: 0.0092742346\n",
      "[19239,     1] loss: 0.0092742287\n",
      "[19240,     1] loss: 0.0092742234\n",
      "[19241,     1] loss: 0.0092742167\n",
      "[19242,     1] loss: 0.0092742130\n",
      "[19243,     1] loss: 0.0092742078\n",
      "[19244,     1] loss: 0.0092742033\n",
      "[19245,     1] loss: 0.0092741989\n",
      "[19246,     1] loss: 0.0092741929\n",
      "[19247,     1] loss: 0.0092741892\n",
      "[19248,     1] loss: 0.0092741825\n",
      "[19249,     1] loss: 0.0092741780\n",
      "[19250,     1] loss: 0.0092741728\n",
      "[19251,     1] loss: 0.0092741683\n",
      "[19252,     1] loss: 0.0092741638\n",
      "[19253,     1] loss: 0.0092741571\n",
      "[19254,     1] loss: 0.0092741534\n",
      "[19255,     1] loss: 0.0092741475\n",
      "[19256,     1] loss: 0.0092741430\n",
      "[19257,     1] loss: 0.0092741378\n",
      "[19258,     1] loss: 0.0092741333\n",
      "[19259,     1] loss: 0.0092741288\n",
      "[19260,     1] loss: 0.0092741229\n",
      "[19261,     1] loss: 0.0092741169\n",
      "[19262,     1] loss: 0.0092741124\n",
      "[19263,     1] loss: 0.0092741080\n",
      "[19264,     1] loss: 0.0092741035\n",
      "[19265,     1] loss: 0.0092740975\n",
      "[19266,     1] loss: 0.0092740908\n",
      "[19267,     1] loss: 0.0092740864\n",
      "[19268,     1] loss: 0.0092740819\n",
      "[19269,     1] loss: 0.0092740774\n",
      "[19270,     1] loss: 0.0092740715\n",
      "[19271,     1] loss: 0.0092740662\n",
      "[19272,     1] loss: 0.0092740625\n",
      "[19273,     1] loss: 0.0092740558\n",
      "[19274,     1] loss: 0.0092740506\n",
      "[19275,     1] loss: 0.0092740454\n",
      "[19276,     1] loss: 0.0092740424\n",
      "[19277,     1] loss: 0.0092740349\n",
      "[19278,     1] loss: 0.0092740290\n",
      "[19279,     1] loss: 0.0092740245\n",
      "[19280,     1] loss: 0.0092740186\n",
      "[19281,     1] loss: 0.0092740141\n",
      "[19282,     1] loss: 0.0092740089\n",
      "[19283,     1] loss: 0.0092740037\n",
      "[19284,     1] loss: 0.0092739999\n",
      "[19285,     1] loss: 0.0092739925\n",
      "[19286,     1] loss: 0.0092739880\n",
      "[19287,     1] loss: 0.0092739850\n",
      "[19288,     1] loss: 0.0092739776\n",
      "[19289,     1] loss: 0.0092739709\n",
      "[19290,     1] loss: 0.0092739686\n",
      "[19291,     1] loss: 0.0092739627\n",
      "[19292,     1] loss: 0.0092739560\n",
      "[19293,     1] loss: 0.0092739515\n",
      "[19294,     1] loss: 0.0092739485\n",
      "[19295,     1] loss: 0.0092739426\n",
      "[19296,     1] loss: 0.0092739373\n",
      "[19297,     1] loss: 0.0092739314\n",
      "[19298,     1] loss: 0.0092739269\n",
      "[19299,     1] loss: 0.0092739217\n",
      "[19300,     1] loss: 0.0092739142\n",
      "[19301,     1] loss: 0.0092739120\n",
      "[19302,     1] loss: 0.0092739068\n",
      "[19303,     1] loss: 0.0092738993\n",
      "[19304,     1] loss: 0.0092738956\n",
      "[19305,     1] loss: 0.0092738904\n",
      "[19306,     1] loss: 0.0092738852\n",
      "[19307,     1] loss: 0.0092738822\n",
      "[19308,     1] loss: 0.0092738755\n",
      "[19309,     1] loss: 0.0092738695\n",
      "[19310,     1] loss: 0.0092738666\n",
      "[19311,     1] loss: 0.0092738591\n",
      "[19312,     1] loss: 0.0092738532\n",
      "[19313,     1] loss: 0.0092738494\n",
      "[19314,     1] loss: 0.0092738450\n",
      "[19315,     1] loss: 0.0092738390\n",
      "[19316,     1] loss: 0.0092738353\n",
      "[19317,     1] loss: 0.0092738301\n",
      "[19318,     1] loss: 0.0092738234\n",
      "[19319,     1] loss: 0.0092738219\n",
      "[19320,     1] loss: 0.0092738129\n",
      "[19321,     1] loss: 0.0092738084\n",
      "[19322,     1] loss: 0.0092738040\n",
      "[19323,     1] loss: 0.0092738003\n",
      "[19324,     1] loss: 0.0092737943\n",
      "[19325,     1] loss: 0.0092737883\n",
      "[19326,     1] loss: 0.0092737839\n",
      "[19327,     1] loss: 0.0092737794\n",
      "[19328,     1] loss: 0.0092737742\n",
      "[19329,     1] loss: 0.0092737697\n",
      "[19330,     1] loss: 0.0092737637\n",
      "[19331,     1] loss: 0.0092737585\n",
      "[19332,     1] loss: 0.0092737541\n",
      "[19333,     1] loss: 0.0092737488\n",
      "[19334,     1] loss: 0.0092737436\n",
      "[19335,     1] loss: 0.0092737369\n",
      "[19336,     1] loss: 0.0092737339\n",
      "[19337,     1] loss: 0.0092737302\n",
      "[19338,     1] loss: 0.0092737228\n",
      "[19339,     1] loss: 0.0092737176\n",
      "[19340,     1] loss: 0.0092737131\n",
      "[19341,     1] loss: 0.0092737086\n",
      "[19342,     1] loss: 0.0092737049\n",
      "[19343,     1] loss: 0.0092736982\n",
      "[19344,     1] loss: 0.0092736945\n",
      "[19345,     1] loss: 0.0092736885\n",
      "[19346,     1] loss: 0.0092736833\n",
      "[19347,     1] loss: 0.0092736788\n",
      "[19348,     1] loss: 0.0092736743\n",
      "[19349,     1] loss: 0.0092736684\n",
      "[19350,     1] loss: 0.0092736647\n",
      "[19351,     1] loss: 0.0092736579\n",
      "[19352,     1] loss: 0.0092736542\n",
      "[19353,     1] loss: 0.0092736498\n",
      "[19354,     1] loss: 0.0092736416\n",
      "[19355,     1] loss: 0.0092736363\n",
      "[19356,     1] loss: 0.0092736326\n",
      "[19357,     1] loss: 0.0092736267\n",
      "[19358,     1] loss: 0.0092736229\n",
      "[19359,     1] loss: 0.0092736170\n",
      "[19360,     1] loss: 0.0092736132\n",
      "[19361,     1] loss: 0.0092736080\n",
      "[19362,     1] loss: 0.0092736013\n",
      "[19363,     1] loss: 0.0092735969\n",
      "[19364,     1] loss: 0.0092735924\n",
      "[19365,     1] loss: 0.0092735879\n",
      "[19366,     1] loss: 0.0092735827\n",
      "[19367,     1] loss: 0.0092735752\n",
      "[19368,     1] loss: 0.0092735715\n",
      "[19369,     1] loss: 0.0092735671\n",
      "[19370,     1] loss: 0.0092735618\n",
      "[19371,     1] loss: 0.0092735566\n",
      "[19372,     1] loss: 0.0092735529\n",
      "[19373,     1] loss: 0.0092735462\n",
      "[19374,     1] loss: 0.0092735417\n",
      "[19375,     1] loss: 0.0092735372\n",
      "[19376,     1] loss: 0.0092735313\n",
      "[19377,     1] loss: 0.0092735268\n",
      "[19378,     1] loss: 0.0092735223\n",
      "[19379,     1] loss: 0.0092735164\n",
      "[19380,     1] loss: 0.0092735112\n",
      "[19381,     1] loss: 0.0092735082\n",
      "[19382,     1] loss: 0.0092735022\n",
      "[19383,     1] loss: 0.0092734955\n",
      "[19384,     1] loss: 0.0092734903\n",
      "[19385,     1] loss: 0.0092734881\n",
      "[19386,     1] loss: 0.0092734814\n",
      "[19387,     1] loss: 0.0092734754\n",
      "[19388,     1] loss: 0.0092734702\n",
      "[19389,     1] loss: 0.0092734657\n",
      "[19390,     1] loss: 0.0092734605\n",
      "[19391,     1] loss: 0.0092734553\n",
      "[19392,     1] loss: 0.0092734508\n",
      "[19393,     1] loss: 0.0092734449\n",
      "[19394,     1] loss: 0.0092734404\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19395,     1] loss: 0.0092734344\n",
      "[19396,     1] loss: 0.0092734315\n",
      "[19397,     1] loss: 0.0092734255\n",
      "[19398,     1] loss: 0.0092734195\n",
      "[19399,     1] loss: 0.0092734165\n",
      "[19400,     1] loss: 0.0092734106\n",
      "[19401,     1] loss: 0.0092734054\n",
      "[19402,     1] loss: 0.0092734009\n",
      "[19403,     1] loss: 0.0092733964\n",
      "[19404,     1] loss: 0.0092733927\n",
      "[19405,     1] loss: 0.0092733867\n",
      "[19406,     1] loss: 0.0092733823\n",
      "[19407,     1] loss: 0.0092733771\n",
      "[19408,     1] loss: 0.0092733733\n",
      "[19409,     1] loss: 0.0092733674\n",
      "[19410,     1] loss: 0.0092733622\n",
      "[19411,     1] loss: 0.0092733577\n",
      "[19412,     1] loss: 0.0092733517\n",
      "[19413,     1] loss: 0.0092733480\n",
      "[19414,     1] loss: 0.0092733443\n",
      "[19415,     1] loss: 0.0092733368\n",
      "[19416,     1] loss: 0.0092733316\n",
      "[19417,     1] loss: 0.0092733271\n",
      "[19418,     1] loss: 0.0092733219\n",
      "[19419,     1] loss: 0.0092733175\n",
      "[19420,     1] loss: 0.0092733130\n",
      "[19421,     1] loss: 0.0092733085\n",
      "[19422,     1] loss: 0.0092733026\n",
      "[19423,     1] loss: 0.0092732981\n",
      "[19424,     1] loss: 0.0092732929\n",
      "[19425,     1] loss: 0.0092732877\n",
      "[19426,     1] loss: 0.0092732817\n",
      "[19427,     1] loss: 0.0092732765\n",
      "[19428,     1] loss: 0.0092732728\n",
      "[19429,     1] loss: 0.0092732668\n",
      "[19430,     1] loss: 0.0092732608\n",
      "[19431,     1] loss: 0.0092732564\n",
      "[19432,     1] loss: 0.0092732519\n",
      "[19433,     1] loss: 0.0092732459\n",
      "[19434,     1] loss: 0.0092732407\n",
      "[19435,     1] loss: 0.0092732362\n",
      "[19436,     1] loss: 0.0092732303\n",
      "[19437,     1] loss: 0.0092732266\n",
      "[19438,     1] loss: 0.0092732228\n",
      "[19439,     1] loss: 0.0092732161\n",
      "[19440,     1] loss: 0.0092732102\n",
      "[19441,     1] loss: 0.0092732079\n",
      "[19442,     1] loss: 0.0092732020\n",
      "[19443,     1] loss: 0.0092731960\n",
      "[19444,     1] loss: 0.0092731923\n",
      "[19445,     1] loss: 0.0092731871\n",
      "[19446,     1] loss: 0.0092731811\n",
      "[19447,     1] loss: 0.0092731774\n",
      "[19448,     1] loss: 0.0092731729\n",
      "[19449,     1] loss: 0.0092731670\n",
      "[19450,     1] loss: 0.0092731625\n",
      "[19451,     1] loss: 0.0092731573\n",
      "[19452,     1] loss: 0.0092731528\n",
      "[19453,     1] loss: 0.0092731453\n",
      "[19454,     1] loss: 0.0092731409\n",
      "[19455,     1] loss: 0.0092731379\n",
      "[19456,     1] loss: 0.0092731312\n",
      "[19457,     1] loss: 0.0092731282\n",
      "[19458,     1] loss: 0.0092731230\n",
      "[19459,     1] loss: 0.0092731163\n",
      "[19460,     1] loss: 0.0092731111\n",
      "[19461,     1] loss: 0.0092731066\n",
      "[19462,     1] loss: 0.0092731014\n",
      "[19463,     1] loss: 0.0092730962\n",
      "[19464,     1] loss: 0.0092730917\n",
      "[19465,     1] loss: 0.0092730865\n",
      "[19466,     1] loss: 0.0092730805\n",
      "[19467,     1] loss: 0.0092730775\n",
      "[19468,     1] loss: 0.0092730723\n",
      "[19469,     1] loss: 0.0092730664\n",
      "[19470,     1] loss: 0.0092730619\n",
      "[19471,     1] loss: 0.0092730559\n",
      "[19472,     1] loss: 0.0092730522\n",
      "[19473,     1] loss: 0.0092730477\n",
      "[19474,     1] loss: 0.0092730418\n",
      "[19475,     1] loss: 0.0092730373\n",
      "[19476,     1] loss: 0.0092730328\n",
      "[19477,     1] loss: 0.0092730284\n",
      "[19478,     1] loss: 0.0092730239\n",
      "[19479,     1] loss: 0.0092730187\n",
      "[19480,     1] loss: 0.0092730150\n",
      "[19481,     1] loss: 0.0092730097\n",
      "[19482,     1] loss: 0.0092730045\n",
      "[19483,     1] loss: 0.0092729993\n",
      "[19484,     1] loss: 0.0092729956\n",
      "[19485,     1] loss: 0.0092729896\n",
      "[19486,     1] loss: 0.0092729859\n",
      "[19487,     1] loss: 0.0092729807\n",
      "[19488,     1] loss: 0.0092729762\n",
      "[19489,     1] loss: 0.0092729710\n",
      "[19490,     1] loss: 0.0092729658\n",
      "[19491,     1] loss: 0.0092729621\n",
      "[19492,     1] loss: 0.0092729546\n",
      "[19493,     1] loss: 0.0092729501\n",
      "[19494,     1] loss: 0.0092729457\n",
      "[19495,     1] loss: 0.0092729419\n",
      "[19496,     1] loss: 0.0092729352\n",
      "[19497,     1] loss: 0.0092729323\n",
      "[19498,     1] loss: 0.0092729256\n",
      "[19499,     1] loss: 0.0092729196\n",
      "[19500,     1] loss: 0.0092729174\n",
      "[19501,     1] loss: 0.0092729121\n",
      "[19502,     1] loss: 0.0092729062\n",
      "[19503,     1] loss: 0.0092729025\n",
      "[19504,     1] loss: 0.0092728950\n",
      "[19505,     1] loss: 0.0092728920\n",
      "[19506,     1] loss: 0.0092728868\n",
      "[19507,     1] loss: 0.0092728823\n",
      "[19508,     1] loss: 0.0092728764\n",
      "[19509,     1] loss: 0.0092728719\n",
      "[19510,     1] loss: 0.0092728674\n",
      "[19511,     1] loss: 0.0092728615\n",
      "[19512,     1] loss: 0.0092728578\n",
      "[19513,     1] loss: 0.0092728503\n",
      "[19514,     1] loss: 0.0092728473\n",
      "[19515,     1] loss: 0.0092728421\n",
      "[19516,     1] loss: 0.0092728376\n",
      "[19517,     1] loss: 0.0092728309\n",
      "[19518,     1] loss: 0.0092728272\n",
      "[19519,     1] loss: 0.0092728235\n",
      "[19520,     1] loss: 0.0092728160\n",
      "[19521,     1] loss: 0.0092728116\n",
      "[19522,     1] loss: 0.0092728078\n",
      "[19523,     1] loss: 0.0092728026\n",
      "[19524,     1] loss: 0.0092727989\n",
      "[19525,     1] loss: 0.0092727944\n",
      "[19526,     1] loss: 0.0092727900\n",
      "[19527,     1] loss: 0.0092727832\n",
      "[19528,     1] loss: 0.0092727795\n",
      "[19529,     1] loss: 0.0092727736\n",
      "[19530,     1] loss: 0.0092727691\n",
      "[19531,     1] loss: 0.0092727639\n",
      "[19532,     1] loss: 0.0092727587\n",
      "[19533,     1] loss: 0.0092727549\n",
      "[19534,     1] loss: 0.0092727512\n",
      "[19535,     1] loss: 0.0092727453\n",
      "[19536,     1] loss: 0.0092727415\n",
      "[19537,     1] loss: 0.0092727348\n",
      "[19538,     1] loss: 0.0092727296\n",
      "[19539,     1] loss: 0.0092727259\n",
      "[19540,     1] loss: 0.0092727207\n",
      "[19541,     1] loss: 0.0092727162\n",
      "[19542,     1] loss: 0.0092727110\n",
      "[19543,     1] loss: 0.0092727065\n",
      "[19544,     1] loss: 0.0092727020\n",
      "[19545,     1] loss: 0.0092726968\n",
      "[19546,     1] loss: 0.0092726909\n",
      "[19547,     1] loss: 0.0092726864\n",
      "[19548,     1] loss: 0.0092726819\n",
      "[19549,     1] loss: 0.0092726775\n",
      "[19550,     1] loss: 0.0092726715\n",
      "[19551,     1] loss: 0.0092726678\n",
      "[19552,     1] loss: 0.0092726626\n",
      "[19553,     1] loss: 0.0092726581\n",
      "[19554,     1] loss: 0.0092726514\n",
      "[19555,     1] loss: 0.0092726476\n",
      "[19556,     1] loss: 0.0092726439\n",
      "[19557,     1] loss: 0.0092726395\n",
      "[19558,     1] loss: 0.0092726335\n",
      "[19559,     1] loss: 0.0092726290\n",
      "[19560,     1] loss: 0.0092726253\n",
      "[19561,     1] loss: 0.0092726208\n",
      "[19562,     1] loss: 0.0092726149\n",
      "[19563,     1] loss: 0.0092726104\n",
      "[19564,     1] loss: 0.0092726052\n",
      "[19565,     1] loss: 0.0092726000\n",
      "[19566,     1] loss: 0.0092725947\n",
      "[19567,     1] loss: 0.0092725895\n",
      "[19568,     1] loss: 0.0092725858\n",
      "[19569,     1] loss: 0.0092725821\n",
      "[19570,     1] loss: 0.0092725761\n",
      "[19571,     1] loss: 0.0092725702\n",
      "[19572,     1] loss: 0.0092725664\n",
      "[19573,     1] loss: 0.0092725612\n",
      "[19574,     1] loss: 0.0092725560\n",
      "[19575,     1] loss: 0.0092725515\n",
      "[19576,     1] loss: 0.0092725478\n",
      "[19577,     1] loss: 0.0092725419\n",
      "[19578,     1] loss: 0.0092725389\n",
      "[19579,     1] loss: 0.0092725337\n",
      "[19580,     1] loss: 0.0092725284\n",
      "[19581,     1] loss: 0.0092725247\n",
      "[19582,     1] loss: 0.0092725188\n",
      "[19583,     1] loss: 0.0092725135\n",
      "[19584,     1] loss: 0.0092725083\n",
      "[19585,     1] loss: 0.0092725053\n",
      "[19586,     1] loss: 0.0092725001\n",
      "[19587,     1] loss: 0.0092724949\n",
      "[19588,     1] loss: 0.0092724904\n",
      "[19589,     1] loss: 0.0092724867\n",
      "[19590,     1] loss: 0.0092724815\n",
      "[19591,     1] loss: 0.0092724763\n",
      "[19592,     1] loss: 0.0092724711\n",
      "[19593,     1] loss: 0.0092724659\n",
      "[19594,     1] loss: 0.0092724606\n",
      "[19595,     1] loss: 0.0092724562\n",
      "[19596,     1] loss: 0.0092724532\n",
      "[19597,     1] loss: 0.0092724472\n",
      "[19598,     1] loss: 0.0092724435\n",
      "[19599,     1] loss: 0.0092724368\n",
      "[19600,     1] loss: 0.0092724316\n",
      "[19601,     1] loss: 0.0092724286\n",
      "[19602,     1] loss: 0.0092724226\n",
      "[19603,     1] loss: 0.0092724182\n",
      "[19604,     1] loss: 0.0092724130\n",
      "[19605,     1] loss: 0.0092724085\n",
      "[19606,     1] loss: 0.0092724040\n",
      "[19607,     1] loss: 0.0092723995\n",
      "[19608,     1] loss: 0.0092723936\n",
      "[19609,     1] loss: 0.0092723906\n",
      "[19610,     1] loss: 0.0092723854\n",
      "[19611,     1] loss: 0.0092723794\n",
      "[19612,     1] loss: 0.0092723742\n",
      "[19613,     1] loss: 0.0092723720\n",
      "[19614,     1] loss: 0.0092723653\n",
      "[19615,     1] loss: 0.0092723601\n",
      "[19616,     1] loss: 0.0092723556\n",
      "[19617,     1] loss: 0.0092723519\n",
      "[19618,     1] loss: 0.0092723466\n",
      "[19619,     1] loss: 0.0092723407\n",
      "[19620,     1] loss: 0.0092723362\n",
      "[19621,     1] loss: 0.0092723325\n",
      "[19622,     1] loss: 0.0092723265\n",
      "[19623,     1] loss: 0.0092723221\n",
      "[19624,     1] loss: 0.0092723183\n",
      "[19625,     1] loss: 0.0092723124\n",
      "[19626,     1] loss: 0.0092723079\n",
      "[19627,     1] loss: 0.0092723042\n",
      "[19628,     1] loss: 0.0092722990\n",
      "[19629,     1] loss: 0.0092722923\n",
      "[19630,     1] loss: 0.0092722878\n",
      "[19631,     1] loss: 0.0092722833\n",
      "[19632,     1] loss: 0.0092722788\n",
      "[19633,     1] loss: 0.0092722744\n",
      "[19634,     1] loss: 0.0092722692\n",
      "[19635,     1] loss: 0.0092722654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19636,     1] loss: 0.0092722595\n",
      "[19637,     1] loss: 0.0092722565\n",
      "[19638,     1] loss: 0.0092722498\n",
      "[19639,     1] loss: 0.0092722453\n",
      "[19640,     1] loss: 0.0092722416\n",
      "[19641,     1] loss: 0.0092722371\n",
      "[19642,     1] loss: 0.0092722312\n",
      "[19643,     1] loss: 0.0092722267\n",
      "[19644,     1] loss: 0.0092722222\n",
      "[19645,     1] loss: 0.0092722185\n",
      "[19646,     1] loss: 0.0092722133\n",
      "[19647,     1] loss: 0.0092722088\n",
      "[19648,     1] loss: 0.0092722028\n",
      "[19649,     1] loss: 0.0092721984\n",
      "[19650,     1] loss: 0.0092721954\n",
      "[19651,     1] loss: 0.0092721887\n",
      "[19652,     1] loss: 0.0092721842\n",
      "[19653,     1] loss: 0.0092721790\n",
      "[19654,     1] loss: 0.0092721745\n",
      "[19655,     1] loss: 0.0092721701\n",
      "[19656,     1] loss: 0.0092721641\n",
      "[19657,     1] loss: 0.0092721596\n",
      "[19658,     1] loss: 0.0092721537\n",
      "[19659,     1] loss: 0.0092721500\n",
      "[19660,     1] loss: 0.0092721447\n",
      "[19661,     1] loss: 0.0092721395\n",
      "[19662,     1] loss: 0.0092721343\n",
      "[19663,     1] loss: 0.0092721291\n",
      "[19664,     1] loss: 0.0092721254\n",
      "[19665,     1] loss: 0.0092721201\n",
      "[19666,     1] loss: 0.0092721149\n",
      "[19667,     1] loss: 0.0092721097\n",
      "[19668,     1] loss: 0.0092721052\n",
      "[19669,     1] loss: 0.0092721008\n",
      "[19670,     1] loss: 0.0092720963\n",
      "[19671,     1] loss: 0.0092720911\n",
      "[19672,     1] loss: 0.0092720874\n",
      "[19673,     1] loss: 0.0092720836\n",
      "[19674,     1] loss: 0.0092720769\n",
      "[19675,     1] loss: 0.0092720732\n",
      "[19676,     1] loss: 0.0092720680\n",
      "[19677,     1] loss: 0.0092720635\n",
      "[19678,     1] loss: 0.0092720576\n",
      "[19679,     1] loss: 0.0092720538\n",
      "[19680,     1] loss: 0.0092720486\n",
      "[19681,     1] loss: 0.0092720442\n",
      "[19682,     1] loss: 0.0092720389\n",
      "[19683,     1] loss: 0.0092720352\n",
      "[19684,     1] loss: 0.0092720307\n",
      "[19685,     1] loss: 0.0092720255\n",
      "[19686,     1] loss: 0.0092720218\n",
      "[19687,     1] loss: 0.0092720173\n",
      "[19688,     1] loss: 0.0092720106\n",
      "[19689,     1] loss: 0.0092720084\n",
      "[19690,     1] loss: 0.0092720032\n",
      "[19691,     1] loss: 0.0092719965\n",
      "[19692,     1] loss: 0.0092719942\n",
      "[19693,     1] loss: 0.0092719883\n",
      "[19694,     1] loss: 0.0092719845\n",
      "[19695,     1] loss: 0.0092719793\n",
      "[19696,     1] loss: 0.0092719749\n",
      "[19697,     1] loss: 0.0092719696\n",
      "[19698,     1] loss: 0.0092719667\n",
      "[19699,     1] loss: 0.0092719615\n",
      "[19700,     1] loss: 0.0092719555\n",
      "[19701,     1] loss: 0.0092719518\n",
      "[19702,     1] loss: 0.0092719458\n",
      "[19703,     1] loss: 0.0092719406\n",
      "[19704,     1] loss: 0.0092719369\n",
      "[19705,     1] loss: 0.0092719324\n",
      "[19706,     1] loss: 0.0092719279\n",
      "[19707,     1] loss: 0.0092719227\n",
      "[19708,     1] loss: 0.0092719175\n",
      "[19709,     1] loss: 0.0092719145\n",
      "[19710,     1] loss: 0.0092719093\n",
      "[19711,     1] loss: 0.0092719041\n",
      "[19712,     1] loss: 0.0092718996\n",
      "[19713,     1] loss: 0.0092718951\n",
      "[19714,     1] loss: 0.0092718907\n",
      "[19715,     1] loss: 0.0092718862\n",
      "[19716,     1] loss: 0.0092718795\n",
      "[19717,     1] loss: 0.0092718758\n",
      "[19718,     1] loss: 0.0092718713\n",
      "[19719,     1] loss: 0.0092718683\n",
      "[19720,     1] loss: 0.0092718631\n",
      "[19721,     1] loss: 0.0092718579\n",
      "[19722,     1] loss: 0.0092718527\n",
      "[19723,     1] loss: 0.0092718489\n",
      "[19724,     1] loss: 0.0092718445\n",
      "[19725,     1] loss: 0.0092718393\n",
      "[19726,     1] loss: 0.0092718348\n",
      "[19727,     1] loss: 0.0092718311\n",
      "[19728,     1] loss: 0.0092718251\n",
      "[19729,     1] loss: 0.0092718191\n",
      "[19730,     1] loss: 0.0092718162\n",
      "[19731,     1] loss: 0.0092718109\n",
      "[19732,     1] loss: 0.0092718057\n",
      "[19733,     1] loss: 0.0092718013\n",
      "[19734,     1] loss: 0.0092717968\n",
      "[19735,     1] loss: 0.0092717916\n",
      "[19736,     1] loss: 0.0092717871\n",
      "[19737,     1] loss: 0.0092717826\n",
      "[19738,     1] loss: 0.0092717774\n",
      "[19739,     1] loss: 0.0092717730\n",
      "[19740,     1] loss: 0.0092717692\n",
      "[19741,     1] loss: 0.0092717655\n",
      "[19742,     1] loss: 0.0092717595\n",
      "[19743,     1] loss: 0.0092717543\n",
      "[19744,     1] loss: 0.0092717506\n",
      "[19745,     1] loss: 0.0092717454\n",
      "[19746,     1] loss: 0.0092717417\n",
      "[19747,     1] loss: 0.0092717372\n",
      "[19748,     1] loss: 0.0092717320\n",
      "[19749,     1] loss: 0.0092717268\n",
      "[19750,     1] loss: 0.0092717238\n",
      "[19751,     1] loss: 0.0092717186\n",
      "[19752,     1] loss: 0.0092717141\n",
      "[19753,     1] loss: 0.0092717089\n",
      "[19754,     1] loss: 0.0092717044\n",
      "[19755,     1] loss: 0.0092716984\n",
      "[19756,     1] loss: 0.0092716947\n",
      "[19757,     1] loss: 0.0092716895\n",
      "[19758,     1] loss: 0.0092716850\n",
      "[19759,     1] loss: 0.0092716806\n",
      "[19760,     1] loss: 0.0092716761\n",
      "[19761,     1] loss: 0.0092716716\n",
      "[19762,     1] loss: 0.0092716679\n",
      "[19763,     1] loss: 0.0092716627\n",
      "[19764,     1] loss: 0.0092716582\n",
      "[19765,     1] loss: 0.0092716537\n",
      "[19766,     1] loss: 0.0092716500\n",
      "[19767,     1] loss: 0.0092716448\n",
      "[19768,     1] loss: 0.0092716388\n",
      "[19769,     1] loss: 0.0092716336\n",
      "[19770,     1] loss: 0.0092716299\n",
      "[19771,     1] loss: 0.0092716239\n",
      "[19772,     1] loss: 0.0092716195\n",
      "[19773,     1] loss: 0.0092716157\n",
      "[19774,     1] loss: 0.0092716113\n",
      "[19775,     1] loss: 0.0092716061\n",
      "[19776,     1] loss: 0.0092716023\n",
      "[19777,     1] loss: 0.0092715979\n",
      "[19778,     1] loss: 0.0092715926\n",
      "[19779,     1] loss: 0.0092715882\n",
      "[19780,     1] loss: 0.0092715837\n",
      "[19781,     1] loss: 0.0092715785\n",
      "[19782,     1] loss: 0.0092715740\n",
      "[19783,     1] loss: 0.0092715703\n",
      "[19784,     1] loss: 0.0092715651\n",
      "[19785,     1] loss: 0.0092715591\n",
      "[19786,     1] loss: 0.0092715554\n",
      "[19787,     1] loss: 0.0092715502\n",
      "[19788,     1] loss: 0.0092715465\n",
      "[19789,     1] loss: 0.0092715405\n",
      "[19790,     1] loss: 0.0092715360\n",
      "[19791,     1] loss: 0.0092715323\n",
      "[19792,     1] loss: 0.0092715271\n",
      "[19793,     1] loss: 0.0092715226\n",
      "[19794,     1] loss: 0.0092715174\n",
      "[19795,     1] loss: 0.0092715122\n",
      "[19796,     1] loss: 0.0092715092\n",
      "[19797,     1] loss: 0.0092715025\n",
      "[19798,     1] loss: 0.0092714988\n",
      "[19799,     1] loss: 0.0092714936\n",
      "[19800,     1] loss: 0.0092714898\n",
      "[19801,     1] loss: 0.0092714854\n",
      "[19802,     1] loss: 0.0092714801\n",
      "[19803,     1] loss: 0.0092714764\n",
      "[19804,     1] loss: 0.0092714719\n",
      "[19805,     1] loss: 0.0092714667\n",
      "[19806,     1] loss: 0.0092714623\n",
      "[19807,     1] loss: 0.0092714578\n",
      "[19808,     1] loss: 0.0092714533\n",
      "[19809,     1] loss: 0.0092714496\n",
      "[19810,     1] loss: 0.0092714421\n",
      "[19811,     1] loss: 0.0092714407\n",
      "[19812,     1] loss: 0.0092714362\n",
      "[19813,     1] loss: 0.0092714310\n",
      "[19814,     1] loss: 0.0092714258\n",
      "[19815,     1] loss: 0.0092714235\n",
      "[19816,     1] loss: 0.0092714161\n",
      "[19817,     1] loss: 0.0092714123\n",
      "[19818,     1] loss: 0.0092714094\n",
      "[19819,     1] loss: 0.0092714041\n",
      "[19820,     1] loss: 0.0092713989\n",
      "[19821,     1] loss: 0.0092713945\n",
      "[19822,     1] loss: 0.0092713900\n",
      "[19823,     1] loss: 0.0092713855\n",
      "[19824,     1] loss: 0.0092713811\n",
      "[19825,     1] loss: 0.0092713766\n",
      "[19826,     1] loss: 0.0092713729\n",
      "[19827,     1] loss: 0.0092713676\n",
      "[19828,     1] loss: 0.0092713624\n",
      "[19829,     1] loss: 0.0092713594\n",
      "[19830,     1] loss: 0.0092713527\n",
      "[19831,     1] loss: 0.0092713468\n",
      "[19832,     1] loss: 0.0092713438\n",
      "[19833,     1] loss: 0.0092713401\n",
      "[19834,     1] loss: 0.0092713341\n",
      "[19835,     1] loss: 0.0092713304\n",
      "[19836,     1] loss: 0.0092713244\n",
      "[19837,     1] loss: 0.0092713222\n",
      "[19838,     1] loss: 0.0092713177\n",
      "[19839,     1] loss: 0.0092713125\n",
      "[19840,     1] loss: 0.0092713080\n",
      "[19841,     1] loss: 0.0092713028\n",
      "[19842,     1] loss: 0.0092712991\n",
      "[19843,     1] loss: 0.0092712946\n",
      "[19844,     1] loss: 0.0092712902\n",
      "[19845,     1] loss: 0.0092712864\n",
      "[19846,     1] loss: 0.0092712812\n",
      "[19847,     1] loss: 0.0092712775\n",
      "[19848,     1] loss: 0.0092712730\n",
      "[19849,     1] loss: 0.0092712693\n",
      "[19850,     1] loss: 0.0092712641\n",
      "[19851,     1] loss: 0.0092712596\n",
      "[19852,     1] loss: 0.0092712551\n",
      "[19853,     1] loss: 0.0092712514\n",
      "[19854,     1] loss: 0.0092712462\n",
      "[19855,     1] loss: 0.0092712417\n",
      "[19856,     1] loss: 0.0092712365\n",
      "[19857,     1] loss: 0.0092712328\n",
      "[19858,     1] loss: 0.0092712276\n",
      "[19859,     1] loss: 0.0092712246\n",
      "[19860,     1] loss: 0.0092712186\n",
      "[19861,     1] loss: 0.0092712156\n",
      "[19862,     1] loss: 0.0092712097\n",
      "[19863,     1] loss: 0.0092712060\n",
      "[19864,     1] loss: 0.0092711993\n",
      "[19865,     1] loss: 0.0092711970\n",
      "[19866,     1] loss: 0.0092711933\n",
      "[19867,     1] loss: 0.0092711888\n",
      "[19868,     1] loss: 0.0092711836\n",
      "[19869,     1] loss: 0.0092711791\n",
      "[19870,     1] loss: 0.0092711747\n",
      "[19871,     1] loss: 0.0092711695\n",
      "[19872,     1] loss: 0.0092711650\n",
      "[19873,     1] loss: 0.0092711613\n",
      "[19874,     1] loss: 0.0092711553\n",
      "[19875,     1] loss: 0.0092711516\n",
      "[19876,     1] loss: 0.0092711471\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19877,     1] loss: 0.0092711426\n",
      "[19878,     1] loss: 0.0092711382\n",
      "[19879,     1] loss: 0.0092711307\n",
      "[19880,     1] loss: 0.0092711292\n",
      "[19881,     1] loss: 0.0092711233\n",
      "[19882,     1] loss: 0.0092711173\n",
      "[19883,     1] loss: 0.0092711128\n",
      "[19884,     1] loss: 0.0092711106\n",
      "[19885,     1] loss: 0.0092711054\n",
      "[19886,     1] loss: 0.0092710994\n",
      "[19887,     1] loss: 0.0092710964\n",
      "[19888,     1] loss: 0.0092710912\n",
      "[19889,     1] loss: 0.0092710860\n",
      "[19890,     1] loss: 0.0092710823\n",
      "[19891,     1] loss: 0.0092710763\n",
      "[19892,     1] loss: 0.0092710719\n",
      "[19893,     1] loss: 0.0092710689\n",
      "[19894,     1] loss: 0.0092710629\n",
      "[19895,     1] loss: 0.0092710584\n",
      "[19896,     1] loss: 0.0092710547\n",
      "[19897,     1] loss: 0.0092710488\n",
      "[19898,     1] loss: 0.0092710465\n",
      "[19899,     1] loss: 0.0092710406\n",
      "[19900,     1] loss: 0.0092710346\n",
      "[19901,     1] loss: 0.0092710309\n",
      "[19902,     1] loss: 0.0092710279\n",
      "[19903,     1] loss: 0.0092710212\n",
      "[19904,     1] loss: 0.0092710182\n",
      "[19905,     1] loss: 0.0092710137\n",
      "[19906,     1] loss: 0.0092710085\n",
      "[19907,     1] loss: 0.0092710055\n",
      "[19908,     1] loss: 0.0092709996\n",
      "[19909,     1] loss: 0.0092709944\n",
      "[19910,     1] loss: 0.0092709914\n",
      "[19911,     1] loss: 0.0092709862\n",
      "[19912,     1] loss: 0.0092709810\n",
      "[19913,     1] loss: 0.0092709772\n",
      "[19914,     1] loss: 0.0092709735\n",
      "[19915,     1] loss: 0.0092709668\n",
      "[19916,     1] loss: 0.0092709631\n",
      "[19917,     1] loss: 0.0092709601\n",
      "[19918,     1] loss: 0.0092709556\n",
      "[19919,     1] loss: 0.0092709504\n",
      "[19920,     1] loss: 0.0092709459\n",
      "[19921,     1] loss: 0.0092709430\n",
      "[19922,     1] loss: 0.0092709363\n",
      "[19923,     1] loss: 0.0092709325\n",
      "[19924,     1] loss: 0.0092709266\n",
      "[19925,     1] loss: 0.0092709228\n",
      "[19926,     1] loss: 0.0092709176\n",
      "[19927,     1] loss: 0.0092709139\n",
      "[19928,     1] loss: 0.0092709087\n",
      "[19929,     1] loss: 0.0092709042\n",
      "[19930,     1] loss: 0.0092708997\n",
      "[19931,     1] loss: 0.0092708945\n",
      "[19932,     1] loss: 0.0092708915\n",
      "[19933,     1] loss: 0.0092708871\n",
      "[19934,     1] loss: 0.0092708804\n",
      "[19935,     1] loss: 0.0092708781\n",
      "[19936,     1] loss: 0.0092708737\n",
      "[19937,     1] loss: 0.0092708670\n",
      "[19938,     1] loss: 0.0092708632\n",
      "[19939,     1] loss: 0.0092708588\n",
      "[19940,     1] loss: 0.0092708550\n",
      "[19941,     1] loss: 0.0092708498\n",
      "[19942,     1] loss: 0.0092708461\n",
      "[19943,     1] loss: 0.0092708394\n",
      "[19944,     1] loss: 0.0092708349\n",
      "[19945,     1] loss: 0.0092708319\n",
      "[19946,     1] loss: 0.0092708267\n",
      "[19947,     1] loss: 0.0092708223\n",
      "[19948,     1] loss: 0.0092708193\n",
      "[19949,     1] loss: 0.0092708141\n",
      "[19950,     1] loss: 0.0092708096\n",
      "[19951,     1] loss: 0.0092708051\n",
      "[19952,     1] loss: 0.0092708007\n",
      "[19953,     1] loss: 0.0092707962\n",
      "[19954,     1] loss: 0.0092707932\n",
      "[19955,     1] loss: 0.0092707880\n",
      "[19956,     1] loss: 0.0092707828\n",
      "[19957,     1] loss: 0.0092707783\n",
      "[19958,     1] loss: 0.0092707746\n",
      "[19959,     1] loss: 0.0092707686\n",
      "[19960,     1] loss: 0.0092707649\n",
      "[19961,     1] loss: 0.0092707612\n",
      "[19962,     1] loss: 0.0092707559\n",
      "[19963,     1] loss: 0.0092707515\n",
      "[19964,     1] loss: 0.0092707470\n",
      "[19965,     1] loss: 0.0092707433\n",
      "[19966,     1] loss: 0.0092707388\n",
      "[19967,     1] loss: 0.0092707336\n",
      "[19968,     1] loss: 0.0092707299\n",
      "[19969,     1] loss: 0.0092707261\n",
      "[19970,     1] loss: 0.0092707217\n",
      "[19971,     1] loss: 0.0092707165\n",
      "[19972,     1] loss: 0.0092707127\n",
      "[19973,     1] loss: 0.0092707083\n",
      "[19974,     1] loss: 0.0092707045\n",
      "[19975,     1] loss: 0.0092706993\n",
      "[19976,     1] loss: 0.0092706941\n",
      "[19977,     1] loss: 0.0092706919\n",
      "[19978,     1] loss: 0.0092706874\n",
      "[19979,     1] loss: 0.0092706822\n",
      "[19980,     1] loss: 0.0092706792\n",
      "[19981,     1] loss: 0.0092706747\n",
      "[19982,     1] loss: 0.0092706680\n",
      "[19983,     1] loss: 0.0092706643\n",
      "[19984,     1] loss: 0.0092706613\n",
      "[19985,     1] loss: 0.0092706569\n",
      "[19986,     1] loss: 0.0092706509\n",
      "[19987,     1] loss: 0.0092706479\n",
      "[19988,     1] loss: 0.0092706434\n",
      "[19989,     1] loss: 0.0092706397\n",
      "[19990,     1] loss: 0.0092706338\n",
      "[19991,     1] loss: 0.0092706285\n",
      "[19992,     1] loss: 0.0092706248\n",
      "[19993,     1] loss: 0.0092706203\n",
      "[19994,     1] loss: 0.0092706159\n",
      "[19995,     1] loss: 0.0092706122\n",
      "[19996,     1] loss: 0.0092706062\n",
      "[19997,     1] loss: 0.0092706017\n",
      "[19998,     1] loss: 0.0092705995\n",
      "[19999,     1] loss: 0.0092705943\n",
      "[20000,     1] loss: 0.0092705883\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_xyz.parameters(), lr=0.005) #0.0001)#0.005)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20000\n",
    "\n",
    "train_loss_xyz = np.zeros(nepochs)\n",
    "test_loss_xyz = np.zeros(nepochs)\n",
    "\n",
    "train_acc_xyz = np.zeros(nepochs)\n",
    "test_acc_xyz = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss_xyz = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss_xyz = 0.0             \n",
    "    epoch_test_loss_xyz = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net_xyz(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "                       \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_xyz += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss_xyz += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set_xyz = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                    net_test_set_xyz[k] = net_xyz(test_set_xyz[k][0],test_set_xyz[k][1],test_set_xyz[k][2])\n",
    "            epoch_test_loss_xyz += criterion(net_test_set_xyz, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.10f' %\n",
    "                  (epoch + 1, i + 1, running_loss_xyz/10))\n",
    "            running_loss_xyz = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss_xyz[epoch] = epoch_train_loss_xyz/counter   # divide by number or training examples in one batch \n",
    "                                                           # to obtain average training loss for each epoch\n",
    "#     if (abs((test_loss_xyz[epoch] - test_loss_xyz[epoch - 1])/test_loss_xyz[epoch - 1])< 0.1):\n",
    "#         break\n",
    "    test_loss_xyz[epoch] = epoch_test_loss_xyz/counter\n",
    "    epoch_train_loss_xyz = 0.0\n",
    "    epoch_test_loss_xyz = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss_xyz[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss_xyz[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "#plt.ylim([-13822,-13800])\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch (training with (x,y,z))',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_training_x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(1,nepochs+1)\n",
    "# # plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# # plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "# plt.plot(x[:1500],train_loss_xyz[:1500],'green',label = 'xyz features')\n",
    "# plt.plot(x[:1500],train_loss_G_3_feat[:1500],'y',label = '3 Symmetry functions')\n",
    "\n",
    "# #plt.ylim([-13822,-13800])\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# plt.xlabel('Epoch',fontsize=14)\n",
    "# plt.ylabel('Loss function',fontsize=14)\n",
    "# plt.title('Convergence: Training with (x,y,z) vs with 3 symmetry functions)',fontsize=14)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.savefig('x_y_z_loss_graph_H2O_compared_with_3G',bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xyz = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_xyz[i]\n",
    "    prediction_xyz[i] = net_xyz(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5770, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, using (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_train_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prediction_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "rotated_test_set_xyz= np.zeros((np.shape(test_set_xyz)))\n",
    "for i in range(test_set_size):\n",
    "    coord = test_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_test_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_test_set_xyz))\n",
    "rotated_test_set_xyz = torch.FloatTensor(rotated_test_set_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-13814.0463, -13813.9741, -13813.7030, -13814.0377, -13813.6886,\n",
      "        -13813.7708, -13812.6262, -13814.0684, -13813.7690, -13813.9794,\n",
      "        -13813.9872, -13814.0684, -13813.8470, -13813.4035, -13813.8464,\n",
      "        -13812.3607, -13814.0387, -13813.3729, -13812.9253, -13813.8811,\n",
      "        -13813.9032, -13813.6016, -13814.0991, -13814.0099, -13813.3479,\n",
      "        -13813.3272, -13813.1525, -13813.8496, -13813.8539, -13813.1377,\n",
      "        -13813.9152, -13814.0058, -13813.9677, -13813.1910, -13814.0254,\n",
      "        -13813.9834, -13813.8865, -13813.8070, -13813.7845, -13814.0116,\n",
      "        -13812.7262, -13813.7112, -13812.3149, -13813.7762, -13814.1316,\n",
      "        -13812.5144, -13814.1117, -13813.9671, -13813.9383, -13812.6900,\n",
      "        -13812.6956, -13813.5373, -13813.5766, -13813.8694, -13813.9441,\n",
      "        -13813.3943, -13813.9472, -13813.1602, -13813.2533, -13814.0164,\n",
      "        -13812.8035, -13813.4035, -13812.6944, -13813.7698, -13813.8953,\n",
      "        -13813.9626, -13814.0396, -13812.0252, -13813.2435, -13813.9237,\n",
      "        -13814.0754, -13814.1007, -13814.1303, -13813.5144, -13813.8727,\n",
      "        -13813.8774, -13813.9083, -13812.4317, -13814.1093, -13813.2352,\n",
      "        -13814.1040, -13814.0537, -13812.9499, -13812.4565, -13814.1067,\n",
      "        -13813.7674, -13814.1344, -13813.7507, -13812.1385, -13813.5639,\n",
      "        -13812.4259, -13813.8395, -13813.5861, -13814.0620, -13813.2059,\n",
      "        -13813.9999, -13814.0536, -13812.3310, -13813.3665, -13814.0262],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "prediction_xyz_rot = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = rotated_test_set_xyz[i]\n",
    "    prediction_xyz_rot[i] = net_xyz(x1, x2, x3)#[0]\n",
    "print(prediction_xyz*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5770, dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-127-3f312519f446>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prediction_xyz = torch.tensor(prediction_xyz)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdsAAAEiCAYAAACx53jlAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABsWklEQVR4nO2dZ5gUxdaA38MSdpGkKEFRMCsZARUDweznVYJyRTFgQkC9hovpYsCcMIsBvYokUblgzgqCERHJqAQBkSQgCOwSdvd8P6qH7Z2d0LM7s7O7nPd5+pnp6u7qUx3qdJ06VUdUFcMwDMMwUkeldAtgGIZhGBUdU7aGYRiGkWJM2RqGYRhGijFlaxiGYRgpxpStYRiGYaQYU7aGYRiGkWJM2RqGYRhGijFlaxiGYRgppljKVkQGi4hGWS5ItpC7EiLS3LuOnb314SIyLcE8/ikifSKkJ5xXSRCRO0TkDxHJF5HhKTzP4yKyLsq2d0XkuwTyqiwi14rIjyKyWUT+FpGPReSo5ElcPEr7/pWVcydKuKypeB/EMVNELi6BqClDRIaKyH/TLEPK3v9o97QsU7kEx24ETouQvrAEeRpFuQfISvCYfwJ7AsOTkFexEJF2wF3Af4BJwJoUnq4FMDvGtk+DZCIitYGPgYOAJ4HvgLrA9cBkETlZVSeXXFwjxYQ/59Heh5LwT2B3YEwS80wmjwA/i8gDqlrqdXIpvP+puKcppSTKNldVA7cYkomIZAAZqro9HeePRbJlU9VFycgn2XkF4DDvd6iq/l2SjAJc0xbAmxGOqwU0Jroi9u8rwARgb6C9qv7m2/YW8CvwGNAuUfmN0qWUnvN/ASNVdUcpnCthVHWJiHwF9Af+nQYRkvb+lwalolNUNeEFGAysjbPPcGAacDIwC9gCfAU0C9vvOOBLIBtYB7wI1IySVzdgLrADON7bdjXwu5f/W8CJgAKdgTOAfGD/sPz299LPCiB/N+BnYKsnf9MEZAtStgE++d/1rpcCnf35R5CvIzAR2IyzMkwC2nj7a9gyOFpeuC/E2cA2T477gMqJ3scI1yRchs7FOGeRaxrhXPW8/K+MsO3Y8HPHkPkKb9+To2y/39teL8AzcwYwz7vv7wN74FrLE73rNw1omch9iHH/gjxjEZ8Vb9skYFzY/p29sjYvybmBZsBHwHqv3POBqxKoZ07w5Njbl/YtkAfU8aXNBu6LJGuUZ7HQ+0ACz7Z33EFePm18aXWA5cCIsH3fwX2oVY+SV8J1FNAkQpkivWf9gVVApTjl6eDJucK7BjOA3mH7BL6XUa65X654z01MeeLc00kk8DxTjHo7kWtRSI6gD36Y8IOBtbiWcaElrDBrvAt1LnCW99DNBcTb51hc5fI68H/AhcAfES7WcO98vwIX4F6ORkB37yIOBU4B7gaWhG4ukIF7AQaH5XcXsJqwyizCOf8EFgO9gR64l/p3IDOAbHHLBnT1ZH0OOBVXof+O7+EkciXX2Xs4PgHOxpnz7wH+ARwIfAFMB472lkaR8vKumQKvennc5Mn8fCL3McK1O9CTR4Eungy1EjxnkWsa5Vwnefmdhqvw/Mt13ra6AZ7pX4AfY2zv7+XVMsY+oWv1o/e8XAD8BYzDvdhXAqd713IeBe9B3GsS5f4FecaiPivFqZwSPPci3MfG/+E+ggcAtyRQz2QB24FzvfXq3noOcIaXtgdOKZ0WSVbivw8JPdvecVfiPlwqhaWf6l23bt76JbgPg2Ni5JVwHQVU85UltLzpXZcDffu18uRpFec69wJu9u7TCcDt3nU+rzj3ktjvf5DnJqY8ce7pJII/z8Wtt4v1XJdE2Ub7smriK0wucLDvuG7ePod561OAiWF5h75mwy+MAq3D9v0BeD8s7VkKK6t7gd8oqNgEp5CHxClj6JzH+NIae2XqF0C2uGUDpgIfhu3zIvGV7be4yjuashsHTIpSJn+F+V0EGW/CVRD+CinmfYwiQx9vnxph6UHPWeSaRjnPDTGeRQVWBMjjUG/fG2Lsc7u3z/4x9gldK3+F97B33EW+tP/z0g4Pek2i3L8gz1i8Z2USxVO2Mc+N609ToEW86x/n3nwLPOPLfy0wFnjQSzvLu061Ysga630ozrM9DPghyrYXcEqyDbABeChAGYtVR/mO/4d3DS4OS6/sle+KBK63eMe9AHzhpSV8L4n+/geq82PJE+eeJvI8J1xvl+S5LsnQn41A+wjLCt8+S1R1gW99nvfbSESq48wFb3geoJVFpDLOjLMDaBt2vj9UdUZoxbOxt8aZG/yEr7+MU5KdvfUu3vorAcq4RlW/Ca2o6lJcq+XIOLLFLZsnfxvg7bC8xscSSER2A44CXlXvSSgO3vmPoGhf5+s4L/UOvrSo9zGF5yx0TWPQAliGu6/hy3yceRAR2VdEPheR+SIyV0Qe9vppwZmF/OWKxGHAJu9csViihfsMQ84pX0RI2yfBa7KTgM9YUp6V4pwbZ2L7HXheRM4VkXrFPN0U4Hjvf0dv/cuwtJla/H7B4jzbDXBKPxL/xpkWv8W1WO8IIEOx6ygROQQYhbOCvOrfpqq5OIXfIE4eu4vIUyKyFHf/dgB9gUO8XZJyL4PW+QHkSRYJ19uU4FqURNnmquq0CIu/g3lD2DGhbZk4T74MXEt0h2/ZBlQB9g07dnXY+l64L54/w9ILravqYtzXziVe0iXAVFWdG7eEkT3o1gAN48gWpGwh+cPPEc9rb3fc197KOPvFY09PlnDZQ+t7+NI2hO3jv4+pOmf4PtFoAUxX1UnhC+4+hZyjcoGbVfVw3EfOUThTL0AN7zdiBeoprbNwVoi8OPJsCFvfHiHdf/0SuSZ+gjxjyXpWEj63qubjzOOrcMpklYhMEZE2CZ5rMtBcROrgFOwUb2knIpm+tOKyIWw9yLOdiStrEVR1M/AeztT7X1WNuF/YMcWqo0SkJs5PZS6uyyQS24j/ng7HmdEfwd2z9rh7lunJl6x7GbTOjylPEkm43i7JtSiJN3JJ2YDXsQ18EGH7irD18C/zP3EV6F5h6eHrAC8BL4rIrbgKNqh3XqSvlnq4hzuWbBuIX7aQ/OHniPel9Beujypc4SfKWtyDFH6++t7v+hLmX9Jzxm2JiUgloCmu/yR82364ftvZAKq6Ek/pqOp2EZlFwcsdUkb74kyu4QwAdsN5Iyeb4t6HDcR/xjYS/1nZClQNS4um4BM5N6r6M3C2iFTBKcWHgPdFpJFXaQXha9wHQ2dc39zNuPdvM66/7AhcpVyarCdKa9Eb8tIf+Am4TUReU9VVAfJMqI7yrDKv4hTEiRrdK7oOMd5l74PlDOBqVX3el16oIZake7mBOM9NUHlikMjzXJx6u9jXIm0zSKnqFlxf1aFRWsjhyjb8+DycY0PXsE1nRdh9PO6LdSyuzGMDillPRI4JrXgV+BG4vtZYssUtWwz5e4TnFyHv74GLfGbQcLYT5yvQO/+PQM+wTf/EVdDfxjq+OKTgnAfjnGgiDe1p4f3OCt8gInVxfXMfe0nf4BTTpRH27YRzvHtKVb9PUL64FPeaBHzGgjwryykYphHi5DgyJ/TuquoOVf0C97HSEKcAAqGqfwFzcGOd84CfPJP4V7h+7ZCZLxZx34cE+QXnLVwIT1GMwD1Xx+GU3LCAeSZaR92G66vt6X1IFkFE9sI5lf0aI59quNbczha412KOVI+W9F4GeW6CyhPtnib8PCcon3//hK5FSVq2lUXk6Ajpv6vqHwHzuAn4XETycR3em4D9cF82g1Q11kMCznt3vIg8g+urPdY7FlxFBYCqbhWR0cBVwGuquiGgfGuBkSJyO87T726cmXd4gGODlC0k/3O4MZ6diDxRSDi3AJ8BH4rIMFwfUQecU8h7uKFKXUWkG+7hWxHl4+VO4GMReQX3crfAeRG+qKrLA8hRHJJ5zpBCjaZs83D9tjsRkWq4+/GEqs4H95KJyC3AcyLyOq4PLGQuuhJ4A+eIlSqKe02CPGPxnpUJwGUi8jjOQtAF51Ubj5jnxlWEQ3B9z4txLbCbcf2r6wHEzZI2Eejimf2jMRn37n7sM+NPwbVoFwRoOQZ9H4LyNXCHiOylqv5uq3txLd4TVTVb3OxSU0Skj6oOh+hljlVHhR8jIsfhvJVfAXLD6uF5vv7rdriW2jdEQVU3isgPXnn+xj33t+A+Pmt5529JnHuZAHGf2XjyeES7p8V9ngPJR4DnOipaPA/BwUT3/rxNI3gEemlNvH3+4Us7Cjdm6W9cRTAP96VQ27dPkbx8267xLnY2runfk8heZqEhIicFLONwnEmxB+7LcBvuJWseab8oeQQp29Vh8oeGgXSOlT9OMU/2jtuAexlba4H34ATcl7USe5ztuThltd2TI+KY13j3MYJ8fYjgjVjcc0Y5x124j6CMCNtG4yoef1oG7gV6LEp+5+Bagjm+a9ozgfci0rUqch0iXb941yRG/kGesajPirf9VpzTxybch8ZZxPFGjndunFl8JK5C2orr43oN2M93fMgru2mc63qut99/ws6twMvx7gOJvQ9F7k2E/Kvixl9e6Es7Fvdxd37Yvo9417xRvDITpY4KP8b3TEVaOvuOe5Iwz9oo5TkI58C3BecAeBO+uRSC3MsE3/+Yz2w8eWLd05I8z8l6rqMtIVfzCoOI3Ib7AtlDVXN86Q/jXtr9NUAfg7i5PJurqs0YVEEQkZdwCvdSrWgPfjlERO4COqpql3TLkigi8iRwkKqeEXfnwsdFLXO0Oqo418nzcl+KG/85KhEZjdSQTgepEuP1SdyK+1LPxnVW34zzAszx9jkU50TTH7griKI1Kh4icixwGa7/7yevC/NlVX0qrYLt2hxDapzOSoNHgF9E5BCN393lp0iZA9RRxblOPXEWmqD+KUaKKdctW3ETx7+GG/daG+dVOga4XT3vPBGZhDMLvIMz+wSa+9JatoZhxEJEegErVfXLEuYziWLUUXHyPA83jtQCZ5QRyrWyNQzDMIzygAWPNwzDMIwUU677bEuDPffcU5s0aZLUPLds2cJuu+2W1DzTTUUsE1TMclXEMkHFLFd5LtOPP/64VlUjTTK0S2LKNg5NmjRh2rRIkwoVn0mTJtG5c+ek5pluKmKZoGKWqyKWCSpmucpzmby5jQ0PMyMbhmEYRooxZWsYhmEYKcaUrWEYhmGkGOuzLQY7duxg+fLlbN26tVjH165dm/nz58ffsRxRXsuUmZlJo0aNqFKlSrpFMQyjAmPKthgsX76cmjVr0qRJE6IHU4nOpk2bqFmzZgokSx/lsUyqyrp161i+fDn7718kiIthGEbSMDNyMdi6dSt169YtlqI1yg4iQt26dYttoTAMwwiKKdtiYoq2YmD30TCM0sCUrWEYhlGUd96B//433VJUGEzZlkPWrVtH69atad26NQ0aNGCfffbZub59e/w5zCdNmsQ330SNJx2YDRs28Oyzz5Y4H8MwyhBr1kCvXtC1q1O2+RYoLRmYsi0FRo+GJk2gUiX3+8YbJfNLq1u3LjNmzGDGjBn069eP66+/fud61apV4x5vytYwjCKowqhRcPjhMGEC3HsvfPmlq7iMEmNXMcWMHg19+8LSpe5ZXroUrrkmk9Gjk3ueH3/8kU6dOtG2bVtOPfVUVq5cCcBTTz1F06ZNadmyJb169WLJkiU8//zzPP7447Ru3ZopU6YUyufLL7/c2Upu06YNmzZtAuCRRx6hffv2tGzZkjvvvBOAW265hUWLFtG6dWtuu+225BbIMIzSY9kyOOMMuPBCOPRQmDEDBg0CGxKXNGzoT4oZNAiyswun5eQIgwZB797JOYeqcs011/D222+z11578frrrzNo0CBefvllHnzwQX777TeqVavGhg0bqFOnDv369aNGjRoMHDiwSF5Dhgxh6NChHHvssWzevJnMzEw++eQTFixYwNSpU1FVzjrrLCZPnsyDDz7InDlzmDFjxk6lbBhGOSI/H154AW66yf1/8km46irIyEi3ZBUOU7YpZtmyxNKLw7Zt25gzZw4nn3wyAHl5eTRs2BCAli1b0rt3b7p160a3bt3i5nXsscdyww030Lt3b3r06EGjRo345JNP+OSTT2jTpg0AmzdvZsGCBey3337JK4RhGKXLr7/C5ZfDlClw8skwbJjr5zJSginbFLPffs50HCk9WagqzZo149tvvy2y7f3332fy5Mm888473HPPPcydOzdmXrfccgtnnHEGH3zwAUcffTSfffYZqsqtt97KlVdeWWjfJUuWJK8QhmGUDrm58OijcOedkJUFr7wCF18MNgwupVifbYq57z6oXr1wWlaWct99yTtHtWrV+PPPP3cq2x07djB37lzy8/P5/fff6dKlCw8//DAbNmxg8+bN1KxZM6rZd9GiRbRo0YKbb76Zdu3a8fPPP3Pqqafy8ssvs3nzZgD++OMP1qxZEzMfwzDKIDNnwlFHwS23wP/9H8ybB336mKItBRJStiJSSUSqx9/TCNG7t7PONG7snufGjeHpp7cmrb8WoFKlSowbN46bb76ZVq1a0bp1a7755hvy8vK44IILaNGiBW3atOH666+nTp06nHnmmUyYMCGig9QTTzxB8+bNadWqFVlZWZx++umccsopnH/++XTo0IEWLVpwzjnnsGnTJurWrcuxxx5L8+bNzUHKMMoy27bBbbdBu3bwxx8wbhyMHw9ed5NRCqhqzAU4HRgBLAVygTxgCzAFGATsHS+P8ry0bdtWw5k3b16RtET4+++/S3R8WaQ8lynW/Zw4cWLpCVJKVMQyqVbMciWlTF9/rXrYYaqgetFFquvWlTzPAADTtAzU4WVlidqyFZFuIvIr8DKwA7gf6A6cClwCTAROAhaLyPMisleqPggMwzCMBNm8Ga69Fo47zg2J+OgjePVV2GOPdEu2SxLLQepW4AbgA1WNNIXIGwAisg9wLXAR8GjSJTQMwzAS49NP3QD/JUvg6qvh/vuhnEXlqmhEVbaqelSQDFT1D+CmpElkGIZhFI+//oJ//9t5GB96qBvWc9xx6ZbKwLyRDcMwKgbjx0PTpjBiBNx6q5sFyhRtmSFqy1ZEngqaiar+KzniGIZhGAmxapUzFf/vf9C6NXzwAXgT0Bhlh1h9ti0C5qHJEMQwDMNIAFXXir3+eucA9cADzoRs8xmXSaKakVW1S8DlhNIU2HBkZGTQunVrmjdvzplnnsmGDRti7j98+HBWrFgRN9+g+/lZsmQJRx1VtIt/yZIljBkzJqG8/Nx///3FPtbPW2+9xbx585KSl2GUCZYuhdNOcxNSNGvmJqu45RZTtGWYhPtsRaSGiOyWCmEqNDkr4dNOkLMqKdllZWUxY8YM5syZwx577MHQoUNj7p9KZRsNU7aGkWTy8+Hpp52C/eYbeOYZFwbv0EPTLZkRh8DKVkSuEpFlwEbgbxFZKiIDUidaBWP2PfDnVzD77qRn3aFDB/744w8AZsyYwdFHH03Lli3p3r07f/31F+PGjWPatGn07t2b1q1bk5OTw91330379u1p3rw5ffv2RVUj7hctdN+PP/5Iq1at6NChQ1RFf8sttzBlyhRat27N448/Tl5eHjfeeOPOUH0vvPACACtXrqRjx447W+pTpkzhlltuIScnh9atW9M7bLqtvLw8+vTpQ/PmzWnRogWPP/444KaaPO2002jbti3HH388P//8M9988w3vvPMON954I61bt2bRokVJv/6GUSr8/DN07Aj/+hccfzzMmeMi9Fi82fJBkJkvgP8Am4A7gRO9ZTDwN3BLSWfWAHoCc4F8oJ0v/UhghrfMBLr7tp0HzAZmAR8Be3rp1YDXgYXA90AT3zEPe+eZDzwFSDzZSjyD1GuZqqMpuryWGTyPCOy2226qqpqbm6vnnHOOfvjhh6qq2qJFC500aZKqqt5+++167bXXqqpqp06d9Icffth5/DrfLDIXXHCBvvPOO0X22759u3bo0EHXrFmjqqpjx47VSy65pMh5Bg4cqIcffngRGSdOnKhnnHHGzvUXXnhB77nnHlVV3bp1q7Zt21YXL16sQ4YM0XvvvXdneUKzUYXKGM60adP0pJNO2rn+119/qarqCSecoL/++quqqn733XfapUsXVVW9+OKL9c0334yYl6rNIFVRqIjlmjhxour27ar33adatarqHnuojhihmp+fbtHigs0gVWgJGvWnH9BXVV/zpX0uIgtwM0s9GDCfaMwBegAvREhvp6q5ItIQmCki73rbngSaqupaEXkYuBr3AXAZ8JeqHiQivYCHgHNF5BjgWKCld/xXQCdgUgllj03XxTB9ICx/C/KyIaM6Oxr8gypHPlmibEOtviVLltC2bVtOPvlkNm7cyIYNG+jUqRMAF198MT179ox4/MSJE3n44YfJzs5m/fr1NGvWjDPPPLPQPr/88kvE0H3h57nwwgt5//3348r8ySefMGvWLMaNGwfAxo0bWbBgAe3bt+fSSy9lx44ddOvWjdatW8fM54ADDmDx4sVcc801nHHGGZxyyils3ryZb775plB5t23bFlcmwyjL1FiwwDlAzZgBPXs6E3L9+ukWyygGQZVtPeCHCOlTgRLfeVWdDyBhkSdU1R92PZMCz2fxlt1EZB1QC9eSBeiKU7oA44BnxGWsXh5VvWOrAKtLKntcshpClVqQtxUqZULeVrRyTchqULJsvT7bjRs38o9//IOhQ4dy8cUXBzp269atDBgwgGnTprHvvvsyePBgtm7dWmQ/1cih+zZs2FDkXgVBVXn66ac59dRTi2ybPHky77//PhdeeCE33ngjF110UdR8dt99d2bOnMnHH3/M0KFDeeONN3jiiSeoU6cOM2bMSFguwyhzbN0Kd91F24cfhnr13Bja7t3TLZVRAoIq21+B84HwDsfzgV+SKlEYInIUbn7mxsCFqprrpffHmZG3AAuAq7xD9gF+B/BaxBuBuqr6rYhMBFbilO0zISUf4Zx9gb4A9evXZ9KkSYW2165dO6HQcpmb/0AbX8qOxpdQZekrsHVVUkLTbdq0iUqVKvHAAw9w3nnnccEFF1C7dm0+/vhjjjnmGF566SU6dOjApk2byMrKYvXq1WzatIkNGzagqlSrVo2VK1fyxhtv0LVr1yL77b333qxevZrPPvuMo446ih07drBw4UIOP/xwatasySeffEKHDh145ZVXUNUiZapUqRIbNmzYmd6pUyeefvpp2rdvT5UqVViwYAF7770369atY++996ZXr16sW7eO7777ju7du1OlShXWr19PlTAPy3Xr1lGlShVOOeUUGjRoQP/+/RER9ttvP0aMGEH37t1RVebMmUOLFi12hiCMds23bt1a5B6H2Lx5c9Rt5ZWKWCaoOOWqPXs2hz7yCNV//53fTz6ZZddcQ27NmlAByrZLE8TWjDPx5gKfAXfhWo6f4QIUdAuYx2c4s3D40tW3zyR8fbZhxx+Oa0ln4lqlnwMH4ilO4DZvv7lAI99xi4C6wEHA+0ANb/kW6BhP7rIa9Se8P/Mf//iHjhgxQn/66Sc96qijtEWLFtq1a1ddv369qqqOGzdODznkEG3VqpVmZ2froEGD9MADD9QTTzxR+/Tpo3feeWfE/X766Sc9/vjjtWXLltq0aVMdNmyYqrp+05YtW+rRRx+td955Z8Q+2+3bt+sJJ5ygLVu21Mcee0zz8vL01ltv1ebNm2uzZs20c+fOumHDBh0+fLg2a9ZMW7durccdd5wuXrxYVVVvuukmPeyww/T8888vlO+MGTO0TZs22qpVK23VqpV+8MEHqqq6ePFiPfXUU7Vly5Z6+OGH61133aWqql999ZUefvjh2rp1a124cGEROa3PtmJQ7sv199+qV12lCqpNmqh++mm5LhPWZ1tYhwXeEdoCo4Afgene/zZJFSaGsvW2TwTaAe2Bz33pHXEBEwA+Bjp4/ysDaz2FfCNwu++YO4Cb4slUVpVtWaM8l8mUbcWgXJfrww9V99tPVUT12mtVN21S1fJdJlO2hZfAPuOq+qOqXqCqbVX1CO//T0GPLw4isr+IVPb+NwYOBZYAfwBNfWH9TsZ5GAO8A4Q6L88BvvBu/DKgk4hUFpEqOOeoiGZkwzCMUmHdOrj4Yjj9dNhtN/j6a3jiCahRI92SGUkmaJ8tACKyB85ZqpCSVtUSzRggIt2Bp4G9gPdFZIaqngocB9wiIjtww4IGqOpa75i7gMnetqVAHy+7/wIjRWQhsB7o5aWPA07A9fMq8JGqhjybDcMwSg9VN5fxVVfB+vVw220waBBkZqZbMiNFBFK2ItIGeIWC+ZJD3r2h34ySCKGqE4AJEdJHAiOjHPM88HyE9K24cbvh6XnAlSWR0zAMo8SsXOmU7IQJ0LYtfPIJtGqVbqmMFBO0ZfsyznR7LW64jAUfMAzDSARVGD4cbrjBDe156CH3v3JCBkajnBL0Lh8M9FTVhXH3NAzDMArz22/Qty989pmbavGll+CQQ9ItlVGKBHWQ+go39MYwDMMISl4ePPkkNG8O338Pzz7rxsuaot3lCNqyvQx4SUQOwI2N3eHfqKqTky2YYRhGuWbePLj8cvj2W+dt/MILsO++6ZbKSBOJmJFbA0Xn2UuCg5RhGEaFYccO1x97zz1QsyaMGgXnnw/FmOLUqDgENSO/gJuxqQVu6M9evqVeakQzYnH77bfz5JMFwQwGDRrEU089FfOYjRs3cuihh/LLL26GzfPOO48XX3wxpXIaxi7FtGnQrh3cfjv06OFat717m6I1ArdsGwH/p6oWDDSc665zETkSICsvDzJiGANat3YD22Nw2WWX0aNHD6699lry8/MZO3YsX3zxRdSIOWPGjKFp06Y888wz9OnTh2uvvZa//vqLK664IiHZDcOIQE4O3HknPPooNGgAb70FXbumWyqjDBFU2X6Km67RlG0ZoUmTJtStW5effvqJ1atX06ZNGxo3bhw36s3JJ5/Mm2++yVVXXcXMmTNLR1jDqMh8+aXrm124EK64Ah5+GOrUSbdURhkjqLL9CHhURFriZmAKd5Aan2zByg1xWqCRyNm0iZo1a5b41JdffjnDhw9n1apVXHrppWzatInjjz8+4r6hlm1+fj7z588nKyuL9evX06hRoxLLYRi7JH//DTffDM8/DwccAJ9/DieckG6pjDJKUGX7rPf7nwjbzEEqTXTv3p077riDHTt2MGbMGDIyMuK2bB9//HEOP/xw7r//fi699FK+/fbbIiHsDMOIw/vvQ79+sGKFm5ji7rvd3MaGEYVAylZVAwcsMEqPqlWr0qVLF+rUqUNGrD5gj19//ZWXXnqJqVOnUrNmTTp27Mi9997LXXfdVQrSGkYFYO1a56cxejQ0bQpvvglHH51uqYxyQExlKyKPAm8BX6tqfqlIZAQmPz+f7777jjfffDPQ/occcgjz5xcEOnrsscdSJZphVCxU4fXX4ZprYMMG5wx1661QrVq6JTPKCfFarNWB14DVIjJcRLqJSFYpyGXEYd68eRx00EGceOKJHHzwwekWxzAqLn/8Ad26wXnnwf77w/TpMHiwKVojIWK2bFW1P9BfRI4EugL3AqNF5HNci/ddVf0z5VIaRWjatCmLFy9OtxiGUXFRdXMYDxzoJqoYMsSZkAN02RhGOIH6YlV1qqoOUtXmQCvgS1z82OUi8pWIDBSRfVIoZ5nDxaM3yjt2H42ILFoEJ57oggcccQTMmgX//rcpWqPYJOz4pKoLVfVRVe2Im+ziZVyQ9/OSLVxZJTMzk3Xr1llFXc5RVdatW0emBew2QuTlwWOPQYsW8OOPbj7jzz+Hgw5Kt2RGOadEgRQ9E/LL3rLL0KhRI5YvX86ffxbPgr5169YKV8GX1zJlZmbaWGPDMWcOXHYZTJ0K//gHPPcc2LNhJImoylZE3gmaiaqelRxxygdVqlRh//33L/bxkyZNok2bNkmUKP1UxDIZuwjbt8MDD8B990Ht2jBmDPTqZfMZG0klVst2XalJYRiGkQ6mTnWt2TlzXGSeJ56AvfZKt1RGBSSqslXVS0pTEMMwjFIjOxvuuAMefxwaNoR333WmY8NIESXqszUMwyh3TJzoAgcsXgxXXuliz9aunW6pjApOYGUrIl1wHsf7AVX921TVZt82DKNss3Ej3HQTDBvmvIsnToTOndMtlbGLEGjoj4j0AT4EagKdgT+B3YEjgHkpks0wDCM5vPuum8s4NEnFzJmmaI1SJeg424HA1ap6Hi683q2q2gYYBWxOlXCGYRgl4s8/nePTWWdB3brw/ffwyCNQvXq6JTN2MYIq2wOAz7z/24Aa3v9ncDNJGYZhlB1U3RCeww+HceNcCLxp06Bdu3RLZuyiBFW263AmZIA/gObe/7qABSYwDKPssHy5a8n27u36Zn/6CW6/HapWjX+sYaSIoMp2CnCK9/8N4CkReQUXEejTkgohIj1FZK6I5ItIO1/6kSIyw1tmikh337bzRGS2iMwSkY9EZE8vvaOITBeRXBE5J+w8F4vIAm+5uKRyG4ZRhsjPd9MrNm0KX3zhpl38+mto1izdkhlGYG/kq4HQXHwPALnAsTjFe28S5JgD9ABeiJDeTlVzRaQhMFNE3vW2PQk0VdW1IvKwJ+NgYBnOtD3Qn5GI7AHcCbQDFPhRRN5R1b+SIL9hGOlkwQJa33CDc3w68UTncXzAAemWyjB2EkjZqup63/984KFkCqGq8wEkbHo0Vc32rWbilCSAeMtuIrIOqAUs9I5Z4uUVHuz+VODTUFlE5FPgNFzr3DCM8khurpuY4o47qJGR4byNL73Uplo0yhyBlK2I9AS2q+rbYelnAVVVdVwqhPPOcRQu0EFj4EJVzfXS+wOzgS3AAuCqOFntA/zuW1/upUU6Z1+gL0D9+vWZNGlSCUpQlM2bNyc9z3RTEcsEFbNcFaVMuy1axKGPPEKtX35h7bHH8tMVV1ClcWP48st0i5Y0Ksq9MoKbkQcDN0RIzwbuB+IqWxH5DGgQYdOgcCXuR1W/B5qJyOHAqyLyIZAH9AfaAIuBp4FbiW3SjvSpGzFGnqoOA4YBtGvXTjsneTzepEmTSHae6aYilgkqZrnKfZm2bXNBAx54AHbfHV5/nT179qTKl1+W73JFoNzfK2MnQZXtAcAvEdIXetvioqonBRUqyvHzRWQLzhNavLRFACLyBnBLnCyW4ybkCNEImFQSmQzDKGW++84FDpg3Dy680JmQ69ZNt1SGEZeg3sh/AQdHSD8E2JQ8cQojIvuLSGXvf2PgUGAJbvhRUxEJhec4GZgfJ7uPgVNEZHcR2R3nXf1xSgQ3DCO5bNkC118PxxwDmzbBBx/AiBGmaI1yQ1Bl+zbwuIgcEkoQkUOBx4C3SiqEiHQXkeVAB+B9EQkpweNwHsgzgAnAAFVdq6orgLuAySIyC2iNM2cjIu29vHoCL4jIXNjp5HUP8IO33O13/DIMo4zy+efQooULf9e/vwuHd/rp6ZbKMBIiqBn5JuAjYJ6IrPTSGgJTgRtLKoSqTsAp0/D0kcDIKMc8DzwfIf0HnIk40jEv45ytDMMo62zY4OYx/u9/4eCDneNTx47plsowikXQoT+bgGNF5GRcK1KA6cDnqhrRycgwDKPYvPUWDBgAa9bAzTfDnXdClk1WZ5Rfgg79OVBVF6nqp4TNGCUiJ6rq5ymRzjCMXYvVq+Gaa+DNN6FVKxetp23bdEtlGCUmaJ/tJyJSZNiOiJxEEvpsDcPYxVGFkSPdVItvv+2G9vzwgylao8IQVNl+BHwqIrVDCZ5J+S3CpkU0DMNIiGXL4P/+Dy66CA47DGbMgP/8B6pUSbdkhpE0girbq3HzFH8gIlk+RXuDqobPZ2wYhhGf/HwYOtQFCpgyBZ56CiZPdmHxDKOCEdRBSkXkQuBdYCJuYonrVPXFVApnGEYF5Zdf4PLL4auv4OSTXeCAJk3SLZVhpIyoylZEjoiQfD8wGhiBi5pzBICqTk+NeIZhVChyc2HIEBg82HkXv/IKXHyxBQ4wKjyxWrbTcHMH+9+C0Ho/4ErvvwIZqRLQMIwKwowZbqrF6dOhe3dnQm7YMN1SGUapEEvZ7l9qUhiGUXHZuhXuuQceegj23BPGjYOzz063VIZRqkRVtqq6tDQFMQyjAvLNN641+/PPzlz82GOwxx7plsowSp2o3sgiclzQTESkhoi0SI5IhmGUezZvhmuvheOOg+xs+OgjGD7cFK2xyxJr6M9/ReRzETlPRGpF2kFEWorIw7hQe61SIqFhGOWLTz6B5s3h6afhqqtc4IBTT023VIaRVmL12TbDOUHdAYwUkYXASmArsDsu3F0mMB44QVXnpVhWwzDKMuvXw7//7Vqwhx7qxsweF9hAZhgVmlh9trnAUGCoiLTDhbtrDGQBPwKPABMtTJ1hGPzvf64Vu3atm/3p9tshMzPdUhlGmSHopBbTcEOBDMMwCli1Cq6+2inbNm1c32zr1umWyjDKHIGmaxSRJ0SkeaqFMQyjnKDqzMVNm8J778EDD8DUqaZo00HOSvi0E+SsSrckRgyCzo3cHpgpIlNFpG80hynDMHYBliyB006DSy5x8xrPnAm33AKVAxnKjGQz+x748yuYfXe6JTFiEEjZquqxQFPcvMh3AitEZISIdEqlcIZhlCHy852HcfPmbvzs0KHw5ZfOGcoofcZmwRiBhc8B+e53jLh0o8wRtGWLqv6iqjcD+wK9gBq4OLcLROQWEbEBdKWFmY2M0ubnn6FjR/jXv+D4491wngEDoFLgKsRINl0XQ+PzIaO6W8+oDk16Q9ff0iuXEZHivClVgFpAbdycyMuAC4FlInJ+EmUzomFmI6O02LED7r8fWrWC+fPh1Vfhgw+gceN0S2ZkNYQqtSBvK1TKdL+Va0FWg3RLZkQgcCeLN/znUlyrNht4FbhcVX/ztl8LPA6MSYGcBjjzUP7WgvWFz7mlUib0ykk8v5yV8FUvOO51e0GNokyf7qZanDEDevZ0JuT69dMtleFn62o4uB8c1BcWDnPvtFEmCeqNPBv4BmdC7gM0VtVBIUXrMQbYK+kSGgUk22xkLWQjEjk5cOutcOSRbmjP+PHwxhumaMsiHcdD+6Gweyv323F8uiUyohDUjPwGsL+qnqmq76hqXvgOqvqnqloHTnEI2gebLLOROVYY0ZgyxQ3fefBB6NMH5s1z4fCKQ2n6FuSspPXaa5N3LvOLMJJMUG/ke1T1j1QLs8uSSAszZDY69Tv3u7UYlYE5VpQ/SlL5Bzl20yY3A1THjrB9O3z6Kbz0Euy+e/FlLk3Lyex7qL19dvLOZVYfI8kE6rMVkZejbFLcXMkLgddVdUWyBNslKE4frN9M1H5o8c6baAvZ+nbTj7/yP/LZ5B770Udw5ZXw++9w3XVw772w227FlzXZvgUBzyXJOFdpym7sUgQ1++4F9AC6AQd5Szcv7VDgJuAXEWldHCFEpKeIzBWRfM8RK5R+pIjM8JaZItLdt+08EZktIrNE5CMR2dNL7ygi00UkV0TO8e3fWkS+9c4zS0TOLY6sSSWdLcxEWsiJfOWb+S25lMTkH+XY41d4EXjWrXMxZk8/3SnXr7+Gxx8vmaKF0n2uk30us/oYKSKosv0a+BBopKodVbUj0Aj4APgEF6DgfeDRYsoxB6e4J0dIb6eqrYHTgBdEpLKIVAaeBLqoaktgFnC1d8wynBNXuFd0NnCRqjbz8npCROoUU97kkE7X/SCOFcWp6M38llzCK3+AGgcHq/yjKI7v642BcePcVItjxrigAT/9BB06JEfmJD/Xo0dDkyZuSG+TJm490rnyqFryd8iG0xgpIujQn2txYfSyQwmqmi0i9wGfq+rDIvIQ8FlxhFDV+QAiEp6e7VvNxJmtwVmMBNhNRNbhxv0u9I5Z4uWVH5bXr77/K0RkDa7FvqE4MieNsuy633UxTB8Iy9+CvGxXWe/bHdoMKbLr8StOhTHbCxLM/JYcshrCstfB75O4eQFMaBj/2kZSHBsyOPjBJ50jVNu2LvZsqxSEoi7ucx3WZTF6NPTt6+LPAyxd6tYBevcufK7pG9rQvs5PJX+HyvI7aZRbRFXj7ySyCeiqql+EpZ8AvK2qNUXkQGC6qtYutjAik4CBXpShUNpRwMu41vOFqjrBSz/HS98CLMC1cvN8xw0H3lPVcRHOcyRunHAzVc2PsL0v0Begfv36bceOHVvcIkVk8+bN1KhRI6l5poqDNzzO3tnvkk8VKrGDFdXPZEGd64vst+PvZbTMG8GeOV+RwTbyqMbarONZVKs/2zPK7+RiZeFetVh3C1m5y6mW9ycZbEepxJqsEwJd22br72B7pT1YUf0MDnrnWWoPn4PuqMSSSy5hec+eaEZGKZUiGKHnLfSc9ep1NKtXFw3VV7/+VsaO/a5QWlm4V8kmXpk++6weL710AGvWVKNevW1cfvliTjppTSlKGJ0uXbr8qKrt4u+5i6CqcRdgBPAb0BNoglN8Pb20V719zgN+iJHHZzizcPjS1bfPJJzZONLxhwNTcS3cKsDnwIG4Fu4zwG1h+w8HzomQT0PgF+DoIGVv27atJpuJEycmPc+U8WV31akDVNfPcL9fdo+428SJE1W/76c6upLqa5nu9/v+pStrScleofpJR9XslTuTysy9Ksm1XbxY9aSTVEG1Y0f9buTI1MlZXF7LVB1NkSX7lUx1IYYKLyJFsygz9yqJxCrTqFGq1asXvi7Vq7v0sgAwTQPUsbvKErTPth/wMTAKWAQs9v5/BAzw9pkPXBFDqZ+kqs0jLG8HEUCdqXkL0Bxo7aUt8m7qG8Ax8fLwohW9j1PM38Xb3yCxQfPJGJaUTspyf3Nxrm1eHjz5pAsc8P338NxzMHEiOY0apV7eRInSv9xxSOS+6f32K0XZyiiDBhWY10NkZ7t0o+wRV9l6zkidgduAPYA2wBHAHqraX1W3AKjqDFWdkUzhRGR/7/yISGOc5/MS4A+gqYiEZqw6GafsY+VVFZgAjFDVN5Mpp+FRXmezKYHHb0znnWSS6LWdNw+OO84N5encGebOhX79Si9wQKJe6VEck667tQHVqxfetXp1uO++5Itc3li2LHL60qWl8DwaCRP3zVPVXGA8UENVt6jqLFWdGVKyyUBEuovIcqAD8L6IfOxtOg4XR3cGTlEOUNW16sbz3gVMFpFZuJbu/V5e7b28euK8l+d6ef0T6Aj08Q0nap2sMhjlmGIO9wg57yxd6ox4S5fChRe6YDhFKK0hUdu3wz33QJs2sGABjBrlgrvvu29qzxtOcawEEVrvvXvDsGEu7oGI+x02zOcctQsTq3Ufeh779jWFW2YIYmsGvgdOSrfNOx3LLt9nG5ByX6bv+2n+qEqaMzxTc0dW0hHX9NdRo2KXq3FjjdqfWKTf7Pv+RfpaR41yeYi43xL3tf3wg2rLlk6IXr1UV6+OuFsq7lWoLNmvRO571dcyk37OcMr9MxiBRPtsIy2NG5eauIXA+mwLLUFtSoOBR0Wkm4jsKyJ7+JcUfQcYRnT+mgFv1oG/ZiUlu2ULVjNsUj+OuuM7nv+8H9UrraJvX+ftGfWYKGY8VV+/WRQTde7orCKt4mK3QrKz4aab4KijYO1aePtteO01qBdd9mTib+EfcP1iRn99PtnbEp8UotRM8hWE8FZ/NKI9p0bpElTZvg+0wJmTlwB/esta79cwSo+clfDJsbBjI3ydnBDKHQeNp99LQ5m1rBVXDx/KOU+OJzsbXnrpgKjHxDLj7azgopioOzzwW3KcW7780o2TfeQRuPRS1zd71lkJZlIy/I46qzY05O+cWlSrspWtO4JPChHJJG8m0Pj07g1LlkB+fvQQw+ZMVjYIqmy7+JYTfEto3TBKhzECE/Z2k2wA/D3XpY2J8WkfgGhf/2vWVIt6zH33RW9R7Kzgojj+/DgvsvIJ3Ar5+2/o3985P+Xnw+efM7rzi7Q/Pocvb+9E++arSk1Rhctcr9Zqnv+8H0ffGdxz2jxrS85992HOZGWYoFF/voy1pFpIwwBieAcLnD6zYLUYzkjRvv7r1dsW9ZjevZ2Db7jCLVLBRXD8iXa+SOnh5tWJA9+HZs2cDfGGG2DWLEavPIG+feGSdvdw3KFfcUn7u0utZRgu8zlPjufq4UPZQHCv9GgfGWYCDY45k5VtAo8DEJEWIvKMiHwoIg29tG4i0iZ14lVAkh13M1mUhwACIZMsYdqtxsGwe8uC9RiesNH6BaO1Ci6/fHFMkZ59FkaOjFPBdRzP6F+H0qRNKyodNZQmF43n//7P5d+gzkom3daJ+rVXRWyF+M2re+ha7l16AV0e/QcbqA3ffAOPPgq77cbZuVlseVEYcPJzZFTKZ8DJz7HlReHs3NTHKI507USczEH7XhP5+DCi4zcrL1liirYsEUjZisgpwA/APjizcegNPhC4MzWiVVCSHXczWZTlCR1ChEyyKDunx666O+RuctvjjJeN1S8YrVUQceq7sA+TeBXcgAFuSJD/vK++6gLuPHyha4k+ctHdjBi2kt71Cn/wOPOqci5jmc/h/JM3GMydtK803TlEeYQck7Z4jklbtlVn1Fe92f+61Eer8V87cNdPvVlgg/a9mgnUqOgEbdneA9ygqt0B32zzTAKOTLZQFRKfIhA0sVBppSRXwiHcYKfiqZq3PqVi7mTrajh4AJz+ExzcH+p1hh5eGOU442Xj9QsGbhX4P0ziWARGj4bnny9QPiHWDs3i2eOEC492LdELj36Os2Vv+HNyoQ+e3KV/8BbdGMt5/Mb+tOVH7mIwi36vWii/qrWcY1Jmla3kbM8ks8pW56hUu3Si1YSuXePGRcsapO+11Eyg5cGCY1RIgirbZrhweuGsx80qZcQj3XEyo1UyJZXLUzyNN72aXHmjEWsmpTjh0UrcLxjpw2RCUQXpZ9CgosoHClqihULnhVj4HIwWuKIKS6o15mQ+4d8MoQPfMocWgGde9d3T++6DvfcocEx6/vN+7FN3Vam3DEtyjUvFBFoeLDhGhSSosv0LZ0IO5whgefLEqcAkO+5mokSrZIobvzNM8eyT/U7KW+qBxmHGmEO4xP2CkWLLhthpESgcoSaaklm1oSGaEbruIY9nLwLPn5nweH14KZeMA/IY36MHj/Fv8r3tO82rvnvauzdsbjOeRyYOZfbvrXhk4lA2txlf6n12ZbbvtaQWHMMoIUGV7RjgERFphOswqywinYAhuIhARhBCcTf3Glp6E/UHqWSKM8l9mOLJo1rsFnEJzXeBx2HGaPmWuF/Q/2EinoKUyoV/9+tV6JBoSkYEOh4Vuu7fQ61mkJ8HH1aGm7bCvNVwGch/4IKzx6CjhexXsqhbFzYOy6K3FL2nvTOy0u4cU2b7XtNtWSpNzFReJgmqbG/DhdNbCtQA5gFfAF8B6X6Nyg+eIthS5aDSm6g/UmusxsGFK5lYptnwFze0jhRqEVdie+wWcQnNd8kYh5mUfsHQh8lpnoLUXJce+l3yaqGPmWieuv36wX69fdd9bQN4sB6MyoX2jVg1qAFbjq0OlQo7O9WoAZV7lF3FUWaHnxTXglMeMVN5mSToONsdqtobOAQ3of/5wGGqeqH6ArYbZZCshrDs9YJJIAA2L4AJDckZnhV/aEb4i+tf97WIV1Q/K3KLOEnmu8B9gXG+6kvcL+j/MKl1COx/MTQ8nZ0m4DDFF0n5jBzphgwBLnDAXXdB38mwOt9NszhlGRMWdfOcnapRvWo2O/Iqs3pjA1feaIoDLVT2dE1/WGaHn5T3EJDxMFN5mSaheFvq4seOU9U3VHVBqoQykkyDU/hbD3bT5wG5eRk7W0pRzbHRXlz/+vIJsOhl2L0Vz027nyYXjS9asSfJfBe4L7A0v+o7jocOw2G3xoBGbTFFVT5Tp0LbtjB4MPzznzB/PvTqBSLs38A5O70z3U292PGwyYCvvJEUh6/sIbP7to0rmTioE1s3rErKJBflev7i8hoCMii7kqm8HJLIpBbnisgwEXlLRN7xL6kU0Cg5o1d8wOtfnkiVjO3kbM9ERPk7pxarNzqFkJ3txnwWqkDDX9xKWVC9CWR4X8m+F3n0aBgy5NDI/akBzXfxKvGYfYE5K2FMRvq+6hNtMWVnw7//DR06wF9/uRB4o0bBnnvu3OWkZh9y1cnPcu7RbyICB9b/DR0tLLzPK49fcSx62X34+MreW4S1Q7O4vZsbx3tH97tLPP2hzV9cxkmiqbxcf1SVUYJOavEIMApoAmwA1oUtRrKIZAYtocPDoEGwR/XCw0Lq1y6cV15eWAU6PuzFzd8GlatD3rYiL/KgQbBtW0ah/ApV7HGUUZBKPGZf4Ox7AHV90en4qg9rMY3+fXz0imriRGjRAh57zBVy3jw444wiWVbusZjf9Hxytrvy5Gyvzm/am8o9IpQnQosmN78SWVW3FppRSkcL8wcX/+PD5i8uByTBVG4fVSkiSBw+YDVwTrrjAaZjKfV4thHinkZM8wgSE1VE48a8jBgD88vuqlMHqK6f4X7/17Dw+pfdC+XfoM4KnXRbR61fe+XOuK5BiBYXNm4czteixE4dTdTrFYjsFaqfdFTNXhksRqpv/2gxRvff/S/9tcsVbuWgg1QnTdp5eNR7+H0/V47XMuOXp9C+omuer6fjru2um1+urjoa3fxydR3Zv7e2axawTBGI9hwFvc+pZleLZ5sqiv0+hoHFsy20BDUjVwJmpEjfGxC9j7SY0w/6Kc4Yx2XLKNrH1WNFxD6vUP5+k6U/PZ5JqtgTIRTxtK7kWrcnfBb7qz6epSDRfl/f/pFaf2fyDlP+asYBE//L05k3MvbWmdCpExDnHibSSvHvW6spe9ZYw+H7/FJoRqnsHbW47tbie9/G7Te3IScVAgsKkRrEfYDE2UnkPmCHqg5OuURljHbt2um0adOSmuekSZPo3Llz4cSclTB9ICx/y3kOZ1SHhqcBCis/Lkjbtzu0GQJZDWjSxFXO4TRu7BxxQoQqdL8S8M9fG4nwPGKROzqLyrK1aLpm8jo5Rc5dvXrh4SBByxGRqf1h4TCoVBXyt8NBV8KRz8Y5ZgAsfKHovmOzIL9oOaiUCb1yiqZH2T9neybVL8lhL9bwFP+iF68zixZcxn+ZRvtC5S9R2QPKs21HVV7/8XI6H7WS/XqPj/z8BSDSc1ToXka7rqVEcctVlklHmZL1TIrIj6raLllylXeCtmzrANeKyNci8pyIPOVfUijfrkMk54bM+m4JMP2gP3pM+BdotOEn0YJNiyQ2CUHlHouZueGMiP2LQfr5Ep0Iwd9S/uit1fyiAVt/8YZGdF0MNQ4q2D+jOquzTore7xuhr3TCT73Z/9rFnM9o5tGUHozndu6mHdOYRvsi5U9qKyKKN2q1fy7loqeGunG9JSBqv3mGDTlJBaNHQ69eR5e6k1KZnZiknBNU2TbFmZG3A4cBLXxL85RItguybMFqRnzXj9a3fMeI7/qxbMGqIqbEZQtW7VQ0lXx3z2/CjWTuizT8JFqFrprg2Mishuy5dwZZVd1HQVbVrex/sPsoCKJMEpkIITyCzukPjOeIvkMZ/UGA4RyxhkaMzXLzHG9eWLB/Xjb1cr6I7s0Z4QOp1b6VeGXzFYzmAhZyEG34iXu5nR0UDhwQKn+QIU2BPUNLYeKGiMOYbMhJ0glZEVavzozZRZQKyuzEJOWddHcal/WltBykIjnWVK/uc5bJXqGrRnXU/RusLLRP9itRnIRey4wrR7IcIVRV1/zv+IjOU8k8x6hR0Z10AucXzekoe4XqV+erjs4ouH5vH6x/jjsqdn4hJ7K101Vv7qhavbLuqFZd/1P9ca1EbmwHNI1/3+M+F9HkCbsPflLidJOIM1eKqEgOUsl8b9IF5iBVaElsZ9gTOAqolm7BS2spLWUb7eWqW9dtG3pJf80dWUmH9ulfaHuDOit09FXn7/Q63TGquurXvVWzV8aVI1SR+72IY1bkCZZJVbV//6IKsrjniHaNInnEjhql2q65K1e7ZisLzhdLGUVQGIEq8F9/Ve3UyQlywgmqixbtlKFu3aKyhpc/lkd5KirdlCilAEo+1VQkZVvWPb+DYMo2TH8G2glqAm8C+UAecICX/jwwON2FSOWSTGVbUKnmF6lUo71c0Vqu2a9k7tzn2UsSb1WEZAHVZy91inzENf0TUoJ+JVG/fk6RYyO1ykScAi4OsYYw+ZVP6LxD+xR8oARS8BEUxsSJE3cO7Rk3amUhpTj61R2qDz+smpmpWru26ksvqebnx7xO0YZnjRu1Qr+7p6M2qLOy0D6pqHRjKaUgspZVKpKyDb2b4cPprGVbfpdgO8GzwNdAa2CzT9n+A5iZ7kKkckmWso1pDsx2FW3ohQpvuY4acH6R8ZL+fd+9qbu+es0AbdV4hr56zQBdOip2qyIkS6ImaH9FXLeuapUqUcrjkWwTckZG5PygsALPGR65XDnD45vWw5k4caLq9/01f1QlfeHyAqtCC2bqtErt3ErXrqp//BE/M9943PCyvXB5YctF6FqmotKNppQSNlmXMSqSsi32B2MZwpRt8ZTtcqC993+TT9keCGxKdyFSuRRX2Ya3ECKZE3dWmhEqc//y7CX9NHdkJc1+JbOIKblq1fhKL5zGjV3l/c3gowtNfJD9SnQTdLTJGmIp0oRbZTGUUdBz9+8f/QOlQZ34pvVCRJo0Yzi6/awM3U5lXUU9/WelN1QIs1Rkr1D96Gi3rJ9RUKZIk5NEmZgj+5XMnR8XIsEr3SAt02hKKfDHUZT7lG7iKdvSarVH7cJIhGgTtgTwxSgrmLItnrLd4lOwfmXbGtiQ7kKkcimOsg2qHGKZiP0Kety13fWZPgO05X4z9Jk+A/R/13UvpMTDWz3xWj5QUHnPf/hgzR+FZr9STXNHFjVB+1tXQZcQCbdso8yUlej5o32gBGkN+ivkds1X6KT7u7mPkNFo9qBq+tfutVVBX+VC3YO1hc5ZtapXgX/fv+B+vtss+ixXr2WqZse2XCRifQjaMo2mlAJ/HMWY0SwdxOqe8e9TGq32pLVIPYe93NHV3P0eG9wXo6xgyrbwEnTozw/AWX4nZu/3SuCbgHlERUR6ishcEckXkXa+9CNFZIa3zBSR7r5t54nIbBGZJSIficieXnpHEZkuIrkick6Ec9USkT9E5JmSyh2N0NhS/9jXSBxw/WJe//58tmxzQya2bKvO1DW9yTr3N5YsgSefhCpV4Jwnx3P18KHMWtaKq4cP5bxnxzNypBt6sX590ZmbwA0VCA0T8Q8d2fpqFjpads6Ze9g+CxCBKhk7eOGLwuNU/cNsohFexgzfFMmBx+vFGf8adMypX5Z6tQrPBb1P3VVxxwmGz+Y0bU5D5i2oT9XcHHJfzSDz/m1kbM/jdD7gYkawnrqFjt/wgj+ou8ffc4ueyD80JqshmlGr0ExP/iARB1y/mNFfF35GJvzUm1u+/a3IcKBoY5qLBJmIQtxhSAmEcCutiewL3zOJOkQmkXmd/bLvuSc0O2AlX97eifbNV8Utx9m5WWx5UQrNSb3lReHs3ATHHHvDuCqxPX3xd21GsOQSRCMDx+BatC8COcDTuODxm4EjSqrxgcOBQ4FJQDtfenWgsve/IbAGqOwta4A9vW0P4zlq4YIltARGEGE+Z+BJYAzwTBDZitOyDbUQ/F+3kVoMlStHboH5+x9jmp9jmCBD+1Wp4lpcofWQiTV/VNHjnDezazHFGmbjXyKV0W+mC+oc9MGt3V3LOoJHddCWbazrHZLFfz3r1nVm55B8kfqEJ3c/VjfWqKkKOvOw5vpW/39EPX+DOit03HXdVcdkFL22obQx1Yq0CpeO6q7PX15guRh3bfe4rfTwc/vvcaylenXVQYPmRnxugww/06/Od62sGK2tlLQio5iug1pPgrbaI8meSCu14e5J6sJQVf2yuy6f0DV9Ht4ltGBgLdtCS9Dg8d94CrcqsAg4EVgBdFDV6cXQ8eH5z1fVXyKkZ6tqrreaSUGLWrxlNxERoJYnD6q6RFVn4TynCyEibYH6wCcllTkW2a8Ubj2GIq7kDM/yBoorjRu7SDvhLbD6tVcxbFhBXuvXRz7HsmVAVxcZJtvX6gnFqQ2xY4eLTx5i1YaG/J1Ti3wV8vNddeI/9pCBv+1sJYW2JVLG7FeyUC0YhA9FJ0EIbzmcf2lD9q39C5Ukn9y8DES38utvBV/xkVrIIsFkgYKZsi65BNb5YlStWwfPPVfQks3LK9hWmw28xGUcP+FrVm1uwPFMptXPs+n23LuF5PC3pldtaMiajfXR/LxCVTqSAZoHtZrBad8XmelqCuMZNKHAcnHOk4Un5oj0jITjv8exyM6Gl146IOr2LF8DrG7dsMkMAk6akZLoQFHmqw46A1fQeMh+2SM9V/FaqVVrufcr3FJRrXYxWqQdx7OgznWlH3/XgtCnhEBzI5cWIjIJGKiq03xpRwEvA42BC1V1gpd+jpe+BVgAdFHVPN9xw4H3VHWct14J1xq/EPex0E5Vr44iR1+gL0D9+vXbjh07NqFyfDdJaLzuBc464m12q5bNlm3VeWd6N5bu0ZejuyibN2+mRo0adOnSCffNEI4yceKXgJuubfXqzCJ71K+/lbFjv+PgDY/TcMu7bM+tRpWM7bzw+ZVcNTz2vLS5IzPIqFTkW4TcvAyqXJRL/fpbWbOmGqqRZHPyHdxoGYO73krXtgVlnPBDdwaOGbLTBOqXM8Rnn9VjyJBDd4bky34ly808FUZefiWmNPq80HEvvXQAa9ZUo169bVx++WLmzKnF22/vQ4M6qxhy/kC6tXsrgiz1GTRoPi+9dACrV2fSoM5Kxl7di3Offn2nnOFpXXmLZxlAPdYwhIHcxZ1sJXJFM7TPAK488YWd133ctT1o3XgGPyxuz7GHfE2trI38md+M3faqQ92t3zKt3itsz9gj6vUoDUSUL774slBaSI7ds9bsvBYbtu7FwIG/cNJJa3bu12z9HWyvtAcrdvsHe295j6r565m7R2EFeMIJnSI+O5HOG4/jV5xKBkW/JPKoypS9P477foSXz3+dq1XLK1I+v+wN6qyM+Fzd+NojvDahSNtg53na5Qzij/V7M+yLvvQ9YRj77LGCaVn3FTpPUEJ1RWlSNW8dB/79HHvmfEUG28ijGmuzjmdRrf6Fnt14dOnSxeZG9lNaTWjgM2BOhKWrb59J+MzIYccfDkzFtXCrAJ/jvKEFeAa4LWz/4fjMyMDVwE3e/z6k0IysqvrLqH6a55n+8kZW0l9GFZhiQg4q0YayZGQU5BPXJOcbG/rqNc4EGclhKtzcOWrA+bpjRMZOs/PPQw7WdweeXsgM7N8/PL/wMkYzb4ab6cLNfiXxHPYPi/njmYaaO1IiyqIa27QfSnv53Iv1dXqqgv5EKz2CaQr5Ea9hkPHPhUyaCTp/xbuHJVnq18+Jei3Dr09xhhgldSKOOKbrREzWQbo0wmUvjqNdMr2e0zacKQkzgmFm5EJL2gUoJEwMZettnwi0A9oDn/vSOwIfhO0brmxHA8uAJcBa4G/gwXgyFXucbYwZdUIvUP/+GrFSCp/0IejLG8kTMrzPNqQAY/UDhs4RTUHtrGx8ZXy5f98ifY1B+86K6znsly9/FDrrwWZF+j1D+UQbe6ujUR2Faj9Ud0O1Mrr97Mpame0Kql27/h6xTzf8I2HLy1m66tl62nzfmYGUcsibOFpfov+aRxreFWSpWzeyIorUZxvr4yFRkt5nG6fiD+KNXFzZw0cCvPXv7qU61jVtyjYJM4KZsi1HyhbYnwIHqca4ftk9gb2BlcBe3rZ7gEfD8iqkbMO2pbxlGwv/C9S/f0ElnpFR/NmVVDWqw9SOUZmFlHX//qr/u66gEnm578W65rm6Wr/2ShUpqKy2j4yeXziDBs0NVMFGavUUt0KLpkBDrUt/WcaNWqFjri7cgh73rx76/iWn6o7mlVRBcw+spG/3+kehiSP89yq8IvZ/JOSNQvNHEXE6zQk3Rm+ZhV+PaEov0j0MMsFIpA+1SBV4u+aFPx5yR1bSN6/toe2aFW+oSVLHtAas+JOlmMInb6lbN30zapXniTpM2ZZBZQt0x02csQ1YDXzspV8IzMVFHJoOdPMd0w+YD8wC3gXqeuntvby2AOuAuRHOV2aUbVIJ6C2qWnjO4p2tqEsKe0Jr9gpdPOr8gnGmr1TXxaMi5zdx4sRAFWykVk+VKsWr0MaNWqFjr4k8RjXStJC/jPK1oF8V/ea443VHtSqaXxXddn5lzX1ViszgFH6v/GX88NbumjsqguexT+FXr+7OG61lFn49GtRxZdoxKv49jCZXvGsYLRDGsCvc9dkxIkPzR6FzHm5WrmYsKs+KKRrluUymbAsvlSkDqHN6mhAhfSQwMsoxz+PmZg5P/wFoFOd8w3Et34pFAiHWnn0Wnjq2cND3ASc9BzwHY71g6VkNXai8hb7QeQdFH+vXu3f8MFyh7YMGOY/R/fZz3saBw3flrISvesFxr3N274b8Si2ytLDnZ2adBowcWjTPQ/ZdDVn9yNJToG9/OsycAu3qwW2n8Onmgaz7fhgN6qykceMCmSZNilXG8ZCzkt/GD6TBjrfIqppN9vbqvD+zO/8aPmRnPqHzclBfF+g+Z2XU61GtdkPaHFXL3ZcExlcGufbxjs9nGJWkwHGu2T5zaYYUPA+GYRSbqMpWRF4OmomqXpoccYwSE4p/G6FiD6dyj8UwfSAsfwvyst1kC/t2hzZDipVfUIqtGHJWwodt3bCZ2XfDkc8WKFBPvgH7rWTAK1GOP+YNGDIEBp/rxri88oqb8UGEMwB6DwWgx+MJyBT2QVK96lZ6nleLno/7laNvyEb7oUWyKHI9JkdXzqmkUo/l8Z8HwzCKRayW7V5h6x1xY1dne+vNccHnJ6dALqO4dIxdsRciSEs4kfxSydgsyPcNEVr4nFsq+VpdseSbMQMuuwymT4cePWDoUGiQpNl4kv1Bkq5rXgrB5w1jVyWqslXVM0P/ReRW3MxRl6jqFi9tN+C/FChfozySgpZr0glXtCGkkpvyMBZbt8I998BDD7kZNMaNg7PPTq58ZeWDJBmUh+fBMMohQfts/wWcGFK0AKq6RUTuwY13jTPrrFFmKQ+Koqtn7l72OqhvmqfGFxa0unx9uTvTvv7atWZ/+QX69IFHH4U9gg/K3yUpD8+DYZRDggYiqIEbbhNOQ9z8xYaROkLmTc1zUx8iburD3L8L9vFP57d5M/zrX3D88a5l+/HHrn/WFK1hGGkiaMv2f8ArInIjEJoD7WjgIQp5fxhGiti6Gg4eUNi82XF8URPz+OfgjOfcoK+rr4H774fw6e4itYINwzBSSFBl2x94FDdcpoqXlovrsx2YfLGMUqE8KZ1o5s2QifnnCfBqDkwBGteC10fCCWcVyQYo3Ao+MvY80oZhGMkgaNSfHFUdANQF2gBHAHuo6gBVzY59tFFmiRJJpVRIVqzMrIYwZS38Owe+BroCI8+NrGjLcjQTix1qGBWaoH22IbK85We/s5RRzigLSicZin7lSudZfOsn0GBPmDgWBg0AXRt5/66LofH5bvwoFA7inm7S+eFjGEbKCWRGFpGauHB2ZwMKHAwsFpHngVWqOjhlEhrJp2uAySxSRZDxsvFQheHD4YYbICcHHnwQ/v1vqFwZODf6cWVxHGkyrodhGGWeoC3bh3DeyEfgxtuGeA83r7FRnkin0ilp63LJEjj1VLj0UmjeHGbOhJtv9hRtAELjSE/9rkgQ97RQllvbhmEkjaAOUmcB3VV1hoioL30+cEDyxTJSTromLyiuos/Lc7M+/ec/IOL+9+sHlRLsCSlr40jLYmvbMIykE1TZ7o4bTBFOTSAvQrpR1kmn0klU0c+fD5dfDt98A6edBs8/D40bl46spYHN2mQYFZ6gyvYHXOv2CW891Lq9EvgmyTIZFZ2gin7HDnj4Ybj7bjdWdsQIuOAC17KtSJS11rZhGEknqLL9D/CxiDTzjrnB+38kLkCBYSSX6dNdv+zMmdCzJzz9NNSvn26pDMMwikXQcbbfAMcAVYFFwInACqCDqk5PnXjGLkdODtxyCxx5JKxeDePHwxtvmKI1DKNcEzh4vKrOBi5OoSzGrs6UKa5v9tdfXQCBRx6B3XdPt1SGYRglJlDLVkTyRKRehPS6ImIOUkbJ2LQJrroKOnaE7dvh00/hpZdM0RqGUWEIOm4imkdKNWB7kmQxdkU+/BCaNYPnnoPrroM5c+Ckk9ItlWEYRlKJaUYWkRu8vwr0E5HNvs0ZwPHAzymSzajIrFsH118PI0dC06Yu9myHDumWyjAMIyXE67O9xvsV4HIKj6ndDiwB+iVfLKPCogpvvglXXw1//QW33w6DBkG1aumWzDAMI2XEVLaquj+AiEwEeqjqX6UilVExWbHC9c2+9Ra0aweffQYtW6ZbKsMwjJQTtM/2NArPiQyAiGSKSNXkimRUOFThv/915uKPPnJext9+a4rWMIxdhqDK9g1gQIT0ft42w4jM4sXO4enyy6F1a5g1CwYODB44wDAMowIQVNkeC3wSIf1T3GQXhlGYvDx44glo0QJ++MHNZ/zFF3DwwemWzDAMo9QJ2ryoDuRGSM/HBSMwjALmznWTUnz/PZxxhlO0jRqlWyrDMIy0EbRlOws4L0L6+cCckgohIj1FZK6I5ItIO1/6kSIyw1tmikh337bzRGS2iMwSkY9EZE8vvaOITBeRXBE5J+w8+4nIJyIyX0TmiUiTkspu+Ni+He65B9q0gYULYfRoePddU7SGYezyBG3Z3gO8JSIHAV94aScCPUlO8Pg5QA/ghQjp7VQ1V0QaAjNF5F1v25NAU1VdKyIPA1cDg4FlQB9gYITzjADuU9VPRaQGrmVuJIGaP/8M//oXzJ4NvXrBk09CvSKTjhmGYeySBA1E8D5wJtAYeMpb9gPOUtX3SiqEqs5X1V8ipGerash8nUlBaD/xlt1ERIBauMAIqOoSVZ1FmCIVkaZAZVX91Ntvs6pml1T2XZ7sbLjpJo646io3UcXbb8NrrxVWtDkr4dNOkLMqfXIahmGkEVHV+HuVEiIyCRioqtN8aUcBL+MU/YWqOsFLP8dL3wIsALqoap7vuOHAe6o6zlvvhpuYYzuwP/AZcIv/GN+xfYG+APXr1287duzYpJZz8+bN1KhRI6l5poPaM2Zw6JAhVP/jD5aeeirLrr6avAjlOnjD4+yd/S4rqp/JgjrXp0HS4lNR7pWfilgmqJjlKs9l6tKly4+q2i7+nrsGpTb+QkQ+AxpE2DRIVd+Odpyqfg80E5HDgVdF5EPcTFb9gTbAYuBp4Fbg3hgiVMZNL9kGZ2p+HWdu/m+Ecw4DhgG0a9dOO3fuHKd0iTFp0iSSnWepsnEj3HwzvPACHHggfPEFv4kULdPYLMjfunN1n+x32Cf7HaiUCb2KDNsuk5T7exWBilgmqJjlqohl2lWJakYWkb99TkebvPWIS5ATqepJqto8whJV0YYdPx/Xim0OtPbSFqlrmr9B/CFIy4GfVHWxZ5p+CzgiyLkNH++/7wIHvPgi3HCDGzfbpUvkfbsuhsbnQ0Z1t55RHZr0hq6/lZ68hmEYZYBYLdtrgE3e/6tLQZYiiMj+wO+eg1Rj4FDcfMxVgaYispeq/gmcDMyPk90PwO6+Y04ApsU5xgjx558uKs+YMdC8uQvqfuSRsY/JaghVakHeVteazdsKlWtBViQDh2EYRsUlqrJV1Vcj/U8F3pCep4G9gPdFZIaqngocB9wiIjtwDk8DVHWtd8xdwGRv21KcSRgRaQ9MAHYHzhSRu1S1marmichA4HPPqepH4MVUlqtCoApjxzpP440bYfBguPVWqBpwls6tq+HgfnBQX1g4zDlLGYZh7GKUiTnzPKenCRHSRwIjoxzzPPB8hPQfgIgDOz1PZJuQNyjLl8OAAW6s7JFHuvmNmzdPLI+O4wv+tx+aXPkMwzDKCVGVrYjkUzDUJiaqmpE0iYz0k58PL70EN94IO3bAo4/CtddCht1mwzCM4hCrZftPCpRtfeBuXOvzWy+tA9ANuDNVwhlpYOFCuOIKmDTJOT69+KLzODYMwzCKTaw+23Gh/yLyDnCrqvr7OF8Wkak4hftsyiQ0SodQ4IDbb4cqVZySvewyEEm3ZIZhGOWeoHMjnwBMjJA+EeicNGmM9DBnDhxzjAt9d9JJMG+eC4lnitYwDCMpBFW2a4FzIqSfA/yZPHGMUmX7duddfMQR8NtvbprFt9+GffZJt2SGYRgViqDeyHcAr4hIFwr6bI8GTgIuS4VgRor5/ntnJp47F3r3dibkPfdMt1SGYRgVkqCBCEbgZmhaC5wFdAXWAcemegyukWS2bHEzP3Xo4MbNvvcejBplitYwDCOFBB5n681R3DuFship5osvnKfx4sXQvz88+CDUqpVuqQzDMCo8QftsEZH6IjJQRJ71zZl8rDelolGW2bDBKdkTT4RKldywnmefNUVrGIZRSgRStiLSFvgF17K9HBc/FtycxPelRjQjKbzzjgsc8PLLcNNNLnBAp07plsowDGOXImjLdgjwpKq2Abb50j8Gjk26VEbJWbMGevWCrl2hbl3nEPXQQ5CVlW7JDMMwdjmCKtu2QCRHqJW42aWMsoIqjB4NTZvChAlwzz0wbRq0sxjOhmEY6SKog1QOLopOOIcBa5InjlEifv8d+vWDDz6Ao4928xs3a5ZuqQzDMHZ5grZs3wbuFJFq3rqKSBPgIeB/qRDMSID8fHjuOadYJ01yY2a/+soUrWEYRhkhqLIdCOyBmy2qOvAVsBDYANyWEsmMYCxY4AIGDBgARx3lpl60CD2GYRhliqBm5FzcHMgdgSNwSnq6qn6WIrmMeOTmwuOPwx13QGam8zbu08fmMzYMwyiDxFW2IpIBbARaqeoXwBcpl8qIzcyZbqrFH3+Ebt3cmNmGDdMtlWEYhhGFuGZkVc0DlgJVUy+OEZNt21wIvHbtnDPUm2/C+PGmaA3DMMo4Qfts7wEeDM0cZaSBb7+FNm3g3nvh/PNdGLxzzjGzsWEYRjkgaJ/tQGB/4A8RWQ5s8W9U1ZbJFszw2LwZbrsNnnoKGjVyw3pOPz3dUhmGYRgJEFTZ/g/QVApiRODTT6FvX1iyxHkbP/gg1KyZbqkMwzCMBAmkbFV1cIrlMPz89RcMHOg8jA85BCZPhuOPT7dUhmEYRjGJ2WcrItVFZKiI/CEia0RkjPXbppgJE9xUi6++Crfe6jyPTdEahmGUa+I5SN0F9AHeB8biovw8l2KZdk1Wr4Z//hN69IAGDWDqVLj/fjeG1jAMwyjXxDMj9wAuU9WxACIyCvhaRDK8IUFGSVGFkSPhuusgOxvuuw9uvBGqVEm3ZIZhGEaSiNey3ReYElpR1am42aT2TqYQItJTROaKSL6ItPOlHykiM7xlpoh09207T0Rmi8gsEfnIF9C+o4hMF5FcETkn7DwPe+eZLyJPiaR53MzSpc6z+OKLnel4xgz4z39M0RqGYVQw4inbDGB7WFouwb2YgzIH14qeHCG9naq2Bk4DXhCRyiJSGXgS6OINO5oFXO0dswxn+h7jz0hEjsHF3m0JNAfaA+mJop6fD0OHQvPmLmDA0087J6jDDkuLOEYC5KyETztBzqp0S2IYRjkintIUYJSI+APGZwIvikh2KEFVzyqJEKo6HyC8oamq2b7VTAqGH4m37CYi64BauMAIqOoSL6/88NN4eVT1jq0CrC6J3MXil19oc+21LmDAqafCCy9A48alLoZRTGbfA39+BbPvhiOfTbc0hmGUE+Ip20gB40elQpBoiMhRwMtAY+BCVc310vsDs3ETbCwAroqVj6p+KyITcQHvBXgmpORLhR07YMgQuOsuqletCsOHw0UX2QxQ5YWxWZC/tWB94XNuqZQJvXLSJ5dhGOUCUS2duSpE5DOgQYRNg1T1bW+fScBAVZ0W4fjDccq/I5AHfAT0BRYDTwOrVPVe3/7DgfdUdZy3fhDO9Hyut8unwM2qGm66RkT6enlTv379tmPHji1GiQuosWABhz7yCDUXLGBNp07MvOwyquy7b4nyLGts3ryZGjVqpFuMpBMqV9W8dRz493PsmfMVGWwjj2qszTqeRbX6sz1jj3SLmRAV/V5VJMpzmbp06fKjqraLv+euQbL7XqOiqieV8Pj5IrIF198qXtoiABF5A7glThbdge9UdbN3zIfA0RTtJ0ZVhwHDANq1a6edO3cuntBbt8Ldd8PDD8Oee8L//ke9Hj2oMmkSxc6zjDKpApYJwso19TNYOBEqZZKRv536+xxM/SN7pFW+4rBL3KsKQkUs065K0EAEaUFE9vecoRCRxsChwBLgD6CpiOzl7XoyEM8kvAzo5DlYVcE5R6XOjPzbb9C6NTzwgDMXz5/vxtAa5Zetq+HgfnDqd+53qzlJGYYRjFJr2cbCG9LzNLAX8L6IzFDVU4HjgFtEZAeQDwxQ1bXeMXcBk71tS3EeyIhIe2ACsDtwpojcparNgHHACbh+XgU+UtV3U1aoffaBgw5yAQROOSVlpzFKkY7jC/63H5o+OQzDKHeUCWWrqhNwCjI8fSQwMsoxzwPPR0j/AWgUIT0PuLLEwgalalV4771SO51hGIZRdinTZmTDMAzDqAiYsjUMwzCMFGPK1jAMwzBSjClbwzAMw0gxpmwNwzAMI8WYsjUMwzCMFGPK1jAMwzBSjClbwzAMw0gxpRaIoLwiIn/iZqhKJnsCa5OcZ7qpiGWCilmuilgmqJjlKs9laqyqe8XfbdfAlG0aEJFpFS0aRkUsE1TMclXEMkHFLFdFLNOuipmRDcMwDCPFmLI1DMMwjBRjyjY9DEu3ACmgIpYJKma5KmKZoGKWqyKWaZfE+mwNwzAMI8VYy9YwDMMwUowpW8MwDMNIMaZsk4CI9BSRuSKSLyLtfOlHisgMb5kpIt19284TkdkiMktEPhKRPb30jiIyXURyReScsPM87J1nvog8JSJSAcq0n4h84pVpnog0SVWZSrNc3vZaIvKHiDxT3sskIq1F5FvvPLNE5NxUlqm0yuVtu1hEFnjLxeWoTNVE5HURWSgi3/vfndKsK4yAqKotJVyAw4FDgUlAO196daCy978hsAao7C1rgD29bQ8Dg73/TYCWwAjgHF9exwBfAxne8i3QuTyXyds2CTjZ+18DqF7e75UvzyeBMcAz5b1MwCHAwd7/vYGVQJ0KUK49gMXe7+7e/93LSZkGAM97/3sBr3v/S7WusCXYUhmjxKjqfIDwj0dVzfatZgIhbzTxlt1EZB1QC1joHbPEyys//DReHlW9Y6sAq5NYjHDZU14mEWmKq2A+9fbbnOxyhFNK9woRaQvUBz4CUjopQWmUSVV/9f1fISJrgL2ADckrSWFK6V6dCnyqquu97Z8CpwGvJbEoftmTViagKzDY+z8OeMZrwZZqXWEEw8zIKUZEjhKRucBsoJ+q5qrqDqC/l7YCaAr8N1Y+qvotMBHXolgJfBx6cUubZJUJ11raICLjReQnEXlERDJSKnwMklUuEakEPArcmGKR45LEe+XP80hcRb4oBSIHlSFZ5doH+N23vtxLK3WKUaadsqtqLrARqFuW6gqjAFO2ARGRz0RkToSla6zjVPV7VW0GtAduFZFMEamCe4Ha4Exys4Bb45z/IJwJqhHuJTtBRDqW5zLhTGTHAwO9vA4A+pSkTFAmyjUA+EBVf4+zX2DKQJlCcjQERgKXqGqRFn2ilIFyRerLLNF4yFIsU0TZU1FXGCXHzMgBUdWTSnj8fBHZAjTHe0lUdRGAiLwB3BIni+7AdyFTq4h8CBwNTC6BTOku03LgJ1Vd7B3zFq5MgVtZUeRKd7k6AMeLyABcP3RVEdmsqvGOiyVTusuEiNQC3gduU9XvSiKPT650l2s50Nm33gjXn1oSmUqrTMuBfYHlIlIZqA2sBy4lyXWFUXKsZZtCRGR/7yVARBrjHCOWAH8ATUUkFBHjZCCemWcZ0ElEKntfu50CHJN0klymH4DdfcecAMxLutABSGa5VLW3qu6nqk1wrfYRJVG0xSWZZRKRqsAEXFneTJnQAUjyM/gxcIqI7C4iuwOneGmlSjHL9A4Q8p4+B/hCVZUyUlcYYaTbQ6siLLhW53JgG84R4WMv/UJgLjADmA508x3TD/cCzALexfW1gDMhLQe2AOuAuV56BvCCd8w84LHyXiZv28ne/rOB4UDVilAu37F9SL03cmk8fxcAO7y8Qkvr8l4ub9ulOKejhTjzeHkpUybwpif3VOAAL71U6wpbgi02XaNhGIZhpBgzIxuGYRhGijFlaxiGYRgpxpStYRiGYaQYU7aGYRiGkWJM2RqGYRhGijFlaxiGYRgpxpStYRiGYaQYU7ZGhUNEhovIe+mWY1fAm3lptYgc6K1PktTH703L/Q0/r4iME5EbSlsOo3xiytYoNiLSRkTyROTrYhyb8ko5zvm/EJHREdLPFRfYu3bAfJqJyEgRWSEi20VkiYg8JCJZyZe6TPIfXNCFtEUAAhCR50Xk8VI+7V3AbUGfFWPXxpStURKuAJ4FmovI4ekWJkHaANMipLcDFqrqxngZiMgFuKn1NuGm4TsMF5GlD/BWsgRNJt4cx8nKqzpwOSUMHJEEOQQ4E3i7NM+rqrNxweYvKM3zGuUTU7ZGsfBabucDL+ICV18Wtl1E5N8iskBEtonIchF5wNs2HDc5+lUiot7SJFJrN4Lp7jQRmSIif4nIehH5OFFF75k86xBd2f4YII/jcHM5X6OqA9SFR1usqq/hQuyd4u0T7XgRkZtEZJGI5IjIbE95h7ZPEpFnReR+EVkrImtEZIi4WLmB8vDl85x37J/A1176biIyQkQ2e2bgW0XkPe96XyQi60SkWlheo0XkHV/S/wH5oTyjlPNEEdkgIlf6ZI74XHjbi3N/2+PmCf4qrMyPenn8KSLXikg1ERnqybNMRC4Mk7WaiDzhXY+tIvJdrHvo8Q5wXpx9DMOUrVFszgGWquosXHzTi8RFGAlxP3A78ADQDOhJQZDua4FvgVeAht4SNPbrbsATwJG40GgbgXcTbLG1xSmJn/yJXgupDQGULfAkMElVh0XYNtH7bRXj+HtxHyhX4QKCPwC8ICJn+PbpDeQCxwBXA9cB5yaYB7iWl+BiB1/kpT2K++Dpjou21MrbDm5y+0rAzvirnqm0O4VbsccDP2qUCdZF5GxcpKC+qvqClxzruYDi3d9uwPvqAqiH6I2zOBwFPOjl+RbwK+6D6lXgJRHZ23fMw7jreynuOZgNfCQuhm80pgJH7kLdBkZxSXckBFvK5wJ8CQz0/gsuHNjZ3noNYCvQL8bxkwiLhhMlbTjwXox8dgPygOMSOOYhXIDwaEsXb799PZnmATOBHl56K2+/7lHy38fbfkUMmXOA48PSn8D1f4auxbdh2z8FXgqahy+fWWH71AC2A73CZPoLGO6tPwN85NveH1gFVPalvQW8GukeAn1xivKUsPPGfC6Kc39x0XJ6hMnwrW9dgD+Bd3xpVbxrcI7vPNuBi3z7ZACLgHujPVdAS+9eH5jud9KWsr1Y8HgjYUTkIOBYPPOZqqo4Z6PLgf/hWlnVgM9TcO4DgXtwLZa9cC2wSsB+CWTTFi8Ielj6GV7e0731XOA6VZ0hIvWAH0XkI+AIb3u0FnBo+4wo25vizJ4fiYi/VVgF99ESYlbYcSuAegnmEUnOA739poYSVHWLiMzx7fMiMF1EGqnqclxr71Ut3HrMwoWJC6crcCXQUVW/9aXHfS4Svb/es3gARWPQ7rx23vO5BtdSDaXtEJG/KLieoWvytW+fPBH51pM7Gjner7VsjZiYsjWKw+W4r/5lzvIKuNYDIrJv6H8xyI9wbJWw9XdxAbWv9H5zcS3PRMzIbYAHVXWGP1FEzsfnHKWqK4GV3v81XuW8p+9cOUTmKk+mSH3CUNB9cyYu0LefHVH+g2tBhY4Nmge4GK5+Qtc4anxNVZ0pItOBPiLyFs70Gu4ItBbYPcLhs7y8LxOR71Q1dJ4gz0Wi97cb8Lmqhpcx0rWLdT1jXZNYcUj38H7/jLGPYVifrZEYIlIZuBjnddvat7TCVbKX4CrHbcCJMbLajlPYfv7E9d/62dnvKSJ1gcOB+1X1M1WdD9QkgY9GEdkfV0FGapUeESUdEWmHU/y/U9DX2ynCfpcBJ+Mcp6JV0qHr01hVF4YtSwMWpSR5LMQpniN9clcHmoft9yLOs/py4GtV/SVs+09EbvX9hutvPQUYJgVfZDGfi2Le364kx/N7Ie6Z3OkQJSIZQAdP7mg0B1aoaqQWvmHsxFq2RqKcgWvdvaiq6/wbRGQsrm/vXpwD0QMisg2YDNQF2qrqc97uS3COJU2AzcB64AvgCRE5C/gF17rZlwKz6F+41tQVIvI7rm/0EVzrJyhtvd/pEba1wTnuFMJTAiOAyzwFOlVE3gee9j4+puKuycW44VCXqeoX0QRQ1U0iMgQY4imiybj+zKOBfI3sdJW0PFR1s4i8DDwkImtxrffbcB/f/g+E14DHcPe0X4SsPvbyqBv+LKjqYhHpgus/HSYifT2ZYz0XCd1fEdnLK+85US9UQDwz+nPAg941+Q24HqiPG94WjeOBj0p6fqPiYy1bI1EuAyaGV64ebwKNgZNwLd+HcJ6n83F9uY18+w7BtSTm4Vq0+wEv+5avcUp4QugAVc3HeYu2BOYAQ738tyUgf1tgsapu8CeKSGMitHi94S8TgAdU9Rvfpp44j9aHcB8G7+AUR3tVHR5AjtuBwcBAnIPPp8DZuEo+KCXJYyAwxZN7Is4qMQ3nwAQ4hQ68gbtPb4RnoG6c6VSgV6QTqJvoojNwGs5LWojxXBTj/p4J/JDEVuXNuHK+gutvbwmc5nUnFEFEMnEe2i8m6fxGBUaiW7oMY9fGUw5jgF9UdXCaxUkp3kfFUuARVX3Ul/4hsFxVr4hy3Gk4K0ZTVc0rFWELzv02zrz9cGme13f+q4CuqnpKOs5vlC/MjGwY0TkW19KaJSLdvLQLvRZduUZE2uD6R6fi+kVv9n5f97bvgbNQnEKM8cKq+pGIDMW1ToP2NyeLr3Gm7nSxA7gmjec3yhHWsjWMXRBP2b4IHIrrE52BGzf9o7d9Cc6sfp+qPpQmMQ2jwmDK1jAMwzBSjDlIGYZhGEaKMWVrGIZhGCnGlK1hGIZhpBhTtoZhGIaRYkzZGoZhGEaKMWVrGIZhGCnGlK1hGIZhpJj/Bzk5WazKnlalAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction_xyz_rot = torch.tensor(prediction_xyz_rot)\n",
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz_rot),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz_rot*var_lab+mean_lab, '*', color='orange', label = 'Rotated test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy(kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, with (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1> Augmenting the dataset and training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N                        = 3           # number of atoms per molecule\n",
    "number_of_features_xyz   = 3           # number of features for each atom\n",
    "\n",
    "\n",
    "training_set_size    = data_size_H2O - test_set_size\n",
    "\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates_water[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(coordinates_water), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates_water)[0]):\n",
    "    for j in range(1,np.shape(coordinates_water)[1]):  # omit first column since for our dataset x=0 always\n",
    "        coordinates_water_norm[i,j] = (coordinates_water[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "data_set_xyz = np.vsplit(coordinates_water_norm,data_size_H2O)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=300, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Predicting Hydrogen ($H_2$) Energies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_H2 = np.genfromtxt('./h2/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_H2),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('h2', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_H2 = (xyz_file[:,1:-1])\n",
    "coordinates_H2 = coordinates_H2.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers_H2 = (xyz_file[:,-1])\n",
    "\n",
    "atomic_numbers_H2 = atomic_numbers_H2.astype(int)\n",
    "atomic_numbers_H2 = np.reshape(atomic_numbers_H2,(len(coordinates_H2),1))\n",
    "#atomic_numbers = torch.from_numpy(atomic_numbers)\n",
    "print(type(atomic_numbers_H2))\n",
    "#print(atomic_numbers_H2)\n",
    "print(np.shape(atomic_numbers_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_H2                    = 2           # number of atoms per molecule\n",
    "# number_of_features_H2   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "#                                    # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_H2 = create_dataset(N_H2,number_of_features_H2,coordinates_H2,energies_H2, \\\n",
    "#                       atomic_numbers_H2,1, 200)\n",
    "\n",
    "# training_set_H2 = data_H2[0]\n",
    "# print('Training set:')\n",
    "# print(type(training_set_H2))\n",
    "# print(np.shape(training_set_H2))\n",
    "# test_set_H2     = data_H2[1]\n",
    "# print('\\n')\n",
    "# print('Test set:')\n",
    "# print(type(test_set_H2))\n",
    "# print(np.shape(test_set_H2))\n",
    "# train_labels_H2 = data_H2[2]\n",
    "# print('\\n')\n",
    "# print('Training labels:')\n",
    "# print(type(train_labels_H2))\n",
    "# print(np.shape(train_labels_H2))\n",
    "# test_labels_H2  = data_H2[3]\n",
    "# print('\\n')\n",
    "# print('Test labels:')\n",
    "# print(type(test_labels_H2))\n",
    "# print(np.shape(test_labels_H2))\n",
    "# dataloader_H2   = data_H2[4]\n",
    "# print('\\n')\n",
    "# print('data_H2loader:')\n",
    "# print(type(dataloader_H2))\n",
    "# print(np.shape(dataloader_H2))\n",
    "\n",
    "# var_lab_H2 = data_H2[5]\n",
    "# print('\\n')\n",
    "# print('Variance of labels:')\n",
    "# print(type(var_lab_H2))\n",
    "# print(var_lab_H2)\n",
    "\n",
    "# mean_lab_H2 = data_H2[6]\n",
    "# print('\\n')\n",
    "# print('Mean value of labels:')\n",
    "# print(type(mean_lab_H2))\n",
    "# print(mean_lab_H2)\n",
    "\n",
    "# test_set_rot_H2 = data_H2[7]\n",
    "# print('\\n')\n",
    "# print('Rotated test set:')\n",
    "# print(type(test_set_rot_H2))\n",
    "# print(np.shape(test_set_rot_H2))\n",
    "\n",
    "# labels_norm_H2 = data_H2[8]\n",
    "# print('\\n')\n",
    "# print('Normalised labels:')\n",
    "# print(type(labels_norm_H2))\n",
    "# print(np.shape(labels_norm_H2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating features using only radial symmetry functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_H2                    = 2           # number of atoms per molecule\n",
    "number_of_features_H2   = 6           # number of features (symmetry functions) for each atom\n",
    "                                       \n",
    "    \n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features_H2)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features_H2)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features_H2)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features_H2)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "\n",
    "data_size_H2            = np.shape(energies_H2)[0]        # We have 500 H2 molecule conformations\n",
    "training_set_size_H2    = data_size_H2 - 50\n",
    "\n",
    "    \n",
    "G_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size_H2):\n",
    "    coord = coordinates_H2[N_H2*i:N_H2*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features_H2):\n",
    "        G_H2[N_H2*i:N_H2*(i+1),j]   = radial_BP_symm_func(Dp,N_H2,heta[j],Rs[j])     \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train_H2 = G_H2[:training_set_size_H2,:]\n",
    "var_train_H2  = np.var(G_train_H2,axis=0)\n",
    "mean_train_H2 = np.mean(G_train_H2,axis=0)\n",
    "\n",
    "G_norm_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G_H2)[0]):\n",
    "    for j in range(np.shape(G_H2)[1]):\n",
    "        G_norm_H2[i,j] = (G_H2[i,j]-mean_train_H2[j])/var_train_H2[j]   \n",
    "\n",
    "G_norm_H2 = np.append(G_norm_H2, atomic_numbers_H2, axis=1)\n",
    "        \n",
    "        \n",
    "data_set_H2 = np.vsplit(G_norm_H2,data_size_H2)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set_H2 = torch.FloatTensor(data_set_H2)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels_H2 = energies_H2          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train_H2 = labels_H2[:training_set_size_H2]\n",
    "var_lab_H2  = np.var(lab_train_H2,axis=0)\n",
    "mean_lab_H2 = np.mean(lab_train_H2,axis=0)\n",
    "print(mean_lab_H2)\n",
    "\n",
    "labels_norm_H2 = np.zeros((np.shape(labels_H2)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels_H2)[0]):\n",
    "    labels_norm_H2[i] = (labels_H2[i]-mean_lab_H2)/var_lab_H2  \n",
    "    \n",
    "    \n",
    "labels_norm_H2 = torch.FloatTensor(labels_norm_H2)      \n",
    "    \n",
    "    \n",
    "# # Splitting the dataset into training and test set\n",
    "# training_set_H2         = data_set_H2[:training_set_size_H2]\n",
    "# test_set_H2             = data_set_H2[training_set_size_H2:]\n",
    "\n",
    "# train_labels_H2         = labels_norm_H2[:training_set_size_H2]\n",
    "# test_labels_H2          = labels_norm_H2[training_set_size_H2:]\n",
    "\n",
    "# # Dataset\n",
    "# dataset_H2 = TensorDataset(training_set_H2, train_labels_H2)\n",
    "# #print(dataset[0])\n",
    "\n",
    "# # Creating the batches\n",
    "# dataloader_H2 = torch.utils.data.DataLoader(dataset_H2, batch_size=400,\n",
    "#                                            shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "# print(np.shape(training_set_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(G_norm[:100,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets_h2, self).__init__()\n",
    "        self.fc1 = nn.Linear(7,3)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(3,3)        \n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN_h2, self).__init__()\n",
    "        self.network1 = Subnets_h2()\n",
    "        self.network2 = Subnets_h2()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model_h2 = BPNN_h2()\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Copying weights!')\n",
    "#'Network1','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network1.fc1.weight = net.network1.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc1.bias = net.network1.fc1.bias\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network1.fc2.weight = net.network1.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc2.bias = net.network1.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network1.fc3.weight = net.network1.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc3.bias = net.network1.fc3.bias\n",
    "\n",
    "\n",
    "\n",
    "#'Network2','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network2.fc1.weight = net.network2.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc1.bias = net.network2.fc1.bias\n",
    "\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network2.fc2.weight = net.network2.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc2.bias = net.network2.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network2.fc3.weight = net.network2.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc3.bias = net.network2.fc3.bias\n",
    "\n",
    "\n",
    "print('Finished copying weights!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, x2 = data_set_H2[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "output = model_h2(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "print(output*var_lab_H2+mean_lab_H2)\n",
    "print(np.shape(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_H2 = torch.zeros(data_size_H2)\n",
    "for i in range(data_size_H2):\n",
    "    x1, x2 = data_set_H2[i]\n",
    "#     print('x1',x1)\n",
    "#     print('x2',x2)\n",
    "    with torch.no_grad():\n",
    "        prediction_H2[i] = model_h2(x1, x2)\n",
    "    print(prediction_H2[i]*var_lab_H2+mean_lab_H2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(prediction_H2*var_lab_H2+mean_lab_H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(labels_norm_H2))\n",
    "labels_norm_H2 = np.array(labels_norm_H2)\n",
    "print(np.mean(labels_norm_H2*var_lab_H2+mean_lab_H2,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_H2 = torch.tensor(prediction_H2)\n",
    "\n",
    "x = np.linspace(min(labels_H2), max(labels_H2))\n",
    "#print(min(torch.cat((test_labels_H2,prediction_H2),0)))\n",
    "y = x\n",
    "plt.plot(labels_H2,prediction_H2*var_lab_H2+mean_lab_H2, 'o', color='blue', label = 'test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2)))])\n",
    "#plt.ylim([-500,-700])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2_using_H2O_trained_net',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Drawing a neural network using matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    for sn in range(3):   \n",
    "        n_layers = len(layer_sizes)\n",
    "        v_spacing = (top - bottom)/float(max(layer_sizes))+0.015  #!!!!!!!!\n",
    "        h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "        # Nodes\n",
    "        for n, layer_size in enumerate(layer_sizes):\n",
    "            layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2. + 0.325*sn  #!!!!!!!!!!!!!\n",
    "            for m in range(layer_size):\n",
    "                x = n*h_spacing + left\n",
    "                y = layer_top - m*v_spacing\n",
    "                circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), 0.024, #v_spacing/4., #!!!!!!!\n",
    "                                color='w', ec='k', zorder=2.7)\n",
    "                ax.add_artist(circle)\n",
    "                if n == 0:\n",
    "                    lab   = \"$G_{0}^{1}$\".format(4-(sn+1),m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 1:\n",
    "                    lab   = '$y^1_{0}$'.format(m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 2:\n",
    "                    lab   = '$y^2_{0}$'.format(m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 3:\n",
    "                    lab   = '$E_{0}$'.format((4-(sn+1)))\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            \n",
    "            \n",
    "            # Edges\n",
    "            for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "                layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.+ 0.325*sn\n",
    "                layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.+ 0.325*sn\n",
    "                for m in range(layer_size_a):\n",
    "                    for o in range(layer_size_b):\n",
    "                        line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                          [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                        ax.add_artist(line)\n",
    "    \n",
    "    \n",
    "  \n",
    "        circle = plt.Circle((0.9,0.526),0.032,color='w', ec='k', zorder=2.7)\n",
    "        ax.add_artist(circle)\n",
    "        label = ax.annotate('$E_{tot}$', xy=(0.9,0.526-0.01), fontsize=20, ha=\"center\")\n",
    "        \n",
    "        line = plt.Line2D([0.68,0.89],[0.525,0.525], c='k')\n",
    "        ax.add_artist(line)\n",
    "\n",
    "        line = plt.Line2D([0.727,0.89],[0.21,0.545], c='k')\n",
    "        ax.add_artist(line)\n",
    "        \n",
    "        line = plt.Line2D([0.72,0.886],[0.85,0.51], c='k')\n",
    "        ax.add_artist(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .7, .1, .3, [6,3, 3, 1])\n",
    "fig.savefig('subnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            x = n*h_spacing + left\n",
    "            y = layer_top - m*v_spacing\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                            color='w', ec='k', zorder=2.7)\n",
    "            ax.add_artist(circle)\n",
    "            if n == 0:\n",
    "                lab   = \"$G_i^{0}$\".format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 1:\n",
    "                lab   = '$y^1_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 2:\n",
    "                lab   = '$y^2_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 3:\n",
    "                lab   = '$E_i$'\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            \n",
    "            \n",
    "        # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                    [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .9, .1, .9, [6,3, 3, 1])\n",
    "fig.savefig('subnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
