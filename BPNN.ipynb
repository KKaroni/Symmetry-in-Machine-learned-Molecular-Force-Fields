{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read\n",
    "\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "energies_water = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_water),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_water = (xyz_file[:,1:-1])\n",
    "coordinates_water = coordinates_water.astype(np.float)\n",
    "atomic_numbers_water = (xyz_file[:,-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers_water = atomic_numbers_water.astype(int)\n",
    "atomic_numbers_water = np.reshape(atomic_numbers_water,(len(coordinates_water),1))\n",
    "#atomic_numbers_water = torch.from_numpy(atomic_numbers_water)\n",
    "print(type(atomic_numbers_water))\n",
    "print(atomic_numbers_water)\n",
    "print(np.shape(atomic_numbers_water))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coordinates_water)\n",
    "print(np.shape(coordinates_water))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0                                     # i-th molecule\n",
    "N = 3                                     # N atoms per molecule\n",
    "coord = coordinates_water[N*i:N*(i+1),:]  # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                          # pairwise distances between all of its 3 atom\n",
    "    \n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "        for j in range(i+1,N):\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65865495 1.67618952 1.79537284]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = 3\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.19473151 5.24520024 5.28298916]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     \n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotation functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Rotation Matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rotation_matrix():\n",
    "    theta = np.arccos(2*np.random.uniform(low = 0,high = 1)-1)\n",
    "    phi = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    u = np.array([np.sin(theta)*np.cos(phi),np.sin(theta)*np.sin(phi),np.cos(theta)])\n",
    "    theta = np.random.uniform(low = 0,high = 2*np.pi)\n",
    "    A = np.zeros((3,3))\n",
    "    A[0][0] = np.cos(theta) + (u[0]**2)*(1-np.cos(theta))\n",
    "    A[0][1] = u[0]*u[1]*(1-np.cos(theta)) - u[2]*np.sin(theta)\n",
    "    A[0][2] = u[0]*u[2]*(1-np.cos(theta)) + u[1]*np.sin(theta)\n",
    "    A[1][0] = u[1]*u[0]*(1-np.cos(theta)) + u[2]*np.sin(theta)\n",
    "    A[1][1] = np.cos(theta) + (u[1]**2)*(1-np.cos(theta))\n",
    "    A[1][2] = u[1]*u[2]*(1-np.cos(theta)) - u[0]*np.sin(theta)\n",
    "    A[2][0] = u[2]*u[0]*(1-np.cos(theta)) - u[1]*np.sin(theta)\n",
    "    A[2][1] = u[2]*u[1]*(1-np.cos(theta)) + u[0]*np.sin(theta)\n",
    "    A[2][2] = np.cos(theta) + (u[2]**2)*(1-np.cos(theta))\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to randomly rotate molecules in the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_data(A,data):\n",
    "    data = np.array(data)\n",
    "    m = np.shape(data)[1]\n",
    "    for i in range(m):\n",
    "        data[:,i] = np.matmul(A,data[:,i])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_H2O                    = 3       # number of atoms per molecule\n",
    "number_of_features_H2O   = 15       # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "data_size_H2O            = np.shape(energies_water)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def create_dataset(N,number_of_features,coordinates,energies,atomic_numbers, use_atom_num_as_feat,batches):\n",
    "    \n",
    "\n",
    "    # Randomly picking the parameters from within a range\n",
    "    heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "    random.shuffle(heta)\n",
    "\n",
    "    Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "    random.shuffle(Rs)\n",
    "\n",
    "    lambdaa = np.ones(number_of_features)\n",
    "    random.shuffle(lambdaa)\n",
    "\n",
    "    zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "    random.shuffle(zeta)\n",
    "\n",
    "\n",
    "    \n",
    "# #     if number_of_features == 6:\n",
    "\n",
    "# or set hand-picked ones\n",
    "    heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808, 1, 2, 0.1 , 0.05,1.2, 0.1, 0.3, 0.5,0.01] \n",
    "    zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. , 9.,  1.9, 4.2, 5.8, 6.4, 0.001, 0.3, 2, 0.6 ]\n",
    "    Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6,   0.8, 0.4, 0.2, 1.,  0.,  0.6,  1, 0.1, 1.2]\n",
    "    lambdaa = [1., 1., 1., 1., 1., 1.,1., 1., 1., 1., 1., 1., 1,1,1]\n",
    "        \n",
    "\n",
    "\n",
    "    data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "    test_set_size        = 100\n",
    "    training_set_size    = data_size - test_set_size\n",
    "\n",
    "    \n",
    "    #Randomly rotate each molecule in the data\n",
    "    rotated_molec_coord = np.zeros((np.shape(coordinates)))\n",
    "    for i in range(data_size):\n",
    "        coord = coordinates_water[N*i:N*(i+1),:]\n",
    "        coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "        coord = np.transpose(coord)\n",
    "        A = random_rotation_matrix()\n",
    "        rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "    \n",
    "    coordinates = np.vstack((coordinates,rotated_molec_coord))  # randomly rotate trainig and test set and compute features for them as well for later use\n",
    "    \n",
    "    \n",
    "    G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "    for i in range(2*data_size):                # factor 2 because we copied the data and randomly translated it\n",
    "        coord = coordinates[N*i:N*(i+1),:]\n",
    "        Dp    = pairwise_distances(coord)\n",
    "        for j in range(0,number_of_features,2):\n",
    "            if j < number_of_features - 1:      # for j = number_of_features compute either 1 or 2 symmetry functions depending on whether number_of_features is odd or even\n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "            else:                  \n",
    "                G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])\n",
    "                if number_of_features % 2 == 0:    #i.e. if number of features is even\n",
    "                    G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "\n",
    "    \n",
    "    # Computing variance and mean on the training data only!\n",
    "    G_train = G[:training_set_size,:]\n",
    "    var  = np.std(G_train,axis=0)\n",
    "    mean = np.mean(G_train,axis=0)\n",
    "    print(mean)\n",
    "    G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "    print(np.shape(G))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(G)[0]):\n",
    "        for j in range(np.shape(G)[1]):\n",
    "            G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "        \n",
    "    print(type(G_norm))  \n",
    "    \n",
    "    if use_atom_num_as_feat == 1:\n",
    "        G_norm = np.append(G_norm, atomic_numbers, axis=1)    # Adding atomic number as a feature\n",
    "       \n",
    "        \n",
    "    data_set = np.vsplit(G_norm,data_size*2)       # Group every three atoms that constitute a molecule together\n",
    "    #data_set = np.random.permutation(training_set)\n",
    "    data_set = torch.FloatTensor(data_set)                    # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "   \n",
    "    # print(data_set[0])\n",
    "    # print(data_set[0][1][1])\n",
    "\n",
    "    labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "    \n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "#     shuffler = np.random.permutation(len(labels))\n",
    "\n",
    "#     data_set = data_set[shuffler]\n",
    "\n",
    "#     labels = labels[shuffler]\n",
    "\n",
    "# #     print(np.shape(labels))\n",
    "# #     print(np.shape(data_set))\n",
    "    \n",
    "    \n",
    "\n",
    "    # Computing variance and mean on the training data only!\n",
    "    lab_train = labels[:training_set_size]\n",
    "    var_lab  = np.std(lab_train,axis=0)\n",
    "    mean_lab = np.mean(lab_train,axis=0)\n",
    "\n",
    "    labels_norm = np.zeros((np.shape(labels)))\n",
    "    # normalize all data (training and test), using training set mean and variance\n",
    "    for i in range(np.shape(labels)[0]):\n",
    "        labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "    labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "    # Splitting the dataset into training and test set\n",
    "    training_set         = data_set[:training_set_size]\n",
    "    test_set             = data_set[training_set_size:data_size]\n",
    "    training_set_rot     = data_set[data_size:data_size+training_set_size]\n",
    "    test_set_rot         = data_set[data_size+training_set_size:]\n",
    "\n",
    "    train_labels         = labels_norm[:training_set_size]\n",
    "    train_labels         = torch.FloatTensor(train_labels)\n",
    "    test_labels          = labels_norm[training_set_size:]\n",
    "    test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    # Dataset\n",
    "    dataset = TensorDataset(training_set, train_labels)\n",
    "    #print(dataset[0])\n",
    "\n",
    "    # Creating the batches\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batches, #25, #300,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "    print(np.shape(training_set))\n",
    "    \n",
    "    return [training_set, test_set, train_labels, test_labels, dataloader,var_lab,mean_lab, test_set_rot,labels_norm,training_set_rot]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.94255889 4.64487745 0.20368879 0.00772632 0.14228216 0.03148129\n",
      " 1.64327903 0.42400896 1.76725392 3.90789223 0.46488069 0.28870981\n",
      " 1.89254946 2.6976953  1.94493484]\n",
      "(6000, 15)\n",
      "<class 'numpy.ndarray'>\n",
      "torch.Size([900, 3, 15])\n"
     ]
    }
   ],
   "source": [
    "data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, 300)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([900, 3, 15])\n",
      "\n",
      "\n",
      "Test set:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([100, 3, 15])\n",
      "\n",
      "\n",
      "Training labels:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([900])\n",
      "\n",
      "\n",
      "Test labels:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([100])\n",
      "\n",
      "\n",
      "data_waterloader:\n",
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "()\n",
      "\n",
      "\n",
      "Variance of labels:\n",
      "<class 'numpy.float64'>\n",
      "2.8763078664728305\n",
      "\n",
      "\n",
      "Mean value of labels:\n",
      "<class 'numpy.float64'>\n",
      "-13813.419735561924\n",
      "\n",
      "\n",
      "Rotated test set:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([100, 3, 15])\n",
      "\n",
      "\n",
      "Normalised labels:\n",
      "<class 'torch.Tensor'>\n",
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "training_set = data_water[0]\n",
    "print('Training set:')\n",
    "print(type(training_set))\n",
    "print(np.shape(training_set))\n",
    "\n",
    "test_set     = data_water[1]\n",
    "print('\\n')\n",
    "print('Test set:')\n",
    "print(type(test_set))\n",
    "print(np.shape(test_set))\n",
    "\n",
    "train_labels = data_water[2]\n",
    "print('\\n')\n",
    "print('Training labels:')\n",
    "print(type(train_labels))\n",
    "print(np.shape(train_labels))\n",
    "\n",
    "test_labels  = data_water[3]\n",
    "print('\\n')\n",
    "print('Test labels:')\n",
    "print(type(test_labels))\n",
    "print(np.shape(test_labels))\n",
    "\n",
    "dataloader   = data_water[4]\n",
    "print('\\n')\n",
    "print('data_waterloader:')\n",
    "print(type(dataloader))\n",
    "print(np.shape(dataloader))\n",
    "\n",
    "var_lab = data_water[5]\n",
    "print('\\n')\n",
    "print('Variance of labels:')\n",
    "print(type(var_lab))\n",
    "print(var_lab)\n",
    "\n",
    "mean_lab = data_water[6]\n",
    "print('\\n')\n",
    "print('Mean value of labels:')\n",
    "print(type(mean_lab))\n",
    "print(mean_lab)\n",
    "\n",
    "test_set_rot = data_water[7]\n",
    "print('\\n')\n",
    "print('Rotated test set:')\n",
    "print(type(test_set_rot))\n",
    "print(np.shape(test_set_rot))\n",
    "\n",
    "labels_norm = data_water[8]\n",
    "print('\\n')\n",
    "print('Normalised labels:')\n",
    "print(type(labels_norm))\n",
    "print(np.shape(labels_norm))\n",
    "\n",
    "training_set_rot = data_water[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.4747,  0.8463, -0.7928,  ..., -0.3087, -0.1886, -0.4595],\n",
      "         [-0.2675,  0.4246, -0.0065,  ..., -0.2980,  0.1494, -0.2683],\n",
      "         [ 1.3817, -1.3350,  1.4544,  ...,  1.2665,  1.2134,  1.3724]],\n",
      "\n",
      "        [[-0.8732,  0.3422, -1.0580,  ..., -0.7609, -1.0526, -0.8638],\n",
      "         [-0.8704,  0.3373, -1.0506,  ..., -0.7599, -1.0484, -0.8611],\n",
      "         [ 1.0067, -1.3833,  0.1988,  ...,  1.2049,  0.2398,  1.0231]],\n",
      "\n",
      "        [[-1.2830,  1.3826, -1.0976,  ..., -1.3326, -1.3189, -1.2902],\n",
      "         [-1.0204,  0.9563, -0.2246,  ..., -1.2881, -0.8945, -1.0456],\n",
      "         [ 1.2488, -1.3560,  1.0196,  ...,  1.2418,  0.3543,  1.2483]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6893,  0.2861, -1.1201,  ..., -0.4776, -0.8107, -0.6706],\n",
      "         [-0.5946,  0.1195, -0.8696,  ..., -0.4451, -0.6732, -0.5811],\n",
      "         [ 1.0350, -1.3750,  0.2844,  ...,  1.2117,  0.4655,  1.0496]],\n",
      "\n",
      "        [[-0.7678,  0.5483, -0.9416,  ..., -0.6590, -0.7943, -0.7583],\n",
      "         [-0.7050,  0.4352, -0.7549,  ..., -0.6429, -0.6986, -0.6994],\n",
      "         [ 1.1507, -1.3660,  0.6025,  ...,  1.2483,  0.5499,  1.1587]],\n",
      "\n",
      "        [[-1.5612,  1.1316, -1.1230,  ..., -1.7282, -1.7443, -1.5814],\n",
      "         [-1.4676,  0.9934, -0.8681,  ..., -1.6980, -1.6012, -1.4931],\n",
      "         [ 1.0641, -1.3813,  0.3625,  ...,  1.2214, -0.1539,  1.0771]]])\n"
     ]
    }
   ],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "a = int(math.sqrt(4))\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets_H2O, self).__init__()\n",
    "        num_hid_feat = math.ceil(number_of_features/2)#int(math.sqrt(number_of_features))##\n",
    "        self.fc1 = nn.Linear(number_of_features, num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat, num_hid_feat)\n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_H2O(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN_H2O, self).__init__()\n",
    "        self.network1 = Subnets_H2O(number_of_features)\n",
    "        self.network2 = Subnets_H2O(number_of_features)\n",
    "        self.network3 = Subnets_H2O(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "# model = BPNN_H2O(3) #+1)                # +1 because he have added the atomic number as a feature\n",
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O)# +1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network1.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc3.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network2.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc3.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "# print(model.network3.fc3.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc3.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# print('x1',x1)\n",
    "# print('x2',x2)\n",
    "# print('x3',x3)\n",
    "\n",
    "\n",
    "# output = model(x1, x2, x3,number_of_features_H2O+1)\n",
    "# print('output')\n",
    "# print(output*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(learning_rate, nepochs,net,dataloader,test_set,test_labels):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), learning_rate) \n",
    "    #torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "    #optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "\n",
    "    train_loss = np.zeros(nepochs)\n",
    "    test_loss = np.zeros(nepochs)\n",
    "\n",
    "    train_acc = np.zeros(nepochs)\n",
    "    test_acc = np.zeros(nepochs)\n",
    "\n",
    "    #===========================================================================\n",
    "    for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "    #===========================================================================\n",
    "    \n",
    "        running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "        epoch_train_loss = 0.0             \n",
    "        epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "        counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                                  # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                                  # be 36 elements in each batch.\n",
    "        #---------------------------------------\n",
    "        for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "        #---------------------------------------\n",
    "            inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                                  # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                                  # after each .step() call. \n",
    "\n",
    "            # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "            outputs = torch.zeros(np.shape(inputs)[0])\n",
    "            for j in range(np.shape(inputs)[0]):\n",
    "                outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "                # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "                # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "                # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "            loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "            loss.backward()                   # performs one back-propagation step \n",
    "            optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                               # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "            epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                               # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                               # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "            net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "            with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                              # the requires_grad flags to false, i.e. makes all the \n",
    "                                                              # operations in the block have no gradients\n",
    "                for k in range(np.shape(test_set)[0]):\n",
    "                       net_test_set[k] = net(test_set[k][0],test_set[k][1],test_set[k][2])\n",
    "                epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch           \n",
    "            if i % 10 == 2:    # print average loss every 10 mini-batches\n",
    "                print('[%d, %5d] loss: %.5f' %\n",
    "                      (epoch + 1, i + 1, running_loss/10))\n",
    "                running_loss = 0.0\n",
    "            counter += 1\n",
    "            #------------------------------------       \n",
    "        # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "        train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                       # to obtain average training loss for each epoch\n",
    "        test_loss[epoch] = epoch_test_loss/counter\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_test_loss = 0.0\n",
    "\n",
    "    #=================================================================================\n",
    "    \n",
    "    print('Finished Training')\n",
    "    \n",
    "    return train_loss, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_for_different_num_feat(num_feat_H2O,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat):\n",
    "    N_H2O           = 3            # number of atoms per molecule\n",
    "    learning_rate   = 0.0001 #0.005\n",
    "    batch_size      = 300    # 25\n",
    "\n",
    "    data_size_H2O   = np.shape(energies_water)[0]\n",
    "    test_set_size   = 100\n",
    "    \n",
    "    data_water = create_dataset(N_H2O, number_of_features_H2O ,coordinates_water,energies_water, \\\n",
    "                      atomic_numbers_water,use_atom_num_as_feat, batch_size)\n",
    "\n",
    "    \n",
    "    training_set = data_water[0];     test_set     = data_water[1];\n",
    "    train_labels = data_water[2];     test_labels  = data_water[3];\n",
    "    dataloader   = data_water[4];     var_lab = data_water[5]; \n",
    "    mean_lab = data_water[6];         test_set_rot = data_water[7];\n",
    "    labels_norm = data_water[8];      training_set_rot = data_water[9]\n",
    "        \n",
    "    # Create and initialise the model\n",
    "    net = BPNN_H2O(number_of_features_H2O)   #+1) # +1 if you are adding atomic number as a feature\n",
    "\n",
    "    # Train for nepochs using learning_rate = 0.0001\n",
    "\n",
    "    print(type(dataloader))\n",
    "    losses = training(learning_rate , nepochs, net, dataloader,test_set,test_labels)\n",
    "    train_loss_G_feat = losses[0]\n",
    "    test_loss_G_feat  = losses[1]\n",
    "    \n",
    "    return train_loss_G_feat, test_loss_G_feat, test_set, net, train_labels,mean_lab, test_labels, var_lab, test_set_rot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 4 Behler and Parinello features, batch size = 300 \n",
    "number_of_features_H2O   = 4       # number of features (radial and angular symmetry functions) to be used\n",
    "use_atom_num_as_feat     = int(0)       # Use atomic number of each element as an extra feature for training?\n",
    "nepochs                  = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses_G4 =  run_for_different_num_feat(4,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_4_feat = losses_G4[0]\n",
    "# test_loss_G_4_feat  = losses_G4[1]\n",
    "# test_set            = losses_G4[2]\n",
    "# net                 = losses_G4[3]\n",
    "# train_labels        = losses_G4[4]\n",
    "# mean_lab            = losses_G4[5]\n",
    "# test_labels         = losses_G4[6]\n",
    "# var_lab             = losses_G4[7]\n",
    "# test_set_rot        = losses_G4[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss functions\n",
    "def plot_train_test_loss(train_loss, test_loss, nepochs, num_feat_G):\n",
    "    x = np.arange(1,nepochs+1)\n",
    "\n",
    "    plt.plot(x,train_loss,'blue',label = 'Training loss')\n",
    "    plt.plot(x,test_loss,'red',label = 'Test loss')\n",
    "\n",
    "    #plt.ylim([0,0.075])\n",
    "\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    plt.xlabel('Epoch',fontsize=14)\n",
    "    plt.ylabel('Loss function',fontsize=14)\n",
    "    plt.title('Average loss function per epoch for $H_2O$ training and test set',fontsize=14)\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.savefig('params_loss_graph_H2O_G_{0}'.format(num_feat_G),bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot_train_test_loss(train_loss_G_4_feat,test_loss_G_4_feat,nepochs,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.shape(train_labels))\n",
    "# train_labels = np.array(train_labels)\n",
    "# print(np.mean(train_labels,axis=0))\n",
    "\n",
    "# print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_prediction(prediction,test_labels,var_lab,mean_lab, number_of_features_H2O):\n",
    "    prediction = torch.tensor(prediction)\n",
    "\n",
    "    x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "    print(min(torch.cat((test_labels,prediction),0)))\n",
    "    y = x\n",
    "    plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "    plt.plot(x,y, color='red',label = 'y=x')\n",
    "    plt.grid()\n",
    "    #plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "    #plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "    #plt.ylim([-13822,-13800])\n",
    "    #plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "    plt.ticklabel_format(useOffset=False, style='plain')\n",
    "    #plt.tick_params(axis='both',labelsize=14)\n",
    "    plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "    plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "    plt.title('Actual vs Predicted energy value for $H_2O$ molecules',fontsize=15)\n",
    "    plt.legend()\n",
    "    plt.savefig('predicted_energies_H2O_G_{0}'.format(number_of_features_H2O),bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(net.network1.fc1.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 6 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 6       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G6 =  run_for_different_num_feat(6,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_6_feat = vars_G6[0]\n",
    "# test_loss_G_6_feat  = vars_G6[1]\n",
    "# test_set            = vars_G6[2]\n",
    "# net                 = vars_G6[3]\n",
    "# train_labels        = vars_G6[4]\n",
    "# mean_lab            = vars_G6[5]\n",
    "# test_labels         = vars_G6[6]\n",
    "# var_lab             = vars_G6[7]\n",
    "# test_set_rot        = vars_G6[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_train_test_loss(train_loss_G_6_feat,test_loss_G_6_feat,nepochs,number_of_features_H2O)\n",
    "\n",
    "# test_set_size = 100\n",
    "\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for 9 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 9       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G9 =  run_for_different_num_feat(9,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_9_feat = vars_G9[0]\n",
    "# test_loss_G_9_feat  = vars_G9[1]\n",
    "# test_set            = vars_G9[2]\n",
    "# net                 = vars_G9[3]\n",
    "# train_labels        = vars_G9[4]\n",
    "# mean_lab            = vars_G9[5]\n",
    "# test_labels         = vars_G9[6]\n",
    "# var_lab             = vars_G9[7]\n",
    "# test_set_rot        = vars_G9[8]\n",
    "# plot_train_test_loss(train_loss_G_9_feat,test_loss_G_9_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Training for 12 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 12       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G12 =  run_for_different_num_feat(12,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_12_feat = vars_G12[0]\n",
    "# test_loss_G_12_feat  = vars_G12[1]\n",
    "# test_set            = vars_G12[2]\n",
    "# net                 = vars_G12[3]\n",
    "# train_labels        = vars_G12[4]\n",
    "# mean_lab            = vars_G12[5]\n",
    "# test_labels         = vars_G12[6]\n",
    "# var_lab             = vars_G12[7]\n",
    "# test_set_rot        = vars_G12[8]\n",
    "# plot_train_test_loss(train_loss_G_12_feat,test_loss_G_12_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Training for 16 Behler and Parinello features, batch size = 600 \n",
    "# number_of_features_H2O   = 16       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "# vars_G16 =  run_for_different_num_feat(16,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "# train_loss_G_16_feat = vars_G16[0]\n",
    "# test_loss_G_16_feat  = vars_G16[1]\n",
    "# test_set            = vars_G16[2]\n",
    "# net                 = vars_G16[3]\n",
    "# train_labels        = vars_G16[4]\n",
    "# mean_lab            = vars_G16[5]\n",
    "# test_labels         = vars_G16[6]\n",
    "# var_lab             = vars_G16[7]\n",
    "#test_set_rot         = vars_G16[8]\n",
    "\n",
    "# plot_train_test_loss(train_loss_G_16_feat,test_loss_G_16_feat,nepochs,number_of_features_H2O)\n",
    "# test_set_size = 100\n",
    "# prediction = np.zeros(test_set_size)\n",
    "# for i in range(test_set_size):\n",
    "#     x1,x2,x3 = test_set[i]\n",
    "#     prediction[i] = net(x1, x2, x3)#[0]    \n",
    "# print(prediction)\n",
    "# plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Comparing convergence plots for different numbers of features\n",
    "# x = np.arange(1,nepochs+1)\n",
    "\n",
    "# plt.plot(x[:2000],train_loss_G_4_feat[:2000],'blue',label = '4 features')\n",
    "# #plt.plot(x[:2000],train_loss_G_6_feat[:2000],'red',label = '6 features')\n",
    "# plt.plot(x[:2000],train_loss_G_9_feat[:2000],'green',label = '9 features')\n",
    "# #plt.plot(x[:2000],train_loss_G_12_feat[:2000],'orange',label = '12 features')\n",
    "# plt.plot(x[:2000],train_loss_G_16_feat[:2000],'cyan',label = '16 features')\n",
    "\n",
    "\n",
    "# #plt.ylim([0,0.075])\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# plt.xlabel('Epoch',fontsize=14)\n",
    "# plt.ylabel('Training loss function',fontsize=14)\n",
    "# plt.title('Convergence plot for different numbers of features',fontsize=14)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.savefig('loss_H2O_compar_diff_num_feat_BP_symm_random_feat',bbox_inches='tight')\n",
    "# plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training for 6 Behler and Parinello features, batch size = 600 \n",
    "number_of_features_H2O   = 6       # number of features (radial and angular symmetry functions) to be used\n",
    "\n",
    "vars_G6 =  run_for_different_num_feat(6,nepochs,coordinates_water,energies_water,atomic_numbers_water,use_atom_num_as_feat)\n",
    "\n",
    "train_loss_G_6_feat = vars_G6[0]\n",
    "test_loss_G_6_feat  = vars_G6[1]\n",
    "test_set            = vars_G6[2]\n",
    "net                 = vars_G6[3]\n",
    "train_labels        = vars_G6[4]\n",
    "mean_lab            = vars_G6[5]\n",
    "test_labels         = vars_G6[6]\n",
    "var_lab             = vars_G6[7]\n",
    "test_set_rot        = vars_G6[8]\n",
    "\n",
    "plot_train_test_loss(train_loss_G_6_feat,test_loss_G_6_feat,nepochs,number_of_features_H2O)\n",
    "test_set_size = 100\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,number_of_features_H2O)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Rotating test set molecules and checking performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(test_set_rot))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_rotated = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_rot[i]\n",
    "    prediction_rotated[i] = net(x1, x2, x3)#[0]\n",
    "\n",
    "\n",
    "prediction_rotated = torch.tensor(prediction_rotated)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_rotated),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set',markersize=8)\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_rotated*var_lab+mean_lab, '*', color='yellow', label = 'Rotated test set')\n",
    "\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for rotated $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('rotated_predicted_energies_H2O_G_feat',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N                        = 3         # number of atoms per molecule\n",
    "number_of_features_xyz   = 3         # number of features for each atom\n",
    "data_size_H2O            = np.shape(energies_water)[0]                                  \n",
    "training_set_size    = data_size_H2O - test_set_size\n",
    "\n",
    "\n",
    "for i in range(np.shape(energies_water)[0]):  # Moving all oxygens to the origin!!!!!\n",
    "    coord = coordinates_water[N*i:N*(i+1),:]\n",
    "    coord = coord - coord[2,:]                \n",
    "    coordinates_water[N*i:N*(i+1),:] = coord\n",
    "\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates_water[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(coordinates_water), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates_water)[0]):\n",
    "    for j in range(1,np.shape(coordinates_water)[1]):  # omit first column since for our dataset x=0 always\n",
    "        coordinates_water_norm[i,j] = (coordinates_water[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "data_set_xyz = np.vsplit(coordinates_water_norm,data_size_H2O)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "#################################################################################\n",
    "#Randomly rotating each molecule in the dataset around its oxygen \n",
    "rotated_training_set_xyz= np.zeros((np.shape(training_set_xyz)))\n",
    "for i in range(training_set_size):\n",
    "    coord = training_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_training_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_training_set_xyz))\n",
    "rotated_training_set_xyz = torch.FloatTensor(rotated_training_set_xyz)\n",
    "training_set_xyz = rotated_training_set_xyz\n",
    "##################################################################################\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=300, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training new neural net on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xyz = BPNN_H2O(3)    # 3 features (x,y and z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_xyz.parameters(), lr=0.005) #0.0001)#0.005)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20000\n",
    "\n",
    "train_loss_xyz = np.zeros(nepochs)\n",
    "test_loss_xyz = np.zeros(nepochs)\n",
    "\n",
    "train_acc_xyz = np.zeros(nepochs)\n",
    "test_acc_xyz = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss_xyz = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss_xyz = 0.0             \n",
    "    epoch_test_loss_xyz = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net_xyz(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "                       \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_xyz += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss_xyz += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set_xyz = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                    net_test_set_xyz[k] = net_xyz(test_set_xyz[k][0],test_set_xyz[k][1],test_set_xyz[k][2])\n",
    "            epoch_test_loss_xyz += criterion(net_test_set_xyz, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.10f' %\n",
    "                  (epoch + 1, i + 1, running_loss_xyz/10))\n",
    "            running_loss_xyz = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss_xyz[epoch] = epoch_train_loss_xyz/counter   # divide by number or training examples in one batch \n",
    "                                                           # to obtain average training loss for each epoch\n",
    "#     if (abs((test_loss_xyz[epoch] - test_loss_xyz[epoch - 1])/test_loss_xyz[epoch - 1])< 0.1):\n",
    "#         break\n",
    "    test_loss_xyz[epoch] = epoch_test_loss_xyz/counter\n",
    "    epoch_train_loss_xyz = 0.0\n",
    "    epoch_test_loss_xyz = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss_xyz[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss_xyz[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "#plt.ylim([-13822,-13800])\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch (training with (x,y,z))',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.savefig('x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_training_x_y_z_loss_graph_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(1,nepochs+1)\n",
    "# # plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# # plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "# plt.plot(x[:1500],train_loss_xyz[:1500],'green',label = 'xyz features')\n",
    "# plt.plot(x[:1500],train_loss_G_3_feat[:1500],'y',label = '3 Symmetry functions')\n",
    "\n",
    "# #plt.ylim([-13822,-13800])\n",
    "\n",
    "# plt.ticklabel_format(useOffset=False, style='plain')\n",
    "# plt.xlabel('Epoch',fontsize=14)\n",
    "# plt.ylabel('Loss function',fontsize=14)\n",
    "# plt.title('Convergence: Training with (x,y,z) vs with 3 symmetry functions)',fontsize=14)\n",
    "# plt.grid()\n",
    "# plt.legend()\n",
    "# plt.savefig('x_y_z_loss_graph_H2O_compared_with_3G',bbox_inches='tight')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xyz = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_xyz[i]\n",
    "    prediction_xyz[i] = net_xyz(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, using (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_train_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(prediction_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotated_test_set_xyz= np.zeros((np.shape(test_set_xyz)))\n",
    "for i in range(test_set_size):\n",
    "    coord = test_set_xyz[N*i:N*(i+1),:]\n",
    "    coord = np.transpose(coord)\n",
    "    A = random_rotation_matrix()\n",
    "    rotated_test_set_xyz[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "print(np.shape(rotated_test_set_xyz))\n",
    "rotated_test_set_xyz = torch.FloatTensor(rotated_test_set_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_xyz_rot = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = rotated_test_set_xyz[i]\n",
    "    prediction_xyz_rot[i] = net_xyz(x1, x2, x3)#[0]\n",
    "print(prediction_xyz*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_xyz_rot = torch.tensor(prediction_xyz_rot)\n",
    "prediction_xyz = torch.tensor(prediction_xyz)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction_xyz_rot),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction_xyz_rot*var_lab+mean_lab, '*', color='orange', label = 'Rotated test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy(kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy(kcal/mol)',fontsize=14)\n",
    "plt.title('Energy prediction for $H_2O$ molecules, with (x,y,z) as features',fontsize=15)\n",
    "plt.legend()\n",
    "#plt.savefig('rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.savefig('rot_rot_xyz_predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1> Augmenting the dataset and training on xyz coordinates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(labels_norm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_size_H2O =  1000\n",
      "torch.Size([22000])\n",
      "(66000, 3)\n",
      "22000\n"
     ]
    }
   ],
   "source": [
    "N                        = 3         # number of atoms per molecule\n",
    "number_of_features_xyz   = 3         # number of features for each atom\n",
    "data_size_H2O            = np.shape(energies_water)[0]  \n",
    "print('data_size_H2O = ', data_size_H2O)\n",
    "number_of_rotations = 21\n",
    "\n",
    "coordinates_water = (xyz_file[:,1:-1])\n",
    "shape_coord_h2o = np.shape(coordinates_water)\n",
    "coordinates_water = coordinates_water.astype(np.float)\n",
    "new_labels        = labels_norm#torch.zeros((data_size_H2O*(number_of_rotations+1)))###### labels_norm\n",
    "\n",
    "\n",
    "    \n",
    "##########################################################################################################################################\n",
    "# for n in range(number_of_rotations):    \n",
    "#     #Randomly rotate each molecule in the data\n",
    "# #    print(np.shape(coordinates_water))\n",
    "#     rotated_molec_coord = np.zeros(shape_coord_h2o)\n",
    "#     for i in range(data_size_H2O):\n",
    "#         coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#         coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "#         coord = np.transpose(coord)\n",
    "#         A = random_rotation_matrix()\n",
    "#         rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "#     coordinates_water = np.vstack((coordinates_water,rotated_molec_coord))  # randomly rotate trainig and test set and compute features for them as well for later use\n",
    "#     new_labels         = torch.cat((new_labels,labels_norm))   \n",
    "##########################################################################################################################################\n",
    "\n",
    "final_coords = np.zeros(((number_of_rotations+1)*len(coordinates_water),3))\n",
    "\n",
    "for j in range(number_of_rotations):\n",
    "    #Randomly rotate each molecule in the data\n",
    "    rotated_molec_coord = np.zeros((np.shape(coordinates_water)))\n",
    "    for i in range(data_size_H2O):\n",
    "        coord = coordinates_water[N*i:N*(i+1),:]\n",
    "        coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "        coord = np.transpose(coord)\n",
    "        A = random_rotation_matrix()\n",
    "        rotated_molec_coord[N*i:N*(i+1),:] = np.transpose(rotate_data(A,coord))\n",
    "    final_coords[len(coordinates_water)*j:len(coordinates_water)*(j+1),:] = rotated_molec_coord\n",
    "    new_labels         = torch.cat((new_labels,labels_norm))\n",
    "\n",
    "print(np.shape(new_labels))\n",
    "print(np.shape(final_coords))\n",
    "\n",
    "##########################################################################################################################################\n",
    "# for i in range(np.shape(energies_water)[0]):  # Moving all oxygens to the origin!!!!!\n",
    "#     coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#     coord = coord - coord[2,:]                \n",
    "#     coordinates_water[N*i:N*(i+1),:] = coord\n",
    "# final_coords = np.zeros(((number_of_rotations+1)*len(coordinates_water),3))\n",
    "#     #Randomly rotate each molecule in the data\n",
    "# #    print(np.shape(coordinates_water))\n",
    "# for i in range(data_size_H2O):\n",
    "#     coord = coordinates_water[N*i:N*(i+1),:]\n",
    "#     coord = coord - coord[2,:]                   # move oxygen to origin for each molecule before rotating\n",
    "#     final_coords[i*(number_of_rotations+1)*N:i*(number_of_rotations+1)*N+N,:] = coord\n",
    "\n",
    "#     coord = np.transpose(coord)\n",
    "#     for n in range(number_of_rotations):    \n",
    "#         A = random_rotation_matrix()\n",
    "#         final_coords[i*(number_of_rotations+1)*N+(n+1)*N:i*(number_of_rotations+1)*N+(n+1)*N+N,:] = np.transpose(rotate_data(A,coord))\n",
    "#         print(i*(number_of_rotations+1)*N+(n+1)*N+N)\n",
    "#     new_labels[i*(number_of_rotations+1):(i+1)*(number_of_rotations+1)] = labels_norm[i]\n",
    "\n",
    "# # print(np.shape(final_coords))        \n",
    "# # print(np.shape(new_labels))        \n",
    "# #for n in range(number_of_rotations):    \n",
    "# #    new_labels         = torch.cat((new_labels,labels_norm))  \n",
    "##########################################################################################################################################\n",
    "\n",
    "\n",
    "\n",
    "#new_labels = torch.reshape(new_labels, (5000, 1))    \n",
    "\n",
    "\n",
    "new_data_size            = len(new_labels)\n",
    "test_set_size            = 2000\n",
    "training_set_size        = new_data_size - test_set_size\n",
    "\n",
    "# Computing standard deviation and mean on the training data only!\n",
    "coord_train = final_coords[:training_set_size,:]          ###########\n",
    "std_train_xyz  = np.std(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "\n",
    "coordinates_water_norm = np.zeros((len(final_coords), number_of_features_xyz)) ###\n",
    "# normalize all data (training and test), using training set mean and standard deviation\n",
    "for i in range(np.shape(final_coords)[0]):###\n",
    "    for j in range(1,np.shape(final_coords)[1]):  # omit first column since for our dataset x=0 always######\n",
    "        coordinates_water_norm[i,j] = (final_coords[i,j]-mean_train_xyz[j])/std_train_xyz[j]####\n",
    "\n",
    "\n",
    "print(new_data_size)\n",
    "data_set_xyz = np.vsplit(final_coords,new_data_size)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "# print(np.shape(data_set_xyz))\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "# #print(np.shape(labels))\n",
    "# print(np.shape(data_set_xyz))\n",
    "# shuffler = np.random.permutation(len(new_labels))\n",
    "\n",
    "# data_set_xyz = data_set_xyz[shuffler]\n",
    "\n",
    "# new_labels = new_labels[shuffler]\n",
    "\n",
    "# #print(np.shape(labels))\n",
    "# #print(np.shape(data_set))\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = new_labels[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = new_labels[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=1000, #300,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(training_set_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.6371, -0.5784, -0.0926, -0.5689, -1.1359, -0.9380, -0.2692,  0.3256,\n",
      "        -0.3583, -0.7963, -0.4665, -0.1007,  0.2436,  1.6948,  0.5976,  0.4114,\n",
      "        -0.8533, -0.8973, -0.1873,  1.5255, -1.1385,  0.1861,  0.0733,  0.1359,\n",
      "         0.2208, -0.5701,  1.2469, -0.3982, -0.1017, -0.8535, -0.4921, -0.3863,\n",
      "        -0.5486, -0.8424, -0.4399,  1.6680,  1.1847, -0.5315, -0.1015,  0.4483,\n",
      "         0.0035, -0.7716, -0.6725,  0.5294, -0.1380,  1.4497, -0.5107,  0.7859,\n",
      "         0.2336, -0.1682, -0.0499,  0.5011,  0.0021, -0.7762,  1.4533, -1.0033,\n",
      "         0.7549, -1.0531, -0.4585, -0.3691, -1.2878, -0.7238])\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0:62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_xyz = BPNN_H2O(3)   #+1) # +1 if you are adding atomic number as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.0807500958\n",
      "[1,    11] loss: 1.0697803199\n",
      "[2,     1] loss: 0.0836114466\n",
      "[2,    11] loss: 0.8114214242\n",
      "[3,     1] loss: 0.0786518514\n",
      "[3,    11] loss: 0.7844876587\n",
      "[4,     1] loss: 0.0779971123\n",
      "[4,    11] loss: 0.7780253708\n",
      "[5,     1] loss: 0.0781477392\n",
      "[5,    11] loss: 0.7762884498\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/queues.py\", line 245, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 411, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/home/s2090086/anaconda3/lib/python3.8/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-c9106aec4c16>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a single value, same as loss.item(), which is the mean loss for each mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# performs one back-propagation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# update the network parameters (perform an update step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net_xyz.parameters(), lr=0.05) #0.0001)#0.005)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 100\n",
    "\n",
    "train_loss_xyz = np.zeros(nepochs)\n",
    "test_loss_xyz = np.zeros(nepochs)\n",
    "\n",
    "train_acc_xyz = np.zeros(nepochs)\n",
    "test_acc_xyz = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss_xyz = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss_xyz = 0.0             \n",
    "    epoch_test_loss_xyz = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net_xyz(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "                       \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss_xyz += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss_xyz += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "                                           # so we ve divided here by the number of train. ex. in one mini-batch (mean)\n",
    "                                           # thus all we need to do at the end of the epoch is divide by the number of mini-batches\n",
    "        \n",
    "        net_test_set_xyz = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                    net_test_set_xyz[k] = net_xyz(test_set_xyz[k][0],test_set_xyz[k][1],test_set_xyz[k][2])\n",
    "            epoch_test_loss_xyz += criterion(net_test_set_xyz, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.10f' %\n",
    "                  (epoch + 1, i + 1, running_loss_xyz/10))\n",
    "            running_loss_xyz = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss_xyz[epoch] = epoch_train_loss_xyz/counter   # divide by number or training examples in one batch \n",
    "                                                           # to obtain average training loss for each epoch\n",
    "#     if (abs((test_loss_xyz[epoch] - test_loss_xyz[epoch - 1])/test_loss_xyz[epoch - 1])< 0.1):\n",
    "#         break\n",
    "    test_loss_xyz[epoch] = epoch_test_loss_xyz/counter\n",
    "    epoch_train_loss_xyz = 0.0\n",
    "    epoch_test_loss_xyz = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[200:600],train_loss_xyz[200:600],'blue',label = 'Training loss')\n",
    "# plt.plot(x[200:600],test_loss_xyz[200:600],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:counter],train_loss_xyz[:counter],'green',label = 'train')\n",
    "plt.plot(x[:counter],test_loss_xyz[:counter],'red',label = 'test')\n",
    "\n",
    "\n",
    "#plt.ylim([-13822,-13800])\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Augmented dataset convergence',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.savefig('augmented_convergence',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.01297557 -0.70780998  0.20899248 ...  3.62129688  3.62129688\n",
      "  3.62129688]\n",
      "tensor(-1.7168, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot_train_test_loss(train_loss_xyz_aug,test_loss_xyz_aug,nepochs,3)\n",
    "\n",
    "test_set_size = 2000\n",
    "\n",
    "prediction = np.zeros(test_set_size)\n",
    "for i in range(test_set_size):\n",
    "    x1,x2,x3 = test_set_xyz[i]\n",
    "    prediction[i] = net_xyz(x1, x2, x3)#[0]    \n",
    "print(prediction)\n",
    "\n",
    "plot_prediction(prediction,test_labels,var_lab,mean_lab,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Predicting Hydrogen ($H_2$) Energies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "energies_H2 = np.genfromtxt('./h2/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies_H2),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('h2', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates_H2 = (xyz_file[:,1:-1])\n",
    "coordinates_H2 = coordinates_H2.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_numbers_H2 = (xyz_file[:,-1])\n",
    "\n",
    "atomic_numbers_H2 = atomic_numbers_H2.astype(int)\n",
    "atomic_numbers_H2 = np.reshape(atomic_numbers_H2,(len(coordinates_H2),1))\n",
    "#atomic_numbers = torch.from_numpy(atomic_numbers)\n",
    "print(type(atomic_numbers_H2))\n",
    "#print(atomic_numbers_H2)\n",
    "print(np.shape(atomic_numbers_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N_H2                    = 2           # number of atoms per molecule\n",
    "# number_of_features_H2   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "#                                    # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data_H2 = create_dataset(N_H2,number_of_features_H2,coordinates_H2,energies_H2, \\\n",
    "#                       atomic_numbers_H2,1, 200)\n",
    "\n",
    "# training_set_H2 = data_H2[0]\n",
    "# print('Training set:')\n",
    "# print(type(training_set_H2))\n",
    "# print(np.shape(training_set_H2))\n",
    "# test_set_H2     = data_H2[1]\n",
    "# print('\\n')\n",
    "# print('Test set:')\n",
    "# print(type(test_set_H2))\n",
    "# print(np.shape(test_set_H2))\n",
    "# train_labels_H2 = data_H2[2]\n",
    "# print('\\n')\n",
    "# print('Training labels:')\n",
    "# print(type(train_labels_H2))\n",
    "# print(np.shape(train_labels_H2))\n",
    "# test_labels_H2  = data_H2[3]\n",
    "# print('\\n')\n",
    "# print('Test labels:')\n",
    "# print(type(test_labels_H2))\n",
    "# print(np.shape(test_labels_H2))\n",
    "# dataloader_H2   = data_H2[4]\n",
    "# print('\\n')\n",
    "# print('data_H2loader:')\n",
    "# print(type(dataloader_H2))\n",
    "# print(np.shape(dataloader_H2))\n",
    "\n",
    "# var_lab_H2 = data_H2[5]\n",
    "# print('\\n')\n",
    "# print('Variance of labels:')\n",
    "# print(type(var_lab_H2))\n",
    "# print(var_lab_H2)\n",
    "\n",
    "# mean_lab_H2 = data_H2[6]\n",
    "# print('\\n')\n",
    "# print('Mean value of labels:')\n",
    "# print(type(mean_lab_H2))\n",
    "# print(mean_lab_H2)\n",
    "\n",
    "# test_set_rot_H2 = data_H2[7]\n",
    "# print('\\n')\n",
    "# print('Rotated test set:')\n",
    "# print(type(test_set_rot_H2))\n",
    "# print(np.shape(test_set_rot_H2))\n",
    "\n",
    "# labels_norm_H2 = data_H2[8]\n",
    "# print('\\n')\n",
    "# print('Normalised labels:')\n",
    "# print(type(labels_norm_H2))\n",
    "# print(np.shape(labels_norm_H2))\n",
    "\n",
    "# training_set_rot = data_H2[9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating features using only radial symmetry functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_H2                    = 2           # number of atoms per molecule\n",
    "number_of_features_H2   = 6           # number of features (symmetry functions) for each atom\n",
    "                                       \n",
    "    \n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features_H2)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features_H2)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features_H2)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features_H2)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "\n",
    "data_size_H2            = np.shape(energies_H2)[0]        # We have 500 H2 molecule conformations\n",
    "training_set_size_H2    = data_size_H2 - 50\n",
    "\n",
    "    \n",
    "G_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size_H2):\n",
    "    coord = coordinates_H2[N_H2*i:N_H2*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features_H2):\n",
    "        G_H2[N_H2*i:N_H2*(i+1),j]   = radial_BP_symm_func(Dp,N_H2,heta[j],Rs[j])     \n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train_H2 = G_H2[:training_set_size_H2,:]\n",
    "var_train_H2  = np.var(G_train_H2,axis=0)\n",
    "mean_train_H2 = np.mean(G_train_H2,axis=0)\n",
    "\n",
    "G_norm_H2 = np.zeros((len(coordinates_H2), number_of_features_H2))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G_H2)[0]):\n",
    "    for j in range(np.shape(G_H2)[1]):\n",
    "        G_norm_H2[i,j] = (G_H2[i,j]-mean_train_H2[j])/var_train_H2[j]   \n",
    "\n",
    "G_norm_H2 = np.append(G_norm_H2, atomic_numbers_H2, axis=1)\n",
    "        \n",
    "        \n",
    "data_set_H2 = np.vsplit(G_norm_H2,data_size_H2)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set_H2 = torch.FloatTensor(data_set_H2)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels_H2 = energies_H2          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train_H2 = labels_H2[:training_set_size_H2]\n",
    "var_lab_H2  = np.var(lab_train_H2,axis=0)\n",
    "mean_lab_H2 = np.mean(lab_train_H2,axis=0)\n",
    "print(mean_lab_H2)\n",
    "\n",
    "labels_norm_H2 = np.zeros((np.shape(labels_H2)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels_H2)[0]):\n",
    "    labels_norm_H2[i] = (labels_H2[i]-mean_lab_H2)/var_lab_H2  \n",
    "    \n",
    "    \n",
    "labels_norm_H2 = torch.FloatTensor(labels_norm_H2)      \n",
    "    \n",
    "    \n",
    "# # Splitting the dataset into training and test set\n",
    "# training_set_H2         = data_set_H2[:training_set_size_H2]\n",
    "# test_set_H2             = data_set_H2[training_set_size_H2:]\n",
    "\n",
    "# train_labels_H2         = labels_norm_H2[:training_set_size_H2]\n",
    "# test_labels_H2          = labels_norm_H2[training_set_size_H2:]\n",
    "\n",
    "# # Dataset\n",
    "# dataset_H2 = TensorDataset(training_set_H2, train_labels_H2)\n",
    "# #print(dataset[0])\n",
    "\n",
    "# # Creating the batches\n",
    "# dataloader_H2 = torch.utils.data.DataLoader(dataset_H2, batch_size=400,\n",
    "#                                            shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "# print(np.shape(training_set_H2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(G_norm[:100,:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class Subnets_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets_h2, self).__init__()\n",
    "        self.fc1 = nn.Linear(7,3)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(3,3)        \n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_h2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN_h2, self).__init__()\n",
    "        self.network1 = Subnets_h2()\n",
    "        self.network2 = Subnets_h2()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model_h2 = BPNN_h2()\n",
    "    \n",
    "    \n",
    "    \n",
    "print('Copying weights!')\n",
    "#'Network1','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network1.fc1.weight = net.network1.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc1.bias = net.network1.fc1.bias\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network1.fc2.weight = net.network1.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc2.bias = net.network1.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network1.fc3.weight = net.network1.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network1.fc3.bias = net.network1.fc3.bias\n",
    "\n",
    "\n",
    "\n",
    "#'Network2','layer 1'\n",
    "# 'weights'\n",
    "model_h2.network2.fc1.weight = net.network2.fc1.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc1.bias = net.network2.fc1.bias\n",
    "\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "model_h2.network2.fc2.weight = net.network2.fc2.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc2.bias = net.network2.fc2.bias\n",
    "\n",
    "# print('layer 3')\n",
    "# print('weights')\n",
    "model_h2.network2.fc3.weight = net.network2.fc3.weight \n",
    "# print('biases')\n",
    "model_h2.network2.fc3.bias = net.network2.fc3.bias\n",
    "\n",
    "\n",
    "print('Finished copying weights!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, x2 = data_set_H2[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "output = model_h2(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "print(output*var_lab_H2+mean_lab_H2)\n",
    "print(np.shape(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_H2 = torch.zeros(data_size_H2)\n",
    "for i in range(data_size_H2):\n",
    "    x1, x2 = data_set_H2[i]\n",
    "#     print('x1',x1)\n",
    "#     print('x2',x2)\n",
    "    with torch.no_grad():\n",
    "        prediction_H2[i] = model_h2(x1, x2)\n",
    "    print(prediction_H2[i]*var_lab_H2+mean_lab_H2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(prediction_H2*var_lab_H2+mean_lab_H2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(np.shape(labels_norm_H2))\n",
    "labels_norm_H2 = np.array(labels_norm_H2)\n",
    "print(np.mean(labels_norm_H2*var_lab_H2+mean_lab_H2,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction_H2 = torch.tensor(prediction_H2)\n",
    "\n",
    "x = np.linspace(min(labels_H2), max(labels_H2))\n",
    "#print(min(torch.cat((test_labels_H2,prediction_H2),0)))\n",
    "y = x\n",
    "plt.plot(labels_H2,prediction_H2*var_lab_H2+mean_lab_H2, 'o', color='blue', label = 'test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction_H2),0)), max(torch.cat((test_labels,prediction_H2)))])\n",
    "#plt.ylim([-500,-700])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2_using_H2O_trained_net',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h1>Drawing a neural network using matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    for sn in range(3):   \n",
    "        n_layers = len(layer_sizes)\n",
    "        v_spacing = (top - bottom)/float(max(layer_sizes))+0.015  #!!!!!!!!\n",
    "        h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "        # Nodes\n",
    "        for n, layer_size in enumerate(layer_sizes):\n",
    "            layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2. + 0.325*sn  #!!!!!!!!!!!!!\n",
    "            for m in range(layer_size):\n",
    "                x = n*h_spacing + left\n",
    "                y = layer_top - m*v_spacing\n",
    "                circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), 0.024, #v_spacing/4., #!!!!!!!\n",
    "                                color='w', ec='k', zorder=2.7)\n",
    "                ax.add_artist(circle)\n",
    "                if n == 0:\n",
    "                    lab   = \"$G_{0}^{1}$\".format(4-(sn+1),m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 1:\n",
    "                    lab   = '$y^1_{0}$'.format(m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 2:\n",
    "                    lab   = '$y^2_{0}$'.format(m+1)\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "                if n == 3:\n",
    "                    lab   = '$E_{0}$'.format((4-(sn+1)))\n",
    "                    label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            \n",
    "            \n",
    "            # Edges\n",
    "            for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "                layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.+ 0.325*sn\n",
    "                layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.+ 0.325*sn\n",
    "                for m in range(layer_size_a):\n",
    "                    for o in range(layer_size_b):\n",
    "                        line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                          [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                        ax.add_artist(line)\n",
    "    \n",
    "    \n",
    "  \n",
    "        circle = plt.Circle((0.9,0.526),0.032,color='w', ec='k', zorder=2.7)\n",
    "        ax.add_artist(circle)\n",
    "        label = ax.annotate('$E_{tot}$', xy=(0.9,0.526-0.01), fontsize=20, ha=\"center\")\n",
    "        \n",
    "        line = plt.Line2D([0.68,0.89],[0.525,0.525], c='k')\n",
    "        ax.add_artist(line)\n",
    "\n",
    "        line = plt.Line2D([0.727,0.89],[0.21,0.545], c='k')\n",
    "        ax.add_artist(line)\n",
    "        \n",
    "        line = plt.Line2D([0.72,0.886],[0.85,0.51], c='k')\n",
    "        ax.add_artist(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .7, .1, .3, [6,3, 3, 1])\n",
    "fig.savefig('subnet.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "    '''\n",
    "    Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "    :usage:\n",
    "        >>> fig = plt.figure(figsize=(12, 12))\n",
    "        >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "    :parameters:\n",
    "        - ax : matplotlib.axes.AxesSubplot\n",
    "            The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "        - left : float\n",
    "            The center of the leftmost node(s) will be placed here\n",
    "        - right : float\n",
    "            The center of the rightmost node(s) will be placed here\n",
    "        - bottom : float\n",
    "            The center of the bottommost node(s) will be placed here\n",
    "        - top : float\n",
    "            The center of the topmost node(s) will be placed here\n",
    "        - layer_sizes : list of int\n",
    "            List of layer sizes, including input and output dimensionality\n",
    "    '''\n",
    "    n_layers = len(layer_sizes)\n",
    "    v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "    h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "    # Nodes\n",
    "    for n, layer_size in enumerate(layer_sizes):\n",
    "        layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size):\n",
    "            x = n*h_spacing + left\n",
    "            y = layer_top - m*v_spacing\n",
    "            circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "                            color='w', ec='k', zorder=2.7)\n",
    "            ax.add_artist(circle)\n",
    "            if n == 0:\n",
    "                lab   = \"$G_{0}$\".format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 1:\n",
    "                lab   = '$y^1_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 2:\n",
    "                lab   = '$y^2_{0}$'.format(m+1)\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            if n == 3:\n",
    "                lab   = '$E_{tot}$'\n",
    "                label = ax.annotate(lab, xy=(x, y-0.01), fontsize=20, ha=\"center\")\n",
    "            \n",
    "            \n",
    "        # Edges\n",
    "    for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "        layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "        layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "        for m in range(layer_size_a):\n",
    "            for o in range(layer_size_b):\n",
    "                line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "                                    [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "                ax.add_artist(line)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 12))\n",
    "ax = fig.gca()\n",
    "ax.axis('off')\n",
    "draw_neural_net(ax, .1, .9, .1, .9, [3,4, 4, 1])\n",
    "fig.savefig('fc_nn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# def draw_neural_net(ax, left, right, bottom, top, layer_sizes):\n",
    "#     '''\n",
    "#     Draw a neural network cartoon using matplotilb.\n",
    "    \n",
    "#     :usage:\n",
    "#         >>> fig = plt.figure(figsize=(12, 12))\n",
    "#         >>> draw_neural_net(fig.gca(), .1, .9, .1, .9, [4, 7, 2])\n",
    "    \n",
    "#     :parameters:\n",
    "#         - ax : matplotlib.axes.AxesSubplot\n",
    "#             The axes on which to plot the cartoon (get e.g. by plt.gca())\n",
    "#         - left : float\n",
    "#             The center of the leftmost node(s) will be placed here\n",
    "#         - right : float\n",
    "#             The center of the rightmost node(s) will be placed here\n",
    "#         - bottom : float\n",
    "#             The center of the bottommost node(s) will be placed here\n",
    "#         - top : float\n",
    "#             The center of the topmost node(s) will be placed here\n",
    "#         - layer_sizes : list of int\n",
    "#             List of layer sizes, including input and output dimensionality\n",
    "#     '''\n",
    "#     n_layers = len(layer_sizes)\n",
    "#     v_spacing = (top - bottom)/float(max(layer_sizes))\n",
    "#     h_spacing = (right - left)/float(len(layer_sizes) - 1)\n",
    "#     # Nodes\n",
    "#     for n, layer_size in enumerate(layer_sizes):\n",
    "#         layer_top = v_spacing*(layer_size - 1)/2. + (top + bottom)/2.\n",
    "#         for m in range(layer_size):\n",
    "#             circle = plt.Circle((n*h_spacing + left, layer_top - m*v_spacing), v_spacing/4.,\n",
    "#                                 color='w', ec='k', zorder=4)\n",
    "#             ax.add_artist(circle)\n",
    "#     # Edges\n",
    "#     for n, (layer_size_a, layer_size_b) in enumerate(zip(layer_sizes[:-1], layer_sizes[1:])):\n",
    "#         layer_top_a = v_spacing*(layer_size_a - 1)/2. + (top + bottom)/2.\n",
    "#         layer_top_b = v_spacing*(layer_size_b - 1)/2. + (top + bottom)/2.\n",
    "#         for m in range(layer_size_a):\n",
    "#             for o in range(layer_size_b):\n",
    "#                 line = plt.Line2D([n*h_spacing + left, (n + 1)*h_spacing + left],\n",
    "#                                   [layer_top_a - m*v_spacing, layer_top_b - o*v_spacing], c='k')\n",
    "#                 ax.add_artist(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig = plt.figure(figsize=(12, 12))\n",
    "# ax = fig.gca()\n",
    "# ax.axis('off')\n",
    "# draw_neural_net(ax, .1, .9, .1, .9, [2, 3, 1])\n",
    "# fig.savefig('nn.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
