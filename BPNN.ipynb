{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for 1000 water molecule configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (1000,) entries\n"
     ]
    }
   ],
   "source": [
    "energies = np.genfromtxt('./water/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-c38e52de7b08>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #6 (got 1 columns instead of 5)\n",
      "    Line #11 (got 1 columns instead of 5)\n",
      "    Line #16 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #26 (got 1 columns instead of 5)\n",
      "    Line #31 (got 1 columns instead of 5)\n",
      "    Line #36 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #46 (got 1 columns instead of 5)\n",
      "    Line #51 (got 1 columns instead of 5)\n",
      "    Line #56 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #66 (got 1 columns instead of 5)\n",
      "    Line #71 (got 1 columns instead of 5)\n",
      "    Line #76 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #86 (got 1 columns instead of 5)\n",
      "    Line #91 (got 1 columns instead of 5)\n",
      "    Line #96 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #106 (got 1 columns instead of 5)\n",
      "    Line #111 (got 1 columns instead of 5)\n",
      "    Line #116 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #126 (got 1 columns instead of 5)\n",
      "    Line #131 (got 1 columns instead of 5)\n",
      "    Line #136 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #146 (got 1 columns instead of 5)\n",
      "    Line #151 (got 1 columns instead of 5)\n",
      "    Line #156 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #166 (got 1 columns instead of 5)\n",
      "    Line #171 (got 1 columns instead of 5)\n",
      "    Line #176 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #186 (got 1 columns instead of 5)\n",
      "    Line #191 (got 1 columns instead of 5)\n",
      "    Line #196 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #206 (got 1 columns instead of 5)\n",
      "    Line #211 (got 1 columns instead of 5)\n",
      "    Line #216 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #226 (got 1 columns instead of 5)\n",
      "    Line #231 (got 1 columns instead of 5)\n",
      "    Line #236 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #246 (got 1 columns instead of 5)\n",
      "    Line #251 (got 1 columns instead of 5)\n",
      "    Line #256 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #266 (got 1 columns instead of 5)\n",
      "    Line #271 (got 1 columns instead of 5)\n",
      "    Line #276 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #286 (got 1 columns instead of 5)\n",
      "    Line #291 (got 1 columns instead of 5)\n",
      "    Line #296 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #306 (got 1 columns instead of 5)\n",
      "    Line #311 (got 1 columns instead of 5)\n",
      "    Line #316 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #326 (got 1 columns instead of 5)\n",
      "    Line #331 (got 1 columns instead of 5)\n",
      "    Line #336 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #346 (got 1 columns instead of 5)\n",
      "    Line #351 (got 1 columns instead of 5)\n",
      "    Line #356 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #366 (got 1 columns instead of 5)\n",
      "    Line #371 (got 1 columns instead of 5)\n",
      "    Line #376 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #386 (got 1 columns instead of 5)\n",
      "    Line #391 (got 1 columns instead of 5)\n",
      "    Line #396 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #406 (got 1 columns instead of 5)\n",
      "    Line #411 (got 1 columns instead of 5)\n",
      "    Line #416 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #426 (got 1 columns instead of 5)\n",
      "    Line #431 (got 1 columns instead of 5)\n",
      "    Line #436 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #446 (got 1 columns instead of 5)\n",
      "    Line #451 (got 1 columns instead of 5)\n",
      "    Line #456 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #466 (got 1 columns instead of 5)\n",
      "    Line #471 (got 1 columns instead of 5)\n",
      "    Line #476 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #486 (got 1 columns instead of 5)\n",
      "    Line #491 (got 1 columns instead of 5)\n",
      "    Line #496 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #506 (got 1 columns instead of 5)\n",
      "    Line #511 (got 1 columns instead of 5)\n",
      "    Line #516 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #526 (got 1 columns instead of 5)\n",
      "    Line #531 (got 1 columns instead of 5)\n",
      "    Line #536 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #546 (got 1 columns instead of 5)\n",
      "    Line #551 (got 1 columns instead of 5)\n",
      "    Line #556 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #566 (got 1 columns instead of 5)\n",
      "    Line #571 (got 1 columns instead of 5)\n",
      "    Line #576 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #586 (got 1 columns instead of 5)\n",
      "    Line #591 (got 1 columns instead of 5)\n",
      "    Line #596 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #606 (got 1 columns instead of 5)\n",
      "    Line #611 (got 1 columns instead of 5)\n",
      "    Line #616 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #626 (got 1 columns instead of 5)\n",
      "    Line #631 (got 1 columns instead of 5)\n",
      "    Line #636 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #646 (got 1 columns instead of 5)\n",
      "    Line #651 (got 1 columns instead of 5)\n",
      "    Line #656 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #666 (got 1 columns instead of 5)\n",
      "    Line #671 (got 1 columns instead of 5)\n",
      "    Line #676 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #686 (got 1 columns instead of 5)\n",
      "    Line #691 (got 1 columns instead of 5)\n",
      "    Line #696 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #706 (got 1 columns instead of 5)\n",
      "    Line #711 (got 1 columns instead of 5)\n",
      "    Line #716 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #726 (got 1 columns instead of 5)\n",
      "    Line #731 (got 1 columns instead of 5)\n",
      "    Line #736 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #746 (got 1 columns instead of 5)\n",
      "    Line #751 (got 1 columns instead of 5)\n",
      "    Line #756 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #766 (got 1 columns instead of 5)\n",
      "    Line #771 (got 1 columns instead of 5)\n",
      "    Line #776 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #786 (got 1 columns instead of 5)\n",
      "    Line #791 (got 1 columns instead of 5)\n",
      "    Line #796 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #806 (got 1 columns instead of 5)\n",
      "    Line #811 (got 1 columns instead of 5)\n",
      "    Line #816 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #826 (got 1 columns instead of 5)\n",
      "    Line #831 (got 1 columns instead of 5)\n",
      "    Line #836 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #846 (got 1 columns instead of 5)\n",
      "    Line #851 (got 1 columns instead of 5)\n",
      "    Line #856 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #866 (got 1 columns instead of 5)\n",
      "    Line #871 (got 1 columns instead of 5)\n",
      "    Line #876 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #886 (got 1 columns instead of 5)\n",
      "    Line #891 (got 1 columns instead of 5)\n",
      "    Line #896 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #906 (got 1 columns instead of 5)\n",
      "    Line #911 (got 1 columns instead of 5)\n",
      "    Line #916 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #926 (got 1 columns instead of 5)\n",
      "    Line #931 (got 1 columns instead of 5)\n",
      "    Line #936 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #946 (got 1 columns instead of 5)\n",
      "    Line #951 (got 1 columns instead of 5)\n",
      "    Line #956 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #966 (got 1 columns instead of 5)\n",
      "    Line #971 (got 1 columns instead of 5)\n",
      "    Line #976 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #986 (got 1 columns instead of 5)\n",
      "    Line #991 (got 1 columns instead of 5)\n",
      "    Line #996 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1006 (got 1 columns instead of 5)\n",
      "    Line #1011 (got 1 columns instead of 5)\n",
      "    Line #1016 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1026 (got 1 columns instead of 5)\n",
      "    Line #1031 (got 1 columns instead of 5)\n",
      "    Line #1036 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1046 (got 1 columns instead of 5)\n",
      "    Line #1051 (got 1 columns instead of 5)\n",
      "    Line #1056 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1066 (got 1 columns instead of 5)\n",
      "    Line #1071 (got 1 columns instead of 5)\n",
      "    Line #1076 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1086 (got 1 columns instead of 5)\n",
      "    Line #1091 (got 1 columns instead of 5)\n",
      "    Line #1096 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1106 (got 1 columns instead of 5)\n",
      "    Line #1111 (got 1 columns instead of 5)\n",
      "    Line #1116 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1126 (got 1 columns instead of 5)\n",
      "    Line #1131 (got 1 columns instead of 5)\n",
      "    Line #1136 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1146 (got 1 columns instead of 5)\n",
      "    Line #1151 (got 1 columns instead of 5)\n",
      "    Line #1156 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1166 (got 1 columns instead of 5)\n",
      "    Line #1171 (got 1 columns instead of 5)\n",
      "    Line #1176 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1186 (got 1 columns instead of 5)\n",
      "    Line #1191 (got 1 columns instead of 5)\n",
      "    Line #1196 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1206 (got 1 columns instead of 5)\n",
      "    Line #1211 (got 1 columns instead of 5)\n",
      "    Line #1216 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1226 (got 1 columns instead of 5)\n",
      "    Line #1231 (got 1 columns instead of 5)\n",
      "    Line #1236 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1246 (got 1 columns instead of 5)\n",
      "    Line #1251 (got 1 columns instead of 5)\n",
      "    Line #1256 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1266 (got 1 columns instead of 5)\n",
      "    Line #1271 (got 1 columns instead of 5)\n",
      "    Line #1276 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1286 (got 1 columns instead of 5)\n",
      "    Line #1291 (got 1 columns instead of 5)\n",
      "    Line #1296 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1306 (got 1 columns instead of 5)\n",
      "    Line #1311 (got 1 columns instead of 5)\n",
      "    Line #1316 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1326 (got 1 columns instead of 5)\n",
      "    Line #1331 (got 1 columns instead of 5)\n",
      "    Line #1336 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1346 (got 1 columns instead of 5)\n",
      "    Line #1351 (got 1 columns instead of 5)\n",
      "    Line #1356 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1366 (got 1 columns instead of 5)\n",
      "    Line #1371 (got 1 columns instead of 5)\n",
      "    Line #1376 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1386 (got 1 columns instead of 5)\n",
      "    Line #1391 (got 1 columns instead of 5)\n",
      "    Line #1396 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1406 (got 1 columns instead of 5)\n",
      "    Line #1411 (got 1 columns instead of 5)\n",
      "    Line #1416 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1426 (got 1 columns instead of 5)\n",
      "    Line #1431 (got 1 columns instead of 5)\n",
      "    Line #1436 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1446 (got 1 columns instead of 5)\n",
      "    Line #1451 (got 1 columns instead of 5)\n",
      "    Line #1456 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1466 (got 1 columns instead of 5)\n",
      "    Line #1471 (got 1 columns instead of 5)\n",
      "    Line #1476 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1486 (got 1 columns instead of 5)\n",
      "    Line #1491 (got 1 columns instead of 5)\n",
      "    Line #1496 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1506 (got 1 columns instead of 5)\n",
      "    Line #1511 (got 1 columns instead of 5)\n",
      "    Line #1516 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1526 (got 1 columns instead of 5)\n",
      "    Line #1531 (got 1 columns instead of 5)\n",
      "    Line #1536 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1546 (got 1 columns instead of 5)\n",
      "    Line #1551 (got 1 columns instead of 5)\n",
      "    Line #1556 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1566 (got 1 columns instead of 5)\n",
      "    Line #1571 (got 1 columns instead of 5)\n",
      "    Line #1576 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1586 (got 1 columns instead of 5)\n",
      "    Line #1591 (got 1 columns instead of 5)\n",
      "    Line #1596 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1606 (got 1 columns instead of 5)\n",
      "    Line #1611 (got 1 columns instead of 5)\n",
      "    Line #1616 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1626 (got 1 columns instead of 5)\n",
      "    Line #1631 (got 1 columns instead of 5)\n",
      "    Line #1636 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1646 (got 1 columns instead of 5)\n",
      "    Line #1651 (got 1 columns instead of 5)\n",
      "    Line #1656 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1666 (got 1 columns instead of 5)\n",
      "    Line #1671 (got 1 columns instead of 5)\n",
      "    Line #1676 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1686 (got 1 columns instead of 5)\n",
      "    Line #1691 (got 1 columns instead of 5)\n",
      "    Line #1696 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1706 (got 1 columns instead of 5)\n",
      "    Line #1711 (got 1 columns instead of 5)\n",
      "    Line #1716 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1726 (got 1 columns instead of 5)\n",
      "    Line #1731 (got 1 columns instead of 5)\n",
      "    Line #1736 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1746 (got 1 columns instead of 5)\n",
      "    Line #1751 (got 1 columns instead of 5)\n",
      "    Line #1756 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1766 (got 1 columns instead of 5)\n",
      "    Line #1771 (got 1 columns instead of 5)\n",
      "    Line #1776 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1786 (got 1 columns instead of 5)\n",
      "    Line #1791 (got 1 columns instead of 5)\n",
      "    Line #1796 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1806 (got 1 columns instead of 5)\n",
      "    Line #1811 (got 1 columns instead of 5)\n",
      "    Line #1816 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1826 (got 1 columns instead of 5)\n",
      "    Line #1831 (got 1 columns instead of 5)\n",
      "    Line #1836 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1846 (got 1 columns instead of 5)\n",
      "    Line #1851 (got 1 columns instead of 5)\n",
      "    Line #1856 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1866 (got 1 columns instead of 5)\n",
      "    Line #1871 (got 1 columns instead of 5)\n",
      "    Line #1876 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1886 (got 1 columns instead of 5)\n",
      "    Line #1891 (got 1 columns instead of 5)\n",
      "    Line #1896 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1906 (got 1 columns instead of 5)\n",
      "    Line #1911 (got 1 columns instead of 5)\n",
      "    Line #1916 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1926 (got 1 columns instead of 5)\n",
      "    Line #1931 (got 1 columns instead of 5)\n",
      "    Line #1936 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1946 (got 1 columns instead of 5)\n",
      "    Line #1951 (got 1 columns instead of 5)\n",
      "    Line #1956 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1966 (got 1 columns instead of 5)\n",
      "    Line #1971 (got 1 columns instead of 5)\n",
      "    Line #1976 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1986 (got 1 columns instead of 5)\n",
      "    Line #1991 (got 1 columns instead of 5)\n",
      "    Line #1996 (got 1 columns instead of 5)\n",
      "    Line #2001 (got 1 columns instead of 5)\n",
      "    Line #2006 (got 1 columns instead of 5)\n",
      "    Line #2011 (got 1 columns instead of 5)\n",
      "    Line #2016 (got 1 columns instead of 5)\n",
      "    Line #2021 (got 1 columns instead of 5)\n",
      "    Line #2026 (got 1 columns instead of 5)\n",
      "    Line #2031 (got 1 columns instead of 5)\n",
      "    Line #2036 (got 1 columns instead of 5)\n",
      "    Line #2041 (got 1 columns instead of 5)\n",
      "    Line #2046 (got 1 columns instead of 5)\n",
      "    Line #2051 (got 1 columns instead of 5)\n",
      "    Line #2056 (got 1 columns instead of 5)\n",
      "    Line #2061 (got 1 columns instead of 5)\n",
      "    Line #2066 (got 1 columns instead of 5)\n",
      "    Line #2071 (got 1 columns instead of 5)\n",
      "    Line #2076 (got 1 columns instead of 5)\n",
      "    Line #2081 (got 1 columns instead of 5)\n",
      "    Line #2086 (got 1 columns instead of 5)\n",
      "    Line #2091 (got 1 columns instead of 5)\n",
      "    Line #2096 (got 1 columns instead of 5)\n",
      "    Line #2101 (got 1 columns instead of 5)\n",
      "    Line #2106 (got 1 columns instead of 5)\n",
      "    Line #2111 (got 1 columns instead of 5)\n",
      "    Line #2116 (got 1 columns instead of 5)\n",
      "    Line #2121 (got 1 columns instead of 5)\n",
      "    Line #2126 (got 1 columns instead of 5)\n",
      "    Line #2131 (got 1 columns instead of 5)\n",
      "    Line #2136 (got 1 columns instead of 5)\n",
      "    Line #2141 (got 1 columns instead of 5)\n",
      "    Line #2146 (got 1 columns instead of 5)\n",
      "    Line #2151 (got 1 columns instead of 5)\n",
      "    Line #2156 (got 1 columns instead of 5)\n",
      "    Line #2161 (got 1 columns instead of 5)\n",
      "    Line #2166 (got 1 columns instead of 5)\n",
      "    Line #2171 (got 1 columns instead of 5)\n",
      "    Line #2176 (got 1 columns instead of 5)\n",
      "    Line #2181 (got 1 columns instead of 5)\n",
      "    Line #2186 (got 1 columns instead of 5)\n",
      "    Line #2191 (got 1 columns instead of 5)\n",
      "    Line #2196 (got 1 columns instead of 5)\n",
      "    Line #2201 (got 1 columns instead of 5)\n",
      "    Line #2206 (got 1 columns instead of 5)\n",
      "    Line #2211 (got 1 columns instead of 5)\n",
      "    Line #2216 (got 1 columns instead of 5)\n",
      "    Line #2221 (got 1 columns instead of 5)\n",
      "    Line #2226 (got 1 columns instead of 5)\n",
      "    Line #2231 (got 1 columns instead of 5)\n",
      "    Line #2236 (got 1 columns instead of 5)\n",
      "    Line #2241 (got 1 columns instead of 5)\n",
      "    Line #2246 (got 1 columns instead of 5)\n",
      "    Line #2251 (got 1 columns instead of 5)\n",
      "    Line #2256 (got 1 columns instead of 5)\n",
      "    Line #2261 (got 1 columns instead of 5)\n",
      "    Line #2266 (got 1 columns instead of 5)\n",
      "    Line #2271 (got 1 columns instead of 5)\n",
      "    Line #2276 (got 1 columns instead of 5)\n",
      "    Line #2281 (got 1 columns instead of 5)\n",
      "    Line #2286 (got 1 columns instead of 5)\n",
      "    Line #2291 (got 1 columns instead of 5)\n",
      "    Line #2296 (got 1 columns instead of 5)\n",
      "    Line #2301 (got 1 columns instead of 5)\n",
      "    Line #2306 (got 1 columns instead of 5)\n",
      "    Line #2311 (got 1 columns instead of 5)\n",
      "    Line #2316 (got 1 columns instead of 5)\n",
      "    Line #2321 (got 1 columns instead of 5)\n",
      "    Line #2326 (got 1 columns instead of 5)\n",
      "    Line #2331 (got 1 columns instead of 5)\n",
      "    Line #2336 (got 1 columns instead of 5)\n",
      "    Line #2341 (got 1 columns instead of 5)\n",
      "    Line #2346 (got 1 columns instead of 5)\n",
      "    Line #2351 (got 1 columns instead of 5)\n",
      "    Line #2356 (got 1 columns instead of 5)\n",
      "    Line #2361 (got 1 columns instead of 5)\n",
      "    Line #2366 (got 1 columns instead of 5)\n",
      "    Line #2371 (got 1 columns instead of 5)\n",
      "    Line #2376 (got 1 columns instead of 5)\n",
      "    Line #2381 (got 1 columns instead of 5)\n",
      "    Line #2386 (got 1 columns instead of 5)\n",
      "    Line #2391 (got 1 columns instead of 5)\n",
      "    Line #2396 (got 1 columns instead of 5)\n",
      "    Line #2401 (got 1 columns instead of 5)\n",
      "    Line #2406 (got 1 columns instead of 5)\n",
      "    Line #2411 (got 1 columns instead of 5)\n",
      "    Line #2416 (got 1 columns instead of 5)\n",
      "    Line #2421 (got 1 columns instead of 5)\n",
      "    Line #2426 (got 1 columns instead of 5)\n",
      "    Line #2431 (got 1 columns instead of 5)\n",
      "    Line #2436 (got 1 columns instead of 5)\n",
      "    Line #2441 (got 1 columns instead of 5)\n",
      "    Line #2446 (got 1 columns instead of 5)\n",
      "    Line #2451 (got 1 columns instead of 5)\n",
      "    Line #2456 (got 1 columns instead of 5)\n",
      "    Line #2461 (got 1 columns instead of 5)\n",
      "    Line #2466 (got 1 columns instead of 5)\n",
      "    Line #2471 (got 1 columns instead of 5)\n",
      "    Line #2476 (got 1 columns instead of 5)\n",
      "    Line #2481 (got 1 columns instead of 5)\n",
      "    Line #2486 (got 1 columns instead of 5)\n",
      "    Line #2491 (got 1 columns instead of 5)\n",
      "    Line #2496 (got 1 columns instead of 5)\n",
      "    Line #2501 (got 1 columns instead of 5)\n",
      "    Line #2506 (got 1 columns instead of 5)\n",
      "    Line #2511 (got 1 columns instead of 5)\n",
      "    Line #2516 (got 1 columns instead of 5)\n",
      "    Line #2521 (got 1 columns instead of 5)\n",
      "    Line #2526 (got 1 columns instead of 5)\n",
      "    Line #2531 (got 1 columns instead of 5)\n",
      "    Line #2536 (got 1 columns instead of 5)\n",
      "    Line #2541 (got 1 columns instead of 5)\n",
      "    Line #2546 (got 1 columns instead of 5)\n",
      "    Line #2551 (got 1 columns instead of 5)\n",
      "    Line #2556 (got 1 columns instead of 5)\n",
      "    Line #2561 (got 1 columns instead of 5)\n",
      "    Line #2566 (got 1 columns instead of 5)\n",
      "    Line #2571 (got 1 columns instead of 5)\n",
      "    Line #2576 (got 1 columns instead of 5)\n",
      "    Line #2581 (got 1 columns instead of 5)\n",
      "    Line #2586 (got 1 columns instead of 5)\n",
      "    Line #2591 (got 1 columns instead of 5)\n",
      "    Line #2596 (got 1 columns instead of 5)\n",
      "    Line #2601 (got 1 columns instead of 5)\n",
      "    Line #2606 (got 1 columns instead of 5)\n",
      "    Line #2611 (got 1 columns instead of 5)\n",
      "    Line #2616 (got 1 columns instead of 5)\n",
      "    Line #2621 (got 1 columns instead of 5)\n",
      "    Line #2626 (got 1 columns instead of 5)\n",
      "    Line #2631 (got 1 columns instead of 5)\n",
      "    Line #2636 (got 1 columns instead of 5)\n",
      "    Line #2641 (got 1 columns instead of 5)\n",
      "    Line #2646 (got 1 columns instead of 5)\n",
      "    Line #2651 (got 1 columns instead of 5)\n",
      "    Line #2656 (got 1 columns instead of 5)\n",
      "    Line #2661 (got 1 columns instead of 5)\n",
      "    Line #2666 (got 1 columns instead of 5)\n",
      "    Line #2671 (got 1 columns instead of 5)\n",
      "    Line #2676 (got 1 columns instead of 5)\n",
      "    Line #2681 (got 1 columns instead of 5)\n",
      "    Line #2686 (got 1 columns instead of 5)\n",
      "    Line #2691 (got 1 columns instead of 5)\n",
      "    Line #2696 (got 1 columns instead of 5)\n",
      "    Line #2701 (got 1 columns instead of 5)\n",
      "    Line #2706 (got 1 columns instead of 5)\n",
      "    Line #2711 (got 1 columns instead of 5)\n",
      "    Line #2716 (got 1 columns instead of 5)\n",
      "    Line #2721 (got 1 columns instead of 5)\n",
      "    Line #2726 (got 1 columns instead of 5)\n",
      "    Line #2731 (got 1 columns instead of 5)\n",
      "    Line #2736 (got 1 columns instead of 5)\n",
      "    Line #2741 (got 1 columns instead of 5)\n",
      "    Line #2746 (got 1 columns instead of 5)\n",
      "    Line #2751 (got 1 columns instead of 5)\n",
      "    Line #2756 (got 1 columns instead of 5)\n",
      "    Line #2761 (got 1 columns instead of 5)\n",
      "    Line #2766 (got 1 columns instead of 5)\n",
      "    Line #2771 (got 1 columns instead of 5)\n",
      "    Line #2776 (got 1 columns instead of 5)\n",
      "    Line #2781 (got 1 columns instead of 5)\n",
      "    Line #2786 (got 1 columns instead of 5)\n",
      "    Line #2791 (got 1 columns instead of 5)\n",
      "    Line #2796 (got 1 columns instead of 5)\n",
      "    Line #2801 (got 1 columns instead of 5)\n",
      "    Line #2806 (got 1 columns instead of 5)\n",
      "    Line #2811 (got 1 columns instead of 5)\n",
      "    Line #2816 (got 1 columns instead of 5)\n",
      "    Line #2821 (got 1 columns instead of 5)\n",
      "    Line #2826 (got 1 columns instead of 5)\n",
      "    Line #2831 (got 1 columns instead of 5)\n",
      "    Line #2836 (got 1 columns instead of 5)\n",
      "    Line #2841 (got 1 columns instead of 5)\n",
      "    Line #2846 (got 1 columns instead of 5)\n",
      "    Line #2851 (got 1 columns instead of 5)\n",
      "    Line #2856 (got 1 columns instead of 5)\n",
      "    Line #2861 (got 1 columns instead of 5)\n",
      "    Line #2866 (got 1 columns instead of 5)\n",
      "    Line #2871 (got 1 columns instead of 5)\n",
      "    Line #2876 (got 1 columns instead of 5)\n",
      "    Line #2881 (got 1 columns instead of 5)\n",
      "    Line #2886 (got 1 columns instead of 5)\n",
      "    Line #2891 (got 1 columns instead of 5)\n",
      "    Line #2896 (got 1 columns instead of 5)\n",
      "    Line #2901 (got 1 columns instead of 5)\n",
      "    Line #2906 (got 1 columns instead of 5)\n",
      "    Line #2911 (got 1 columns instead of 5)\n",
      "    Line #2916 (got 1 columns instead of 5)\n",
      "    Line #2921 (got 1 columns instead of 5)\n",
      "    Line #2926 (got 1 columns instead of 5)\n",
      "    Line #2931 (got 1 columns instead of 5)\n",
      "    Line #2936 (got 1 columns instead of 5)\n",
      "    Line #2941 (got 1 columns instead of 5)\n",
      "    Line #2946 (got 1 columns instead of 5)\n",
      "    Line #2951 (got 1 columns instead of 5)\n",
      "    Line #2956 (got 1 columns instead of 5)\n",
      "    Line #2961 (got 1 columns instead of 5)\n",
      "    Line #2966 (got 1 columns instead of 5)\n",
      "    Line #2971 (got 1 columns instead of 5)\n",
      "    Line #2976 (got 1 columns instead of 5)\n",
      "    Line #2981 (got 1 columns instead of 5)\n",
      "    Line #2986 (got 1 columns instead of 5)\n",
      "    Line #2991 (got 1 columns instead of 5)\n",
      "    Line #2996 (got 1 columns instead of 5)\n",
      "    Line #3001 (got 1 columns instead of 5)\n",
      "    Line #3006 (got 1 columns instead of 5)\n",
      "    Line #3011 (got 1 columns instead of 5)\n",
      "    Line #3016 (got 1 columns instead of 5)\n",
      "    Line #3021 (got 1 columns instead of 5)\n",
      "    Line #3026 (got 1 columns instead of 5)\n",
      "    Line #3031 (got 1 columns instead of 5)\n",
      "    Line #3036 (got 1 columns instead of 5)\n",
      "    Line #3041 (got 1 columns instead of 5)\n",
      "    Line #3046 (got 1 columns instead of 5)\n",
      "    Line #3051 (got 1 columns instead of 5)\n",
      "    Line #3056 (got 1 columns instead of 5)\n",
      "    Line #3061 (got 1 columns instead of 5)\n",
      "    Line #3066 (got 1 columns instead of 5)\n",
      "    Line #3071 (got 1 columns instead of 5)\n",
      "    Line #3076 (got 1 columns instead of 5)\n",
      "    Line #3081 (got 1 columns instead of 5)\n",
      "    Line #3086 (got 1 columns instead of 5)\n",
      "    Line #3091 (got 1 columns instead of 5)\n",
      "    Line #3096 (got 1 columns instead of 5)\n",
      "    Line #3101 (got 1 columns instead of 5)\n",
      "    Line #3106 (got 1 columns instead of 5)\n",
      "    Line #3111 (got 1 columns instead of 5)\n",
      "    Line #3116 (got 1 columns instead of 5)\n",
      "    Line #3121 (got 1 columns instead of 5)\n",
      "    Line #3126 (got 1 columns instead of 5)\n",
      "    Line #3131 (got 1 columns instead of 5)\n",
      "    Line #3136 (got 1 columns instead of 5)\n",
      "    Line #3141 (got 1 columns instead of 5)\n",
      "    Line #3146 (got 1 columns instead of 5)\n",
      "    Line #3151 (got 1 columns instead of 5)\n",
      "    Line #3156 (got 1 columns instead of 5)\n",
      "    Line #3161 (got 1 columns instead of 5)\n",
      "    Line #3166 (got 1 columns instead of 5)\n",
      "    Line #3171 (got 1 columns instead of 5)\n",
      "    Line #3176 (got 1 columns instead of 5)\n",
      "    Line #3181 (got 1 columns instead of 5)\n",
      "    Line #3186 (got 1 columns instead of 5)\n",
      "    Line #3191 (got 1 columns instead of 5)\n",
      "    Line #3196 (got 1 columns instead of 5)\n",
      "    Line #3201 (got 1 columns instead of 5)\n",
      "    Line #3206 (got 1 columns instead of 5)\n",
      "    Line #3211 (got 1 columns instead of 5)\n",
      "    Line #3216 (got 1 columns instead of 5)\n",
      "    Line #3221 (got 1 columns instead of 5)\n",
      "    Line #3226 (got 1 columns instead of 5)\n",
      "    Line #3231 (got 1 columns instead of 5)\n",
      "    Line #3236 (got 1 columns instead of 5)\n",
      "    Line #3241 (got 1 columns instead of 5)\n",
      "    Line #3246 (got 1 columns instead of 5)\n",
      "    Line #3251 (got 1 columns instead of 5)\n",
      "    Line #3256 (got 1 columns instead of 5)\n",
      "    Line #3261 (got 1 columns instead of 5)\n",
      "    Line #3266 (got 1 columns instead of 5)\n",
      "    Line #3271 (got 1 columns instead of 5)\n",
      "    Line #3276 (got 1 columns instead of 5)\n",
      "    Line #3281 (got 1 columns instead of 5)\n",
      "    Line #3286 (got 1 columns instead of 5)\n",
      "    Line #3291 (got 1 columns instead of 5)\n",
      "    Line #3296 (got 1 columns instead of 5)\n",
      "    Line #3301 (got 1 columns instead of 5)\n",
      "    Line #3306 (got 1 columns instead of 5)\n",
      "    Line #3311 (got 1 columns instead of 5)\n",
      "    Line #3316 (got 1 columns instead of 5)\n",
      "    Line #3321 (got 1 columns instead of 5)\n",
      "    Line #3326 (got 1 columns instead of 5)\n",
      "    Line #3331 (got 1 columns instead of 5)\n",
      "    Line #3336 (got 1 columns instead of 5)\n",
      "    Line #3341 (got 1 columns instead of 5)\n",
      "    Line #3346 (got 1 columns instead of 5)\n",
      "    Line #3351 (got 1 columns instead of 5)\n",
      "    Line #3356 (got 1 columns instead of 5)\n",
      "    Line #3361 (got 1 columns instead of 5)\n",
      "    Line #3366 (got 1 columns instead of 5)\n",
      "    Line #3371 (got 1 columns instead of 5)\n",
      "    Line #3376 (got 1 columns instead of 5)\n",
      "    Line #3381 (got 1 columns instead of 5)\n",
      "    Line #3386 (got 1 columns instead of 5)\n",
      "    Line #3391 (got 1 columns instead of 5)\n",
      "    Line #3396 (got 1 columns instead of 5)\n",
      "    Line #3401 (got 1 columns instead of 5)\n",
      "    Line #3406 (got 1 columns instead of 5)\n",
      "    Line #3411 (got 1 columns instead of 5)\n",
      "    Line #3416 (got 1 columns instead of 5)\n",
      "    Line #3421 (got 1 columns instead of 5)\n",
      "    Line #3426 (got 1 columns instead of 5)\n",
      "    Line #3431 (got 1 columns instead of 5)\n",
      "    Line #3436 (got 1 columns instead of 5)\n",
      "    Line #3441 (got 1 columns instead of 5)\n",
      "    Line #3446 (got 1 columns instead of 5)\n",
      "    Line #3451 (got 1 columns instead of 5)\n",
      "    Line #3456 (got 1 columns instead of 5)\n",
      "    Line #3461 (got 1 columns instead of 5)\n",
      "    Line #3466 (got 1 columns instead of 5)\n",
      "    Line #3471 (got 1 columns instead of 5)\n",
      "    Line #3476 (got 1 columns instead of 5)\n",
      "    Line #3481 (got 1 columns instead of 5)\n",
      "    Line #3486 (got 1 columns instead of 5)\n",
      "    Line #3491 (got 1 columns instead of 5)\n",
      "    Line #3496 (got 1 columns instead of 5)\n",
      "    Line #3501 (got 1 columns instead of 5)\n",
      "    Line #3506 (got 1 columns instead of 5)\n",
      "    Line #3511 (got 1 columns instead of 5)\n",
      "    Line #3516 (got 1 columns instead of 5)\n",
      "    Line #3521 (got 1 columns instead of 5)\n",
      "    Line #3526 (got 1 columns instead of 5)\n",
      "    Line #3531 (got 1 columns instead of 5)\n",
      "    Line #3536 (got 1 columns instead of 5)\n",
      "    Line #3541 (got 1 columns instead of 5)\n",
      "    Line #3546 (got 1 columns instead of 5)\n",
      "    Line #3551 (got 1 columns instead of 5)\n",
      "    Line #3556 (got 1 columns instead of 5)\n",
      "    Line #3561 (got 1 columns instead of 5)\n",
      "    Line #3566 (got 1 columns instead of 5)\n",
      "    Line #3571 (got 1 columns instead of 5)\n",
      "    Line #3576 (got 1 columns instead of 5)\n",
      "    Line #3581 (got 1 columns instead of 5)\n",
      "    Line #3586 (got 1 columns instead of 5)\n",
      "    Line #3591 (got 1 columns instead of 5)\n",
      "    Line #3596 (got 1 columns instead of 5)\n",
      "    Line #3601 (got 1 columns instead of 5)\n",
      "    Line #3606 (got 1 columns instead of 5)\n",
      "    Line #3611 (got 1 columns instead of 5)\n",
      "    Line #3616 (got 1 columns instead of 5)\n",
      "    Line #3621 (got 1 columns instead of 5)\n",
      "    Line #3626 (got 1 columns instead of 5)\n",
      "    Line #3631 (got 1 columns instead of 5)\n",
      "    Line #3636 (got 1 columns instead of 5)\n",
      "    Line #3641 (got 1 columns instead of 5)\n",
      "    Line #3646 (got 1 columns instead of 5)\n",
      "    Line #3651 (got 1 columns instead of 5)\n",
      "    Line #3656 (got 1 columns instead of 5)\n",
      "    Line #3661 (got 1 columns instead of 5)\n",
      "    Line #3666 (got 1 columns instead of 5)\n",
      "    Line #3671 (got 1 columns instead of 5)\n",
      "    Line #3676 (got 1 columns instead of 5)\n",
      "    Line #3681 (got 1 columns instead of 5)\n",
      "    Line #3686 (got 1 columns instead of 5)\n",
      "    Line #3691 (got 1 columns instead of 5)\n",
      "    Line #3696 (got 1 columns instead of 5)\n",
      "    Line #3701 (got 1 columns instead of 5)\n",
      "    Line #3706 (got 1 columns instead of 5)\n",
      "    Line #3711 (got 1 columns instead of 5)\n",
      "    Line #3716 (got 1 columns instead of 5)\n",
      "    Line #3721 (got 1 columns instead of 5)\n",
      "    Line #3726 (got 1 columns instead of 5)\n",
      "    Line #3731 (got 1 columns instead of 5)\n",
      "    Line #3736 (got 1 columns instead of 5)\n",
      "    Line #3741 (got 1 columns instead of 5)\n",
      "    Line #3746 (got 1 columns instead of 5)\n",
      "    Line #3751 (got 1 columns instead of 5)\n",
      "    Line #3756 (got 1 columns instead of 5)\n",
      "    Line #3761 (got 1 columns instead of 5)\n",
      "    Line #3766 (got 1 columns instead of 5)\n",
      "    Line #3771 (got 1 columns instead of 5)\n",
      "    Line #3776 (got 1 columns instead of 5)\n",
      "    Line #3781 (got 1 columns instead of 5)\n",
      "    Line #3786 (got 1 columns instead of 5)\n",
      "    Line #3791 (got 1 columns instead of 5)\n",
      "    Line #3796 (got 1 columns instead of 5)\n",
      "    Line #3801 (got 1 columns instead of 5)\n",
      "    Line #3806 (got 1 columns instead of 5)\n",
      "    Line #3811 (got 1 columns instead of 5)\n",
      "    Line #3816 (got 1 columns instead of 5)\n",
      "    Line #3821 (got 1 columns instead of 5)\n",
      "    Line #3826 (got 1 columns instead of 5)\n",
      "    Line #3831 (got 1 columns instead of 5)\n",
      "    Line #3836 (got 1 columns instead of 5)\n",
      "    Line #3841 (got 1 columns instead of 5)\n",
      "    Line #3846 (got 1 columns instead of 5)\n",
      "    Line #3851 (got 1 columns instead of 5)\n",
      "    Line #3856 (got 1 columns instead of 5)\n",
      "    Line #3861 (got 1 columns instead of 5)\n",
      "    Line #3866 (got 1 columns instead of 5)\n",
      "    Line #3871 (got 1 columns instead of 5)\n",
      "    Line #3876 (got 1 columns instead of 5)\n",
      "    Line #3881 (got 1 columns instead of 5)\n",
      "    Line #3886 (got 1 columns instead of 5)\n",
      "    Line #3891 (got 1 columns instead of 5)\n",
      "    Line #3896 (got 1 columns instead of 5)\n",
      "    Line #3901 (got 1 columns instead of 5)\n",
      "    Line #3906 (got 1 columns instead of 5)\n",
      "    Line #3911 (got 1 columns instead of 5)\n",
      "    Line #3916 (got 1 columns instead of 5)\n",
      "    Line #3921 (got 1 columns instead of 5)\n",
      "    Line #3926 (got 1 columns instead of 5)\n",
      "    Line #3931 (got 1 columns instead of 5)\n",
      "    Line #3936 (got 1 columns instead of 5)\n",
      "    Line #3941 (got 1 columns instead of 5)\n",
      "    Line #3946 (got 1 columns instead of 5)\n",
      "    Line #3951 (got 1 columns instead of 5)\n",
      "    Line #3956 (got 1 columns instead of 5)\n",
      "    Line #3961 (got 1 columns instead of 5)\n",
      "    Line #3966 (got 1 columns instead of 5)\n",
      "    Line #3971 (got 1 columns instead of 5)\n",
      "    Line #3976 (got 1 columns instead of 5)\n",
      "    Line #3981 (got 1 columns instead of 5)\n",
      "    Line #3986 (got 1 columns instead of 5)\n",
      "    Line #3991 (got 1 columns instead of 5)\n",
      "    Line #3996 (got 1 columns instead of 5)\n",
      "    Line #4001 (got 1 columns instead of 5)\n",
      "    Line #4006 (got 1 columns instead of 5)\n",
      "    Line #4011 (got 1 columns instead of 5)\n",
      "    Line #4016 (got 1 columns instead of 5)\n",
      "    Line #4021 (got 1 columns instead of 5)\n",
      "    Line #4026 (got 1 columns instead of 5)\n",
      "    Line #4031 (got 1 columns instead of 5)\n",
      "    Line #4036 (got 1 columns instead of 5)\n",
      "    Line #4041 (got 1 columns instead of 5)\n",
      "    Line #4046 (got 1 columns instead of 5)\n",
      "    Line #4051 (got 1 columns instead of 5)\n",
      "    Line #4056 (got 1 columns instead of 5)\n",
      "    Line #4061 (got 1 columns instead of 5)\n",
      "    Line #4066 (got 1 columns instead of 5)\n",
      "    Line #4071 (got 1 columns instead of 5)\n",
      "    Line #4076 (got 1 columns instead of 5)\n",
      "    Line #4081 (got 1 columns instead of 5)\n",
      "    Line #4086 (got 1 columns instead of 5)\n",
      "    Line #4091 (got 1 columns instead of 5)\n",
      "    Line #4096 (got 1 columns instead of 5)\n",
      "    Line #4101 (got 1 columns instead of 5)\n",
      "    Line #4106 (got 1 columns instead of 5)\n",
      "    Line #4111 (got 1 columns instead of 5)\n",
      "    Line #4116 (got 1 columns instead of 5)\n",
      "    Line #4121 (got 1 columns instead of 5)\n",
      "    Line #4126 (got 1 columns instead of 5)\n",
      "    Line #4131 (got 1 columns instead of 5)\n",
      "    Line #4136 (got 1 columns instead of 5)\n",
      "    Line #4141 (got 1 columns instead of 5)\n",
      "    Line #4146 (got 1 columns instead of 5)\n",
      "    Line #4151 (got 1 columns instead of 5)\n",
      "    Line #4156 (got 1 columns instead of 5)\n",
      "    Line #4161 (got 1 columns instead of 5)\n",
      "    Line #4166 (got 1 columns instead of 5)\n",
      "    Line #4171 (got 1 columns instead of 5)\n",
      "    Line #4176 (got 1 columns instead of 5)\n",
      "    Line #4181 (got 1 columns instead of 5)\n",
      "    Line #4186 (got 1 columns instead of 5)\n",
      "    Line #4191 (got 1 columns instead of 5)\n",
      "    Line #4196 (got 1 columns instead of 5)\n",
      "    Line #4201 (got 1 columns instead of 5)\n",
      "    Line #4206 (got 1 columns instead of 5)\n",
      "    Line #4211 (got 1 columns instead of 5)\n",
      "    Line #4216 (got 1 columns instead of 5)\n",
      "    Line #4221 (got 1 columns instead of 5)\n",
      "    Line #4226 (got 1 columns instead of 5)\n",
      "    Line #4231 (got 1 columns instead of 5)\n",
      "    Line #4236 (got 1 columns instead of 5)\n",
      "    Line #4241 (got 1 columns instead of 5)\n",
      "    Line #4246 (got 1 columns instead of 5)\n",
      "    Line #4251 (got 1 columns instead of 5)\n",
      "    Line #4256 (got 1 columns instead of 5)\n",
      "    Line #4261 (got 1 columns instead of 5)\n",
      "    Line #4266 (got 1 columns instead of 5)\n",
      "    Line #4271 (got 1 columns instead of 5)\n",
      "    Line #4276 (got 1 columns instead of 5)\n",
      "    Line #4281 (got 1 columns instead of 5)\n",
      "    Line #4286 (got 1 columns instead of 5)\n",
      "    Line #4291 (got 1 columns instead of 5)\n",
      "    Line #4296 (got 1 columns instead of 5)\n",
      "    Line #4301 (got 1 columns instead of 5)\n",
      "    Line #4306 (got 1 columns instead of 5)\n",
      "    Line #4311 (got 1 columns instead of 5)\n",
      "    Line #4316 (got 1 columns instead of 5)\n",
      "    Line #4321 (got 1 columns instead of 5)\n",
      "    Line #4326 (got 1 columns instead of 5)\n",
      "    Line #4331 (got 1 columns instead of 5)\n",
      "    Line #4336 (got 1 columns instead of 5)\n",
      "    Line #4341 (got 1 columns instead of 5)\n",
      "    Line #4346 (got 1 columns instead of 5)\n",
      "    Line #4351 (got 1 columns instead of 5)\n",
      "    Line #4356 (got 1 columns instead of 5)\n",
      "    Line #4361 (got 1 columns instead of 5)\n",
      "    Line #4366 (got 1 columns instead of 5)\n",
      "    Line #4371 (got 1 columns instead of 5)\n",
      "    Line #4376 (got 1 columns instead of 5)\n",
      "    Line #4381 (got 1 columns instead of 5)\n",
      "    Line #4386 (got 1 columns instead of 5)\n",
      "    Line #4391 (got 1 columns instead of 5)\n",
      "    Line #4396 (got 1 columns instead of 5)\n",
      "    Line #4401 (got 1 columns instead of 5)\n",
      "    Line #4406 (got 1 columns instead of 5)\n",
      "    Line #4411 (got 1 columns instead of 5)\n",
      "    Line #4416 (got 1 columns instead of 5)\n",
      "    Line #4421 (got 1 columns instead of 5)\n",
      "    Line #4426 (got 1 columns instead of 5)\n",
      "    Line #4431 (got 1 columns instead of 5)\n",
      "    Line #4436 (got 1 columns instead of 5)\n",
      "    Line #4441 (got 1 columns instead of 5)\n",
      "    Line #4446 (got 1 columns instead of 5)\n",
      "    Line #4451 (got 1 columns instead of 5)\n",
      "    Line #4456 (got 1 columns instead of 5)\n",
      "    Line #4461 (got 1 columns instead of 5)\n",
      "    Line #4466 (got 1 columns instead of 5)\n",
      "    Line #4471 (got 1 columns instead of 5)\n",
      "    Line #4476 (got 1 columns instead of 5)\n",
      "    Line #4481 (got 1 columns instead of 5)\n",
      "    Line #4486 (got 1 columns instead of 5)\n",
      "    Line #4491 (got 1 columns instead of 5)\n",
      "    Line #4496 (got 1 columns instead of 5)\n",
      "    Line #4501 (got 1 columns instead of 5)\n",
      "    Line #4506 (got 1 columns instead of 5)\n",
      "    Line #4511 (got 1 columns instead of 5)\n",
      "    Line #4516 (got 1 columns instead of 5)\n",
      "    Line #4521 (got 1 columns instead of 5)\n",
      "    Line #4526 (got 1 columns instead of 5)\n",
      "    Line #4531 (got 1 columns instead of 5)\n",
      "    Line #4536 (got 1 columns instead of 5)\n",
      "    Line #4541 (got 1 columns instead of 5)\n",
      "    Line #4546 (got 1 columns instead of 5)\n",
      "    Line #4551 (got 1 columns instead of 5)\n",
      "    Line #4556 (got 1 columns instead of 5)\n",
      "    Line #4561 (got 1 columns instead of 5)\n",
      "    Line #4566 (got 1 columns instead of 5)\n",
      "    Line #4571 (got 1 columns instead of 5)\n",
      "    Line #4576 (got 1 columns instead of 5)\n",
      "    Line #4581 (got 1 columns instead of 5)\n",
      "    Line #4586 (got 1 columns instead of 5)\n",
      "    Line #4591 (got 1 columns instead of 5)\n",
      "    Line #4596 (got 1 columns instead of 5)\n",
      "    Line #4601 (got 1 columns instead of 5)\n",
      "    Line #4606 (got 1 columns instead of 5)\n",
      "    Line #4611 (got 1 columns instead of 5)\n",
      "    Line #4616 (got 1 columns instead of 5)\n",
      "    Line #4621 (got 1 columns instead of 5)\n",
      "    Line #4626 (got 1 columns instead of 5)\n",
      "    Line #4631 (got 1 columns instead of 5)\n",
      "    Line #4636 (got 1 columns instead of 5)\n",
      "    Line #4641 (got 1 columns instead of 5)\n",
      "    Line #4646 (got 1 columns instead of 5)\n",
      "    Line #4651 (got 1 columns instead of 5)\n",
      "    Line #4656 (got 1 columns instead of 5)\n",
      "    Line #4661 (got 1 columns instead of 5)\n",
      "    Line #4666 (got 1 columns instead of 5)\n",
      "    Line #4671 (got 1 columns instead of 5)\n",
      "    Line #4676 (got 1 columns instead of 5)\n",
      "    Line #4681 (got 1 columns instead of 5)\n",
      "    Line #4686 (got 1 columns instead of 5)\n",
      "    Line #4691 (got 1 columns instead of 5)\n",
      "    Line #4696 (got 1 columns instead of 5)\n",
      "    Line #4701 (got 1 columns instead of 5)\n",
      "    Line #4706 (got 1 columns instead of 5)\n",
      "    Line #4711 (got 1 columns instead of 5)\n",
      "    Line #4716 (got 1 columns instead of 5)\n",
      "    Line #4721 (got 1 columns instead of 5)\n",
      "    Line #4726 (got 1 columns instead of 5)\n",
      "    Line #4731 (got 1 columns instead of 5)\n",
      "    Line #4736 (got 1 columns instead of 5)\n",
      "    Line #4741 (got 1 columns instead of 5)\n",
      "    Line #4746 (got 1 columns instead of 5)\n",
      "    Line #4751 (got 1 columns instead of 5)\n",
      "    Line #4756 (got 1 columns instead of 5)\n",
      "    Line #4761 (got 1 columns instead of 5)\n",
      "    Line #4766 (got 1 columns instead of 5)\n",
      "    Line #4771 (got 1 columns instead of 5)\n",
      "    Line #4776 (got 1 columns instead of 5)\n",
      "    Line #4781 (got 1 columns instead of 5)\n",
      "    Line #4786 (got 1 columns instead of 5)\n",
      "    Line #4791 (got 1 columns instead of 5)\n",
      "    Line #4796 (got 1 columns instead of 5)\n",
      "    Line #4801 (got 1 columns instead of 5)\n",
      "    Line #4806 (got 1 columns instead of 5)\n",
      "    Line #4811 (got 1 columns instead of 5)\n",
      "    Line #4816 (got 1 columns instead of 5)\n",
      "    Line #4821 (got 1 columns instead of 5)\n",
      "    Line #4826 (got 1 columns instead of 5)\n",
      "    Line #4831 (got 1 columns instead of 5)\n",
      "    Line #4836 (got 1 columns instead of 5)\n",
      "    Line #4841 (got 1 columns instead of 5)\n",
      "    Line #4846 (got 1 columns instead of 5)\n",
      "    Line #4851 (got 1 columns instead of 5)\n",
      "    Line #4856 (got 1 columns instead of 5)\n",
      "    Line #4861 (got 1 columns instead of 5)\n",
      "    Line #4866 (got 1 columns instead of 5)\n",
      "    Line #4871 (got 1 columns instead of 5)\n",
      "    Line #4876 (got 1 columns instead of 5)\n",
      "    Line #4881 (got 1 columns instead of 5)\n",
      "    Line #4886 (got 1 columns instead of 5)\n",
      "    Line #4891 (got 1 columns instead of 5)\n",
      "    Line #4896 (got 1 columns instead of 5)\n",
      "    Line #4901 (got 1 columns instead of 5)\n",
      "    Line #4906 (got 1 columns instead of 5)\n",
      "    Line #4911 (got 1 columns instead of 5)\n",
      "    Line #4916 (got 1 columns instead of 5)\n",
      "    Line #4921 (got 1 columns instead of 5)\n",
      "    Line #4926 (got 1 columns instead of 5)\n",
      "    Line #4931 (got 1 columns instead of 5)\n",
      "    Line #4936 (got 1 columns instead of 5)\n",
      "    Line #4941 (got 1 columns instead of 5)\n",
      "    Line #4946 (got 1 columns instead of 5)\n",
      "    Line #4951 (got 1 columns instead of 5)\n",
      "    Line #4956 (got 1 columns instead of 5)\n",
      "    Line #4961 (got 1 columns instead of 5)\n",
      "    Line #4966 (got 1 columns instead of 5)\n",
      "    Line #4971 (got 1 columns instead of 5)\n",
      "    Line #4976 (got 1 columns instead of 5)\n",
      "    Line #4981 (got 1 columns instead of 5)\n",
      "    Line #4986 (got 1 columns instead of 5)\n",
      "    Line #4991 (got 1 columns instead of 5)\n",
      "    Line #4996 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('water', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates = (xyz_file[:,1:-1])\n",
    "coordinates = coordinates.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[0.         0.769767   0.55746937]\n",
      "[ 0.         -0.71017975  0.50340914]\n",
      "[ 0.         -0.0037242  -0.06630491]\n",
      "gfjkhgfjhgfkgh\n",
      "[0.         0.77715107 0.59586089]\n",
      "[ 0.         -0.77642641  0.59515778]\n",
      "[ 0.000000e+00 -4.529000e-05 -7.443867e-02]\n",
      "-13815.2523726009\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(coordinates))\n",
    "print(type(coordinates))\n",
    "print(coordinates[0])\n",
    "print(coordinates[1])\n",
    "print(coordinates[2])\n",
    "\n",
    "print('gfjkhgfjhgfkgh')\n",
    "print(coordinates[3])\n",
    "print(coordinates[4])\n",
    "print(coordinates[5])\n",
    "\n",
    "print(energies[0])\n",
    "# There is 1000 water molecules and each of them consists of 3 atoms, so we have 3000 atoms in total and each\n",
    "# of them has 3 coordinates.\n",
    "# Thus the coordinates array has 3000 lines, each of them corresponding to one atom (the first three lines\n",
    "# correspopnd to the first water molecule) and 3 columns corresponding to the x, y and z coordinates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.769767    0.55746937]\n",
      " [ 0.         -0.71017975  0.50340914]\n",
      " [ 0.         -0.0037242  -0.06630491]\n",
      " ...\n",
      " [ 0.          0.81441381  0.59863567]\n",
      " [ 0.         -0.76145415  0.54978922]\n",
      " [ 0.         -0.00330998 -0.07177656]]\n"
     ]
    }
   ],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.48093379 0.99367147]\n",
      " [1.48093379 0.         0.9075536 ]\n",
      " [0.99367147 0.9075536  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                # i-th water molecule\n",
    "N = 3                                # 3 atoms per molecule\n",
    "coord = coordinates[N*i:N*(i+1),:]  # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                     # pairwise distances between all of its 3 atom\n",
    "    \n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "#        print('i=',i)\n",
    "        for j in range(i+1,N):\n",
    "#            print(j)\n",
    "#            pairwise_dist_matrix[i][j] = \\\n",
    "#            np.sqrt(  (coord[i][0] - coord[j][0] )**2 + (coord[i][1] - coord[j][1] )**2 +(coord[i][2] - coord[j][2] )**2   )\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.65865495 1.67618952 1.79537284]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = len(coord)\n",
    "\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.19473151 5.24520024 5.28298916]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     #1\n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n",
      "torch.Size([900, 3, 6])\n"
     ]
    }
   ],
   "source": [
    "N                    = 3           # number of atoms per molecule\n",
    "number_of_features   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "    \n",
    "    \n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1., 1., 1.]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_size            = np.shape(energies)[0]        # We have 1000 water molecule conformations\n",
    "training_set_size    = data_size - 100\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features,2):\n",
    "        G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])     \n",
    "        G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.var(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "\n",
    "G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set = np.vsplit(G_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.var(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "print(mean_lab)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=25,\n",
    "                                           shuffle=False, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n",
      "[[0.33333333 2.        ]\n",
      " [3.         4.        ]\n",
      " [5.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "# for i in range(3):\n",
    "#     a[i,0] = a[i,0]/(sum(a[:,0])/3)\n",
    "# print(a)\n",
    "\n",
    "b = (sum(a[:,0])/3)\n",
    "print(b)\n",
    "print(type(b))\n",
    "a[0,0] = a[0,0]/b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([ -29.2808,    1.3305,   -9.2254, -144.8022,  -13.5390,  -43.2955])\n",
      "x2 tensor([-16.4986,   0.6675,  -0.0754,  74.7737,  -0.4341,  16.6143])\n",
      "x3 tensor([ 85.2207,  -2.0988,  16.9244, 191.8523,  25.0267,  65.6689])\n",
      "output\n",
      "tensor([-1.0741], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Subnets, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, 3)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(3, 3)\n",
    "        self.fc3 = nn.Linear(3, 1)\n",
    "#         self.fc6 = nn.Linear(6, 4)\n",
    "#         self.fc7 = nn.Linear(4, 2)\n",
    "#         self.fc8 = nn.Linear(2, 1)\n",
    "\n",
    "\n",
    "    def forward(self, x,train = True):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "#         x = torch.relu(self.fc3(x))\n",
    "#         x = torch.tanh(self.fc4(x))\n",
    "#         x = torch.tanh(self.fc5(x))\n",
    "#         x = torch.tanh(self.fc6(x))\n",
    "#         x = torch.tanh(self.fc7(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BPNN, self).__init__()\n",
    "        self.network1 = Subnets()\n",
    "        self.network2 = Subnets()\n",
    "        self.network3 = Subnets()\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2, x3,train = True):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        x3 = self.network3(x3)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "#         print(x3)\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), 0) \n",
    "#        x = self.fc_out(x)\n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN()\n",
    "N = 1\n",
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class simplenn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(simplenn, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 3)        # where fc stands for fully connected \n",
    "#         self.fc2 = nn.Linear(3, 1)        \n",
    "   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "#         x = self.fc2(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "#         return x\n",
    "\n",
    "# mod = simplenn()\n",
    "\n",
    "# print(mod.fc1.weight)\n",
    "# print(mod.fc1.bias)\n",
    "\n",
    "# print(mod.fc2.weight)\n",
    "# print(mod.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# x1 = x1[:2]\n",
    "# x2 = x2[:2]\n",
    "\n",
    "# x1[0] = -18650\n",
    "# x1[1] = 109075\n",
    "# print('x1',x1)\n",
    "\n",
    "# x2[0] = -6\n",
    "# x2[1] = 7\n",
    "# print('x2',x2)\n",
    "\n",
    "# output1 = mod(x1)\n",
    "# print('output1')\n",
    "# print(output1)\n",
    "\n",
    "# output2 = mod(x2)\n",
    "# print('output2')\n",
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([ -29.2808,    1.3305,   -9.2254, -144.8022,  -13.5390,  -43.2955])\n",
      "x2 tensor([-16.4986,   0.6675,  -0.0754,  74.7737,  -0.4341,  16.6143])\n",
      "x3 tensor([ 85.2207,  -2.0988,  16.9244, 191.8523,  25.0267,  65.6689])\n",
      "output\n",
      "tensor([-1.0741], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "print('x3',x3)\n",
    "\n",
    "\n",
    "output = model(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.1208,  0.0488,  0.3083, -0.0268, -0.0415,  0.1877],\n",
      "        [-0.1126,  0.4042, -0.2837, -0.0191,  0.2523, -0.3126],\n",
      "        [ 0.1143,  0.0774, -0.0577, -0.3593,  0.0507,  0.1800]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.2704,  0.3794,  0.3167], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.1397, -0.5493, -0.4365],\n",
      "        [ 0.1685, -0.3821,  0.3778],\n",
      "        [-0.3662, -0.5161, -0.4073]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.1217,  0.0333, -0.3269], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(model.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(model.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    10] loss: 1.38084\n",
      "[1,    20] loss: 1.15221\n",
      "[1,    30] loss: 1.00584\n",
      "[2,    10] loss: 0.71115\n",
      "[2,    20] loss: 0.59904\n",
      "[2,    30] loss: 0.49858\n",
      "[3,    10] loss: 0.37988\n",
      "[3,    20] loss: 0.34127\n",
      "[3,    30] loss: 0.27653\n",
      "[4,    10] loss: 0.23169\n",
      "[4,    20] loss: 0.22864\n",
      "[4,    30] loss: 0.18359\n",
      "[5,    10] loss: 0.16126\n",
      "[5,    20] loss: 0.16718\n",
      "[5,    30] loss: 0.12648\n",
      "[6,    10] loss: 0.11948\n",
      "[6,    20] loss: 0.13206\n",
      "[6,    30] loss: 0.10299\n",
      "[7,    10] loss: 0.10073\n",
      "[7,    20] loss: 0.11411\n",
      "[7,    30] loss: 0.09048\n",
      "[8,    10] loss: 0.08203\n",
      "[8,    20] loss: 0.09029\n",
      "[8,    30] loss: 0.07481\n",
      "[9,    10] loss: 0.07551\n",
      "[9,    20] loss: 0.08355\n",
      "[9,    30] loss: 0.06831\n",
      "[10,    10] loss: 0.06955\n",
      "[10,    20] loss: 0.07606\n",
      "[10,    30] loss: 0.06564\n",
      "[11,    10] loss: 0.06694\n",
      "[11,    20] loss: 0.07269\n",
      "[11,    30] loss: 0.06319\n",
      "[12,    10] loss: 0.06461\n",
      "[12,    20] loss: 0.07005\n",
      "[12,    30] loss: 0.06100\n",
      "[13,    10] loss: 0.06152\n",
      "[13,    20] loss: 0.06693\n",
      "[13,    30] loss: 0.05934\n",
      "[14,    10] loss: 0.05979\n",
      "[14,    20] loss: 0.06531\n",
      "[14,    30] loss: 0.05813\n",
      "[15,    10] loss: 0.05833\n",
      "[15,    20] loss: 0.06356\n",
      "[15,    30] loss: 0.05691\n",
      "[16,    10] loss: 0.05684\n",
      "[16,    20] loss: 0.06178\n",
      "[16,    30] loss: 0.05571\n",
      "[17,    10] loss: 0.05538\n",
      "[17,    20] loss: 0.06025\n",
      "[17,    30] loss: 0.05447\n",
      "[18,    10] loss: 0.05402\n",
      "[18,    20] loss: 0.05894\n",
      "[18,    30] loss: 0.05329\n",
      "[19,    10] loss: 0.05287\n",
      "[19,    20] loss: 0.05763\n",
      "[19,    30] loss: 0.05197\n",
      "[20,    10] loss: 0.05162\n",
      "[20,    20] loss: 0.05623\n",
      "[20,    30] loss: 0.04945\n",
      "[21,    10] loss: 0.05026\n",
      "[21,    20] loss: 0.05527\n",
      "[21,    30] loss: 0.04799\n",
      "[22,    10] loss: 0.04877\n",
      "[22,    20] loss: 0.05430\n",
      "[22,    30] loss: 0.04693\n",
      "[23,    10] loss: 0.04727\n",
      "[23,    20] loss: 0.05301\n",
      "[23,    30] loss: 0.04562\n",
      "[24,    10] loss: 0.04610\n",
      "[24,    20] loss: 0.05190\n",
      "[24,    30] loss: 0.04452\n",
      "[25,    10] loss: 0.04490\n",
      "[25,    20] loss: 0.05066\n",
      "[25,    30] loss: 0.04328\n",
      "[26,    10] loss: 0.04346\n",
      "[26,    20] loss: 0.04858\n",
      "[26,    30] loss: 0.04038\n",
      "[27,    10] loss: 0.03951\n",
      "[27,    20] loss: 0.04281\n",
      "[27,    30] loss: 0.03717\n",
      "[28,    10] loss: 0.03486\n",
      "[28,    20] loss: 0.04136\n",
      "[28,    30] loss: 0.03575\n",
      "[29,    10] loss: 0.03330\n",
      "[29,    20] loss: 0.03992\n",
      "[29,    30] loss: 0.03442\n",
      "[30,    10] loss: 0.03187\n",
      "[30,    20] loss: 0.03867\n",
      "[30,    30] loss: 0.03346\n",
      "[31,    10] loss: 0.03070\n",
      "[31,    20] loss: 0.03777\n",
      "[31,    30] loss: 0.03267\n",
      "[32,    10] loss: 0.02957\n",
      "[32,    20] loss: 0.03709\n",
      "[32,    30] loss: 0.03189\n",
      "[33,    10] loss: 0.02813\n",
      "[33,    20] loss: 0.03571\n",
      "[33,    30] loss: 0.03056\n",
      "[34,    10] loss: 0.02639\n",
      "[34,    20] loss: 0.03296\n",
      "[34,    30] loss: 0.02904\n",
      "[35,    10] loss: 0.02467\n",
      "[35,    20] loss: 0.03173\n",
      "[35,    30] loss: 0.02813\n",
      "[36,    10] loss: 0.02375\n",
      "[36,    20] loss: 0.03029\n",
      "[36,    30] loss: 0.02751\n",
      "[37,    10] loss: 0.02327\n",
      "[37,    20] loss: 0.02974\n",
      "[37,    30] loss: 0.02679\n",
      "[38,    10] loss: 0.02265\n",
      "[38,    20] loss: 0.02934\n",
      "[38,    30] loss: 0.02637\n",
      "[39,    10] loss: 0.02231\n",
      "[39,    20] loss: 0.02895\n",
      "[39,    30] loss: 0.02574\n",
      "[40,    10] loss: 0.02183\n",
      "[40,    20] loss: 0.02859\n",
      "[40,    30] loss: 0.02528\n",
      "[41,    10] loss: 0.02150\n",
      "[41,    20] loss: 0.02817\n",
      "[41,    30] loss: 0.02459\n",
      "[42,    10] loss: 0.02110\n",
      "[42,    20] loss: 0.02754\n",
      "[42,    30] loss: 0.02395\n",
      "[43,    10] loss: 0.02081\n",
      "[43,    20] loss: 0.02697\n",
      "[43,    30] loss: 0.02312\n",
      "[44,    10] loss: 0.02011\n",
      "[44,    20] loss: 0.02547\n",
      "[44,    30] loss: 0.02182\n",
      "[45,    10] loss: 0.01844\n",
      "[45,    20] loss: 0.02386\n",
      "[45,    30] loss: 0.02190\n",
      "[46,    10] loss: 0.01798\n",
      "[46,    20] loss: 0.02260\n",
      "[46,    30] loss: 0.02117\n",
      "[47,    10] loss: 0.01723\n",
      "[47,    20] loss: 0.02221\n",
      "[47,    30] loss: 0.02069\n",
      "[48,    10] loss: 0.01644\n",
      "[48,    20] loss: 0.02162\n",
      "[48,    30] loss: 0.02014\n",
      "[49,    10] loss: 0.01599\n",
      "[49,    20] loss: 0.02108\n",
      "[49,    30] loss: 0.01975\n",
      "[50,    10] loss: 0.01568\n",
      "[50,    20] loss: 0.02054\n",
      "[50,    30] loss: 0.01941\n",
      "[51,    10] loss: 0.01544\n",
      "[51,    20] loss: 0.02006\n",
      "[51,    30] loss: 0.01912\n",
      "[52,    10] loss: 0.01522\n",
      "[52,    20] loss: 0.01966\n",
      "[52,    30] loss: 0.01883\n",
      "[53,    10] loss: 0.01496\n",
      "[53,    20] loss: 0.01939\n",
      "[53,    30] loss: 0.01843\n",
      "[54,    10] loss: 0.01473\n",
      "[54,    20] loss: 0.01870\n",
      "[54,    30] loss: 0.01890\n",
      "[55,    10] loss: 0.01502\n",
      "[55,    20] loss: 0.01804\n",
      "[55,    30] loss: 0.01902\n",
      "[56,    10] loss: 0.01489\n",
      "[56,    20] loss: 0.01818\n",
      "[56,    30] loss: 0.01805\n",
      "[57,    10] loss: 0.01431\n",
      "[57,    20] loss: 0.01758\n",
      "[57,    30] loss: 0.01745\n",
      "[58,    10] loss: 0.01394\n",
      "[58,    20] loss: 0.01711\n",
      "[58,    30] loss: 0.01698\n",
      "[59,    10] loss: 0.01348\n",
      "[59,    20] loss: 0.01661\n",
      "[59,    30] loss: 0.01662\n",
      "[60,    10] loss: 0.01303\n",
      "[60,    20] loss: 0.01616\n",
      "[60,    30] loss: 0.01625\n",
      "[61,    10] loss: 0.01269\n",
      "[61,    20] loss: 0.01579\n",
      "[61,    30] loss: 0.01591\n",
      "[62,    10] loss: 0.01243\n",
      "[62,    20] loss: 0.01533\n",
      "[62,    30] loss: 0.01567\n",
      "[63,    10] loss: 0.01226\n",
      "[63,    20] loss: 0.01500\n",
      "[63,    30] loss: 0.01554\n",
      "[64,    10] loss: 0.01204\n",
      "[64,    20] loss: 0.01477\n",
      "[64,    30] loss: 0.01550\n",
      "[65,    10] loss: 0.01195\n",
      "[65,    20] loss: 0.01457\n",
      "[65,    30] loss: 0.01536\n",
      "[66,    10] loss: 0.01187\n",
      "[66,    20] loss: 0.01433\n",
      "[66,    30] loss: 0.01521\n",
      "[67,    10] loss: 0.01177\n",
      "[67,    20] loss: 0.01411\n",
      "[67,    30] loss: 0.01506\n",
      "[68,    10] loss: 0.01168\n",
      "[68,    20] loss: 0.01388\n",
      "[68,    30] loss: 0.01493\n",
      "[69,    10] loss: 0.01160\n",
      "[69,    20] loss: 0.01365\n",
      "[69,    30] loss: 0.01482\n",
      "[70,    10] loss: 0.01153\n",
      "[70,    20] loss: 0.01341\n",
      "[70,    30] loss: 0.01473\n",
      "[71,    10] loss: 0.01146\n",
      "[71,    20] loss: 0.01315\n",
      "[71,    30] loss: 0.01467\n",
      "[72,    10] loss: 0.01139\n",
      "[72,    20] loss: 0.01295\n",
      "[72,    30] loss: 0.01463\n",
      "[73,    10] loss: 0.01131\n",
      "[73,    20] loss: 0.01283\n",
      "[73,    30] loss: 0.01453\n",
      "[74,    10] loss: 0.01122\n",
      "[74,    20] loss: 0.01267\n",
      "[74,    30] loss: 0.01441\n",
      "[75,    10] loss: 0.01111\n",
      "[75,    20] loss: 0.01250\n",
      "[75,    30] loss: 0.01426\n",
      "[76,    10] loss: 0.01104\n",
      "[76,    20] loss: 0.01231\n",
      "[76,    30] loss: 0.01410\n",
      "[77,    10] loss: 0.01097\n",
      "[77,    20] loss: 0.01214\n",
      "[77,    30] loss: 0.01392\n",
      "[78,    10] loss: 0.01087\n",
      "[78,    20] loss: 0.01199\n",
      "[78,    30] loss: 0.01371\n",
      "[79,    10] loss: 0.01079\n",
      "[79,    20] loss: 0.01186\n",
      "[79,    30] loss: 0.01349\n",
      "[80,    10] loss: 0.01071\n",
      "[80,    20] loss: 0.01173\n",
      "[80,    30] loss: 0.01324\n",
      "[81,    10] loss: 0.01066\n",
      "[81,    20] loss: 0.01158\n",
      "[81,    30] loss: 0.01294\n",
      "[82,    10] loss: 0.01061\n",
      "[82,    20] loss: 0.01162\n",
      "[82,    30] loss: 0.01272\n",
      "[83,    10] loss: 0.01047\n",
      "[83,    20] loss: 0.01160\n",
      "[83,    30] loss: 0.01265\n",
      "[84,    10] loss: 0.01047\n",
      "[84,    20] loss: 0.01148\n",
      "[84,    30] loss: 0.01254\n",
      "[85,    10] loss: 0.01042\n",
      "[85,    20] loss: 0.01139\n",
      "[85,    30] loss: 0.01244\n",
      "[86,    10] loss: 0.01040\n",
      "[86,    20] loss: 0.01129\n",
      "[86,    30] loss: 0.01234\n",
      "[87,    10] loss: 0.01036\n",
      "[87,    20] loss: 0.01120\n",
      "[87,    30] loss: 0.01224\n",
      "[88,    10] loss: 0.01032\n",
      "[88,    20] loss: 0.01111\n",
      "[88,    30] loss: 0.01214\n",
      "[89,    10] loss: 0.01028\n",
      "[89,    20] loss: 0.01103\n",
      "[89,    30] loss: 0.01204\n",
      "[90,    10] loss: 0.01023\n",
      "[90,    20] loss: 0.01094\n",
      "[90,    30] loss: 0.01195\n",
      "[91,    10] loss: 0.01018\n",
      "[91,    20] loss: 0.01086\n",
      "[91,    30] loss: 0.01185\n",
      "[92,    10] loss: 0.01012\n",
      "[92,    20] loss: 0.01078\n",
      "[92,    30] loss: 0.01176\n",
      "[93,    10] loss: 0.01006\n",
      "[93,    20] loss: 0.01070\n",
      "[93,    30] loss: 0.01166\n",
      "[94,    10] loss: 0.01001\n",
      "[94,    20] loss: 0.01063\n",
      "[94,    30] loss: 0.01157\n",
      "[95,    10] loss: 0.00996\n",
      "[95,    20] loss: 0.01056\n",
      "[95,    30] loss: 0.01148\n",
      "[96,    10] loss: 0.00991\n",
      "[96,    20] loss: 0.01048\n",
      "[96,    30] loss: 0.01139\n",
      "[97,    10] loss: 0.00987\n",
      "[97,    20] loss: 0.01041\n",
      "[97,    30] loss: 0.01131\n",
      "[98,    10] loss: 0.00983\n",
      "[98,    20] loss: 0.01033\n",
      "[98,    30] loss: 0.01124\n",
      "[99,    10] loss: 0.00980\n",
      "[99,    20] loss: 0.01026\n",
      "[99,    30] loss: 0.01117\n",
      "[100,    10] loss: 0.00977\n",
      "[100,    20] loss: 0.01018\n",
      "[100,    30] loss: 0.01111\n",
      "[101,    10] loss: 0.00974\n",
      "[101,    20] loss: 0.01011\n",
      "[101,    30] loss: 0.01106\n",
      "[102,    10] loss: 0.00971\n",
      "[102,    20] loss: 0.01003\n",
      "[102,    30] loss: 0.01101\n",
      "[103,    10] loss: 0.00967\n",
      "[103,    20] loss: 0.00995\n",
      "[103,    30] loss: 0.01096\n",
      "[104,    10] loss: 0.00963\n",
      "[104,    20] loss: 0.00986\n",
      "[104,    30] loss: 0.01092\n",
      "[105,    10] loss: 0.00960\n",
      "[105,    20] loss: 0.00974\n",
      "[105,    30] loss: 0.01083\n",
      "[106,    10] loss: 0.00956\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[106,    20] loss: 0.00962\n",
      "[106,    30] loss: 0.01069\n",
      "[107,    10] loss: 0.00948\n",
      "[107,    20] loss: 0.00951\n",
      "[107,    30] loss: 0.01059\n",
      "[108,    10] loss: 0.00944\n",
      "[108,    20] loss: 0.00944\n",
      "[108,    30] loss: 0.01039\n",
      "[109,    10] loss: 0.00908\n",
      "[109,    20] loss: 0.00934\n",
      "[109,    30] loss: 0.01042\n",
      "[110,    10] loss: 0.00934\n",
      "[110,    20] loss: 0.00920\n",
      "[110,    30] loss: 0.01018\n",
      "[111,    10] loss: 0.00928\n",
      "[111,    20] loss: 0.00924\n",
      "[111,    30] loss: 0.00995\n",
      "[112,    10] loss: 0.00872\n",
      "[112,    20] loss: 0.00893\n",
      "[112,    30] loss: 0.00990\n",
      "[113,    10] loss: 0.00892\n",
      "[113,    20] loss: 0.00897\n",
      "[113,    30] loss: 0.00962\n",
      "[114,    10] loss: 0.00883\n",
      "[114,    20] loss: 0.00898\n",
      "[114,    30] loss: 0.00954\n",
      "[115,    10] loss: 0.00850\n",
      "[115,    20] loss: 0.00870\n",
      "[115,    30] loss: 0.00922\n",
      "[116,    10] loss: 0.00814\n",
      "[116,    20] loss: 0.00845\n",
      "[116,    30] loss: 0.00886\n",
      "[117,    10] loss: 0.00784\n",
      "[117,    20] loss: 0.00823\n",
      "[117,    30] loss: 0.00853\n",
      "[118,    10] loss: 0.00759\n",
      "[118,    20] loss: 0.00803\n",
      "[118,    30] loss: 0.00819\n",
      "[119,    10] loss: 0.00736\n",
      "[119,    20] loss: 0.00782\n",
      "[119,    30] loss: 0.00784\n",
      "[120,    10] loss: 0.00714\n",
      "[120,    20] loss: 0.00760\n",
      "[120,    30] loss: 0.00749\n",
      "[121,    10] loss: 0.00693\n",
      "[121,    20] loss: 0.00736\n",
      "[121,    30] loss: 0.00713\n",
      "[122,    10] loss: 0.00672\n",
      "[122,    20] loss: 0.00709\n",
      "[122,    30] loss: 0.00677\n",
      "[123,    10] loss: 0.00652\n",
      "[123,    20] loss: 0.00678\n",
      "[123,    30] loss: 0.00641\n",
      "[124,    10] loss: 0.00633\n",
      "[124,    20] loss: 0.00645\n",
      "[124,    30] loss: 0.00608\n",
      "[125,    10] loss: 0.00617\n",
      "[125,    20] loss: 0.00613\n",
      "[125,    30] loss: 0.00581\n",
      "[126,    10] loss: 0.00599\n",
      "[126,    20] loss: 0.00584\n",
      "[126,    30] loss: 0.00561\n",
      "[127,    10] loss: 0.00578\n",
      "[127,    20] loss: 0.00555\n",
      "[127,    30] loss: 0.00525\n",
      "[128,    10] loss: 0.00520\n",
      "[128,    20] loss: 0.00530\n",
      "[128,    30] loss: 0.00512\n",
      "[129,    10] loss: 0.00497\n",
      "[129,    20] loss: 0.00534\n",
      "[129,    30] loss: 0.00505\n",
      "[130,    10] loss: 0.00497\n",
      "[130,    20] loss: 0.00486\n",
      "[130,    30] loss: 0.00472\n",
      "[131,    10] loss: 0.00475\n",
      "[131,    20] loss: 0.00474\n",
      "[131,    30] loss: 0.00449\n",
      "[132,    10] loss: 0.00465\n",
      "[132,    20] loss: 0.00448\n",
      "[132,    30] loss: 0.00426\n",
      "[133,    10] loss: 0.00451\n",
      "[133,    20] loss: 0.00431\n",
      "[133,    30] loss: 0.00403\n",
      "[134,    10] loss: 0.00440\n",
      "[134,    20] loss: 0.00419\n",
      "[134,    30] loss: 0.00388\n",
      "[135,    10] loss: 0.00428\n",
      "[135,    20] loss: 0.00403\n",
      "[135,    30] loss: 0.00367\n",
      "[136,    10] loss: 0.00413\n",
      "[136,    20] loss: 0.00393\n",
      "[136,    30] loss: 0.00355\n",
      "[137,    10] loss: 0.00400\n",
      "[137,    20] loss: 0.00380\n",
      "[137,    30] loss: 0.00337\n",
      "[138,    10] loss: 0.00386\n",
      "[138,    20] loss: 0.00371\n",
      "[138,    30] loss: 0.00327\n",
      "[139,    10] loss: 0.00375\n",
      "[139,    20] loss: 0.00361\n",
      "[139,    30] loss: 0.00315\n",
      "[140,    10] loss: 0.00366\n",
      "[140,    20] loss: 0.00354\n",
      "[140,    30] loss: 0.00308\n",
      "[141,    10] loss: 0.00360\n",
      "[141,    20] loss: 0.00347\n",
      "[141,    30] loss: 0.00300\n",
      "[142,    10] loss: 0.00354\n",
      "[142,    20] loss: 0.00343\n",
      "[142,    30] loss: 0.00295\n",
      "[143,    10] loss: 0.00350\n",
      "[143,    20] loss: 0.00339\n",
      "[143,    30] loss: 0.00289\n",
      "[144,    10] loss: 0.00346\n",
      "[144,    20] loss: 0.00335\n",
      "[144,    30] loss: 0.00284\n",
      "[145,    10] loss: 0.00343\n",
      "[145,    20] loss: 0.00333\n",
      "[145,    30] loss: 0.00279\n",
      "[146,    10] loss: 0.00340\n",
      "[146,    20] loss: 0.00330\n",
      "[146,    30] loss: 0.00275\n",
      "[147,    10] loss: 0.00337\n",
      "[147,    20] loss: 0.00329\n",
      "[147,    30] loss: 0.00270\n",
      "[148,    10] loss: 0.00335\n",
      "[148,    20] loss: 0.00327\n",
      "[148,    30] loss: 0.00266\n",
      "[149,    10] loss: 0.00333\n",
      "[149,    20] loss: 0.00326\n",
      "[149,    30] loss: 0.00261\n",
      "[150,    10] loss: 0.00332\n",
      "[150,    20] loss: 0.00325\n",
      "[150,    30] loss: 0.00257\n",
      "[151,    10] loss: 0.00330\n",
      "[151,    20] loss: 0.00324\n",
      "[151,    30] loss: 0.00253\n",
      "[152,    10] loss: 0.00329\n",
      "[152,    20] loss: 0.00322\n",
      "[152,    30] loss: 0.00249\n",
      "[153,    10] loss: 0.00328\n",
      "[153,    20] loss: 0.00321\n",
      "[153,    30] loss: 0.00245\n",
      "[154,    10] loss: 0.00327\n",
      "[154,    20] loss: 0.00320\n",
      "[154,    30] loss: 0.00242\n",
      "[155,    10] loss: 0.00326\n",
      "[155,    20] loss: 0.00319\n",
      "[155,    30] loss: 0.00239\n",
      "[156,    10] loss: 0.00325\n",
      "[156,    20] loss: 0.00318\n",
      "[156,    30] loss: 0.00235\n",
      "[157,    10] loss: 0.00325\n",
      "[157,    20] loss: 0.00316\n",
      "[157,    30] loss: 0.00232\n",
      "[158,    10] loss: 0.00324\n",
      "[158,    20] loss: 0.00315\n",
      "[158,    30] loss: 0.00230\n",
      "[159,    10] loss: 0.00323\n",
      "[159,    20] loss: 0.00313\n",
      "[159,    30] loss: 0.00227\n",
      "[160,    10] loss: 0.00322\n",
      "[160,    20] loss: 0.00312\n",
      "[160,    30] loss: 0.00224\n",
      "[161,    10] loss: 0.00322\n",
      "[161,    20] loss: 0.00310\n",
      "[161,    30] loss: 0.00222\n",
      "[162,    10] loss: 0.00321\n",
      "[162,    20] loss: 0.00309\n",
      "[162,    30] loss: 0.00220\n",
      "[163,    10] loss: 0.00320\n",
      "[163,    20] loss: 0.00307\n",
      "[163,    30] loss: 0.00218\n",
      "[164,    10] loss: 0.00319\n",
      "[164,    20] loss: 0.00306\n",
      "[164,    30] loss: 0.00216\n",
      "[165,    10] loss: 0.00318\n",
      "[165,    20] loss: 0.00304\n",
      "[165,    30] loss: 0.00214\n",
      "[166,    10] loss: 0.00318\n",
      "[166,    20] loss: 0.00303\n",
      "[166,    30] loss: 0.00212\n",
      "[167,    10] loss: 0.00317\n",
      "[167,    20] loss: 0.00302\n",
      "[167,    30] loss: 0.00210\n",
      "[168,    10] loss: 0.00316\n",
      "[168,    20] loss: 0.00300\n",
      "[168,    30] loss: 0.00208\n",
      "[169,    10] loss: 0.00315\n",
      "[169,    20] loss: 0.00299\n",
      "[169,    30] loss: 0.00207\n",
      "[170,    10] loss: 0.00315\n",
      "[170,    20] loss: 0.00298\n",
      "[170,    30] loss: 0.00205\n",
      "[171,    10] loss: 0.00314\n",
      "[171,    20] loss: 0.00297\n",
      "[171,    30] loss: 0.00203\n",
      "[172,    10] loss: 0.00313\n",
      "[172,    20] loss: 0.00295\n",
      "[172,    30] loss: 0.00202\n",
      "[173,    10] loss: 0.00312\n",
      "[173,    20] loss: 0.00294\n",
      "[173,    30] loss: 0.00200\n",
      "[174,    10] loss: 0.00311\n",
      "[174,    20] loss: 0.00293\n",
      "[174,    30] loss: 0.00199\n",
      "[175,    10] loss: 0.00310\n",
      "[175,    20] loss: 0.00291\n",
      "[175,    30] loss: 0.00197\n",
      "[176,    10] loss: 0.00308\n",
      "[176,    20] loss: 0.00290\n",
      "[176,    30] loss: 0.00195\n",
      "[177,    10] loss: 0.00307\n",
      "[177,    20] loss: 0.00289\n",
      "[177,    30] loss: 0.00194\n",
      "[178,    10] loss: 0.00305\n",
      "[178,    20] loss: 0.00287\n",
      "[178,    30] loss: 0.00192\n",
      "[179,    10] loss: 0.00303\n",
      "[179,    20] loss: 0.00286\n",
      "[179,    30] loss: 0.00191\n",
      "[180,    10] loss: 0.00301\n",
      "[180,    20] loss: 0.00284\n",
      "[180,    30] loss: 0.00189\n",
      "[181,    10] loss: 0.00299\n",
      "[181,    20] loss: 0.00283\n",
      "[181,    30] loss: 0.00187\n",
      "[182,    10] loss: 0.00297\n",
      "[182,    20] loss: 0.00282\n",
      "[182,    30] loss: 0.00186\n",
      "[183,    10] loss: 0.00295\n",
      "[183,    20] loss: 0.00281\n",
      "[183,    30] loss: 0.00184\n",
      "[184,    10] loss: 0.00293\n",
      "[184,    20] loss: 0.00281\n",
      "[184,    30] loss: 0.00183\n",
      "[185,    10] loss: 0.00291\n",
      "[185,    20] loss: 0.00281\n",
      "[185,    30] loss: 0.00181\n",
      "[186,    10] loss: 0.00289\n",
      "[186,    20] loss: 0.00284\n",
      "[186,    30] loss: 0.00180\n",
      "[187,    10] loss: 0.00291\n",
      "[187,    20] loss: 0.00306\n",
      "[187,    30] loss: 0.00180\n",
      "[188,    10] loss: 0.00310\n",
      "[188,    20] loss: 0.00347\n",
      "[188,    30] loss: 0.00187\n",
      "[189,    10] loss: 0.00312\n",
      "[189,    20] loss: 0.00348\n",
      "[189,    30] loss: 0.00193\n",
      "[190,    10] loss: 0.00313\n",
      "[190,    20] loss: 0.00347\n",
      "[190,    30] loss: 0.00204\n",
      "[191,    10] loss: 0.00307\n",
      "[191,    20] loss: 0.00336\n",
      "[191,    30] loss: 0.00211\n",
      "[192,    10] loss: 0.00299\n",
      "[192,    20] loss: 0.00320\n",
      "[192,    30] loss: 0.00212\n",
      "[193,    10] loss: 0.00290\n",
      "[193,    20] loss: 0.00304\n",
      "[193,    30] loss: 0.00208\n",
      "[194,    10] loss: 0.00281\n",
      "[194,    20] loss: 0.00291\n",
      "[194,    30] loss: 0.00202\n",
      "[195,    10] loss: 0.00275\n",
      "[195,    20] loss: 0.00283\n",
      "[195,    30] loss: 0.00197\n",
      "[196,    10] loss: 0.00270\n",
      "[196,    20] loss: 0.00278\n",
      "[196,    30] loss: 0.00194\n",
      "[197,    10] loss: 0.00266\n",
      "[197,    20] loss: 0.00275\n",
      "[197,    30] loss: 0.00191\n",
      "[198,    10] loss: 0.00263\n",
      "[198,    20] loss: 0.00272\n",
      "[198,    30] loss: 0.00188\n",
      "[199,    10] loss: 0.00260\n",
      "[199,    20] loss: 0.00270\n",
      "[199,    30] loss: 0.00186\n",
      "[200,    10] loss: 0.00258\n",
      "[200,    20] loss: 0.00268\n",
      "[200,    30] loss: 0.00184\n",
      "[201,    10] loss: 0.00255\n",
      "[201,    20] loss: 0.00266\n",
      "[201,    30] loss: 0.00182\n",
      "[202,    10] loss: 0.00254\n",
      "[202,    20] loss: 0.00265\n",
      "[202,    30] loss: 0.00180\n",
      "[203,    10] loss: 0.00252\n",
      "[203,    20] loss: 0.00264\n",
      "[203,    30] loss: 0.00179\n",
      "[204,    10] loss: 0.00250\n",
      "[204,    20] loss: 0.00263\n",
      "[204,    30] loss: 0.00177\n",
      "[205,    10] loss: 0.00249\n",
      "[205,    20] loss: 0.00262\n",
      "[205,    30] loss: 0.00175\n",
      "[206,    10] loss: 0.00247\n",
      "[206,    20] loss: 0.00262\n",
      "[206,    30] loss: 0.00173\n",
      "[207,    10] loss: 0.00246\n",
      "[207,    20] loss: 0.00261\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[207,    30] loss: 0.00172\n",
      "[208,    10] loss: 0.00245\n",
      "[208,    20] loss: 0.00260\n",
      "[208,    30] loss: 0.00170\n",
      "[209,    10] loss: 0.00243\n",
      "[209,    20] loss: 0.00260\n",
      "[209,    30] loss: 0.00169\n",
      "[210,    10] loss: 0.00242\n",
      "[210,    20] loss: 0.00259\n",
      "[210,    30] loss: 0.00167\n",
      "[211,    10] loss: 0.00241\n",
      "[211,    20] loss: 0.00259\n",
      "[211,    30] loss: 0.00166\n",
      "[212,    10] loss: 0.00240\n",
      "[212,    20] loss: 0.00259\n",
      "[212,    30] loss: 0.00165\n",
      "[213,    10] loss: 0.00239\n",
      "[213,    20] loss: 0.00258\n",
      "[213,    30] loss: 0.00163\n",
      "[214,    10] loss: 0.00238\n",
      "[214,    20] loss: 0.00258\n",
      "[214,    30] loss: 0.00162\n",
      "[215,    10] loss: 0.00237\n",
      "[215,    20] loss: 0.00258\n",
      "[215,    30] loss: 0.00161\n",
      "[216,    10] loss: 0.00236\n",
      "[216,    20] loss: 0.00258\n",
      "[216,    30] loss: 0.00159\n",
      "[217,    10] loss: 0.00235\n",
      "[217,    20] loss: 0.00257\n",
      "[217,    30] loss: 0.00158\n",
      "[218,    10] loss: 0.00234\n",
      "[218,    20] loss: 0.00257\n",
      "[218,    30] loss: 0.00157\n",
      "[219,    10] loss: 0.00233\n",
      "[219,    20] loss: 0.00257\n",
      "[219,    30] loss: 0.00156\n",
      "[220,    10] loss: 0.00232\n",
      "[220,    20] loss: 0.00257\n",
      "[220,    30] loss: 0.00155\n",
      "[221,    10] loss: 0.00231\n",
      "[221,    20] loss: 0.00256\n",
      "[221,    30] loss: 0.00154\n",
      "[222,    10] loss: 0.00230\n",
      "[222,    20] loss: 0.00256\n",
      "[222,    30] loss: 0.00152\n",
      "[223,    10] loss: 0.00229\n",
      "[223,    20] loss: 0.00256\n",
      "[223,    30] loss: 0.00151\n",
      "[224,    10] loss: 0.00228\n",
      "[224,    20] loss: 0.00256\n",
      "[224,    30] loss: 0.00150\n",
      "[225,    10] loss: 0.00227\n",
      "[225,    20] loss: 0.00256\n",
      "[225,    30] loss: 0.00149\n",
      "[226,    10] loss: 0.00227\n",
      "[226,    20] loss: 0.00255\n",
      "[226,    30] loss: 0.00149\n",
      "[227,    10] loss: 0.00226\n",
      "[227,    20] loss: 0.00255\n",
      "[227,    30] loss: 0.00148\n",
      "[228,    10] loss: 0.00225\n",
      "[228,    20] loss: 0.00255\n",
      "[228,    30] loss: 0.00147\n",
      "[229,    10] loss: 0.00224\n",
      "[229,    20] loss: 0.00255\n",
      "[229,    30] loss: 0.00146\n",
      "[230,    10] loss: 0.00224\n",
      "[230,    20] loss: 0.00254\n",
      "[230,    30] loss: 0.00145\n",
      "[231,    10] loss: 0.00223\n",
      "[231,    20] loss: 0.00254\n",
      "[231,    30] loss: 0.00144\n",
      "[232,    10] loss: 0.00222\n",
      "[232,    20] loss: 0.00254\n",
      "[232,    30] loss: 0.00143\n",
      "[233,    10] loss: 0.00222\n",
      "[233,    20] loss: 0.00254\n",
      "[233,    30] loss: 0.00143\n",
      "[234,    10] loss: 0.00221\n",
      "[234,    20] loss: 0.00253\n",
      "[234,    30] loss: 0.00142\n",
      "[235,    10] loss: 0.00221\n",
      "[235,    20] loss: 0.00253\n",
      "[235,    30] loss: 0.00141\n",
      "[236,    10] loss: 0.00220\n",
      "[236,    20] loss: 0.00252\n",
      "[236,    30] loss: 0.00140\n",
      "[237,    10] loss: 0.00220\n",
      "[237,    20] loss: 0.00252\n",
      "[237,    30] loss: 0.00139\n",
      "[238,    10] loss: 0.00219\n",
      "[238,    20] loss: 0.00251\n",
      "[238,    30] loss: 0.00139\n",
      "[239,    10] loss: 0.00219\n",
      "[239,    20] loss: 0.00251\n",
      "[239,    30] loss: 0.00138\n",
      "[240,    10] loss: 0.00218\n",
      "[240,    20] loss: 0.00250\n",
      "[240,    30] loss: 0.00137\n",
      "[241,    10] loss: 0.00217\n",
      "[241,    20] loss: 0.00250\n",
      "[241,    30] loss: 0.00136\n",
      "[242,    10] loss: 0.00217\n",
      "[242,    20] loss: 0.00249\n",
      "[242,    30] loss: 0.00135\n",
      "[243,    10] loss: 0.00216\n",
      "[243,    20] loss: 0.00248\n",
      "[243,    30] loss: 0.00134\n",
      "[244,    10] loss: 0.00215\n",
      "[244,    20] loss: 0.00247\n",
      "[244,    30] loss: 0.00132\n",
      "[245,    10] loss: 0.00214\n",
      "[245,    20] loss: 0.00245\n",
      "[245,    30] loss: 0.00131\n",
      "[246,    10] loss: 0.00213\n",
      "[246,    20] loss: 0.00243\n",
      "[246,    30] loss: 0.00129\n",
      "[247,    10] loss: 0.00211\n",
      "[247,    20] loss: 0.00242\n",
      "[247,    30] loss: 0.00127\n",
      "[248,    10] loss: 0.00208\n",
      "[248,    20] loss: 0.00240\n",
      "[248,    30] loss: 0.00125\n",
      "[249,    10] loss: 0.00205\n",
      "[249,    20] loss: 0.00239\n",
      "[249,    30] loss: 0.00125\n",
      "[250,    10] loss: 0.00201\n",
      "[250,    20] loss: 0.00236\n",
      "[250,    30] loss: 0.00126\n",
      "[251,    10] loss: 0.00198\n",
      "[251,    20] loss: 0.00227\n",
      "[251,    30] loss: 0.00124\n",
      "[252,    10] loss: 0.00196\n",
      "[252,    20] loss: 0.00223\n",
      "[252,    30] loss: 0.00122\n",
      "[253,    10] loss: 0.00194\n",
      "[253,    20] loss: 0.00221\n",
      "[253,    30] loss: 0.00120\n",
      "[254,    10] loss: 0.00192\n",
      "[254,    20] loss: 0.00220\n",
      "[254,    30] loss: 0.00119\n",
      "[255,    10] loss: 0.00191\n",
      "[255,    20] loss: 0.00220\n",
      "[255,    30] loss: 0.00118\n",
      "[256,    10] loss: 0.00190\n",
      "[256,    20] loss: 0.00220\n",
      "[256,    30] loss: 0.00117\n",
      "[257,    10] loss: 0.00189\n",
      "[257,    20] loss: 0.00220\n",
      "[257,    30] loss: 0.00117\n",
      "[258,    10] loss: 0.00189\n",
      "[258,    20] loss: 0.00220\n",
      "[258,    30] loss: 0.00117\n",
      "[259,    10] loss: 0.00188\n",
      "[259,    20] loss: 0.00220\n",
      "[259,    30] loss: 0.00116\n",
      "[260,    10] loss: 0.00188\n",
      "[260,    20] loss: 0.00220\n",
      "[260,    30] loss: 0.00116\n",
      "[261,    10] loss: 0.00187\n",
      "[261,    20] loss: 0.00221\n",
      "[261,    30] loss: 0.00116\n",
      "[262,    10] loss: 0.00187\n",
      "[262,    20] loss: 0.00221\n",
      "[262,    30] loss: 0.00117\n",
      "[263,    10] loss: 0.00186\n",
      "[263,    20] loss: 0.00221\n",
      "[263,    30] loss: 0.00117\n",
      "[264,    10] loss: 0.00186\n",
      "[264,    20] loss: 0.00220\n",
      "[264,    30] loss: 0.00117\n",
      "[265,    10] loss: 0.00185\n",
      "[265,    20] loss: 0.00220\n",
      "[265,    30] loss: 0.00117\n",
      "[266,    10] loss: 0.00185\n",
      "[266,    20] loss: 0.00219\n",
      "[266,    30] loss: 0.00116\n",
      "[267,    10] loss: 0.00184\n",
      "[267,    20] loss: 0.00218\n",
      "[267,    30] loss: 0.00116\n",
      "[268,    10] loss: 0.00183\n",
      "[268,    20] loss: 0.00217\n",
      "[268,    30] loss: 0.00116\n",
      "[269,    10] loss: 0.00182\n",
      "[269,    20] loss: 0.00216\n",
      "[269,    30] loss: 0.00115\n",
      "[270,    10] loss: 0.00181\n",
      "[270,    20] loss: 0.00214\n",
      "[270,    30] loss: 0.00115\n",
      "[271,    10] loss: 0.00180\n",
      "[271,    20] loss: 0.00213\n",
      "[271,    30] loss: 0.00114\n",
      "[272,    10] loss: 0.00179\n",
      "[272,    20] loss: 0.00212\n",
      "[272,    30] loss: 0.00114\n",
      "[273,    10] loss: 0.00178\n",
      "[273,    20] loss: 0.00211\n",
      "[273,    30] loss: 0.00113\n",
      "[274,    10] loss: 0.00177\n",
      "[274,    20] loss: 0.00209\n",
      "[274,    30] loss: 0.00112\n",
      "[275,    10] loss: 0.00177\n",
      "[275,    20] loss: 0.00208\n",
      "[275,    30] loss: 0.00112\n",
      "[276,    10] loss: 0.00176\n",
      "[276,    20] loss: 0.00207\n",
      "[276,    30] loss: 0.00111\n",
      "[277,    10] loss: 0.00175\n",
      "[277,    20] loss: 0.00206\n",
      "[277,    30] loss: 0.00111\n",
      "[278,    10] loss: 0.00175\n",
      "[278,    20] loss: 0.00206\n",
      "[278,    30] loss: 0.00111\n",
      "[279,    10] loss: 0.00174\n",
      "[279,    20] loss: 0.00205\n",
      "[279,    30] loss: 0.00111\n",
      "[280,    10] loss: 0.00175\n",
      "[280,    20] loss: 0.00205\n",
      "[280,    30] loss: 0.00111\n",
      "[281,    10] loss: 0.00175\n",
      "[281,    20] loss: 0.00205\n",
      "[281,    30] loss: 0.00111\n",
      "[282,    10] loss: 0.00175\n",
      "[282,    20] loss: 0.00204\n",
      "[282,    30] loss: 0.00111\n",
      "[283,    10] loss: 0.00175\n",
      "[283,    20] loss: 0.00203\n",
      "[283,    30] loss: 0.00111\n",
      "[284,    10] loss: 0.00175\n",
      "[284,    20] loss: 0.00203\n",
      "[284,    30] loss: 0.00111\n",
      "[285,    10] loss: 0.00175\n",
      "[285,    20] loss: 0.00202\n",
      "[285,    30] loss: 0.00111\n",
      "[286,    10] loss: 0.00174\n",
      "[286,    20] loss: 0.00201\n",
      "[286,    30] loss: 0.00110\n",
      "[287,    10] loss: 0.00174\n",
      "[287,    20] loss: 0.00200\n",
      "[287,    30] loss: 0.00110\n",
      "[288,    10] loss: 0.00174\n",
      "[288,    20] loss: 0.00200\n",
      "[288,    30] loss: 0.00110\n",
      "[289,    10] loss: 0.00174\n",
      "[289,    20] loss: 0.00199\n",
      "[289,    30] loss: 0.00110\n",
      "[290,    10] loss: 0.00174\n",
      "[290,    20] loss: 0.00198\n",
      "[290,    30] loss: 0.00110\n",
      "[291,    10] loss: 0.00173\n",
      "[291,    20] loss: 0.00198\n",
      "[291,    30] loss: 0.00110\n",
      "[292,    10] loss: 0.00173\n",
      "[292,    20] loss: 0.00197\n",
      "[292,    30] loss: 0.00110\n",
      "[293,    10] loss: 0.00173\n",
      "[293,    20] loss: 0.00196\n",
      "[293,    30] loss: 0.00110\n",
      "[294,    10] loss: 0.00173\n",
      "[294,    20] loss: 0.00196\n",
      "[294,    30] loss: 0.00110\n",
      "[295,    10] loss: 0.00173\n",
      "[295,    20] loss: 0.00195\n",
      "[295,    30] loss: 0.00110\n",
      "[296,    10] loss: 0.00173\n",
      "[296,    20] loss: 0.00195\n",
      "[296,    30] loss: 0.00110\n",
      "[297,    10] loss: 0.00173\n",
      "[297,    20] loss: 0.00194\n",
      "[297,    30] loss: 0.00109\n",
      "[298,    10] loss: 0.00172\n",
      "[298,    20] loss: 0.00194\n",
      "[298,    30] loss: 0.00109\n",
      "[299,    10] loss: 0.00172\n",
      "[299,    20] loss: 0.00193\n",
      "[299,    30] loss: 0.00109\n",
      "[300,    10] loss: 0.00172\n",
      "[300,    20] loss: 0.00193\n",
      "[300,    30] loss: 0.00109\n",
      "[301,    10] loss: 0.00172\n",
      "[301,    20] loss: 0.00193\n",
      "[301,    30] loss: 0.00109\n",
      "[302,    10] loss: 0.00172\n",
      "[302,    20] loss: 0.00192\n",
      "[302,    30] loss: 0.00109\n",
      "[303,    10] loss: 0.00172\n",
      "[303,    20] loss: 0.00192\n",
      "[303,    30] loss: 0.00109\n",
      "[304,    10] loss: 0.00172\n",
      "[304,    20] loss: 0.00191\n",
      "[304,    30] loss: 0.00109\n",
      "[305,    10] loss: 0.00172\n",
      "[305,    20] loss: 0.00191\n",
      "[305,    30] loss: 0.00109\n",
      "[306,    10] loss: 0.00172\n",
      "[306,    20] loss: 0.00191\n",
      "[306,    30] loss: 0.00109\n",
      "[307,    10] loss: 0.00171\n",
      "[307,    20] loss: 0.00190\n",
      "[307,    30] loss: 0.00109\n",
      "[308,    10] loss: 0.00171\n",
      "[308,    20] loss: 0.00190\n",
      "[308,    30] loss: 0.00109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[309,    10] loss: 0.00171\n",
      "[309,    20] loss: 0.00190\n",
      "[309,    30] loss: 0.00109\n",
      "[310,    10] loss: 0.00171\n",
      "[310,    20] loss: 0.00189\n",
      "[310,    30] loss: 0.00109\n",
      "[311,    10] loss: 0.00171\n",
      "[311,    20] loss: 0.00189\n",
      "[311,    30] loss: 0.00109\n",
      "[312,    10] loss: 0.00171\n",
      "[312,    20] loss: 0.00189\n",
      "[312,    30] loss: 0.00109\n",
      "[313,    10] loss: 0.00171\n",
      "[313,    20] loss: 0.00188\n",
      "[313,    30] loss: 0.00109\n",
      "[314,    10] loss: 0.00171\n",
      "[314,    20] loss: 0.00188\n",
      "[314,    30] loss: 0.00109\n",
      "[315,    10] loss: 0.00171\n",
      "[315,    20] loss: 0.00188\n",
      "[315,    30] loss: 0.00109\n",
      "[316,    10] loss: 0.00170\n",
      "[316,    20] loss: 0.00187\n",
      "[316,    30] loss: 0.00109\n",
      "[317,    10] loss: 0.00170\n",
      "[317,    20] loss: 0.00187\n",
      "[317,    30] loss: 0.00109\n",
      "[318,    10] loss: 0.00170\n",
      "[318,    20] loss: 0.00187\n",
      "[318,    30] loss: 0.00108\n",
      "[319,    10] loss: 0.00170\n",
      "[319,    20] loss: 0.00186\n",
      "[319,    30] loss: 0.00108\n",
      "[320,    10] loss: 0.00170\n",
      "[320,    20] loss: 0.00186\n",
      "[320,    30] loss: 0.00108\n",
      "[321,    10] loss: 0.00170\n",
      "[321,    20] loss: 0.00186\n",
      "[321,    30] loss: 0.00108\n",
      "[322,    10] loss: 0.00170\n",
      "[322,    20] loss: 0.00185\n",
      "[322,    30] loss: 0.00108\n",
      "[323,    10] loss: 0.00170\n",
      "[323,    20] loss: 0.00185\n",
      "[323,    30] loss: 0.00108\n",
      "[324,    10] loss: 0.00170\n",
      "[324,    20] loss: 0.00185\n",
      "[324,    30] loss: 0.00108\n",
      "[325,    10] loss: 0.00169\n",
      "[325,    20] loss: 0.00184\n",
      "[325,    30] loss: 0.00108\n",
      "[326,    10] loss: 0.00169\n",
      "[326,    20] loss: 0.00184\n",
      "[326,    30] loss: 0.00108\n",
      "[327,    10] loss: 0.00169\n",
      "[327,    20] loss: 0.00184\n",
      "[327,    30] loss: 0.00108\n",
      "[328,    10] loss: 0.00169\n",
      "[328,    20] loss: 0.00184\n",
      "[328,    30] loss: 0.00108\n",
      "[329,    10] loss: 0.00169\n",
      "[329,    20] loss: 0.00183\n",
      "[329,    30] loss: 0.00108\n",
      "[330,    10] loss: 0.00169\n",
      "[330,    20] loss: 0.00183\n",
      "[330,    30] loss: 0.00108\n",
      "[331,    10] loss: 0.00169\n",
      "[331,    20] loss: 0.00183\n",
      "[331,    30] loss: 0.00108\n",
      "[332,    10] loss: 0.00169\n",
      "[332,    20] loss: 0.00182\n",
      "[332,    30] loss: 0.00108\n",
      "[333,    10] loss: 0.00169\n",
      "[333,    20] loss: 0.00182\n",
      "[333,    30] loss: 0.00108\n",
      "[334,    10] loss: 0.00168\n",
      "[334,    20] loss: 0.00182\n",
      "[334,    30] loss: 0.00108\n",
      "[335,    10] loss: 0.00168\n",
      "[335,    20] loss: 0.00182\n",
      "[335,    30] loss: 0.00108\n",
      "[336,    10] loss: 0.00168\n",
      "[336,    20] loss: 0.00181\n",
      "[336,    30] loss: 0.00108\n",
      "[337,    10] loss: 0.00168\n",
      "[337,    20] loss: 0.00181\n",
      "[337,    30] loss: 0.00108\n",
      "[338,    10] loss: 0.00168\n",
      "[338,    20] loss: 0.00181\n",
      "[338,    30] loss: 0.00108\n",
      "[339,    10] loss: 0.00168\n",
      "[339,    20] loss: 0.00181\n",
      "[339,    30] loss: 0.00108\n",
      "[340,    10] loss: 0.00168\n",
      "[340,    20] loss: 0.00180\n",
      "[340,    30] loss: 0.00108\n",
      "[341,    10] loss: 0.00168\n",
      "[341,    20] loss: 0.00180\n",
      "[341,    30] loss: 0.00108\n",
      "[342,    10] loss: 0.00168\n",
      "[342,    20] loss: 0.00180\n",
      "[342,    30] loss: 0.00108\n",
      "[343,    10] loss: 0.00167\n",
      "[343,    20] loss: 0.00180\n",
      "[343,    30] loss: 0.00107\n",
      "[344,    10] loss: 0.00167\n",
      "[344,    20] loss: 0.00179\n",
      "[344,    30] loss: 0.00107\n",
      "[345,    10] loss: 0.00167\n",
      "[345,    20] loss: 0.00179\n",
      "[345,    30] loss: 0.00107\n",
      "[346,    10] loss: 0.00167\n",
      "[346,    20] loss: 0.00179\n",
      "[346,    30] loss: 0.00107\n",
      "[347,    10] loss: 0.00167\n",
      "[347,    20] loss: 0.00179\n",
      "[347,    30] loss: 0.00107\n",
      "[348,    10] loss: 0.00167\n",
      "[348,    20] loss: 0.00178\n",
      "[348,    30] loss: 0.00107\n",
      "[349,    10] loss: 0.00167\n",
      "[349,    20] loss: 0.00178\n",
      "[349,    30] loss: 0.00107\n",
      "[350,    10] loss: 0.00167\n",
      "[350,    20] loss: 0.00178\n",
      "[350,    30] loss: 0.00107\n",
      "[351,    10] loss: 0.00167\n",
      "[351,    20] loss: 0.00178\n",
      "[351,    30] loss: 0.00107\n",
      "[352,    10] loss: 0.00167\n",
      "[352,    20] loss: 0.00177\n",
      "[352,    30] loss: 0.00107\n",
      "[353,    10] loss: 0.00166\n",
      "[353,    20] loss: 0.00177\n",
      "[353,    30] loss: 0.00107\n",
      "[354,    10] loss: 0.00166\n",
      "[354,    20] loss: 0.00177\n",
      "[354,    30] loss: 0.00107\n",
      "[355,    10] loss: 0.00166\n",
      "[355,    20] loss: 0.00177\n",
      "[355,    30] loss: 0.00107\n",
      "[356,    10] loss: 0.00166\n",
      "[356,    20] loss: 0.00177\n",
      "[356,    30] loss: 0.00107\n",
      "[357,    10] loss: 0.00166\n",
      "[357,    20] loss: 0.00176\n",
      "[357,    30] loss: 0.00107\n",
      "[358,    10] loss: 0.00166\n",
      "[358,    20] loss: 0.00176\n",
      "[358,    30] loss: 0.00107\n",
      "[359,    10] loss: 0.00166\n",
      "[359,    20] loss: 0.00176\n",
      "[359,    30] loss: 0.00107\n",
      "[360,    10] loss: 0.00166\n",
      "[360,    20] loss: 0.00176\n",
      "[360,    30] loss: 0.00107\n",
      "[361,    10] loss: 0.00166\n",
      "[361,    20] loss: 0.00175\n",
      "[361,    30] loss: 0.00107\n",
      "[362,    10] loss: 0.00166\n",
      "[362,    20] loss: 0.00175\n",
      "[362,    30] loss: 0.00107\n",
      "[363,    10] loss: 0.00166\n",
      "[363,    20] loss: 0.00175\n",
      "[363,    30] loss: 0.00107\n",
      "[364,    10] loss: 0.00165\n",
      "[364,    20] loss: 0.00175\n",
      "[364,    30] loss: 0.00107\n",
      "[365,    10] loss: 0.00165\n",
      "[365,    20] loss: 0.00174\n",
      "[365,    30] loss: 0.00106\n",
      "[366,    10] loss: 0.00165\n",
      "[366,    20] loss: 0.00174\n",
      "[366,    30] loss: 0.00106\n",
      "[367,    10] loss: 0.00165\n",
      "[367,    20] loss: 0.00174\n",
      "[367,    30] loss: 0.00106\n",
      "[368,    10] loss: 0.00165\n",
      "[368,    20] loss: 0.00174\n",
      "[368,    30] loss: 0.00106\n",
      "[369,    10] loss: 0.00165\n",
      "[369,    20] loss: 0.00174\n",
      "[369,    30] loss: 0.00106\n",
      "[370,    10] loss: 0.00165\n",
      "[370,    20] loss: 0.00173\n",
      "[370,    30] loss: 0.00106\n",
      "[371,    10] loss: 0.00165\n",
      "[371,    20] loss: 0.00173\n",
      "[371,    30] loss: 0.00106\n",
      "[372,    10] loss: 0.00165\n",
      "[372,    20] loss: 0.00173\n",
      "[372,    30] loss: 0.00106\n",
      "[373,    10] loss: 0.00165\n",
      "[373,    20] loss: 0.00173\n",
      "[373,    30] loss: 0.00106\n",
      "[374,    10] loss: 0.00165\n",
      "[374,    20] loss: 0.00172\n",
      "[374,    30] loss: 0.00106\n",
      "[375,    10] loss: 0.00165\n",
      "[375,    20] loss: 0.00172\n",
      "[375,    30] loss: 0.00106\n",
      "[376,    10] loss: 0.00165\n",
      "[376,    20] loss: 0.00172\n",
      "[376,    30] loss: 0.00106\n",
      "[377,    10] loss: 0.00165\n",
      "[377,    20] loss: 0.00172\n",
      "[377,    30] loss: 0.00106\n",
      "[378,    10] loss: 0.00164\n",
      "[378,    20] loss: 0.00171\n",
      "[378,    30] loss: 0.00106\n",
      "[379,    10] loss: 0.00164\n",
      "[379,    20] loss: 0.00171\n",
      "[379,    30] loss: 0.00106\n",
      "[380,    10] loss: 0.00164\n",
      "[380,    20] loss: 0.00171\n",
      "[380,    30] loss: 0.00106\n",
      "[381,    10] loss: 0.00164\n",
      "[381,    20] loss: 0.00171\n",
      "[381,    30] loss: 0.00106\n",
      "[382,    10] loss: 0.00164\n",
      "[382,    20] loss: 0.00171\n",
      "[382,    30] loss: 0.00106\n",
      "[383,    10] loss: 0.00164\n",
      "[383,    20] loss: 0.00170\n",
      "[383,    30] loss: 0.00106\n",
      "[384,    10] loss: 0.00164\n",
      "[384,    20] loss: 0.00170\n",
      "[384,    30] loss: 0.00106\n",
      "[385,    10] loss: 0.00164\n",
      "[385,    20] loss: 0.00170\n",
      "[385,    30] loss: 0.00105\n",
      "[386,    10] loss: 0.00164\n",
      "[386,    20] loss: 0.00170\n",
      "[386,    30] loss: 0.00105\n",
      "[387,    10] loss: 0.00164\n",
      "[387,    20] loss: 0.00169\n",
      "[387,    30] loss: 0.00105\n",
      "[388,    10] loss: 0.00164\n",
      "[388,    20] loss: 0.00169\n",
      "[388,    30] loss: 0.00105\n",
      "[389,    10] loss: 0.00164\n",
      "[389,    20] loss: 0.00169\n",
      "[389,    30] loss: 0.00105\n",
      "[390,    10] loss: 0.00164\n",
      "[390,    20] loss: 0.00169\n",
      "[390,    30] loss: 0.00105\n",
      "[391,    10] loss: 0.00164\n",
      "[391,    20] loss: 0.00168\n",
      "[391,    30] loss: 0.00105\n",
      "[392,    10] loss: 0.00164\n",
      "[392,    20] loss: 0.00168\n",
      "[392,    30] loss: 0.00105\n",
      "[393,    10] loss: 0.00164\n",
      "[393,    20] loss: 0.00168\n",
      "[393,    30] loss: 0.00105\n",
      "[394,    10] loss: 0.00164\n",
      "[394,    20] loss: 0.00168\n",
      "[394,    30] loss: 0.00105\n",
      "[395,    10] loss: 0.00164\n",
      "[395,    20] loss: 0.00167\n",
      "[395,    30] loss: 0.00105\n",
      "[396,    10] loss: 0.00164\n",
      "[396,    20] loss: 0.00167\n",
      "[396,    30] loss: 0.00105\n",
      "[397,    10] loss: 0.00163\n",
      "[397,    20] loss: 0.00167\n",
      "[397,    30] loss: 0.00105\n",
      "[398,    10] loss: 0.00163\n",
      "[398,    20] loss: 0.00167\n",
      "[398,    30] loss: 0.00105\n",
      "[399,    10] loss: 0.00163\n",
      "[399,    20] loss: 0.00166\n",
      "[399,    30] loss: 0.00105\n",
      "[400,    10] loss: 0.00163\n",
      "[400,    20] loss: 0.00166\n",
      "[400,    30] loss: 0.00105\n",
      "[401,    10] loss: 0.00163\n",
      "[401,    20] loss: 0.00166\n",
      "[401,    30] loss: 0.00105\n",
      "[402,    10] loss: 0.00163\n",
      "[402,    20] loss: 0.00165\n",
      "[402,    30] loss: 0.00105\n",
      "[403,    10] loss: 0.00163\n",
      "[403,    20] loss: 0.00165\n",
      "[403,    30] loss: 0.00105\n",
      "[404,    10] loss: 0.00163\n",
      "[404,    20] loss: 0.00165\n",
      "[404,    30] loss: 0.00105\n",
      "[405,    10] loss: 0.00163\n",
      "[405,    20] loss: 0.00165\n",
      "[405,    30] loss: 0.00104\n",
      "[406,    10] loss: 0.00163\n",
      "[406,    20] loss: 0.00164\n",
      "[406,    30] loss: 0.00104\n",
      "[407,    10] loss: 0.00163\n",
      "[407,    20] loss: 0.00164\n",
      "[407,    30] loss: 0.00104\n",
      "[408,    10] loss: 0.00163\n",
      "[408,    20] loss: 0.00164\n",
      "[408,    30] loss: 0.00104\n",
      "[409,    10] loss: 0.00163\n",
      "[409,    20] loss: 0.00164\n",
      "[409,    30] loss: 0.00104\n",
      "[410,    10] loss: 0.00163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[410,    20] loss: 0.00163\n",
      "[410,    30] loss: 0.00104\n",
      "[411,    10] loss: 0.00163\n",
      "[411,    20] loss: 0.00163\n",
      "[411,    30] loss: 0.00104\n",
      "[412,    10] loss: 0.00163\n",
      "[412,    20] loss: 0.00163\n",
      "[412,    30] loss: 0.00104\n",
      "[413,    10] loss: 0.00163\n",
      "[413,    20] loss: 0.00162\n",
      "[413,    30] loss: 0.00104\n",
      "[414,    10] loss: 0.00163\n",
      "[414,    20] loss: 0.00162\n",
      "[414,    30] loss: 0.00104\n",
      "[415,    10] loss: 0.00163\n",
      "[415,    20] loss: 0.00162\n",
      "[415,    30] loss: 0.00104\n",
      "[416,    10] loss: 0.00163\n",
      "[416,    20] loss: 0.00161\n",
      "[416,    30] loss: 0.00104\n",
      "[417,    10] loss: 0.00163\n",
      "[417,    20] loss: 0.00161\n",
      "[417,    30] loss: 0.00104\n",
      "[418,    10] loss: 0.00163\n",
      "[418,    20] loss: 0.00161\n",
      "[418,    30] loss: 0.00104\n",
      "[419,    10] loss: 0.00163\n",
      "[419,    20] loss: 0.00161\n",
      "[419,    30] loss: 0.00103\n",
      "[420,    10] loss: 0.00163\n",
      "[420,    20] loss: 0.00160\n",
      "[420,    30] loss: 0.00103\n",
      "[421,    10] loss: 0.00163\n",
      "[421,    20] loss: 0.00160\n",
      "[421,    30] loss: 0.00103\n",
      "[422,    10] loss: 0.00163\n",
      "[422,    20] loss: 0.00160\n",
      "[422,    30] loss: 0.00103\n",
      "[423,    10] loss: 0.00163\n",
      "[423,    20] loss: 0.00159\n",
      "[423,    30] loss: 0.00103\n",
      "[424,    10] loss: 0.00163\n",
      "[424,    20] loss: 0.00159\n",
      "[424,    30] loss: 0.00103\n",
      "[425,    10] loss: 0.00163\n",
      "[425,    20] loss: 0.00159\n",
      "[425,    30] loss: 0.00103\n",
      "[426,    10] loss: 0.00163\n",
      "[426,    20] loss: 0.00158\n",
      "[426,    30] loss: 0.00102\n",
      "[427,    10] loss: 0.00163\n",
      "[427,    20] loss: 0.00158\n",
      "[427,    30] loss: 0.00102\n",
      "[428,    10] loss: 0.00163\n",
      "[428,    20] loss: 0.00158\n",
      "[428,    30] loss: 0.00102\n",
      "[429,    10] loss: 0.00163\n",
      "[429,    20] loss: 0.00157\n",
      "[429,    30] loss: 0.00101\n",
      "[430,    10] loss: 0.00163\n",
      "[430,    20] loss: 0.00157\n",
      "[430,    30] loss: 0.00101\n",
      "[431,    10] loss: 0.00163\n",
      "[431,    20] loss: 0.00157\n",
      "[431,    30] loss: 0.00101\n",
      "[432,    10] loss: 0.00164\n",
      "[432,    20] loss: 0.00156\n",
      "[432,    30] loss: 0.00100\n",
      "[433,    10] loss: 0.00164\n",
      "[433,    20] loss: 0.00156\n",
      "[433,    30] loss: 0.00100\n",
      "[434,    10] loss: 0.00164\n",
      "[434,    20] loss: 0.00156\n",
      "[434,    30] loss: 0.00099\n",
      "[435,    10] loss: 0.00164\n",
      "[435,    20] loss: 0.00155\n",
      "[435,    30] loss: 0.00099\n",
      "[436,    10] loss: 0.00165\n",
      "[436,    20] loss: 0.00154\n",
      "[436,    30] loss: 0.00098\n",
      "[437,    10] loss: 0.00165\n",
      "[437,    20] loss: 0.00153\n",
      "[437,    30] loss: 0.00098\n",
      "[438,    10] loss: 0.00165\n",
      "[438,    20] loss: 0.00152\n",
      "[438,    30] loss: 0.00098\n",
      "[439,    10] loss: 0.00166\n",
      "[439,    20] loss: 0.00151\n",
      "[439,    30] loss: 0.00098\n",
      "[440,    10] loss: 0.00166\n",
      "[440,    20] loss: 0.00149\n",
      "[440,    30] loss: 0.00098\n",
      "[441,    10] loss: 0.00166\n",
      "[441,    20] loss: 0.00148\n",
      "[441,    30] loss: 0.00098\n",
      "[442,    10] loss: 0.00167\n",
      "[442,    20] loss: 0.00146\n",
      "[442,    30] loss: 0.00099\n",
      "[443,    10] loss: 0.00167\n",
      "[443,    20] loss: 0.00144\n",
      "[443,    30] loss: 0.00099\n",
      "[444,    10] loss: 0.00167\n",
      "[444,    20] loss: 0.00143\n",
      "[444,    30] loss: 0.00099\n",
      "[445,    10] loss: 0.00168\n",
      "[445,    20] loss: 0.00141\n",
      "[445,    30] loss: 0.00099\n",
      "[446,    10] loss: 0.00168\n",
      "[446,    20] loss: 0.00140\n",
      "[446,    30] loss: 0.00099\n",
      "[447,    10] loss: 0.00168\n",
      "[447,    20] loss: 0.00138\n",
      "[447,    30] loss: 0.00099\n",
      "[448,    10] loss: 0.00168\n",
      "[448,    20] loss: 0.00137\n",
      "[448,    30] loss: 0.00099\n",
      "[449,    10] loss: 0.00168\n",
      "[449,    20] loss: 0.00137\n",
      "[449,    30] loss: 0.00099\n",
      "[450,    10] loss: 0.00167\n",
      "[450,    20] loss: 0.00136\n",
      "[450,    30] loss: 0.00099\n",
      "[451,    10] loss: 0.00167\n",
      "[451,    20] loss: 0.00135\n",
      "[451,    30] loss: 0.00099\n",
      "[452,    10] loss: 0.00167\n",
      "[452,    20] loss: 0.00135\n",
      "[452,    30] loss: 0.00099\n",
      "[453,    10] loss: 0.00166\n",
      "[453,    20] loss: 0.00134\n",
      "[453,    30] loss: 0.00099\n",
      "[454,    10] loss: 0.00166\n",
      "[454,    20] loss: 0.00134\n",
      "[454,    30] loss: 0.00099\n",
      "[455,    10] loss: 0.00165\n",
      "[455,    20] loss: 0.00133\n",
      "[455,    30] loss: 0.00099\n",
      "[456,    10] loss: 0.00165\n",
      "[456,    20] loss: 0.00133\n",
      "[456,    30] loss: 0.00098\n",
      "[457,    10] loss: 0.00165\n",
      "[457,    20] loss: 0.00132\n",
      "[457,    30] loss: 0.00098\n",
      "[458,    10] loss: 0.00164\n",
      "[458,    20] loss: 0.00132\n",
      "[458,    30] loss: 0.00098\n",
      "[459,    10] loss: 0.00164\n",
      "[459,    20] loss: 0.00131\n",
      "[459,    30] loss: 0.00098\n",
      "[460,    10] loss: 0.00164\n",
      "[460,    20] loss: 0.00131\n",
      "[460,    30] loss: 0.00098\n",
      "[461,    10] loss: 0.00163\n",
      "[461,    20] loss: 0.00131\n",
      "[461,    30] loss: 0.00098\n",
      "[462,    10] loss: 0.00163\n",
      "[462,    20] loss: 0.00130\n",
      "[462,    30] loss: 0.00098\n",
      "[463,    10] loss: 0.00163\n",
      "[463,    20] loss: 0.00130\n",
      "[463,    30] loss: 0.00098\n",
      "[464,    10] loss: 0.00162\n",
      "[464,    20] loss: 0.00130\n",
      "[464,    30] loss: 0.00097\n",
      "[465,    10] loss: 0.00162\n",
      "[465,    20] loss: 0.00129\n",
      "[465,    30] loss: 0.00097\n",
      "[466,    10] loss: 0.00162\n",
      "[466,    20] loss: 0.00129\n",
      "[466,    30] loss: 0.00097\n",
      "[467,    10] loss: 0.00162\n",
      "[467,    20] loss: 0.00129\n",
      "[467,    30] loss: 0.00097\n",
      "[468,    10] loss: 0.00162\n",
      "[468,    20] loss: 0.00129\n",
      "[468,    30] loss: 0.00097\n",
      "[469,    10] loss: 0.00161\n",
      "[469,    20] loss: 0.00128\n",
      "[469,    30] loss: 0.00097\n",
      "[470,    10] loss: 0.00161\n",
      "[470,    20] loss: 0.00128\n",
      "[470,    30] loss: 0.00097\n",
      "[471,    10] loss: 0.00161\n",
      "[471,    20] loss: 0.00128\n",
      "[471,    30] loss: 0.00097\n",
      "[472,    10] loss: 0.00161\n",
      "[472,    20] loss: 0.00128\n",
      "[472,    30] loss: 0.00097\n",
      "[473,    10] loss: 0.00161\n",
      "[473,    20] loss: 0.00127\n",
      "[473,    30] loss: 0.00097\n",
      "[474,    10] loss: 0.00161\n",
      "[474,    20] loss: 0.00127\n",
      "[474,    30] loss: 0.00096\n",
      "[475,    10] loss: 0.00160\n",
      "[475,    20] loss: 0.00127\n",
      "[475,    30] loss: 0.00096\n",
      "[476,    10] loss: 0.00160\n",
      "[476,    20] loss: 0.00127\n",
      "[476,    30] loss: 0.00096\n",
      "[477,    10] loss: 0.00160\n",
      "[477,    20] loss: 0.00127\n",
      "[477,    30] loss: 0.00096\n",
      "[478,    10] loss: 0.00160\n",
      "[478,    20] loss: 0.00126\n",
      "[478,    30] loss: 0.00096\n",
      "[479,    10] loss: 0.00160\n",
      "[479,    20] loss: 0.00126\n",
      "[479,    30] loss: 0.00096\n",
      "[480,    10] loss: 0.00160\n",
      "[480,    20] loss: 0.00126\n",
      "[480,    30] loss: 0.00096\n",
      "[481,    10] loss: 0.00160\n",
      "[481,    20] loss: 0.00126\n",
      "[481,    30] loss: 0.00096\n",
      "[482,    10] loss: 0.00160\n",
      "[482,    20] loss: 0.00126\n",
      "[482,    30] loss: 0.00096\n",
      "[483,    10] loss: 0.00159\n",
      "[483,    20] loss: 0.00125\n",
      "[483,    30] loss: 0.00096\n",
      "[484,    10] loss: 0.00159\n",
      "[484,    20] loss: 0.00125\n",
      "[484,    30] loss: 0.00096\n",
      "[485,    10] loss: 0.00159\n",
      "[485,    20] loss: 0.00125\n",
      "[485,    30] loss: 0.00096\n",
      "[486,    10] loss: 0.00159\n",
      "[486,    20] loss: 0.00125\n",
      "[486,    30] loss: 0.00095\n",
      "[487,    10] loss: 0.00159\n",
      "[487,    20] loss: 0.00125\n",
      "[487,    30] loss: 0.00095\n",
      "[488,    10] loss: 0.00159\n",
      "[488,    20] loss: 0.00125\n",
      "[488,    30] loss: 0.00095\n",
      "[489,    10] loss: 0.00159\n",
      "[489,    20] loss: 0.00124\n",
      "[489,    30] loss: 0.00095\n",
      "[490,    10] loss: 0.00159\n",
      "[490,    20] loss: 0.00124\n",
      "[490,    30] loss: 0.00095\n",
      "[491,    10] loss: 0.00159\n",
      "[491,    20] loss: 0.00124\n",
      "[491,    30] loss: 0.00095\n",
      "[492,    10] loss: 0.00159\n",
      "[492,    20] loss: 0.00124\n",
      "[492,    30] loss: 0.00095\n",
      "[493,    10] loss: 0.00159\n",
      "[493,    20] loss: 0.00124\n",
      "[493,    30] loss: 0.00095\n",
      "[494,    10] loss: 0.00159\n",
      "[494,    20] loss: 0.00124\n",
      "[494,    30] loss: 0.00095\n",
      "[495,    10] loss: 0.00159\n",
      "[495,    20] loss: 0.00124\n",
      "[495,    30] loss: 0.00095\n",
      "[496,    10] loss: 0.00159\n",
      "[496,    20] loss: 0.00124\n",
      "[496,    30] loss: 0.00095\n",
      "[497,    10] loss: 0.00159\n",
      "[497,    20] loss: 0.00123\n",
      "[497,    30] loss: 0.00095\n",
      "[498,    10] loss: 0.00158\n",
      "[498,    20] loss: 0.00123\n",
      "[498,    30] loss: 0.00095\n",
      "[499,    10] loss: 0.00158\n",
      "[499,    20] loss: 0.00123\n",
      "[499,    30] loss: 0.00094\n",
      "[500,    10] loss: 0.00158\n",
      "[500,    20] loss: 0.00123\n",
      "[500,    30] loss: 0.00094\n",
      "[501,    10] loss: 0.00158\n",
      "[501,    20] loss: 0.00123\n",
      "[501,    30] loss: 0.00094\n",
      "[502,    10] loss: 0.00158\n",
      "[502,    20] loss: 0.00123\n",
      "[502,    30] loss: 0.00094\n",
      "[503,    10] loss: 0.00158\n",
      "[503,    20] loss: 0.00123\n",
      "[503,    30] loss: 0.00094\n",
      "[504,    10] loss: 0.00158\n",
      "[504,    20] loss: 0.00123\n",
      "[504,    30] loss: 0.00094\n",
      "[505,    10] loss: 0.00158\n",
      "[505,    20] loss: 0.00122\n",
      "[505,    30] loss: 0.00094\n",
      "[506,    10] loss: 0.00158\n",
      "[506,    20] loss: 0.00122\n",
      "[506,    30] loss: 0.00094\n",
      "[507,    10] loss: 0.00158\n",
      "[507,    20] loss: 0.00122\n",
      "[507,    30] loss: 0.00094\n",
      "[508,    10] loss: 0.00158\n",
      "[508,    20] loss: 0.00122\n",
      "[508,    30] loss: 0.00094\n",
      "[509,    10] loss: 0.00158\n",
      "[509,    20] loss: 0.00122\n",
      "[509,    30] loss: 0.00094\n",
      "[510,    10] loss: 0.00158\n",
      "[510,    20] loss: 0.00122\n",
      "[510,    30] loss: 0.00094\n",
      "[511,    10] loss: 0.00158\n",
      "[511,    20] loss: 0.00122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[511,    30] loss: 0.00094\n",
      "[512,    10] loss: 0.00158\n",
      "[512,    20] loss: 0.00122\n",
      "[512,    30] loss: 0.00094\n",
      "[513,    10] loss: 0.00158\n",
      "[513,    20] loss: 0.00121\n",
      "[513,    30] loss: 0.00094\n",
      "[514,    10] loss: 0.00158\n",
      "[514,    20] loss: 0.00121\n",
      "[514,    30] loss: 0.00094\n",
      "[515,    10] loss: 0.00158\n",
      "[515,    20] loss: 0.00121\n",
      "[515,    30] loss: 0.00094\n",
      "[516,    10] loss: 0.00158\n",
      "[516,    20] loss: 0.00121\n",
      "[516,    30] loss: 0.00094\n",
      "[517,    10] loss: 0.00158\n",
      "[517,    20] loss: 0.00121\n",
      "[517,    30] loss: 0.00094\n",
      "[518,    10] loss: 0.00158\n",
      "[518,    20] loss: 0.00121\n",
      "[518,    30] loss: 0.00094\n",
      "[519,    10] loss: 0.00158\n",
      "[519,    20] loss: 0.00121\n",
      "[519,    30] loss: 0.00094\n",
      "[520,    10] loss: 0.00158\n",
      "[520,    20] loss: 0.00121\n",
      "[520,    30] loss: 0.00093\n",
      "[521,    10] loss: 0.00158\n",
      "[521,    20] loss: 0.00121\n",
      "[521,    30] loss: 0.00093\n",
      "[522,    10] loss: 0.00158\n",
      "[522,    20] loss: 0.00120\n",
      "[522,    30] loss: 0.00093\n",
      "[523,    10] loss: 0.00157\n",
      "[523,    20] loss: 0.00120\n",
      "[523,    30] loss: 0.00093\n",
      "[524,    10] loss: 0.00157\n",
      "[524,    20] loss: 0.00120\n",
      "[524,    30] loss: 0.00093\n",
      "[525,    10] loss: 0.00157\n",
      "[525,    20] loss: 0.00120\n",
      "[525,    30] loss: 0.00093\n",
      "[526,    10] loss: 0.00157\n",
      "[526,    20] loss: 0.00120\n",
      "[526,    30] loss: 0.00093\n",
      "[527,    10] loss: 0.00157\n",
      "[527,    20] loss: 0.00120\n",
      "[527,    30] loss: 0.00093\n",
      "[528,    10] loss: 0.00157\n",
      "[528,    20] loss: 0.00120\n",
      "[528,    30] loss: 0.00093\n",
      "[529,    10] loss: 0.00157\n",
      "[529,    20] loss: 0.00120\n",
      "[529,    30] loss: 0.00093\n",
      "[530,    10] loss: 0.00157\n",
      "[530,    20] loss: 0.00120\n",
      "[530,    30] loss: 0.00093\n",
      "[531,    10] loss: 0.00157\n",
      "[531,    20] loss: 0.00119\n",
      "[531,    30] loss: 0.00093\n",
      "[532,    10] loss: 0.00157\n",
      "[532,    20] loss: 0.00119\n",
      "[532,    30] loss: 0.00093\n",
      "[533,    10] loss: 0.00157\n",
      "[533,    20] loss: 0.00119\n",
      "[533,    30] loss: 0.00093\n",
      "[534,    10] loss: 0.00157\n",
      "[534,    20] loss: 0.00119\n",
      "[534,    30] loss: 0.00093\n",
      "[535,    10] loss: 0.00157\n",
      "[535,    20] loss: 0.00119\n",
      "[535,    30] loss: 0.00093\n",
      "[536,    10] loss: 0.00157\n",
      "[536,    20] loss: 0.00119\n",
      "[536,    30] loss: 0.00093\n",
      "[537,    10] loss: 0.00157\n",
      "[537,    20] loss: 0.00119\n",
      "[537,    30] loss: 0.00093\n",
      "[538,    10] loss: 0.00157\n",
      "[538,    20] loss: 0.00119\n",
      "[538,    30] loss: 0.00093\n",
      "[539,    10] loss: 0.00157\n",
      "[539,    20] loss: 0.00119\n",
      "[539,    30] loss: 0.00093\n",
      "[540,    10] loss: 0.00157\n",
      "[540,    20] loss: 0.00118\n",
      "[540,    30] loss: 0.00093\n",
      "[541,    10] loss: 0.00157\n",
      "[541,    20] loss: 0.00118\n",
      "[541,    30] loss: 0.00093\n",
      "[542,    10] loss: 0.00157\n",
      "[542,    20] loss: 0.00118\n",
      "[542,    30] loss: 0.00093\n",
      "[543,    10] loss: 0.00157\n",
      "[543,    20] loss: 0.00118\n",
      "[543,    30] loss: 0.00093\n",
      "[544,    10] loss: 0.00157\n",
      "[544,    20] loss: 0.00118\n",
      "[544,    30] loss: 0.00093\n",
      "[545,    10] loss: 0.00156\n",
      "[545,    20] loss: 0.00118\n",
      "[545,    30] loss: 0.00093\n",
      "[546,    10] loss: 0.00156\n",
      "[546,    20] loss: 0.00118\n",
      "[546,    30] loss: 0.00093\n",
      "[547,    10] loss: 0.00156\n",
      "[547,    20] loss: 0.00118\n",
      "[547,    30] loss: 0.00093\n",
      "[548,    10] loss: 0.00156\n",
      "[548,    20] loss: 0.00118\n",
      "[548,    30] loss: 0.00093\n",
      "[549,    10] loss: 0.00156\n",
      "[549,    20] loss: 0.00117\n",
      "[549,    30] loss: 0.00093\n",
      "[550,    10] loss: 0.00156\n",
      "[550,    20] loss: 0.00117\n",
      "[550,    30] loss: 0.00093\n",
      "[551,    10] loss: 0.00156\n",
      "[551,    20] loss: 0.00117\n",
      "[551,    30] loss: 0.00093\n",
      "[552,    10] loss: 0.00156\n",
      "[552,    20] loss: 0.00117\n",
      "[552,    30] loss: 0.00093\n",
      "[553,    10] loss: 0.00156\n",
      "[553,    20] loss: 0.00117\n",
      "[553,    30] loss: 0.00093\n",
      "[554,    10] loss: 0.00156\n",
      "[554,    20] loss: 0.00117\n",
      "[554,    30] loss: 0.00093\n",
      "[555,    10] loss: 0.00156\n",
      "[555,    20] loss: 0.00117\n",
      "[555,    30] loss: 0.00093\n",
      "[556,    10] loss: 0.00156\n",
      "[556,    20] loss: 0.00117\n",
      "[556,    30] loss: 0.00093\n",
      "[557,    10] loss: 0.00156\n",
      "[557,    20] loss: 0.00117\n",
      "[557,    30] loss: 0.00093\n",
      "[558,    10] loss: 0.00156\n",
      "[558,    20] loss: 0.00117\n",
      "[558,    30] loss: 0.00093\n",
      "[559,    10] loss: 0.00156\n",
      "[559,    20] loss: 0.00116\n",
      "[559,    30] loss: 0.00093\n",
      "[560,    10] loss: 0.00156\n",
      "[560,    20] loss: 0.00116\n",
      "[560,    30] loss: 0.00093\n",
      "[561,    10] loss: 0.00156\n",
      "[561,    20] loss: 0.00116\n",
      "[561,    30] loss: 0.00093\n",
      "[562,    10] loss: 0.00156\n",
      "[562,    20] loss: 0.00116\n",
      "[562,    30] loss: 0.00093\n",
      "[563,    10] loss: 0.00156\n",
      "[563,    20] loss: 0.00116\n",
      "[563,    30] loss: 0.00093\n",
      "[564,    10] loss: 0.00156\n",
      "[564,    20] loss: 0.00116\n",
      "[564,    30] loss: 0.00093\n",
      "[565,    10] loss: 0.00155\n",
      "[565,    20] loss: 0.00116\n",
      "[565,    30] loss: 0.00093\n",
      "[566,    10] loss: 0.00155\n",
      "[566,    20] loss: 0.00116\n",
      "[566,    30] loss: 0.00093\n",
      "[567,    10] loss: 0.00155\n",
      "[567,    20] loss: 0.00116\n",
      "[567,    30] loss: 0.00093\n",
      "[568,    10] loss: 0.00155\n",
      "[568,    20] loss: 0.00116\n",
      "[568,    30] loss: 0.00093\n",
      "[569,    10] loss: 0.00155\n",
      "[569,    20] loss: 0.00115\n",
      "[569,    30] loss: 0.00093\n",
      "[570,    10] loss: 0.00155\n",
      "[570,    20] loss: 0.00115\n",
      "[570,    30] loss: 0.00093\n",
      "[571,    10] loss: 0.00155\n",
      "[571,    20] loss: 0.00115\n",
      "[571,    30] loss: 0.00093\n",
      "[572,    10] loss: 0.00155\n",
      "[572,    20] loss: 0.00115\n",
      "[572,    30] loss: 0.00092\n",
      "[573,    10] loss: 0.00155\n",
      "[573,    20] loss: 0.00115\n",
      "[573,    30] loss: 0.00092\n",
      "[574,    10] loss: 0.00155\n",
      "[574,    20] loss: 0.00115\n",
      "[574,    30] loss: 0.00092\n",
      "[575,    10] loss: 0.00155\n",
      "[575,    20] loss: 0.00115\n",
      "[575,    30] loss: 0.00092\n",
      "[576,    10] loss: 0.00155\n",
      "[576,    20] loss: 0.00115\n",
      "[576,    30] loss: 0.00092\n",
      "[577,    10] loss: 0.00155\n",
      "[577,    20] loss: 0.00115\n",
      "[577,    30] loss: 0.00092\n",
      "[578,    10] loss: 0.00155\n",
      "[578,    20] loss: 0.00115\n",
      "[578,    30] loss: 0.00092\n",
      "[579,    10] loss: 0.00155\n",
      "[579,    20] loss: 0.00114\n",
      "[579,    30] loss: 0.00092\n",
      "[580,    10] loss: 0.00155\n",
      "[580,    20] loss: 0.00114\n",
      "[580,    30] loss: 0.00092\n",
      "[581,    10] loss: 0.00155\n",
      "[581,    20] loss: 0.00114\n",
      "[581,    30] loss: 0.00092\n",
      "[582,    10] loss: 0.00155\n",
      "[582,    20] loss: 0.00114\n",
      "[582,    30] loss: 0.00092\n",
      "[583,    10] loss: 0.00154\n",
      "[583,    20] loss: 0.00114\n",
      "[583,    30] loss: 0.00092\n",
      "[584,    10] loss: 0.00154\n",
      "[584,    20] loss: 0.00114\n",
      "[584,    30] loss: 0.00092\n",
      "[585,    10] loss: 0.00154\n",
      "[585,    20] loss: 0.00114\n",
      "[585,    30] loss: 0.00092\n",
      "[586,    10] loss: 0.00154\n",
      "[586,    20] loss: 0.00114\n",
      "[586,    30] loss: 0.00092\n",
      "[587,    10] loss: 0.00154\n",
      "[587,    20] loss: 0.00114\n",
      "[587,    30] loss: 0.00092\n",
      "[588,    10] loss: 0.00154\n",
      "[588,    20] loss: 0.00114\n",
      "[588,    30] loss: 0.00092\n",
      "[589,    10] loss: 0.00154\n",
      "[589,    20] loss: 0.00113\n",
      "[589,    30] loss: 0.00092\n",
      "[590,    10] loss: 0.00154\n",
      "[590,    20] loss: 0.00113\n",
      "[590,    30] loss: 0.00092\n",
      "[591,    10] loss: 0.00154\n",
      "[591,    20] loss: 0.00113\n",
      "[591,    30] loss: 0.00092\n",
      "[592,    10] loss: 0.00154\n",
      "[592,    20] loss: 0.00113\n",
      "[592,    30] loss: 0.00092\n",
      "[593,    10] loss: 0.00154\n",
      "[593,    20] loss: 0.00113\n",
      "[593,    30] loss: 0.00092\n",
      "[594,    10] loss: 0.00154\n",
      "[594,    20] loss: 0.00113\n",
      "[594,    30] loss: 0.00092\n",
      "[595,    10] loss: 0.00154\n",
      "[595,    20] loss: 0.00113\n",
      "[595,    30] loss: 0.00092\n",
      "[596,    10] loss: 0.00154\n",
      "[596,    20] loss: 0.00113\n",
      "[596,    30] loss: 0.00092\n",
      "[597,    10] loss: 0.00154\n",
      "[597,    20] loss: 0.00113\n",
      "[597,    30] loss: 0.00092\n",
      "[598,    10] loss: 0.00154\n",
      "[598,    20] loss: 0.00113\n",
      "[598,    30] loss: 0.00092\n",
      "[599,    10] loss: 0.00153\n",
      "[599,    20] loss: 0.00113\n",
      "[599,    30] loss: 0.00092\n",
      "[600,    10] loss: 0.00153\n",
      "[600,    20] loss: 0.00112\n",
      "[600,    30] loss: 0.00092\n",
      "[601,    10] loss: 0.00153\n",
      "[601,    20] loss: 0.00112\n",
      "[601,    30] loss: 0.00092\n",
      "[602,    10] loss: 0.00153\n",
      "[602,    20] loss: 0.00112\n",
      "[602,    30] loss: 0.00092\n",
      "[603,    10] loss: 0.00153\n",
      "[603,    20] loss: 0.00112\n",
      "[603,    30] loss: 0.00092\n",
      "[604,    10] loss: 0.00153\n",
      "[604,    20] loss: 0.00112\n",
      "[604,    30] loss: 0.00092\n",
      "[605,    10] loss: 0.00153\n",
      "[605,    20] loss: 0.00112\n",
      "[605,    30] loss: 0.00092\n",
      "[606,    10] loss: 0.00153\n",
      "[606,    20] loss: 0.00112\n",
      "[606,    30] loss: 0.00092\n",
      "[607,    10] loss: 0.00153\n",
      "[607,    20] loss: 0.00112\n",
      "[607,    30] loss: 0.00092\n",
      "[608,    10] loss: 0.00153\n",
      "[608,    20] loss: 0.00112\n",
      "[608,    30] loss: 0.00092\n",
      "[609,    10] loss: 0.00153\n",
      "[609,    20] loss: 0.00112\n",
      "[609,    30] loss: 0.00092\n",
      "[610,    10] loss: 0.00153\n",
      "[610,    20] loss: 0.00112\n",
      "[610,    30] loss: 0.00092\n",
      "[611,    10] loss: 0.00153\n",
      "[611,    20] loss: 0.00111\n",
      "[611,    30] loss: 0.00092\n",
      "[612,    10] loss: 0.00153\n",
      "[612,    20] loss: 0.00111\n",
      "[612,    30] loss: 0.00092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[613,    10] loss: 0.00153\n",
      "[613,    20] loss: 0.00111\n",
      "[613,    30] loss: 0.00092\n",
      "[614,    10] loss: 0.00152\n",
      "[614,    20] loss: 0.00111\n",
      "[614,    30] loss: 0.00092\n",
      "[615,    10] loss: 0.00152\n",
      "[615,    20] loss: 0.00111\n",
      "[615,    30] loss: 0.00092\n",
      "[616,    10] loss: 0.00152\n",
      "[616,    20] loss: 0.00111\n",
      "[616,    30] loss: 0.00092\n",
      "[617,    10] loss: 0.00152\n",
      "[617,    20] loss: 0.00111\n",
      "[617,    30] loss: 0.00092\n",
      "[618,    10] loss: 0.00152\n",
      "[618,    20] loss: 0.00111\n",
      "[618,    30] loss: 0.00092\n",
      "[619,    10] loss: 0.00152\n",
      "[619,    20] loss: 0.00111\n",
      "[619,    30] loss: 0.00092\n",
      "[620,    10] loss: 0.00152\n",
      "[620,    20] loss: 0.00111\n",
      "[620,    30] loss: 0.00092\n",
      "[621,    10] loss: 0.00152\n",
      "[621,    20] loss: 0.00111\n",
      "[621,    30] loss: 0.00092\n",
      "[622,    10] loss: 0.00152\n",
      "[622,    20] loss: 0.00110\n",
      "[622,    30] loss: 0.00092\n",
      "[623,    10] loss: 0.00152\n",
      "[623,    20] loss: 0.00110\n",
      "[623,    30] loss: 0.00092\n",
      "[624,    10] loss: 0.00152\n",
      "[624,    20] loss: 0.00110\n",
      "[624,    30] loss: 0.00092\n",
      "[625,    10] loss: 0.00152\n",
      "[625,    20] loss: 0.00110\n",
      "[625,    30] loss: 0.00092\n",
      "[626,    10] loss: 0.00152\n",
      "[626,    20] loss: 0.00110\n",
      "[626,    30] loss: 0.00092\n",
      "[627,    10] loss: 0.00152\n",
      "[627,    20] loss: 0.00110\n",
      "[627,    30] loss: 0.00092\n",
      "[628,    10] loss: 0.00152\n",
      "[628,    20] loss: 0.00110\n",
      "[628,    30] loss: 0.00092\n",
      "[629,    10] loss: 0.00151\n",
      "[629,    20] loss: 0.00110\n",
      "[629,    30] loss: 0.00092\n",
      "[630,    10] loss: 0.00151\n",
      "[630,    20] loss: 0.00110\n",
      "[630,    30] loss: 0.00092\n",
      "[631,    10] loss: 0.00151\n",
      "[631,    20] loss: 0.00110\n",
      "[631,    30] loss: 0.00092\n",
      "[632,    10] loss: 0.00151\n",
      "[632,    20] loss: 0.00110\n",
      "[632,    30] loss: 0.00092\n",
      "[633,    10] loss: 0.00151\n",
      "[633,    20] loss: 0.00110\n",
      "[633,    30] loss: 0.00092\n",
      "[634,    10] loss: 0.00151\n",
      "[634,    20] loss: 0.00109\n",
      "[634,    30] loss: 0.00092\n",
      "[635,    10] loss: 0.00151\n",
      "[635,    20] loss: 0.00109\n",
      "[635,    30] loss: 0.00092\n",
      "[636,    10] loss: 0.00151\n",
      "[636,    20] loss: 0.00109\n",
      "[636,    30] loss: 0.00092\n",
      "[637,    10] loss: 0.00151\n",
      "[637,    20] loss: 0.00109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-48daeb147928>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# The net is designed to take a (3 x num_features)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m             \u001b[0;31m# tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0;31m# output vector with as many elements as the batch size. If our net was designed to take one row as input we\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cb48679ecb8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x1, x2, x3, train)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0mx3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-cb48679ecb8f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, train)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m           \u001b[0;31m# Apply a tanh activation on fully connected layer 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;31m#         x = torch.relu(self.fc3(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m#         x = torch.tanh(self.fc4(x))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1751\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "net = BPNN()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 2000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1],inputs[j][2])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[k][0],test_set[k][1],test_set[k][2])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch          \n",
    "            \n",
    "        if i % 10 == 9:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.5f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f6bd57977f0>]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "plt.plot(x[200:600],train_loss[200:600],'blue')\n",
    "plt.plot(x[200:600],test_loss[200:600],'red')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ -47.1594,    1.0143,   -9.9748, -152.5495,  -14.7487,  -46.0137])\n",
      "output\n",
      "tensor([-0.5350], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3 = test_set[10]\n",
    "x1 = x1\n",
    "print(x1)\n",
    "\n",
    "output = net(x1, x2, x3)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x1,x2,x3 = test_set[i]\n",
    "    prediction[i] = net(x1, x2, x3)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.04084563e-01 -4.50614154e-01 -1.67387545e-01 -2.89092422e-01\n",
      " -3.22757006e-01 -6.19412959e-02  1.33346871e-01  9.45320427e-02\n",
      "  5.64078838e-02  1.09385610e-01 -5.34994125e-01 -3.78985405e-01\n",
      " -2.00914592e-01 -4.23097312e-01 -4.55649137e-01 -6.98107183e-02\n",
      " -8.07289034e-02  2.20012963e-02 -1.83144078e-01 -5.73959589e-01\n",
      "  4.81161296e-01  8.53701085e-02 -1.42732382e-01  7.31229037e-02\n",
      " -4.92983133e-01 -1.59770250e-04 -3.66372287e-01  4.30981219e-02\n",
      " -3.27933192e-01  1.05896816e-01  4.99014139e-01  2.16671154e-01\n",
      "  4.98510003e-01 -3.65626574e-01 -1.94147080e-02 -3.74017984e-01\n",
      " -3.98321509e-01 -1.96756110e-01  2.34165072e-01 -4.35509443e-01\n",
      " -1.36948362e-01 -4.81148750e-01  6.66735470e-01 -1.97580293e-01\n",
      " -1.86409816e-01  3.82351339e-01 -2.52295196e-01  1.00428209e-01\n",
      "  2.38771737e-02  9.19894427e-02  1.39687955e-02 -1.65461838e-01\n",
      " -2.10587323e-01 -1.68899342e-01 -8.65594447e-02  9.02757049e-04\n",
      " -5.03543496e-01  2.22642362e-01 -2.09463879e-01 -5.44671774e-01\n",
      "  1.50862932e-02 -2.43749455e-01  2.73129821e-01 -5.59934378e-01\n",
      "  1.21541619e-02  1.27782539e-01 -1.55973047e-01  3.63960087e-01\n",
      "  8.30216289e-01 -3.57008576e-01 -1.06626719e-01  2.13765949e-02\n",
      "  2.28827655e-01  5.98522246e-01  5.81574142e-01  1.62636340e-01\n",
      " -8.94988328e-02 -1.18141875e-01 -3.74952853e-01 -4.36310589e-01\n",
      " -1.55474544e-02 -2.84834564e-01  3.83233726e-02  3.23469788e-02\n",
      " -1.88679263e-01 -3.32918465e-01 -3.81867290e-02 -3.95689845e-01\n",
      "  6.50569916e-01 -4.84385699e-01  2.82489747e-01 -3.53622675e-01\n",
      " -5.07915616e-01 -3.86781782e-01 -4.34343040e-01  5.11833012e-01\n",
      " -1.11657202e-01 -7.49442577e-02 -1.90205574e-02 -1.75032526e-01]\n"
     ]
    }
   ],
   "source": [
    "print(prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900])\n",
      "-2.6490954e-09\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13813.419735561924\n"
     ]
    }
   ],
   "source": [
    "print(mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.5770, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAboAAAEiCAYAAACPwherAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABa9UlEQVR4nO2dZ5gUxdaA38OSBMQAwkWRBb1XZQmCIKIkUQzXhHhB0TUgAgLiZ74GzIoBFURRUQwoLHJNKIqCgKwRAyAZQdQFEZGkkuOe70f1sM3szGzv7syG2fM+Tz8zU11Vfaq7p09X1alzRFUxDMMwjGSlXHELYBiGYRiJxBSdYRiGkdSYojMMwzCSGlN0hmEYRlJjis4wDMNIakzRGYZhGEmNKTrDMAwjqTFFZxiGYSQ1SaHoxPGLiKiI/LMA5S8UkR4JEM1/jFEiMjORx4hy3Hu98xLaVonI2yJyZAKP2dg71sm+tHy1P97XJJJMRuEojntaRO4Wkd9EJFtERiXoGENFZH2Ufe+LyNf5rK+8iFwnIrNEZLOIbBSRySJyQnwkLhjF9UwqjmMnhaIDTgTqe9+7F6D8hUCPeAlTAvkbd45OBG4GmgHTRKRqEcrwAPk7x8l+TYx8IiItgfuA4UAb3D2VCJoA8wuwLxcicgDwBXAX8C7QBegDHAh8JiLtCyOoEYzyxS1AnLgY2AIs8L4/WLzilDh2q2roLfRrEVkBfA6cBbwZnllEUoAUVd0ZLwFU9ad41ZXMJOLcJxHHeJ/PqOrGglYS4Bw3IfL/ojqQSkBFJyICjAcOBY5X1V98+94FlgJDgJb5kd/IP6W+R+fdtN2ACcDLQJqINI2Qr72ITPeGDv4WkUwRae4Nf/wH6OAb3rvXK5MpIm+F1XOyl6ex9/tEEZngDQluEZE5IpJegHZcKSI7ROTAsPRG3vFO9b5PEpEN3rEWi8g1+T0WMMv7rO8dY5SIzBSR80VkIbAdOMHb11ZEPhWRrSKyXkRGisj+YTL2F5FfPZneB+pEaF+uoYqCXJN4yhSNvOr3na/TRGSed4wvRKRRfuoJcO4H+NrwrncPqHcPni1u+K5BWH0NvPTzorQtz/vM+53v+zrI/yU/5yb8PAGjvZ9/i28YWtww93yvXb+KyCARKe8vG+0cRzhOLaAWkZVZE+9zXozT4KcX0BG4yq/kAFR1OzAGaOEdM5IsIbnPFpFF3rmaKCIHi8g/vf/OFi9PpGdezPMS5ZhB7tmI/1vf/sD3QQGOXaBnYKlXdMApQG1gHPAWsAvXq9uL94eY5u27ArgI16M5DDf8MR34npzhvRfzcfxU4EvcTX0u8DbwiohcHLNUbt7xPruEpV8ErAEyccp8D3ApcB7wNBD14RCD+t7n6rC0wcDDuJ7eLyLSBnfeVgNdgeu9fa+EColIZ+AZ4APgAtwD4uW8BCjoNUmkTEHr96gHPAYMwt1vtYA3RETyWQ9EPvddcNd3Au6emAe85CszCViFO3d+egBrgQ+jNDHIfQbxu69zkc9zE+IBckZqTsHdE7NF5HTgf8BsoDPunN2MG970U5+wcxzlOCGFsVxEDvRvwPHevqBDlzcDs1V1SpT9v3qf/4hRRz3gfuBO3JDnScALuOfdONz5Kw+MC917APk4L/jKBPlvnUz0/22Bycc9UbBnoKqW6g33APsTqOj9noi7icWXZwYw058WVsdbQGaE9EzgrbC0kwEFGkfIL7ib7nngk7B9o4CZebTlPWBSWNoS3M1Z0ztuk3yen3uBdZ5c5YGjcEpkI1DHJ5sCzcLKfg5MD0s7xd9+4Fvgo7A8I708J0drfyGuSdxkinLcIPWPAnYD//LlOd/Lc0zQevI4998BE8PSnvW3Affg33uve/dfFvB4Qe+zKPkj3tcRrmkmAf4vQc9NBDl6eHmq+dK+jlDXf3EPw7qxznGUY9zo5Y22rQr4vzvay39jjDx3eXkaRNkfus+O9KUN9spc7ks7y0trWIDz4r9+Qe79mP/boPdBAY9doGegqpbuHp2IVMK9mY7XnPH213Fvb629PFVxwxSvqne24izDQSLylIgsx73l7MK9eR1VgOr+B5wqIjW9upt59fwP2IB7AxwhIhdFG+6IQg2fbEuAI4CLVPV3X57fVHWOr11VcG/Nb4izGivvDXt84dXTQtywcXPcg9PPO8SgoNckkTIFrd+XPUtVf/T9XuR91s1nPZD73KfgDIYmhOUL//0yrud1sve7o/c7Vs8IYt9nIRnieV/vpQDnJlZdKcBx5J5P+x9utOpEX9o+5zgGTYAVuHMZvi3GG7YUkcNFZJo3dLZQRAb7e1RAaBh7EdE5BtjkHS8aWbrv/PYy7/OTCGmHebLl57zglQny30rIszQf90SBn4GlWtEB/8ZZL33oG17IBHaQM3x5EO6N9PcI5ePBKFz3/THgdNzwxstA5QLUNQF3YS/wfl8E/AZ8oarZXv2rvfpXi8jn/rHxGPztydUSqAvUV9WPwvL8Efb7ICAF14vY5dt2ABWAw4FDcG/6a8LKhv8Op6DXJJEyBa0/xF9hZUMvWpXzWQ/kPvehNqwNS9/nt6r+jLvfr/SSrgS+VdWFUVvoiHqf+fKMIn73tZ/8nptY1PTKhJ+/0O+DI6TlRRPccGNm+Iab5w0NW+4GblXVhrgXqxPIOZ8A1bzPdZEO4imN83AjD3tiyPNX2O+dEdL99x7k77yECHJdEvUsDXRPFOYZWNqtLkPKLJeFFHChiNyAG9bMJh/GCD62AxXD0vbeJCJSGTgbGKCqI3zpBXqBUNXNIjIR94B5AWdi/0bo7UlVfwD+IyIVgHbAo8BEEanr3QTR2K2qea1ZCX9D+8tLu5fI8z2rcA/e3bj5KT95vWkV9JokUqag9Qchv/WEn/tQGw4JSw//DW7ucqSI3I570N6Ul3B53WeFuK9j/l88/iI+5xicEtlF7mtb2/vc4EvLswfitS8NN/0Rvq8e7qV6PoA3GvK7932niMxjXyUdUgaH44b6wukPVMVZXcab/JyXEH+R93X5m2D/2yD3QX6PDRT8GVhqe3QiUg04BzdUGT7EcCPuonZU1S3AN8DlYUMLfnYS+U11JTkmzSFO832vhHsT2eGTa3/cm1pBGYezNjwXN8Q4LjyDqu5S1U9wf5I6uD9gXPHO29fA0ao6M8K2ynsTnYOb7PZzQXh9EerO9zVJpExB68+rjnjUE6MNke6rd3Dnahzu/5zrfolCrPusoPd1Xv+XuJ1jr649OAvibmG7LsQ9kGcErcvjX8B+5NPiUkRq4OZoJ/uSv8Iphp4R8nfAGZg8parf5FPGPCnIeQn43wryv4UA90F+jx2hTL6egYF6dCJyBNABN/e1H+6NczbwpToz2eKgM1AFGBZ+s4jIl8BAXI9vKnCb9/mRiLyAW3N3Im4y9APgB6CziJyPu0irvJM7HrhKRIbi3vI6AmeEjqOqf4vId8DdIrIRdxPdhrvBqxewXROBrbiJ/19U9VuvTU2Bx3Hj7D/juvu3AnNVNdIbWjz4L25heTbOOGQTzgrsbGCgqi4FHgLeEZHncOerA3BmgLoLek0SKVPQNhdFPaE2DMcNNbbxyoK7zwBnpi4iGcA1wOuq+ldA+SLeZ16dBb2vY/5ffMTrHAPcA0wWkVdwyroJzkJzpKquzEc9kKPMoim6Pbh5ur14dgJvAU+q6t59qrpFRG4DnhOR/+GWEoSG3q4G3sC9kCeKgpyXINclr/8tBL8P8nXsQj0DNbblUDrOgi0b1xWfhRvHX4R72/sbN66aGqueRGw40/GlMfY/ixsiq+T97gB8hvtz/4WzPGymOdY843FdegXu9dVzO24CdBPuZj2PfS2B/ombGN6Cm1T+L56lY5g8o8jD6tKXd4x3jId9abVw64h+xg0NrMb1ZuvlUVcuWSLkiSobbu5hEs5Kc4t37YcAB/jyDMApo624oYfTycPqspDXJC4yxTgfMeuP0pb6Xv3n5FPOWOf+2rA2dCOyhWYnL71TPv9Due4z37487+so5yHm/yU/5yaCTD0Is7r00i/CKaed3vkaBJTP738P53VlG24xefi+DGBRWFoK7qE8JEadXXG9oG3k3OPdAl6fSOc31zmIdO8V9LwEvGej/m+D3gcFOTYFfAaq6l6z5FyIyPe4eYJRwARV/TVsfyWcJu+OW9zbX1UjzZUZhhEHRORO3EjFwaq6zZc+GPdQa6Cx52qNOCIiL+KUXU+N9iA1SgSxFN3ZqpprUjZK3pq4P9l38RTOMMoqInII7q14Ou7NuR1umOYlVR3g5TkaZzzxGnCfqj5eTOKWOcQtcP4C53YwZDX5sqo+VXxSGdGIqugMwyg+xDkDfh1oBRyAmzoYC9ylqru8PJm44Z4JwGVq/jENIyKxenSxzEH3QQtpDCEi3XDj/w2BVuqZwotIK5z5M7j1G/eq6nhv38XAHXjeCoBLVXWdN6T6Gm6R4Xrcwugsr0w9nDn24V65s0L7DMMwjOQklqLLJu+1JwKoqqYUSgiRhjiDl+eBm32KrgqwU1V3i0gdYC7OEzg45ZbmKbfBwFZVvVdE+gNNVbWviHQHuqjqRV59mcAgVZ3iLU/IVtWthZHdMAzDKNnEWl7QsaiEUM8sN3xpRpgSqkyO4hVvqyouQGJ1ctzgdMb1DsFZRA331nw0xFkcTfHq3hxEtpo1a2r9+vXz16A82LJlC1WrFmUouMSTjG2C5GxXMrYJkrNdpblNs2bNWqeqkZwcFDlRFZ2qflqUgkRDXBTekE+/y1R1t5feD2c6uwX4EbeOCJy/t18BvJ7g3zhfj0cBf4nIO0ADvPV1GsH9joj0wfn1o3bt2jz+eHzn+Ddv3ky1atXyzliKSMY2QXK2KxnbBMnZrtLcpo4dOy4vbhn2EmQ9hze8WQm3yv9xnP+7Hnhr1AKWn4qzUArfOvvyZAIto5RviFvTVxnn/2wacCSuZzccuNPLtxDPO7f3+yecouuKW/d3BE7Bv42LExVT7hYtWmi8mT59etzrLG6SsU2qydmuZGyTanK2qzS3iYDrhotiC+oZJQ23kK86OV4DegP3iciZ6vMIEEOhdgpyrBjlF4vIFqAxTrmhnldvEXkDt2If3MLIw4GVngfsA3CLjlcC36tzhIu4CL+t2TfGl2EYhpFkBPV1OQwXBLOeqrZT1XY49yxzgScTJFsoWnJ573sqLsZTFs7Tepq31gicH7WQsp1ATjDKrrj4WYqL73WQr8wpxA6hYRiGYSQBQaMXtAGOV9WNoQRV3SgiA3HOOAuF5ERTPgTniXqOqp4BtAVuE5FdOKvM/qq6zitzH/CZt285bigVXA9ttIgsw/Xkunvy7hGRm3H+1ATnzmxkQeTdtWsXK1euZPv2grn5POCAA1i8OM9OcKki0W2qXLkydevWpUKFCgk7hmEYyUlQRbedyN6hD/D2FQp1a+PGR0gfjfNtFqnMCGBEhPTt5PbaHdo3BWhaKGGBlStXsv/++1O/fv1clqJB2LRpE/vvn3f099JEItukqqxfv56VK1fSoEGDhBzDMIzkJejQ5fu4mFdtRCTF29ri1r2FRz1OerZv306NGjUKpOSM/CMi1KhRo8A9aMMw8kdGBtSvD+XKuc+MjOKWqHAE7dFdB7wKfE6OX7dyOCV3ffzFKvmYkita7HwbRtGQkQF9+sBWbxXz8uXuN0B6evHJVRgCKTp1Ma46i8i/cAH1BBeyYlnMgoZhGEapYuDAHCUXYutWl15aFV2+Ioyr6o+q+r6qTjAlV7ykpKTQrFkzGjVqxLHHHsuQIUPIzo4doSUrK4uxY8cmXLZevXqxaFFsg9Z33303zzyGYRQ9K1bkL700EFjRiUgXEXlKRMaJyBv+LZECJgPh491vvBF0xDg6++23H3PmzGHhwoVMmTKFDz/8kPvuuy9mmaJSdC+++CJpaWkx85iiM4ySSb16+UsvDQRSdCLyBC58eSjU/J6wzYhCaLx7+XJQdZ/XXls5rpO7tWrV4oUXXmD48OGoKllZWbRr147jjjuO4447jq+++gqA2267jc8//5xmzZoxdOjQqPn8ZGVlccwxx3DFFVfQtGlTunbtylZvXGPatGk0b96cJk2a0L9/f3bs2AHAySefzMyZMwGoVq0aAwcO5Nhjj6V169b88ccffPXVV0yYMIFbbrmFZs2a8dNPP/HUU0+RlpZG06ZN6d69e/xOjmEY+WLQIKhSZd+0KlVceqkliPsUYB0+V11laYvkAmzRokW50qKRmqrqVNy+W2pq4CoiUrVq1VxpBx54oK5evVq3bNmi27ZtU1XVpUuXaqgN06dP17PPPntv/mj5/Pzyyy8K6BdffKGqqldeeaU+9thjum3bNq1bt64uWbJEVVW7d++uQ4cOVVXVDh066HfffaeqqoBOmDBBVVVvueUWfeCBB1RV9YorrtA333xz73Hq1Kmj27dvV1XVP//8M2Kb83Pe40VpdsEUjWRsk2pytqu42jRmjGqHQ5dqBzI1NdX9zi+UIBdgQYcutwI/JEbVJjdFOd7t7i23oL137940adKEbt26RR0iDJrv8MMPp02bNgBceumlfPHFFyxZsoQGDRpw1FFHAXDJJZfw2Wef5SpbsWJFzjnnHABatGhBVlZWxGM0bdqU9PR0xowZQ/nyhR/aNQyjgOzaRfryh8hc34TMhv3I+jm71BqhhAiq6B4B/htyx2UEp6jGu3/++WdSUlKoVasWQ4cOpXbt2sydO5eZM2eyc2fkwNNB84Wb9ovIXqWaFxUqVNhbPiUlhd27d0fMN3HiRK655hpmzZpFixYtouYzDCOBfPMNtGjhTCzPPhumTnXGBaWcoC0YCdQBfhORz0XkE/+WQPlKPZHGu/fbT+M63r127Vr69u3LgAEDEBH+/vtv6tSpQ7ly5Rg9ejR79rhp1P33359NmzbtLRctXzgrVqxgxowZALz++uu0bduWY445hqysLJYtc8a348aNo0OHDoFl9suSnZ3Nr7/+SseOHRk8eDB//fUXmzcHChdoGEY82LQJrrsOTjwRNmyA8ePh7bfh0EPzLlsKCKroRgDtgM9wjpwXhm1GFNLT4YUXIDUVRNzn009vL/RQwLZt2/YuL+jUqROnn34699xzDwD9+/fn1VdfpXXr1ixdunRv4MamTZtSvnx5jj32WIYOHRo1XzgNGzbk1VdfpWnTpmzYsIF+/fpRuXJlXnnlFbp160aTJk0oV64cffv2DSx/9+7deeyxx2jevDk//vgjl156KU2aNKF58+bccMMNHHjggYU7QYZhBGPiRGjUCJ5+Gvr1g0WL4Pzzi1uq+BJkIg/YBJxW3BOKxbEV1hglEhs3bixU+aLkl19+0UaNGuWZryjaZMYo8SEZ26SanO1KaJt+/131wguddVxamuqXX8a1ekqhMco6XGgcwzAMozSjCi+9BA0bwrvvwv33w/ffw0knFbdkCSOoorsHuF9ESmdMd6PA1K9fnwULFhS3GIZhxIOlS+GUU6BXL2jSBObOhbvugooVi1uyhBLUivIWoD7wh4isAHb5d6pqoUPfGIZhGAli1y547DHXe6tc2RkOXHVVUlhUBiGoonsroVIYhmEYieGbb1wPbsEC6NoVnnoK6tQpbqmKlJiKTkQOUtU/VTW2E0XDMAyjZLFpk1sPN3w4HHYYvPcenHdecUtVLOTVb13trZW7TkRSi0QiwzAMo3C8/z6kpTkld801sHBhmVVykLeiSwXGAacDP4jIHBG5T0SOS7xohmEYRr5YvRouvNAptQMPhK++cuvjqlcvbsmKlZiKTlVXq+oLqno2cAjwIHAEMEVElovI0yLSSURSikJYwzAMIwLZ2TBypFsyMGGCc8k0axa0bl3ckpUIApvcqOpmVX1LVS8DagE9gWyce7B1IlLK3X6WLu666y6GDRu29/fAgQN56qmnYpb5+++/Ofroo1myZAkAF198MSNHjkyonIZhJJglS6BjRxcPrFkzmDcP7rgj6ZcM5IcCOWlW1T3ANG+7TkSaARXiKFfp4frrYc6cfBXZb88eSInRCW7WDJ58MmYdV111FRdccAHXXXcd2dnZjBs3jk8++YRmzZpFzD927FjS0tIYPnw4PXr04LrrruPPP/+kd+/e+ZLdMIwSws6d8Oij8OCDULWqWwR+5ZXO16CxD1EVXT7m4VRVv4+TPEZA6tevT40aNfj+++/5448/aN68OampqczJQ+medtppvPnmm1xzzTXMnTu3aIQ1DCO+zJgBvXs7I5OLLoJhw6B27eKWqsQSq0c3E1Agr9cDBcruHF0ePa9IbNu0if3337/Qh+7VqxejRo1i9erV9OzZk02bNtGuXbuIeUM9uuzsbBYvXsx+++3Hhg0bqFu3bqHlMAyjiNi4EW6/HZ57DurWhQ8+cOF0PDIy3IqCFStcKLBBgyj1seTiQSxF16DIpDAKRJcuXbj77rvZtWsXY8eOJSUlJc8e3dChQ2nYsCEPPfQQPXv2ZMaMGVSoUDZHnQ2jVPHuuzBgAKxaBdde64YsfS/MGRlumm7rVvd7+XL3G0zZRTVGUdXlQbfCCiEi3URkoYhki0hLX3orb0nDHBGZKyJdfPsuFpH5IjJPRCaJSE0vvZKI/E9ElonINyJS31dmsHecxSLylIRHFC1lVKxYkY4dO3LhhReSEmvOz2Pp0qW8+OKLPPHEE7Rr14727dvz4IMPFoGkhmEUmFWr4D//gS5d4OCD4euv3VDl/vuTkQH16ztPXldckaPkQmzd6np4ZZ18GaOIyKFAPWAfcx5V/ayQciwALgCej5DeUlV3i0gdYK6IvO/tGwakqeo6ERkMDADuBa4C/lTVf4pId+BR4CIROQloA4T8cn4BdAAyCyl7sZGdnc3XX3/Nm2++GSj/UUcdxeLFi/f+HjJkSKJEMwyjsGRnU2fCBHj5Zdi+HR56CG6+GbwRmPAeXJS4yaxYUUTylmACKTpPwY0F2pMzb6e+LIWao1PVxd5xwtP97yeVfccUb6sqIuuB6sAyb19nnMID56NzuNdzU6+Oil7ZCsAfhZG7OFm0aBHnnHMOXbp04V//+ldxi2MYRjz54Qfo3Zujv/jCLR14/nkI+58PHJi7BxeJevUSJGMpImiP7klgD5AGfAecCdQG7gduSIhkHiJyAvAyzkvLZaq620vvB8wHtgA/Atd4RQ4DfgXweoJ/AzVUdYaITAd+xym64SEFG+GYfYA+ALVr1yYzM3Of/QcccACbNm0qcJv27NlTqPIAhx9++F6rycLWFQ/i0aa82L59e65rkWg2b95c5MdMNMnYJkiOdsnOndR7/XVSMzLYU7kyC6+7jr86d4bffnObjxUrOpCXrWClSnu49NIlZGauSaDUpYAg0VlxPZ+W3veNwFHe97OBrwPWMRU3FBm+dfblyQwdJ0L5hsC3uF5ZBdwaviPxlBZwp5dvIVDXV+4noAbwT2AiUM3bZgDt85I7WoTx7OzsXOlBKU0RxoOS6DZlZ2dbhPE4kYxtUk2Cdn3xhWrDhi7i98UXq65eHbNNqakua/iWkqIq4vaPGVNUwueGUhhhfD9clHGADTjPKACLyJnzykuhdlLVxhG29wKWX4zrvTUGmnlpP3kn9A0gFB53JXA4gIiUBw7wZO6CU8qbVXUz8BFQIP84lStXZv369SFFaiQYVWX9+vVUrly5uEUxjPjz99/Qvz+0bQtbtsDEiTB2bJ7r4gYNgipV9k2rUgVefdV5BMvKMmvLEEGHLn8AjgGygDlAXxH5FTdc+Fv0YoVDRBoAv6obgkwFjvZkqAikicghqroWOA0IDUNOAK7A9di6Ap+oqnoBY3uLyMO4XmAH3JBsvqlbty4rV65k7dq1BWrX9u3bk+6hneg2Va5c2db8GcnH+PFuycDq1c7L0gMPQLVqgYqGlJitm8uboIpuGPAP7/v9wCTgYmAHTqkUCm/ZwNM4x9ETRWSOqp4BtAVuE5FdOL+a/VV1nVfmPuAzb99yoIdX3UvAaBFZhuvJdffS3wJOwc3rKTBJVUMWnPmiQoUKNGhQ8GWGmZmZNG/evMDlSyLJ2CbDSBi//eYU3LvvwrHHus/jj893NenpptiCEEjRqWqG7/tsb23aMcCKkOIpDKo6HhgfIX00MDpKmRHAiAjp24FuEdL3AFcXVlbDMIwCk50NI0bAbbfBrl3OV+UNN+xdMuAnIwNuuqk1a9ZYb62wBF1eUBEo5ymRkNn/bBGpLCIVVXVnIoU0DMMo9Sxc6Ba+ffUVdOrkFN6RR0bMmrNGzk0HmJeTwhHUGOVNoH+E9L44QxDDMAwD9vFWUr8+vD5qB9x9NzRv7tbHvfoqfPxxVCUHkdfImZeTghN0jq4NEOkUTwHuiJ84hmEYpZdwbyWHL/+c43r2Bl3iumJDh8Ihh+RZTzRvJublpGAE7dFVAXZHSM8GCu+G3zAMIwkI9cQO4C9GcDWf056KuoPLa02CMWMCKTmI7s3EvJwUjKCKbh7OyjKcS3CLvg3DMMo8K5YrF/A2i2lIL17kcW6iMQsYs/aMfNUTbY3coEFxFLYMEXTo8gHgXRH5J/CJl3YqzrqxS9RShmEYZYWVK5m83zWctm0Cs2nOOXzAbFoAkJrPnljI4OSmm7azZk1ls7osJEGXF0wUkXOBO4GnvOTvgfNU9aNECWcYhlHiyc52gVBvv52Oe3ZzR4XHGLzrevZ4j9eC9sTS0+Gww77m5JNPjq+8ZZCgQ5eo6iRVbauqVb2trap+JCIWtdMwjLLJggXOddeAAdC6NeUXL6DRKzdTN7U8IpCaCi+8YD2x4iaQohORB6KkVwTejqtEhmEYJZ3t2+Guu+C442DpUnjtNZg8GY44gvR052fS/E2WHILO0V0lImtVNTRsGVJy7wDmgNAwjLLDp5+6NQRLl8Jll8GQIVCzZnFLZcQg6NDlv4F7RCQd9iq58bgoAacmSDbDMIySw59/Qu/ecPLJzn3Xxx+7npwpuRJPIEWnqnOB84FnReQ/OCV3GHCKqq5PnHiGYRjFR0YG1E9VLpQ3WVuzIdkvvwK33OLm5k47rbjFMwKSH2OUz3Hr5l4HDsWUnGEYSUxGBtzX61eeWtGZN7iQFdmH0bbid2QcOzj3IjejRBNV0YnIhPAN5/1/HS4A6ihfumEYRrEQ8i15yikdqF/f/S5sfUek7uHrS59m1vY0TmUaN/E4J/ANM7Y3N3+TpZBYxijRemuTEyGIYRhGftnXt6QU2st/RgYM6zWfsdt705pvmMQZ9OM5ssiJP2n+JksfURWdql5ZlIIYhmHkl1he/sMVXUZGHtG4t2/nz/4P8OX2wfzFgaQzhrFcAsg+9Zi/ydJH0OUFhmEYJY6gXv7Dowrk6vlNnw5XX82AjT8yiiu4iSfYQI1c9Zq/ydJJrDm6qSLSNq8KRORAERkoItfGVzTDMIzYBPXyH63nN/i2DXDVVXDKKbBnD+m1pnAloyIqOfNyUnqJZXU5BnhdRJaKyOMi0l1EOojICSJypojcKCLvAKuARrglB4ZhGEVGUC//uXt+yoX8j49XNnSBUG+9FebP56whnSLWN2aMeTkpzcSaoxslIhm4CAUXA1cBB4R2A4twhinNVXVJogU1DMMIJ6R43NybUq+eRPTyX6+eG64EOJwVPEt/zmEicyu2pPY3k6FZswj1RZnLM0odMefoVHUXMNbbEJEDgP2A9d4+wzCMYiU93W2ZmZ9G9fQ/aBD07b2HntuGM4iBCMqtFYZw7MhrObZZ+Yj1GclD4AXjAKr6t6quNiVnGEZpIr3JPH459CSGcT2f044zDltI01du4JLLzR6vLJAvRWcYhlGq2LYNbr8dWrSg5sZfYOxY/p39IV+srG+9tjKEvc4YhpGcfPIJXH01LFsGV14Jjz8OBx9c3FIZxYD16AzDSC7Wr4eePeFUL7DKtGnw8sum5MowJULRiUg3EVkoItki0tKX3kpE5njbXBHp4tt3sYjMF5F5IjJJRGp66e1FZLaI7BaRrmHHuUJEfvS2K4quhYZhJBxVeP11aNgQRo+G226DefPcGjmjTJOvoUtPCR0JfKCqW0SkKrBDVXcXUo4FwAXA8xHSW6rqbhGpA8wVkfe9fcOANFVdJyKDgQHAvcAKoAdwc5jsBwP3AC1xyyNmicgEVf2zkLIbhlHcZGXR5Pbb4Ztv4PjjYepUaNq0uKUySgiBenQiUltEvgG+xS01qO3tGgI8UVghVHVxpLV4qrrVp0Qr4xQUOOdzAlQVEQGq4xauo6pZqjoPyA6r7gxgiqpu8JTbFODMwspuGEYxsns3DB0KjRpx4Ny5MGwYzJhhSs7Yh6A9uqHAaqAGrscU4k3g6XgL5UdETgBeBlKBy0KKT0T6AfNxIYN+BK7Jo6rDgF99v1d6aZGO2QfoA1C7dm0yMzML0YLcbN68Oe51FjfJ2CZIznYlS5uqLVvGUY8/TvUlS1jfujXf9+5N+SOOgM8/L27R4kayXKtiR1Xz3IA/gMbe903AEd73BsCWgHVMxQ1Fhm+dfXkycUOVkco3xPUoKwMVgGm4YVQBhgN3huUfBXT1/b7Fnwe4C7gpL7lbtGih8Wb69Olxr7O4ScY2qSZnu0pqm8aMUU1NVRVxn2PGRMm4ZYvqf/+rmpKiWru26rhxqtnZJbZdhaE0twmYqQF0Q1FsQXt0+wE7I6QfAmwPUoGqdgp4rGjlF4vIFqAxXtwMVf0JQETeAG7Lo4qVwMm+33VxitUwjGImz+gCIaZOdUsGfv7ZOWN+7DE46KAil9coXQS1uvwMZ+ARQkUkBbgV17NKCCLSQETKe99TgaOBLOA3IE1EDvGyngYszqO6ycDpInKQiBwEnI4FkTWMEkGsuHKAWzLQowecdhqkpLiwOi++aErOCETQHt1/gU9F5HigEs4ApRHOyXObwgrhLRt4GtdDnCgic1T1DKAtcJuI7MIZl/RX1XVemfuAz7x9y/EUsSfjeOAg4FwRuU9VG6nqBhF5APjOO+z9qrqhsLIbhlF4osaVW6582X8sxzx/PdWz/2JE9YHUvP1OLj65ctEKaJRqAik6VV0kIk2AfsAO3DzZm8Azqvp7YYVQ1fFECPOjqqOB0VHKjABGREj/DjcsGanMyzjDFsMwShD+6AIh6vMLL1boR5vnJvM1J9CbkSzY2IQqAyC7ojleNoITeMG4OmfO96jqOap6lqreGQ8lZxiG4Y8rl8JubuQJFtCYE3Z9yQCepg1fsoAmQNiQpmEEIFCPTkSOi7VfVWfHRxzDMMoiod7Z2Jtnc//q3rRgNiubn8tJ3z/DrxyeK3+0oU7DiETQObqZuMXa4ktT3/eUuElkGEbZY8sW0ufcS/raoVC7Jjz9BnW7dqVcA3Ez8GHUq1f0Ihqll6BDlw2AI7zPBsBRQHfcgu1zEiOaYRhlgo8/hiZNXHSBnj1h8WLo1g1E9hnSDFGlihvqNIygBDVGifBOxTIR+RvnP/KjuEplGEbys3Yt3HgjjBkDRx0FmZnQocM+WUJDmgMHuuHKevWckjNDFCM/FDYe3S9AszjIYRhGWUHVRRe48UbYuBHuugvuuAMqR14ykJ5uis0oHEGNUcIDOQlQBxctIJczZsMwjIj8/DP07QtTpsCJJ8LIkdCoUXFLZSQ5QXt069jX+AScsvsVuCiuEhmGkXzs3g1DhsC990L58jB8OPTrB+VKREhMI8kJqug6hv3OBtYCy7TwsegMw0hmZs2CXr1gzhzo3NkpuboRfToYRkII9Dqlqp+GbZ+r6g+m5AzDAOeUuX5910GrX9/9ZssWuOkmaNUK/vgD3n4bxo83JWcUOVF7dHktEvdjC8YNo+wSKfLA6EsncxJ9aUAWP55yNf96+xE48MBildMou8Qauoy0SDwSii0YN4wyiz/ywCGsYSg3kM5YFnMM7fiM2V+344WJZjlpFB+xFF2DIpPCMIxSi3PHpVzOawzhRvZnE/dyDw9zOzupBJ5vSlN0RnERVdFFWSRuGIaxD23r/MRdq/pyGlP5kpPozUgWk7ZPHvNNaRQn+VowLiKHAvWAiv50Vf0snkIZhlEK2LULhgzhk7X3soWK9ONZnudqNIKNm/mmNIqToAvGDwXGAu3Jmbczp86GUVaZOdMtGZg7l/Lnn8+0U4bz0ROHoctBxDk/CWG+KY3iJuhqzSeBPUAasBVoB3QDFgNnJkQywzCKnIjLBPxs3uxcd51wAqxZA++8A+PHc8G1h5GVlePdKzXVKbzUVHjhBZufM4qXoEOXHYCzVfUHEVFgrap+KSI7gAeAKQmT0DCMIqF/fxgxIqc3tny5WzYAnqL66CPnzWT5cufG65FH4IADctVjvimNkkbQHt1+ODdgABuAWt73RUDTeAtlGEbRkpGxr5ILsXUrDLltDVx8MZx1Fn/vqkLX2p9T7vnnqH/sAbl7fL76YvYMDaMICarofgCO8b7PAfqKSCpwDfBbAuQyDKMIGTgwt5IDpQevMGXlMfDOO8y74F7q//k9b//RFtWcHl+4EgstIF++nJj5DKOoCKrohgH/8L7fD5wO/Az0B+5IgFyGYRQh4eb/R7KMqXTiFXqyrFIjmDOH82bdw1/bKu2Tb6u3Rs6PfwF5rHyGUVQE9XWZoaqjvO+zgfrA8UA9VX0zYdIZhlEkhMz/y7OL23iY+TShJTPpywh+HPkpNGwYdS1ceHrQfIZRVARSdCLSWUT2Gq6o6lZVna2q62KVMwyjdDBoELSv9A2zaMHD3MFEziaNxZTrdzXpl7nHRLS1cOHpQfMZRlERdOjydWC1iDwnIiclUiDDMIqYTZtI//Y6MneeyCEpGzifd7k59S0GjzmUZ5/NyTZokFsT5yfSGrmg+QyjqAiq6GoDtwD/BD4TkZ9F5AEROTpxohmGkXAmTnQRvp9+GunfnzobFvGudiYrK/cSgfR0tyYurzVyQfMZRlERdI5uk6q+oqqnAYcDw4F/A4tE5NvCCiEi3URkoYhki0hLX3orEZnjbXNFpItv38UiMl9E5onIJBGp6aW3F5HZIrJbRLr68jcTkRneceaJiEVGN8ouf/wB3bvDOedA9erwxRcuIGr16jGLpadDVhZkZxNRGeY3n2EUBfmOY6+qv+MU3cPAPKBFHORYAFwAhPvMXAC0VNVmOA8sz4tIeW++cBjQUVWbenIM8MqsAHrgXJb52QpcrqqNvLqeFJED4yC7YZQY8ly/pso/Jk6EY45xQVDvvx9mz4aTbEbCSF7y69S5I5AO/MdLGg/cWFghVHWxV394ut9IuTI5/jXF26qKyHqgOrDMK5Pl1ZUdVtdS3/dVIrIGOAT4q7DyG0ZJIFIA1H08myxdCn36cMynn0L79m488WibfTCSH9Hcq0RzZxJ5DOiO84gyGRgDvKeqO+IqjEgmcLOqzvSlnQC8DKQCl6nqeC+9q5e+BfgR17vb4ys3CvhAVd+KcJxWwKtAI1XNjrC/D9AHoHbt2i3GjRsXryYCsHnzZqpVqxbXOoubZGwTlK52de/emj/+qJwrvW6tjXx67o3Uf+019lSqxKIePfizSxfX7UsiStO1CkppblPHjh1nqWrLvHMWAaqa5wZ8hVscfnCQ/FHqmIobigzfOvvyZOKGKiOVbwh8i+vZVQCmAUfienbDgTvD8o8Cukaopw6wBGgdRO4WLVpovJk+fXrc6yxukrFNqqWrXSKqzhdJznYCM3Qejd2PCy9U/f33UtWm/JCM7SrNbQJmagH1Rby3QEOXqlroAXxV7VTI8otFZAvQGKfcUNWfAETkDeC2vOoQkerARJxS/Low8hhGSePgg2H9eve9Gpt4iDu4hmdYnXIYjJ8A557rdv7wQ/EJaRjFQIkeuxCRBqGF6p5vzaOBLJx/zTQROcTLehouZFCsuiri5hRfU/PmYpRywo1O+veHjRvdvnOZwCLSuIZneK7cAD4fsShHyRlGGSRfxiiJwls28DTOOGSiiMxR1TOAtsBtIrILyAb6q+eNRUTuw63p2wUsx1laIiLH4xTaQcC5InKfOkvLC3GBY2uISA/v0D1UdU7RtNIw4kMko5MRI6C2/s5T/B/deIv5NKYbb7LsoNas61W88hpGcVMiFJ06A5PxEdJHA6OjlBkBjIiQ/h1QN0L6GJwRjWGUasKdJgvZXKUv8Ri3UJnt3MEgHuMWdlMB1rse34oVzgXXoEFw2GHFJrphFAsleujSMIzc+J0jH80PTKcjI+nD9zSnKfN4mDuckvMID5czdWqtCLUaRvIS1KnzkyLSONHCGIaR96LvevWgAju5kweYy7E0YT49eYlT+YQfOSpm3Vu3wosvHpEw2Q2jJBK0R3c8MFdEvhWRPp71omEYcSZS0NIrr4SaNXMU34DjvmKONOcB7mY8XWjIYjIq9qRK1RyHC7GWyK1ZUyn6TsNIQoL6umwDpAHTgXuAVSLymoh0SKRwhlHWiBS0dNcut2ygmm7kluXXcOP4tvyj6iauPOQDLpFx7KlRG1XYsiWnTHYuNwg51KoVVz8PhlHiCTxHp6pLVPVWnFPn7kA14GMR+VFEbhORgxMlpGGUFaIFJ+3MuywijX48x9Ncy3EVF/LKmrPJzoZq1ZwyDIII9Or1c/wENoxSQEGMUSrgfEseAKTgnChfBqwQkUviKJthlDnCg5PWYRVv8R/epQvrqUFrvuZ6hrF8w/578wSN3C0CfftCp05r4iixYZR8Ais6EWkpIs8CvwODga+Bf6nqqd46tYHA0MSIaRhlg1DQUiGbPjzPItI4iw+5jYdpyUy+o9XevCEjlWiRu2vU2Dcm3OjR7BNI1TDKCkGtLufj/F0ejluYnaqqA1X1F1+2sbgF34Zh5JOQpeVll0HTCov5snwHnqcvs2hBU+bxKLfts2QA3HweRI/oPWyYxYQzDAjeo3sDaKCq56rqBPVFCQihqmtV1dblGUY+CVla/r58B3fpfWT+3Yyjdy9kRu+XWT16Ksv4V8RyoSFLi+htGLEJ6tT5gUQLYhhllYEDodnWLxlJb9JYzFgu5nqeZN2LtdCR0cv5hyzT002xGUY0Aik6EXk5yi4FtuOCnv5PVVfFSzDDSHYyMuCR2//m1l9vox8jWE49zmIiH3GWyxAjVGSVKm7I0jCMvAnq6/IQoB3OsfICLy0ULmcWcAFwv4i0MyfJhpE3GRnwfs/xTNo5gH+wmiHcwN3czxbyDrKZmuqUnPXgDCMYQefUvgQ+AuqqantVbY9znPwh8DEu+vdE4ImESGkYJZC8XHVF5bffqNm7C+N2XsBaDqE1X3MTQwIpOREzLDGM/BJU0V0H3K+qe302eN8HATeo6k7gUaBZ3CU0jBJIRoZzzRXuqiuSssvI8Fx4STb95Dk2Hp5G+22TuJVHOJ7vmMnxgY8bbSmBYRjRCaroqgF1IqT/w9sHsJESEvbHMBLNddfl9kaya5dL95ORAT17Qq31i/icdjxHf77V42nMAgZza64lA7GoWNHm5QyjIARVdOOBl0Skm4jUF5FUEekGvAS84+VpBSxNhJCGUdJYvz56un9Is/flO7hj5z3MoRnH8AOX8yqnMYWfORKRyHWEqFFj3+8vv2xDloZREIL2wPoCQ3CBS0NldgMvAzd7vxcDveMqnWGUQkLRv9vyOSO1N8ewhDGkcwNDWefzqaAxrCpTUmDduiIQ1jDKAHn26ESkPHAycCdwMNAcOA44WFX7qeoWAFWdYxaXRlnB39sKp8LWvxjB1XxOeyqxgzOYxGWM2UfJgbOe7Ncvch19+sRRWMMo4+Sp6FR1N254spqqblHVeao6N6TgDKMsERqWjDR0mVJOuYC3WUQavXiRx7mJxizgY86IWNegQc73ZL9+rgcH7rNfP/NJaRjxJOgc3Vzgn4kUxDBKOv6gqCFC82wnHLaSCSldeJuurOYftOJbbuFxtlI1Yl01auTMtz37LOze7YYyd+82JWcY8SaoorsXeEJEzheRw0XkYP+WQPkMo8QQKSgqms0tVZ9hyqo0Tt71MTfzGK34ltm0iFpPyOGyYRhFQ1BjlIne5zvs65hIvN8p8RTKMEoi4XHfGrGAF+jDSVtm8DGn0ZcR/MIRedZjDpcNo2gJqug6JlQKwygF1Kvnhi0rsZ2BDOJWHmUj1bmU0WSQjnvvi01qqik5wyhqgkYv+DTRghhGSWfQIHj1qs94akcfjmEJr3EZNzKE9dSMmF9k3yUE5ojZMIqH/EQYbyIiw0XkIxGp46WdLyLNEyeeYZQQ/vyTE17szcc7OlCRnZzOZG6s8RrUiKzkQhG9LUacYRQ/QSOMnw58BxwGnALs5+06ErinsEJ4HlcWiki2iLT0pbcSkTneNldEuvj2XSwi80VknohMEpGaXnp7EZktIrtFpGuEY1UXkd9EZHhh5TbKAKrw5ptsa9CQ+pmvMJhbaMwCpnA627ZBs2a5i4RcdaWnW4RvwygJBO3RPQDcqKpdgJ2+9Eyc66/CsgAX6uezCOktVbUZcCbwvIiU9xaxDwM6qmpTYB4wwCuzAugBjI1yrAcAG4otg+Q72sCvv9L4zjvhwgtZtu0wjuc7bmUw26gCOAvMadNyF9u9O96SG4ZRGIIquka4kDzhbMB5SykUqrpYVZdESN/qLVgHqEyOxad4W1UREaA6sMork6Wq83Cx8/ZBRFoAtXGhhYwyhH8NXCjaQJ8+uZVdRgYckbqHa2U4m1PTqPbN9zx40OM03/kNcwg2Sp+d7ZYiGIZRMghqdfknbtgyKyz9OGBlPAUKR0ROwPnUTAUuCyk+EekHzAe2AD8C1+RRTzlcvLzLgFPzyNsH6ANQu3ZtMjMzC9eIMDZv3hz3Ooubkt6mm25qzdatlfdJ27oVbrppO4cd9jUAU6fW4qPBOxm7qy+t+YbJejp99zxH1p95LxkIZ8UKJTOzZA4clPRrVVCSsV3J2KZiQVXz3HCx5r7CBVvdCBwFdAB+Ae4OWMdU3FBk+NbZlycTN1QZqXxD4Ftcz64CMA03RyjAcODOsPyjgK6+3wOA/3rfewDDg8jdokULjTfTp0+Pe53FTUlvk4iq68vtu4l4GbZu1aer36E7Ka9rqKmXMEYhO2IZf9lo+1JTi7O1sSnp16qgJGO7SnObgJka4BlbFFvQHt2dnuJY7imWRd7nWFzw1SAKtVPAY0Urv1hEtgCNvWOjqj8BiMgbwG15VHEi0E5E+uNi6FUUkc2qmlc5IwkIrYEL5+CDgenToU8fBmxcxiiu4CaeYAMxvDbjrCjPOgteegl27tx3X4UKtozAMEoSQdfR7QLSReRuXPSCcsD3qvpjIoUTkQbAr6q6W0RSgaNxw6cVgTQROURV1wKn4cIExWrDXps3EemB6zmakisjDBrkIoD7g6UexAYGb7gFTnmZnziCfjKFKQHex1JTnRUlQJs2LthqyMlzjRrOvZdZWBpGySHwOjpwPShVfUtV34inkhORLiKyEtfrmigik71dbYG5IjIHF/y1v6quU9VVwH3AZyIyD2gGPOTVdbxXVzeclebCeMlplF7S06F69dAv5SLGsZiGXK6v8gi30oT5UZTcvkHjwhd9p6e7uHGhQct160zJGUZJI+jQJSJyEc6IoxZhClJVzyuMEKo6HqfIwtNHA6OjlBkBjIiQ/h1uLjHW8UbhhmKNMsSGDXA4K3iOfpzNh3zL8ZzOx8zj2Khl9ttvD7VqlWfFCjf8GVofZxhG6SGQohORx4Drgek4M/4YsZENowSyZw/3HDicm/50dv/XM5SnuZbsPPyRb9+esneY0jCM0knQHt3lwMWq+lYihTGMhDBvHvTuzT1/fstEzqI/z7KC1EBFa9XagTP0NQyjtBJ0jq4cMCeBchhG/Nm2DW6/HVq0gKwsrq35OufwQWAlV7Ei9Or1c4KFNAwj0QRVdC8AlyZSEMMoKBFde02bBk2awCOPwOWXw+LFPLO+O0FC6YTYf3/o1GlNgqQ2DKOoCDp0eSBwiYichvMrucu/U1X/L85yGUZUMjKci60VK9w6uE2bctaybVq+nuwrboY9o/il/D+5ik/4eVpHBn0UfS1dNDZsSIz8hmEULUF7dGm4ocudwDFAE9/WOCGSGUYEwn1Wrl8fUnJKd15nMQ3pvmcMD3E7abvnMZ2Oe/1annWWWx7gp0oVt/YtEvXqJbo1hmEUBUEXjFuEcaNEMHCg81HpJ5UsnqMf/2YS39CKTkxlPk33ybN1K3z4oYsJF+oNhpYLgFOE/notSKphJA+B19EBeDHfjgTmqOqOxIhkGJHJyNh36DGF3VzL0zzInSjC/zGMZ7gm6pKBFSvcGrho6+DCFWB6Opg/XcMo/QRdR7c/LoLAf3Br6P4F/CwiI4DVqnpvwiQ0DKB/fxjhcw9wLHN4kV60ZBYfcDb9eZZfiT3WGGsoMpYCNAyjdBN0ju5R4FBcWJ5tvvQPgC4RSxhGnMjIcEpOFfZjK49wKzNpSV1WciH/41zez1PJ2VCkYZRdgiq684DrVXUO+3pFWQzkP1iXYYQRK/r3wIFOyZ3KVObThFsZzCtcSRqLeJMLyWvJQI0abm7OemyGUTYJOkd3ELA+Qvr+wJ74iWOURUKWlCFjkJCVJDjltGX5OkZxE1fwGkv5FycznU85Oc96RWD0aFNwhlHWCdqj+w7XqwsR6tVdjQvIahgFJpIl5datMPAOhTFj+KFcQy5hLA8ykKbMC6TkqlQxJWcYhiNoj+4OYLKINPLK3Oh9bwW0T5RwRtlgxYrcafX5hedW9IPLJqNHnkCb30by3fYmgepLSbGhSsMwcgjUo1PVr4CTcAFPf8KF61kFnKiqsxMnnlEW8FtDprCbm3ichTSirXwJTz1FzSVfct2LTUiJHWgAcD25V181JWcYRg6BA6+q6nxVvUJVG6tqmqpeqqrzEymckRzEMjQBZw1ZpQo0ZzbfcAKPcwvTUzox5clFcO21kJJCerpTYOGeTQCqVnXzcamp1pMzDCM3+Yowbhj5Jdxl1/Ll0LMn1KyZo/hSdmzlu5Nv4VtacSir6FfzTf4a9R5d/u/wfepKT3eKLDU1R7GNGQObN0N2NmRlmZIzDCM3+fKMYhj5JZKhyc6dzkclwFHLP+aEXn1poL9A797UefRRnjvooKj12cJuwzDyi/XojIQSydAEoCZreY3L+Jgz2KEVubD2p667FkPJGYZhFARTdEZCye12S7mM11hMQy7if9zPXTRjDm+tMeNdwzASgyk6I6GEDE0AGvAzkzmD17iCpRxFc77nHu5nB5UtJI5hGAkj6hydiLwctBJV7RkfcYzSjD8gqj8CgOzZTdZ1Q7n+r3vYI+W5Vp7hmey+qPeeVaGCMygpV27fcoZhGPEgljHKIWG/2wPZQGhJQWNcj/CzBMhllDKmTq3F0KG53Xgd/MssLnmnN/z1PXTuDMOH0/rTurwfFiE8ZJwS7v7LMAyjsEQdulTVc0Mbzs3XZKCuqrZX1fbA4cAk4JuiEdUoybz44hH7Bi5lC/dvvYnT72oFq1fD22/D+PFQty7p6W4pQHY2VKsWihCew9atrmdoGIYRD4LO0f0fcK+qbgkleN8fAK5NhGBG6WLNmkp7v5/OZBbQmJsYwov0hkWL4IIL3OK3MKJZZUZLNwzDyC9BFV01XDy6cOoAEXxV5A8R6SYiC0UkW0Ra+tJbicgcb5srIl18+y4WkfkiMk9EJnnRzxGR9iIyW0R2i0jXsOPUE5GPRWSxiCwSkfqFld1w1Kq1g5qsZQzpTOZMtlOZtnzOw6kj4MADo5aLZoRiximGYcSLoIrubeAVEekuIvW9rTvwEvBOHORYAFxA7vm+BUBLVW0GnAk8LyLlRaQ8MAzoqKpNgXnAAK/MCqAHMDbCcV4DHlPVhjiH1GviILuhyjOthvADx9CNN7mXe2jGHL6v0pazzgrm/suPBUk1DCOeBPWM0g94AhgFVPDSduMU3c2FFUJVFwNI2NCWqvp9alQmJzyQeFtVEVkPVAeWeWWyvLqy/XWJSBpQXlWnePk2F1busk5GBjz/35+4e9XV/IdpLPtHGy6SF/hkdRr16sFZZzn/lNHizPk/I1lrGoZhxANR1bxzhTKLVAWOxCmZZf45u7gII5IJ3KyqM31pJwAvA6nAZao63kvv6qVvAX7E9e72+MqNAj5Q1be83+cDvYCdQANgKnCbv4yvbB+gD0Dt2rVbjBs3Lp7NZPPmzVSrVi2udRY1n0w+iF2PfsTdeh87qcitPMpL5Xpx6+1L6dTJdZS7d2/NH39UzlW2du3tjBv3dVGLXCCS4VqFk4xtguRsV2luU8eOHWepasu8cxYBqhp4A2oCJwCV8lPOKzsVNxQZvnX25cnEDVVGKt8Q+BbXs6sATCNH6Q4H7gzLPwro6vvdFfgbOALXk30buCovuVu0aKHxZvr06XGvs0j59ludl3KsKujbdNFDWanOZbNqjRo52UR0b7p/Eyk2yfNNqb9WEUjGNqkmZ7tKc5uAmZpPPZGoLdAcnYjsLyJv4ua0vgIO89JHiMi9ARVqJ3UhfsK39wKWX4zrvTUGmnlpP3kn9A1cvLxYrAS+V9WfVXU38C5wXJBjGx6bN8MNN0Dr1hy8Zy1deIf/8A6r3O0A5KyHAzM0MQyjZBDUGOVRnNXlccA2X/oHQJeIJeKAiDTwDE8QkVTgaCAL+A1IE5HQovbTgMV5VPcdcJCvzCnAorgLnWSEYsmdLR/y6wGN4MkneS77atJYxLt5XHozNDEMoyQQVNGdB1yvqnPIMQgBp1yOKKwQItJFRFYCJwITRWSyt6stMFdE5gDjgf6quk5VVwH3AZ+JyDxcD+8hr67jvbq64aw0FwKom4u7GZgmIvNxQ54jCyt7aSSvQKj+fHf2/oOHl1/MRM5mY3Y12vAF/XmWjRwQtf5QnZHix1lgVMMwipqgVpcHAesjpO8P5DLmyC/qDEzGR0gfDYyOUmYEMCJC+ndA3ShlpgBNCyVsKScUCDWWJSQAqnxz9SvM2nYzVdnC3dzHo9zKTirlqjOc8DpNsRmGUZwE7dF9h+vVhQj16q7GzdkZpYRIgVC3boUrrvD17H78kdWNT+WpLVexgMYcy1we4O5ASs5fp7nxMgyjJBC0R3cHMFlEGnllbvS+t8I5ezZKCdFca+3ZA/1776LRe4/RbML9VNlZmT48z4v0IpbNUmqqqzPSKhVz42UYRkkgUI9OVb/CWTVWBH4CTgVWASeq6uzEiWfEm2gWj634hs+3taDZmwPh3HM5Rhczkj4xlVzIsMSsKw3DKMkEDryqqvNV9QpvSUCaql6qqvPzLmmUJMItHquxiWH8HzM4kYPZQGfegzffpGJqnZj1+A1LBg2CSpX2nao160rDMEoKQdfR7RGRWhHSa4hIoY1RjKLDbxhyNh+wkEYMYDjP0p80FvFlDTcVO2hQxGADgFNyWVn7uvG6+eYlZl1pGEaJJGiPLsojj0o4l1pGKaI2qxnHRXzAuWykOm34kmsZziaqs3FjztKAvn1zK7toPbVOndbsjTHnV4KGYRjFTUxjFBG50fuqQF8R8TtCTgHaAT8kSDYj3qjCSy+xmFuowlbu5AEG8192UXFvll27nLVkejo8+yy0aWMOlw3DKN3kZXUZCqoqOIfI/mHKnTgvJX3jL5YRd5Ysgauvhk8/ZXH59ly5+wWWcnTErMuX53y3dXCGYZR2Yio6VW0AICLTgQtU9c8ikcqIHzt3Mjd9MEe/9SDb2I9bGMkblXqyJbscZEcuEm1uzjAMozQSdI7uTPb1cQmAiFQWkYoR8hslgRkz+OvI4zj2rbt4j840ZDEv0YtNW8pRLsaVV43uFswwDKO0EVTRvQH0j5De19tnlBAyMqBxvY08IwPIPqkNW1b9zblMoDv/4w/+sTff7t2QkhK9HvNqYhhGshBU0bUBPo6QPoW8w+MYCSbkpFkE3rh0ApN+TaMfzzKcARyTvYgPODdiuT0xFoaYVxPDMJKFoIquCrA7Qno2zrGzUUyEnDTvWP47b9CN9+jMBg7mRGZwHU+xOcblSU2FGjUi7zOvJoZhJAtBFd084OII6ZfgooQbxcSdd2STvvUFFtOQc3mfOxhEC2bxLSfELFexolsqMGyYxYwzDCO5CerU+QHgXRH5J/CJl3YqLuZbwgKvGq7HFnUd2w8/MGrF1XTgMz6hI1fzPMv4V646Qr22UPTvGjWcgvMvG7C1coZhJCtBnTpPBM4FUoGnvK0ecJ6qfpA48co2oWHJ5cudJeTy5XDllVCnxk7ulgfY0fBYmsp8evISpzItopKrUsUptXXrXB2q7rtfkaWnY15NDMNIWoL26FDVScCkBMpihBEpdlzLXV8xckNvGrGI1+nOzeWeZNWe2vvkEXEKLTXVemeGYRiBoxcYRY/f8nF/NjKca/iCtlRjM2cxkUt4PaKS69vXKTrrnRmGYcRQdCKyUURqet83eb8jbkUnbnITWiZQrpz7PPhgl34e77GINPoygmFcRyMW8hFnRaxDFT78sMhENgzDKPHEGrq8FtjkfR9QBLKUaULzcaGhyuXLIbXCKl7gWi7gHebSlC6MZybH51mXrYEzDMPIIaqiU9VXI303EoN/Pk7Ipg8v8OiuW6nITm7lEYZwI7upEKguWwNnGIaRQ2BjFCOxhHphx7CYF+hDO75gGqdwNc/zE/+MWKZGDdi40YXWCWFr4AzDMPYl1hxdthdZPM+tKAVONkLzchV0B3dzH3NoRhqL6MErdGIqWSmRlVxqqlsm8MorWGRvwzCMGMTq0V2IC7gKUBu4HxgPzPDSTgTOB+5JlHDJTkaGWxd3/K4v+ZDepLGYsVzM9TzJWmpRpQpccQW8+uq+ywz8vTaLF2cYhhGbqD06VX1LVd9W1bdxYXpuV9Xeqvqyt/UG7gDOLqwQItJNRBZ6vciWvvRWIjLH2+aKSBffvotFZL6IzBORST4L0fYiMltEdotI17DjDPaOs1hEnhIp3shrd177N8N29eNL2lKFrfybD0lnLGuptbd39uyz7tN6bYZhGAUj6BzdKcCNEdKnA0/GQY4FwAXA8xHSW6rqbhGpA8wVkfe9fcOANFVdJyKDcZah9wIrgB7Azf6KROQkXBSGpl7SF0AHIDMO8uef8eP54s8B/IPVDOEG7uZ+tlBt7+6srJys1mszDMMoOEEXjK8DukZI7wqsLawQqrpYVZdESN+qqqGoCZXJGUoVb6vq9cqqA6u8MlmqOo/c8bPVq6MiUAmoAPxRWNnzQ0YGnFD3N/7sOAwuuIC1HEJrvuYmhuyj5AzDMIz4EbRHdzfwioh0JGeOrjXQCbgqEYKFEJETgJdxfjYvCyk+EekHzAe2AD8C18SqR1VniMh04HeckhyuqosTJXe4M+az/51N+Zee5+Ndt+W5ZCBa6BzDMAwj/wRSdKr6mogsAf4POA+nKBYBbVT1myB1iMhU8IW4zmGgqr4X49jfAI1EpCHwqoh8BOwB+gHNgZ+Bp4HbgQdjHP+fQEOgrpc0RUTaq+pnEfL2AfoA1K5dm8zMzLwb6GPq1Fo8/vjR7NjhQnhXWb6IS0b0pg1fMZVTuZrn+ZkjQy3EnU5H+fLZ9O37A5mZa/J1zOJm8+bN+T5PpYFkbFcytgmSs13J2KZiQVVLzIabL2sZY/90oCVwPDDNl94e+DAs7yigq+/3LcBdvt93A//NS6YWLVpofklNdXECKrJd7+Vu3UEFXcfBehmvKmRrThwBt6Wmqoq4zzFj8n24EsH06dOLW4SEkIztSsY2qSZnu0pzm4CZWgL0iqoGXzAuIrWBy4AjgLvVGYG0AVap6i+F1riRj9kA+FWdMUoqcDSQhZtnSxORQ1R1LXAakNcw5Aqgt4g8jOtCdSA+hjS5D7QC6vMLH3IWDfmBMaRzA0NZxyG58qam7mt4YhiGYcSXQMYoItICWAKkA71wxh/gFEyh/XCISBcRWYlbmzdRRCZ7u9riLC3n4Nbw9VfVdaq6CrgP+ExE5gHNgIe8uo736uoGPC8iC7263gJ+ws3rzQXmqmrIgjOu1KsHv3EYP3EkZzCJyxjDOg4hfDGDeTExDMNIPEF7dI8Dw1T1HhHZ5EufDFxZWCFUdTxOkYWnjwZGRykzAhgRIf07cubh/Ol7gKsLK2sQBg2CPn0qcu7WnJi0ocXfH34IK1Yo9eqJxYozDMMoAoIquhZEtq78Hec1xfARUl5+q0u/UsvM/JSTTz652OQzDMMoSwRVdNuAgyKkHwOULvPAIsIWeRuGYZQMgi4Yfw+4R0Qqeb9VROoDjwJvJ0IwwzAMw4gHQRXdzcDBOC8oVXDus5YBfwF3JkQywzAMw4gDQYcudwMn49arHYdTkLNVdWqC5DIMwzCMuJCnohORFOBv4FhV/QT4JOFSGYZhGEacyHPo0jPLX45bpG0YhmEYpYqgc3QPAI+EYr4ZhmEYRmlBnEuyPDKJzAca4ELbrMRFDNiLqjaNVC4ZEJG1uB5tPKmJC32UTCRjmyA525WMbYLkbFdpblOqqub2e1gMBDVGeZucWHBlikRcKBGZqaot885ZekjGNkFytisZ2wTJ2a5kbFNxEDRMz70JlsMwDMMwEkLMOToRqSIiz4jIbyKyRkTG2jydYRiGUZrIyxjlPqAHMBEYh4tW8FyCZSoLvFDcAiSAZGwTJGe7krFNkJztSsY2FTkxjVFE5CdcBPBx3u9WwJdAZW/ZgWEYhmGUaPJSdDuBBqr6my9tG3CUqv5aBPIZhmEYRqHIa+gyBdgZlrab4NaahmEYhlGs5KXoBBgjIhNCG1AZGBmWVqYRkW4islBEskWkpS+9lYjM8ba5ItLFt+9iEZkvIvNEZFLIyEdE2ovIbBHZLSJdw44z2DvOYhF5SiQ8ZnmpbFM9EfnYa9MiLypGwiiqdnn7q3uGXMNLe5tEpJmIzPCOM09ELkpkm4qqXd6+K0TkR2+7ohS1qZKI/E9ElonIN/7/TlE+K0oFqhp1A14JssWqoyxsQEPgaCATaOlLrwKU977XwcXuK+9ta4Ca3r7BwL3e9/pAU+A1oKuvrpNw86Mp3jYDOLk0t8nblwmc5n2vBlQp7dfKV+cwYCwwvLS3CTgK+Jf3/VBc0OUDk6BdBwM/e58Hed8PKiVt6g+M8L53B/7nfS/SZ0Vp2GIOQarqlbH2Gw5VXQwQ/tKkqlt9PyuTs+hevK2qiKwHquPCHqGqWV5d2eGH8eqo6JWtAPwRx2aEy57wNolIGu7PPcXLtzne7QiniK4VItICqA1MAhK64Lco2qSqS33fV4nIGuAQXKiuhFBE1+oMYIqqbvD2TwHOBF6PY1P8ssetTUBn4F7v+1vAcK/nVqTPitJAUF+XRgERkRNEZCEwH+irqrtVdRfQz0tbBaQBL8WqR1VnANNxb9K/A5NDf5qiJl5twvUS/hKRd0TkexF5TFy0jGIhXu0SkXLAE8AtCRY5T+J4rfx1tsI9RH9KgMhBZYhXuw4D/IZ1K720IqcAbdoru6ruxkWZqVGSnhUlBVN0ARGRqSKyIMLWOVY5Vf1GVRsBxwO3i0hlEamAu3mb44aB5gG353H8f+KGPeribvBTRKR9aW4TblimHS6w7/HAEbh1m4WiBLSrP/ChxtEyuQS0KSRHHWA0cKWq5urJ5pcS0K5Ic1eFcndYhG2KKHsinhWlHbOeDIiqdipk+cUisgVojHeDqupPACLyBnBbHlV0Ab4ODe+JyEdAa+CzQshU3G1aCXyvqj97Zd7FtSlw7yKKXMXdrhOBdiLSHzfvWFFENqtqXuViyVTcbUJEquOcR9ypql8XRh6fXMXdrpW4oNIh6uLmzwojU1G1aSVwOLBSRMoDBwAbgJ7E+VlR2rEeXQIRkQbeDYiIpOImobOA34A0EQk5jD4NyGtoYQXQQUTKe295HQKUiTtxbtN3wEG+MqcAi+IudADi2S5VTVfVeqpaH9dbfa0wSq6gxLNNIlIRGI9ry5sJEzoAcb4HJwOni8hBInIQcLqXVqQUsE0TgJCVaFfgE1VVSsizokRR3NYwybDhelsrgR24Sd/JXvplwEJgDjAbON9Xpi/u5psHvI8bWwc3bBEKhbQeWOilpwDPe2UWAUNKe5u8fad5+ecDo4CKydAuX9keJN7qsijuv0uBXV5doa1ZaW+Xt68nzsBjGW5ItrS0qTLwpif3t8ARXnqRPitKwxYoHp1hGIZhlFZs6NIwDMNIakzRGYZhGEmNKTrDMAwjqTFFZxiGYSQ1pugMwzCMpMYUnWEYhpHUmKIzDMMwkhpTdEaxICKjROSD4pbDyD9Fee08jyV/iMiR3u9MSXx8v2K7N/3HFpG3ROTG4pAj2TBFl+SISHMR2SMiXxagbMIfKnkc/xMRyYiQfpG4wJUHBKynkYiMFpFVIrJTRLJE5FER2S/+Uhtx5g6cg+xii5QQQkRGiMjQIjzkfcCdQe9zIzqm6JKf3sCzQGMRaVjcwuST5sDMCOktgWWq+ndeFYjIpTiXSptw7peOwXl/7wG8Gy9B44nnV7LMIyJVgF4U0sl3nGQR4FzgvaI6pqrOxwWCvbSojpmsmKJLYrweyyXASFxgxqvC9ouI3CQiP4rIDhFZKSIPe/tG4ZzBXiMi6m31I/Xywod6RORMEflcRP4UkQ0iMjm/StYbqjqQ6IpuVoA62uL8Z16rqv3VhUH5WVVfx4XSOd3LE628iMh/ReQnEdkmIvM9xRnanykiz4rIQyKyTkTWiMjj4uLRBarDV89zXtm1uOjQiEhVEXlNRDZ7w3e3i8gH3vm+XETWi0ilsLoyRGRClPZc7dVTPix9rIi8533P97ULeE/keR4icBaQHTofUY59qoj8JSJX+44T8Z4uaPs8jsf5lvzCd72e8OpYKyLXiUglEXnGk2eFiFwWJmslEXnSuwbbReTrWPefxwTg4gDyGTEwRZfcdAWWq+o8XAyxy8V5Mw/xEHAX8DDQCOhGThDK64AZwCtAHW8LGl+tKvAk0AoXAuVv4H3JX0+lBe4h970/UUQE19PLU9EBw4BMVX0hwr7p3uexMco/iHs5uAYX8PJh4HkROduXJx3YDZwEDACuBy7KZx3g3toFF5/vci/tCdzLRhdcZIdjvf3gnPmWw0WZBkDcEFcXoveA3sC9PHTylanq1THGS4rHtYtE0PPgpx0wS6M45BWR/+AiKvRR1ee95Fj3NBS8fecDE9UFOAV33TcBJwCPeHW+CyzFvYi9CrwoIof66hiMuzd64u7h+cAkcTH+ovEt0EpsmL1wFLdXadsStwGfAjd73wUX9uM/3u9qwHZcJONo5TMJ87wfJW0U8EGMeqoCe4C2+SjzKC4AZrSto5fvcE+mRcBc4AIv/VgvX5co9R/m7e8dQ+ZtQLuw9Cdxc0ahczEjbP8U4MWgdfjqmReWpxqwE+geJtOfwCjv93Bgkm9/P2A1UD7GeR0PjPb9vhT3sK9c0GuX1z0R9DxEOPa7wKuR7j+gjyf36WHnLOY9XdB7ExdZ4AKfDDN8+wRYC0zwpVXwrl9X33F2Apf78qTgorQ/GOPcNvXu0yODtsm23JsFXk1SxEUZboM37KGqKs6woxfwNu6tuhIwLQHHPhJ4APe2ewiu51EOqJePalrgBfkMSz/bq3u293s3cL2qzhGRWsAsEZkEHOftj9bzC+2fE2V/Gm6oapKI+HsUFXAvDCHmhZVbBdTKZx2R5DzSy/dtKEFVt4jIAl+ekcBsEamrqitxPYVXNafXEYkxwCgRqaKqW3E9k7dUdTvE7dqFk5/z4Gc/XCibcDoDVwPtVXVG2HFi3tMFaZ/3XzqCfePU7b3u3n9rDa6HFkrbJSJ/knMvhK7nl748e0Rkhid3NLZ5n9ajKwSm6JKXXrg3xhVutA/wohWLyOGh7wUgO0LZCmG/38cFjLza+9yN63HlZ/irOfCIqs7xJ4rIJfgMUVT1d+B37/sa7+FS03esbUTmGk+mSHOAkDOsfy4ukKWfXVG+g3v7DpUNWge4OGl+Quc4ahwtVZ0rIrOBHuKis7ckb8OFD3DXo7OITMMNY57u21+Qa5fXPZGf8+BnHXBQhPR5uPNylYh8rV7XJ4IMkShI+84Hpqmq/xpFuu6x7oVY1zNWrLSDvc+1MfIYeWCKLgnxjA2uwFkXhq8HGg1cCQzFBX88FfgxSlU7ccrSz1rcfJ2fY/HezEWkBtAQuEZVp3tpx5GPe01EGuD+4JF6Y8dFSUdEWuIesL+SM7fXAWeI4893FS7g62m+h2Q4i3DnJ1VVPwkqexzrWIZ7cLYCfoG9VoiNccNdIUYC/8Up9y9VdUmsSlV1h4i8hevJ1cQNdX7q1V/QaxfznqDg5+F7nHVsOL8A1+KGEF8QkT7edQwdJ+I9XYj2dcbNuRWGZbj/U1ucJSUikgKcCIyNUa4xsEpVI/VsjYCYoktOzsY9xEaq6nr/DhEZh5vLeRBnrPGwiOwAPgNqAC1U9TkvexZuIrw+sBnYAHwCPCki5wFLcG/Gh5PzUPsT9ybeW0R+xc2FPYZ7cw5KC+9zdoR9zXGGBvvgPcReA67yHnrfishE4GlP8X+LOydX4JZcXBXroauqm0TkceBxzwDmM9wcUGsgWyMbuMStDlXdLCIvA4+KyDpcr/VOXA/Br5xfB4bgrmnfvGTyGANMBRoAY1U120sv6LWLeU8U4jxMxrW/Rvh9rKo/i0hH9lV2m0Qk1j2d7/aJyCGenF3zOAcx8YadnwMe8a7nL8ANQG3c8p9otAMmFebYhlldJitXAdPDHw4ebwKpuCGr23FGH3cBi3Fzd3V9eR/HvYUuwr211wNe9m1f4hTg+FAB76F5EW4SfQHwjFf/jnzI3wL4WVX/8ieKSCoRenriTOzHAw+r6le+Xd1wb+KP4h7AE3APvuNVdVQAOe4C7gVuxhkjTAH+g9fDCkhh6rgZ+NyTezpuyG4mzuACcEoEZ0250/sMwme4Ybs0cqwtC3PtYt4THvk+D+rWkX0LdI+y/yec5eSZOAtOIcY9XcD2nQt8F6ce1a24a/QKbm64KXCmN/yeCxGpjLOiHRmHY5dpJPrIjWGUfLyH21hgiareW8ziJBRPoS8HHlPVJ3zpHwErVbV3sQmXIETkTNzIQ5qq7imG47+HGxIeXAzHvgborKqn55nZiIkNXRqlnTa4t/R5InK+l3aZ1xso1YhIc9yc0rfA/rgewf7A/7z9B5NjTBJrPWCpRVUnicgzuF7Z8mIQ4Uvc8HBxsAs3F2kUEuvRGUYJxVN0I4GjcfNIc3DrImd5+7NwQ7mDVPXRYhLTMEo8pugMwzCMpMaMUQzDMIykxhSdYRiGkdSYojMMwzCSGlN0hmEYRlJjis4wDMNIakzRGYZhGEmNKTrDMAwjqfl/+1/sE1Vr9UcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Data points')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2O$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2O$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2O',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(type(prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.0047,  0.1966,  0.3590,  0.0692, -0.1098, -0.2643],\n",
      "        [ 0.0222,  0.2488,  0.0647, -0.0402, -0.1613,  0.1564],\n",
      "        [ 0.0333, -0.8185,  0.1459,  0.0855,  0.1969, -0.3953]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.6568,  0.8407,  0.4474], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.5096, -0.0593,  0.4107],\n",
      "        [ 0.2871,  1.0128, -0.3358],\n",
      "        [-0.4071,  0.1870, -0.2271]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([ 0.5503,  0.3201, -0.5740], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(net.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heta    = [0.01,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  1.6, 3.2, 4.8, 6.4, 0. ]\n",
    "Rs      = [0.8, 0.4, 0.2, 1.,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1., 1., 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
