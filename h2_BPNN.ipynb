{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for our 500 H2 molecular configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (500,) entries\n"
     ]
    }
   ],
   "source": [
    "energies = np.genfromtxt('./h2/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-943f3a945352>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #5 (got 1 columns instead of 5)\n",
      "    Line #9 (got 1 columns instead of 5)\n",
      "    Line #13 (got 1 columns instead of 5)\n",
      "    Line #17 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #25 (got 1 columns instead of 5)\n",
      "    Line #29 (got 1 columns instead of 5)\n",
      "    Line #33 (got 1 columns instead of 5)\n",
      "    Line #37 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #45 (got 1 columns instead of 5)\n",
      "    Line #49 (got 1 columns instead of 5)\n",
      "    Line #53 (got 1 columns instead of 5)\n",
      "    Line #57 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #65 (got 1 columns instead of 5)\n",
      "    Line #69 (got 1 columns instead of 5)\n",
      "    Line #73 (got 1 columns instead of 5)\n",
      "    Line #77 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #85 (got 1 columns instead of 5)\n",
      "    Line #89 (got 1 columns instead of 5)\n",
      "    Line #93 (got 1 columns instead of 5)\n",
      "    Line #97 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #105 (got 1 columns instead of 5)\n",
      "    Line #109 (got 1 columns instead of 5)\n",
      "    Line #113 (got 1 columns instead of 5)\n",
      "    Line #117 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #125 (got 1 columns instead of 5)\n",
      "    Line #129 (got 1 columns instead of 5)\n",
      "    Line #133 (got 1 columns instead of 5)\n",
      "    Line #137 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #145 (got 1 columns instead of 5)\n",
      "    Line #149 (got 1 columns instead of 5)\n",
      "    Line #153 (got 1 columns instead of 5)\n",
      "    Line #157 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #165 (got 1 columns instead of 5)\n",
      "    Line #169 (got 1 columns instead of 5)\n",
      "    Line #173 (got 1 columns instead of 5)\n",
      "    Line #177 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #185 (got 1 columns instead of 5)\n",
      "    Line #189 (got 1 columns instead of 5)\n",
      "    Line #193 (got 1 columns instead of 5)\n",
      "    Line #197 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #205 (got 1 columns instead of 5)\n",
      "    Line #209 (got 1 columns instead of 5)\n",
      "    Line #213 (got 1 columns instead of 5)\n",
      "    Line #217 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #225 (got 1 columns instead of 5)\n",
      "    Line #229 (got 1 columns instead of 5)\n",
      "    Line #233 (got 1 columns instead of 5)\n",
      "    Line #237 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #245 (got 1 columns instead of 5)\n",
      "    Line #249 (got 1 columns instead of 5)\n",
      "    Line #253 (got 1 columns instead of 5)\n",
      "    Line #257 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #265 (got 1 columns instead of 5)\n",
      "    Line #269 (got 1 columns instead of 5)\n",
      "    Line #273 (got 1 columns instead of 5)\n",
      "    Line #277 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #285 (got 1 columns instead of 5)\n",
      "    Line #289 (got 1 columns instead of 5)\n",
      "    Line #293 (got 1 columns instead of 5)\n",
      "    Line #297 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #305 (got 1 columns instead of 5)\n",
      "    Line #309 (got 1 columns instead of 5)\n",
      "    Line #313 (got 1 columns instead of 5)\n",
      "    Line #317 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #325 (got 1 columns instead of 5)\n",
      "    Line #329 (got 1 columns instead of 5)\n",
      "    Line #333 (got 1 columns instead of 5)\n",
      "    Line #337 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #345 (got 1 columns instead of 5)\n",
      "    Line #349 (got 1 columns instead of 5)\n",
      "    Line #353 (got 1 columns instead of 5)\n",
      "    Line #357 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #365 (got 1 columns instead of 5)\n",
      "    Line #369 (got 1 columns instead of 5)\n",
      "    Line #373 (got 1 columns instead of 5)\n",
      "    Line #377 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #385 (got 1 columns instead of 5)\n",
      "    Line #389 (got 1 columns instead of 5)\n",
      "    Line #393 (got 1 columns instead of 5)\n",
      "    Line #397 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #405 (got 1 columns instead of 5)\n",
      "    Line #409 (got 1 columns instead of 5)\n",
      "    Line #413 (got 1 columns instead of 5)\n",
      "    Line #417 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #425 (got 1 columns instead of 5)\n",
      "    Line #429 (got 1 columns instead of 5)\n",
      "    Line #433 (got 1 columns instead of 5)\n",
      "    Line #437 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #445 (got 1 columns instead of 5)\n",
      "    Line #449 (got 1 columns instead of 5)\n",
      "    Line #453 (got 1 columns instead of 5)\n",
      "    Line #457 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #465 (got 1 columns instead of 5)\n",
      "    Line #469 (got 1 columns instead of 5)\n",
      "    Line #473 (got 1 columns instead of 5)\n",
      "    Line #477 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #485 (got 1 columns instead of 5)\n",
      "    Line #489 (got 1 columns instead of 5)\n",
      "    Line #493 (got 1 columns instead of 5)\n",
      "    Line #497 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #505 (got 1 columns instead of 5)\n",
      "    Line #509 (got 1 columns instead of 5)\n",
      "    Line #513 (got 1 columns instead of 5)\n",
      "    Line #517 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #525 (got 1 columns instead of 5)\n",
      "    Line #529 (got 1 columns instead of 5)\n",
      "    Line #533 (got 1 columns instead of 5)\n",
      "    Line #537 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #545 (got 1 columns instead of 5)\n",
      "    Line #549 (got 1 columns instead of 5)\n",
      "    Line #553 (got 1 columns instead of 5)\n",
      "    Line #557 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #565 (got 1 columns instead of 5)\n",
      "    Line #569 (got 1 columns instead of 5)\n",
      "    Line #573 (got 1 columns instead of 5)\n",
      "    Line #577 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #585 (got 1 columns instead of 5)\n",
      "    Line #589 (got 1 columns instead of 5)\n",
      "    Line #593 (got 1 columns instead of 5)\n",
      "    Line #597 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #605 (got 1 columns instead of 5)\n",
      "    Line #609 (got 1 columns instead of 5)\n",
      "    Line #613 (got 1 columns instead of 5)\n",
      "    Line #617 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #625 (got 1 columns instead of 5)\n",
      "    Line #629 (got 1 columns instead of 5)\n",
      "    Line #633 (got 1 columns instead of 5)\n",
      "    Line #637 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #645 (got 1 columns instead of 5)\n",
      "    Line #649 (got 1 columns instead of 5)\n",
      "    Line #653 (got 1 columns instead of 5)\n",
      "    Line #657 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #665 (got 1 columns instead of 5)\n",
      "    Line #669 (got 1 columns instead of 5)\n",
      "    Line #673 (got 1 columns instead of 5)\n",
      "    Line #677 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #685 (got 1 columns instead of 5)\n",
      "    Line #689 (got 1 columns instead of 5)\n",
      "    Line #693 (got 1 columns instead of 5)\n",
      "    Line #697 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #705 (got 1 columns instead of 5)\n",
      "    Line #709 (got 1 columns instead of 5)\n",
      "    Line #713 (got 1 columns instead of 5)\n",
      "    Line #717 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #725 (got 1 columns instead of 5)\n",
      "    Line #729 (got 1 columns instead of 5)\n",
      "    Line #733 (got 1 columns instead of 5)\n",
      "    Line #737 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #745 (got 1 columns instead of 5)\n",
      "    Line #749 (got 1 columns instead of 5)\n",
      "    Line #753 (got 1 columns instead of 5)\n",
      "    Line #757 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #765 (got 1 columns instead of 5)\n",
      "    Line #769 (got 1 columns instead of 5)\n",
      "    Line #773 (got 1 columns instead of 5)\n",
      "    Line #777 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #785 (got 1 columns instead of 5)\n",
      "    Line #789 (got 1 columns instead of 5)\n",
      "    Line #793 (got 1 columns instead of 5)\n",
      "    Line #797 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #805 (got 1 columns instead of 5)\n",
      "    Line #809 (got 1 columns instead of 5)\n",
      "    Line #813 (got 1 columns instead of 5)\n",
      "    Line #817 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #825 (got 1 columns instead of 5)\n",
      "    Line #829 (got 1 columns instead of 5)\n",
      "    Line #833 (got 1 columns instead of 5)\n",
      "    Line #837 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #845 (got 1 columns instead of 5)\n",
      "    Line #849 (got 1 columns instead of 5)\n",
      "    Line #853 (got 1 columns instead of 5)\n",
      "    Line #857 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #865 (got 1 columns instead of 5)\n",
      "    Line #869 (got 1 columns instead of 5)\n",
      "    Line #873 (got 1 columns instead of 5)\n",
      "    Line #877 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #885 (got 1 columns instead of 5)\n",
      "    Line #889 (got 1 columns instead of 5)\n",
      "    Line #893 (got 1 columns instead of 5)\n",
      "    Line #897 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #905 (got 1 columns instead of 5)\n",
      "    Line #909 (got 1 columns instead of 5)\n",
      "    Line #913 (got 1 columns instead of 5)\n",
      "    Line #917 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #925 (got 1 columns instead of 5)\n",
      "    Line #929 (got 1 columns instead of 5)\n",
      "    Line #933 (got 1 columns instead of 5)\n",
      "    Line #937 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #945 (got 1 columns instead of 5)\n",
      "    Line #949 (got 1 columns instead of 5)\n",
      "    Line #953 (got 1 columns instead of 5)\n",
      "    Line #957 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #965 (got 1 columns instead of 5)\n",
      "    Line #969 (got 1 columns instead of 5)\n",
      "    Line #973 (got 1 columns instead of 5)\n",
      "    Line #977 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #985 (got 1 columns instead of 5)\n",
      "    Line #989 (got 1 columns instead of 5)\n",
      "    Line #993 (got 1 columns instead of 5)\n",
      "    Line #997 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1005 (got 1 columns instead of 5)\n",
      "    Line #1009 (got 1 columns instead of 5)\n",
      "    Line #1013 (got 1 columns instead of 5)\n",
      "    Line #1017 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1025 (got 1 columns instead of 5)\n",
      "    Line #1029 (got 1 columns instead of 5)\n",
      "    Line #1033 (got 1 columns instead of 5)\n",
      "    Line #1037 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1045 (got 1 columns instead of 5)\n",
      "    Line #1049 (got 1 columns instead of 5)\n",
      "    Line #1053 (got 1 columns instead of 5)\n",
      "    Line #1057 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1065 (got 1 columns instead of 5)\n",
      "    Line #1069 (got 1 columns instead of 5)\n",
      "    Line #1073 (got 1 columns instead of 5)\n",
      "    Line #1077 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1085 (got 1 columns instead of 5)\n",
      "    Line #1089 (got 1 columns instead of 5)\n",
      "    Line #1093 (got 1 columns instead of 5)\n",
      "    Line #1097 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1105 (got 1 columns instead of 5)\n",
      "    Line #1109 (got 1 columns instead of 5)\n",
      "    Line #1113 (got 1 columns instead of 5)\n",
      "    Line #1117 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1125 (got 1 columns instead of 5)\n",
      "    Line #1129 (got 1 columns instead of 5)\n",
      "    Line #1133 (got 1 columns instead of 5)\n",
      "    Line #1137 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1145 (got 1 columns instead of 5)\n",
      "    Line #1149 (got 1 columns instead of 5)\n",
      "    Line #1153 (got 1 columns instead of 5)\n",
      "    Line #1157 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1165 (got 1 columns instead of 5)\n",
      "    Line #1169 (got 1 columns instead of 5)\n",
      "    Line #1173 (got 1 columns instead of 5)\n",
      "    Line #1177 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1185 (got 1 columns instead of 5)\n",
      "    Line #1189 (got 1 columns instead of 5)\n",
      "    Line #1193 (got 1 columns instead of 5)\n",
      "    Line #1197 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1205 (got 1 columns instead of 5)\n",
      "    Line #1209 (got 1 columns instead of 5)\n",
      "    Line #1213 (got 1 columns instead of 5)\n",
      "    Line #1217 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1225 (got 1 columns instead of 5)\n",
      "    Line #1229 (got 1 columns instead of 5)\n",
      "    Line #1233 (got 1 columns instead of 5)\n",
      "    Line #1237 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1245 (got 1 columns instead of 5)\n",
      "    Line #1249 (got 1 columns instead of 5)\n",
      "    Line #1253 (got 1 columns instead of 5)\n",
      "    Line #1257 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1265 (got 1 columns instead of 5)\n",
      "    Line #1269 (got 1 columns instead of 5)\n",
      "    Line #1273 (got 1 columns instead of 5)\n",
      "    Line #1277 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1285 (got 1 columns instead of 5)\n",
      "    Line #1289 (got 1 columns instead of 5)\n",
      "    Line #1293 (got 1 columns instead of 5)\n",
      "    Line #1297 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1305 (got 1 columns instead of 5)\n",
      "    Line #1309 (got 1 columns instead of 5)\n",
      "    Line #1313 (got 1 columns instead of 5)\n",
      "    Line #1317 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1325 (got 1 columns instead of 5)\n",
      "    Line #1329 (got 1 columns instead of 5)\n",
      "    Line #1333 (got 1 columns instead of 5)\n",
      "    Line #1337 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1345 (got 1 columns instead of 5)\n",
      "    Line #1349 (got 1 columns instead of 5)\n",
      "    Line #1353 (got 1 columns instead of 5)\n",
      "    Line #1357 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1365 (got 1 columns instead of 5)\n",
      "    Line #1369 (got 1 columns instead of 5)\n",
      "    Line #1373 (got 1 columns instead of 5)\n",
      "    Line #1377 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1385 (got 1 columns instead of 5)\n",
      "    Line #1389 (got 1 columns instead of 5)\n",
      "    Line #1393 (got 1 columns instead of 5)\n",
      "    Line #1397 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1405 (got 1 columns instead of 5)\n",
      "    Line #1409 (got 1 columns instead of 5)\n",
      "    Line #1413 (got 1 columns instead of 5)\n",
      "    Line #1417 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1425 (got 1 columns instead of 5)\n",
      "    Line #1429 (got 1 columns instead of 5)\n",
      "    Line #1433 (got 1 columns instead of 5)\n",
      "    Line #1437 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1445 (got 1 columns instead of 5)\n",
      "    Line #1449 (got 1 columns instead of 5)\n",
      "    Line #1453 (got 1 columns instead of 5)\n",
      "    Line #1457 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1465 (got 1 columns instead of 5)\n",
      "    Line #1469 (got 1 columns instead of 5)\n",
      "    Line #1473 (got 1 columns instead of 5)\n",
      "    Line #1477 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1485 (got 1 columns instead of 5)\n",
      "    Line #1489 (got 1 columns instead of 5)\n",
      "    Line #1493 (got 1 columns instead of 5)\n",
      "    Line #1497 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1505 (got 1 columns instead of 5)\n",
      "    Line #1509 (got 1 columns instead of 5)\n",
      "    Line #1513 (got 1 columns instead of 5)\n",
      "    Line #1517 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1525 (got 1 columns instead of 5)\n",
      "    Line #1529 (got 1 columns instead of 5)\n",
      "    Line #1533 (got 1 columns instead of 5)\n",
      "    Line #1537 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1545 (got 1 columns instead of 5)\n",
      "    Line #1549 (got 1 columns instead of 5)\n",
      "    Line #1553 (got 1 columns instead of 5)\n",
      "    Line #1557 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1565 (got 1 columns instead of 5)\n",
      "    Line #1569 (got 1 columns instead of 5)\n",
      "    Line #1573 (got 1 columns instead of 5)\n",
      "    Line #1577 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1585 (got 1 columns instead of 5)\n",
      "    Line #1589 (got 1 columns instead of 5)\n",
      "    Line #1593 (got 1 columns instead of 5)\n",
      "    Line #1597 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1605 (got 1 columns instead of 5)\n",
      "    Line #1609 (got 1 columns instead of 5)\n",
      "    Line #1613 (got 1 columns instead of 5)\n",
      "    Line #1617 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1625 (got 1 columns instead of 5)\n",
      "    Line #1629 (got 1 columns instead of 5)\n",
      "    Line #1633 (got 1 columns instead of 5)\n",
      "    Line #1637 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1645 (got 1 columns instead of 5)\n",
      "    Line #1649 (got 1 columns instead of 5)\n",
      "    Line #1653 (got 1 columns instead of 5)\n",
      "    Line #1657 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1665 (got 1 columns instead of 5)\n",
      "    Line #1669 (got 1 columns instead of 5)\n",
      "    Line #1673 (got 1 columns instead of 5)\n",
      "    Line #1677 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1685 (got 1 columns instead of 5)\n",
      "    Line #1689 (got 1 columns instead of 5)\n",
      "    Line #1693 (got 1 columns instead of 5)\n",
      "    Line #1697 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1705 (got 1 columns instead of 5)\n",
      "    Line #1709 (got 1 columns instead of 5)\n",
      "    Line #1713 (got 1 columns instead of 5)\n",
      "    Line #1717 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1725 (got 1 columns instead of 5)\n",
      "    Line #1729 (got 1 columns instead of 5)\n",
      "    Line #1733 (got 1 columns instead of 5)\n",
      "    Line #1737 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1745 (got 1 columns instead of 5)\n",
      "    Line #1749 (got 1 columns instead of 5)\n",
      "    Line #1753 (got 1 columns instead of 5)\n",
      "    Line #1757 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1765 (got 1 columns instead of 5)\n",
      "    Line #1769 (got 1 columns instead of 5)\n",
      "    Line #1773 (got 1 columns instead of 5)\n",
      "    Line #1777 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1785 (got 1 columns instead of 5)\n",
      "    Line #1789 (got 1 columns instead of 5)\n",
      "    Line #1793 (got 1 columns instead of 5)\n",
      "    Line #1797 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1805 (got 1 columns instead of 5)\n",
      "    Line #1809 (got 1 columns instead of 5)\n",
      "    Line #1813 (got 1 columns instead of 5)\n",
      "    Line #1817 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1825 (got 1 columns instead of 5)\n",
      "    Line #1829 (got 1 columns instead of 5)\n",
      "    Line #1833 (got 1 columns instead of 5)\n",
      "    Line #1837 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1845 (got 1 columns instead of 5)\n",
      "    Line #1849 (got 1 columns instead of 5)\n",
      "    Line #1853 (got 1 columns instead of 5)\n",
      "    Line #1857 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1865 (got 1 columns instead of 5)\n",
      "    Line #1869 (got 1 columns instead of 5)\n",
      "    Line #1873 (got 1 columns instead of 5)\n",
      "    Line #1877 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1885 (got 1 columns instead of 5)\n",
      "    Line #1889 (got 1 columns instead of 5)\n",
      "    Line #1893 (got 1 columns instead of 5)\n",
      "    Line #1897 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1905 (got 1 columns instead of 5)\n",
      "    Line #1909 (got 1 columns instead of 5)\n",
      "    Line #1913 (got 1 columns instead of 5)\n",
      "    Line #1917 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1925 (got 1 columns instead of 5)\n",
      "    Line #1929 (got 1 columns instead of 5)\n",
      "    Line #1933 (got 1 columns instead of 5)\n",
      "    Line #1937 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1945 (got 1 columns instead of 5)\n",
      "    Line #1949 (got 1 columns instead of 5)\n",
      "    Line #1953 (got 1 columns instead of 5)\n",
      "    Line #1957 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1965 (got 1 columns instead of 5)\n",
      "    Line #1969 (got 1 columns instead of 5)\n",
      "    Line #1973 (got 1 columns instead of 5)\n",
      "    Line #1977 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1985 (got 1 columns instead of 5)\n",
      "    Line #1989 (got 1 columns instead of 5)\n",
      "    Line #1993 (got 1 columns instead of 5)\n",
      "    Line #1997 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('h2', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates = (xyz_file[:,1:-1])\n",
    "coordinates = coordinates.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[-0.12741596  0.          0.        ]\n",
      "[0.12741596 0.         0.        ]\n",
      "[-0.13270776  0.          0.        ]\n",
      "coord\n",
      "[0.13270776 0.         0.        ]\n",
      "[-0.13988344  0.          0.        ]\n",
      "[0.13988344 0.         0.        ]\n",
      "-274.25385732225\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(coordinates))\n",
    "print(type(coordinates))\n",
    "print(coordinates[0])\n",
    "print(coordinates[1])\n",
    "print(coordinates[2])\n",
    "\n",
    "print('coord')\n",
    "print(coordinates[3])\n",
    "print(coordinates[4])\n",
    "print(coordinates[5])\n",
    "\n",
    "print(energies[0])\n",
    "# There is 500 h2 molecules and each of them consists of 2 atoms, so we have 1000 atoms in total and each\n",
    "# of them has 3 coordinates.\n",
    "# Thus the coordinates array has 1000 lines, each of them corresponding to one atom (the first 2 lines\n",
    "# correspopnd to the first H2 molecule) and 3 columns corresponding to the x, y and z coordinates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12741596  0.          0.        ]\n",
      " [ 0.12741596  0.          0.        ]\n",
      " [-0.13270776  0.          0.        ]\n",
      " ...\n",
      " [ 1.49673271  0.          0.        ]\n",
      " [-1.49918811  0.          0.        ]\n",
      " [ 1.49918811  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[0.         0.25483192]\n",
      " [0.25483192 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                # i-th water molecule\n",
    "N = 2 # 2 atoms per molecule\n",
    "coord = coordinates[N*i:N*(i+1),:]   # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                     # pairwise distances between all of its 3 atom\n",
    "print(np.shape(coord))\n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "#        print('i=',i)\n",
    "        for j in range(i+1,N):\n",
    "#            print(j)\n",
    "#            pairwise_dist_matrix[i][j] = \\\n",
    "#            np.sqrt(  (coord[i][0] - coord[j][0] )**2 + (coord[i][1] - coord[j][1] )**2 +(coord[i][2] - coord[j][2] )**2   )\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99228933 0.99228933]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = len(coord)\n",
    "\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.96927623 1.96927623]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     #1\n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-651.4754354084254\n",
      "torch.Size([450, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "N                    = 2           # number of atoms per molecule\n",
    "number_of_features   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "# heta   = np.linspace(0.01, 4, num=number_of_features)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "\n",
    "data_size            = np.shape(energies)[0]        # We have 500 H2 molecule conformations\n",
    "training_set_size    = data_size - 50\n",
    "\n",
    "    \n",
    "G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features):#,2):\n",
    "        G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])     \n",
    "#        G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.var(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "\n",
    "G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set = np.vsplit(G_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(training_set)\n",
    "data_set = torch.FloatTensor(data_set)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.var(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "print(mean_lab)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)      \n",
    "    \n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=50,   #400,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.13557381e-01 9.18014856e-01 5.81028960e-05 4.09417844e-01]\n",
      " [9.13557381e-01 9.18014856e-01 5.81028960e-05 4.09417844e-01]\n",
      " [9.16597795e-01 9.28853479e-01 6.53680777e-05 4.19801624e-01]\n",
      " [9.16597795e-01 9.28853479e-01 6.53680777e-05 4.19801624e-01]\n",
      " [9.20631594e-01 9.42398939e-01 7.66035343e-05 4.34051456e-01]\n",
      " [9.20631594e-01 9.42398939e-01 7.66035343e-05 4.34051456e-01]\n",
      " [9.21188541e-01 9.44187740e-01 7.83167255e-05 4.36063057e-01]\n",
      " [9.21188541e-01 9.44187740e-01 7.83167255e-05 4.36063057e-01]\n",
      " [9.21801740e-01 9.46133014e-01 8.02524389e-05 4.38290560e-01]\n",
      " [9.21801740e-01 9.46133014e-01 8.02524389e-05 4.38290560e-01]\n",
      " [9.22910762e-01 9.49585156e-01 8.38902444e-05 4.42353495e-01]\n",
      " [9.22910762e-01 9.49585156e-01 8.38902444e-05 4.42353495e-01]\n",
      " [9.23263203e-01 9.50664007e-01 8.50845596e-05 4.43654033e-01]\n",
      " [9.23263203e-01 9.50664007e-01 8.50845596e-05 4.43654033e-01]\n",
      " [9.28564108e-01 9.65748392e-01 1.05552550e-04 4.63780830e-01]\n",
      " [9.28564108e-01 9.65748392e-01 1.05552550e-04 4.63780830e-01]\n",
      " [9.30401103e-01 9.70433716e-01 1.13896470e-04 4.71014467e-01]\n",
      " [9.30401103e-01 9.70433716e-01 1.13896470e-04 4.71014467e-01]\n",
      " [9.32568010e-01 9.75564354e-01 1.24714636e-04 4.79728477e-01]\n",
      " [9.32568010e-01 9.75564354e-01 1.24714636e-04 4.79728477e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(G[0:20,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n",
      "[[0.33333333 2.        ]\n",
      " [3.         4.        ]\n",
      " [5.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "# for i in range(3):\n",
    "#     a[i,0] = a[i,0]/(sum(a[:,0])/3)\n",
    "# print(a)\n",
    "\n",
    "b = (sum(a[:,0])/3)\n",
    "print(b)\n",
    "print(type(b))\n",
    "a[0,0] = a[0,0]/b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "x1 tensor([-13.5463,   3.1992,  -5.1321, -14.9051,   8.9152,   2.3141])\n",
      "x2 tensor([-13.5463,   3.1992,  -5.1321, -14.9051,   8.9152,   2.3141])\n",
      "output\n",
      "tensor([-0.8142], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets, self).__init__()\n",
    "        num_hid_feat = int(number_of_features/2)\n",
    "        self.fc1 = nn.Linear(number_of_features,num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat,num_hid_feat)        \n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN, self).__init__()\n",
    "        self.network1 = Subnets(number_of_features)\n",
    "        self.network2 = Subnets(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN(number_of_features)\n",
    "print(np.shape(training_set[0]))\n",
    "x1, x2 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class simplenn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(simplenn, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 3)        # where fc stands for fully connected \n",
    "#         self.fc2 = nn.Linear(3, 1)        \n",
    "   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "#         x = self.fc2(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "#         return x\n",
    "\n",
    "# mod = simplenn()\n",
    "\n",
    "# print(mod.fc1.weight)\n",
    "# print(mod.fc1.bias)\n",
    "\n",
    "# print(mod.fc2.weight)\n",
    "# print(mod.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# x1 = x1[:2]\n",
    "# x2 = x2[:2]\n",
    "\n",
    "# x1[0] = -18650\n",
    "# x1[1] = 109075\n",
    "# print('x1',x1)\n",
    "\n",
    "# x2[0] = -6\n",
    "# x2[1] = 7\n",
    "# print('x2',x2)\n",
    "\n",
    "# output1 = mod(x1)\n",
    "# print('output1')\n",
    "# print(output1)\n",
    "\n",
    "# output2 = mod(x2)\n",
    "# print('output2')\n",
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([-13.5463,   3.1992,  -5.1321, -14.9051,   8.9152,   2.3141])\n",
      "x2 tensor([-13.5463,   3.1992,  -5.1321, -14.9051,   8.9152,   2.3141])\n",
      "output\n",
      "tensor([-0.8142], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.0899, -0.1200, -0.0207, -0.3637, -0.1801,  0.2438],\n",
      "        [-0.0889, -0.3772,  0.3619,  0.2454,  0.3056,  0.3226],\n",
      "        [-0.1486, -0.1253,  0.2268,  0.3178, -0.2109, -0.1893]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.2549, -0.1573, -0.2640], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.3110,  0.4367, -0.4628],\n",
      "        [ 0.0268, -0.3124, -0.2391],\n",
      "        [ 0.3999, -0.3843,  0.0801]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.0635,  0.4716, -0.0993], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(model.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(model.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BPNN(number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.0037973426282\n",
      "[2,     1] loss: 0.0003804118140\n",
      "[3,     1] loss: 0.0000900528568\n",
      "[4,     1] loss: 0.0000329444272\n",
      "[5,     1] loss: 0.0000339191785\n",
      "[6,     1] loss: 0.0000161235133\n",
      "[7,     1] loss: 0.0000108457280\n",
      "[8,     1] loss: 0.0000095538504\n",
      "[9,     1] loss: 0.0000076235781\n",
      "[10,     1] loss: 0.0000091986556\n",
      "[11,     1] loss: 0.0000057816458\n",
      "[12,     1] loss: 0.0000089796456\n",
      "[13,     1] loss: 0.0000048942893\n",
      "[14,     1] loss: 0.0000101941238\n",
      "[15,     1] loss: 0.0000034501907\n",
      "[16,     1] loss: 0.0000035127381\n",
      "[17,     1] loss: 0.0000054786869\n",
      "[18,     1] loss: 0.0000041358966\n",
      "[19,     1] loss: 0.0000051007577\n",
      "[20,     1] loss: 0.0000045066859\n",
      "[21,     1] loss: 0.0000045856210\n",
      "[22,     1] loss: 0.0000029030232\n",
      "[23,     1] loss: 0.0000059266604\n",
      "[24,     1] loss: 0.0000028413098\n",
      "[25,     1] loss: 0.0000037026173\n",
      "[26,     1] loss: 0.0000027300135\n",
      "[27,     1] loss: 0.0000029187206\n",
      "[28,     1] loss: 0.0000023403321\n",
      "[29,     1] loss: 0.0000021782911\n",
      "[30,     1] loss: 0.0000034424549\n",
      "[31,     1] loss: 0.0000033162316\n",
      "[32,     1] loss: 0.0000033465541\n",
      "[33,     1] loss: 0.0000036450543\n",
      "[34,     1] loss: 0.0000024792007\n",
      "[35,     1] loss: 0.0000020863192\n",
      "[36,     1] loss: 0.0000024318038\n",
      "[37,     1] loss: 0.0000027437087\n",
      "[38,     1] loss: 0.0000028423581\n",
      "[39,     1] loss: 0.0000041613155\n",
      "[40,     1] loss: 0.0000034993242\n",
      "[41,     1] loss: 0.0000033964232\n",
      "[42,     1] loss: 0.0000018841936\n",
      "[43,     1] loss: 0.0000027438076\n",
      "[44,     1] loss: 0.0000019083884\n",
      "[45,     1] loss: 0.0000023918088\n",
      "[46,     1] loss: 0.0000025643878\n",
      "[47,     1] loss: 0.0000034711877\n",
      "[48,     1] loss: 0.0000020989613\n",
      "[49,     1] loss: 0.0000030649280\n",
      "[50,     1] loss: 0.0000031098411\n",
      "[51,     1] loss: 0.0000027943977\n",
      "[52,     1] loss: 0.0000021366684\n",
      "[53,     1] loss: 0.0000027702506\n",
      "[54,     1] loss: 0.0000042054980\n",
      "[55,     1] loss: 0.0000025649611\n",
      "[56,     1] loss: 0.0000035787434\n",
      "[57,     1] loss: 0.0000023779750\n",
      "[58,     1] loss: 0.0000025785312\n",
      "[59,     1] loss: 0.0000041932468\n",
      "[60,     1] loss: 0.0000025298405\n",
      "[61,     1] loss: 0.0000028887745\n",
      "[62,     1] loss: 0.0000025657580\n",
      "[63,     1] loss: 0.0000029828947\n",
      "[64,     1] loss: 0.0000027565948\n",
      "[65,     1] loss: 0.0000035628102\n",
      "[66,     1] loss: 0.0000020531321\n",
      "[67,     1] loss: 0.0000033974160\n",
      "[68,     1] loss: 0.0000047910424\n",
      "[69,     1] loss: 0.0000046449331\n",
      "[70,     1] loss: 0.0000036980462\n",
      "[71,     1] loss: 0.0000038426879\n",
      "[72,     1] loss: 0.0000039098795\n",
      "[73,     1] loss: 0.0000028512306\n",
      "[74,     1] loss: 0.0000028109214\n",
      "[75,     1] loss: 0.0000037476508\n",
      "[76,     1] loss: 0.0000026952512\n",
      "[77,     1] loss: 0.0000032727996\n",
      "[78,     1] loss: 0.0000036915990\n",
      "[79,     1] loss: 0.0000036015594\n",
      "[80,     1] loss: 0.0000026924188\n",
      "[81,     1] loss: 0.0000031815653\n",
      "[82,     1] loss: 0.0000023921808\n",
      "[83,     1] loss: 0.0000032908483\n",
      "[84,     1] loss: 0.0000022993845\n",
      "[85,     1] loss: 0.0000022784980\n",
      "[86,     1] loss: 0.0000021911723\n",
      "[87,     1] loss: 0.0000040673596\n",
      "[88,     1] loss: 0.0000025665438\n",
      "[89,     1] loss: 0.0000055446439\n",
      "[90,     1] loss: 0.0000049663606\n",
      "[91,     1] loss: 0.0000028308887\n",
      "[92,     1] loss: 0.0000048675844\n",
      "[93,     1] loss: 0.0000022379541\n",
      "[94,     1] loss: 0.0000033803986\n",
      "[95,     1] loss: 0.0000027742019\n",
      "[96,     1] loss: 0.0000022500119\n",
      "[97,     1] loss: 0.0000033271870\n",
      "[98,     1] loss: 0.0000043403925\n",
      "[99,     1] loss: 0.0000040445029\n",
      "[100,     1] loss: 0.0000026535396\n",
      "[101,     1] loss: 0.0000023731294\n",
      "[102,     1] loss: 0.0000024761437\n",
      "[103,     1] loss: 0.0000029554256\n",
      "[104,     1] loss: 0.0000024240973\n",
      "[105,     1] loss: 0.0000017101494\n",
      "[106,     1] loss: 0.0000044399061\n",
      "[107,     1] loss: 0.0000031151721\n",
      "[108,     1] loss: 0.0000050297167\n",
      "[109,     1] loss: 0.0000049164119\n",
      "[110,     1] loss: 0.0000034322202\n",
      "[111,     1] loss: 0.0000036110869\n",
      "[112,     1] loss: 0.0000037731450\n",
      "[113,     1] loss: 0.0000031235053\n",
      "[114,     1] loss: 0.0000018590310\n",
      "[115,     1] loss: 0.0000027205535\n",
      "[116,     1] loss: 0.0000023580898\n",
      "[117,     1] loss: 0.0000018126879\n",
      "[118,     1] loss: 0.0000044398588\n",
      "[119,     1] loss: 0.0000022584682\n",
      "[120,     1] loss: 0.0000033570010\n",
      "[121,     1] loss: 0.0000039373404\n",
      "[122,     1] loss: 0.0000023523047\n",
      "[123,     1] loss: 0.0000050626790\n",
      "[124,     1] loss: 0.0000023706847\n",
      "[125,     1] loss: 0.0000022115599\n",
      "[126,     1] loss: 0.0000022839773\n",
      "[127,     1] loss: 0.0000043080261\n",
      "[128,     1] loss: 0.0000018292641\n",
      "[129,     1] loss: 0.0000030824562\n",
      "[130,     1] loss: 0.0000057319114\n",
      "[131,     1] loss: 0.0000070673930\n",
      "[132,     1] loss: 0.0000045515550\n",
      "[133,     1] loss: 0.0000028373041\n",
      "[134,     1] loss: 0.0000030556010\n",
      "[135,     1] loss: 0.0000032527969\n",
      "[136,     1] loss: 0.0000021276363\n",
      "[137,     1] loss: 0.0000031321815\n",
      "[138,     1] loss: 0.0000030069272\n",
      "[139,     1] loss: 0.0000017520440\n",
      "[140,     1] loss: 0.0000033878605\n",
      "[141,     1] loss: 0.0000019067327\n",
      "[142,     1] loss: 0.0000022843064\n",
      "[143,     1] loss: 0.0000024267774\n",
      "[144,     1] loss: 0.0000026500707\n",
      "[145,     1] loss: 0.0000030868094\n",
      "[146,     1] loss: 0.0000033483389\n",
      "[147,     1] loss: 0.0000033195822\n",
      "[148,     1] loss: 0.0000024397126\n",
      "[149,     1] loss: 0.0000053086002\n",
      "[150,     1] loss: 0.0000025217256\n",
      "[151,     1] loss: 0.0000021452506\n",
      "[152,     1] loss: 0.0000013240136\n",
      "[153,     1] loss: 0.0000025316702\n",
      "[154,     1] loss: 0.0000008965164\n",
      "[155,     1] loss: 0.0000006003390\n",
      "[156,     1] loss: 0.0000012848191\n",
      "[157,     1] loss: 0.0000072599614\n",
      "[158,     1] loss: 0.0000045591867\n",
      "[159,     1] loss: 0.0000016558903\n",
      "[160,     1] loss: 0.0000015380027\n",
      "[161,     1] loss: 0.0000013508207\n",
      "[162,     1] loss: 0.0000011399246\n",
      "[163,     1] loss: 0.0000007825481\n",
      "[164,     1] loss: 0.0000005684403\n",
      "[165,     1] loss: 0.0000009688273\n",
      "[166,     1] loss: 0.0000004348489\n",
      "[167,     1] loss: 0.0000002497409\n",
      "[168,     1] loss: 0.0000005296327\n",
      "[169,     1] loss: 0.0000003346195\n",
      "[170,     1] loss: 0.0000007786236\n",
      "[171,     1] loss: 0.0000006946467\n",
      "[172,     1] loss: 0.0000007331806\n",
      "[173,     1] loss: 0.0000004413348\n",
      "[174,     1] loss: 0.0000005322931\n",
      "[175,     1] loss: 0.0000009174528\n",
      "[176,     1] loss: 0.0000004989004\n",
      "[177,     1] loss: 0.0000004449169\n",
      "[178,     1] loss: 0.0000016153364\n",
      "[179,     1] loss: 0.0000005419119\n",
      "[180,     1] loss: 0.0000003176358\n",
      "[181,     1] loss: 0.0000005436892\n",
      "[182,     1] loss: 0.0000004078238\n",
      "[183,     1] loss: 0.0000005754051\n",
      "[184,     1] loss: 0.0000004689868\n",
      "[185,     1] loss: 0.0000004261684\n",
      "[186,     1] loss: 0.0000004821573\n",
      "[187,     1] loss: 0.0000004790164\n",
      "[188,     1] loss: 0.0000025340940\n",
      "[189,     1] loss: 0.0000010407256\n",
      "[190,     1] loss: 0.0000007042437\n",
      "[191,     1] loss: 0.0000007136904\n",
      "[192,     1] loss: 0.0000013779346\n",
      "[193,     1] loss: 0.0000004894131\n",
      "[194,     1] loss: 0.0000009944142\n",
      "[195,     1] loss: 0.0000004703665\n",
      "[196,     1] loss: 0.0000004845738\n",
      "[197,     1] loss: 0.0000003276562\n",
      "[198,     1] loss: 0.0000009145405\n",
      "[199,     1] loss: 0.0000014963766\n",
      "[200,     1] loss: 0.0000005920854\n",
      "[201,     1] loss: 0.0000004294296\n",
      "[202,     1] loss: 0.0000005774004\n",
      "[203,     1] loss: 0.0000007891631\n",
      "[204,     1] loss: 0.0000008490511\n",
      "[205,     1] loss: 0.0000010029516\n",
      "[206,     1] loss: 0.0000005911174\n",
      "[207,     1] loss: 0.0000004694716\n",
      "[208,     1] loss: 0.0000006363209\n",
      "[209,     1] loss: 0.0000004722119\n",
      "[210,     1] loss: 0.0000006631236\n",
      "[211,     1] loss: 0.0000008219602\n",
      "[212,     1] loss: 0.0000006640174\n",
      "[213,     1] loss: 0.0000005378645\n",
      "[214,     1] loss: 0.0000002957095\n",
      "[215,     1] loss: 0.0000005746483\n",
      "[216,     1] loss: 0.0000008343854\n",
      "[217,     1] loss: 0.0000005288874\n",
      "[218,     1] loss: 0.0000005804033\n",
      "[219,     1] loss: 0.0000007552930\n",
      "[220,     1] loss: 0.0000004446596\n",
      "[221,     1] loss: 0.0000003094049\n",
      "[222,     1] loss: 0.0000004625899\n",
      "[223,     1] loss: 0.0000016924047\n",
      "[224,     1] loss: 0.0000005890040\n",
      "[225,     1] loss: 0.0000015129061\n",
      "[226,     1] loss: 0.0000005637265\n",
      "[227,     1] loss: 0.0000008481887\n",
      "[228,     1] loss: 0.0000011833700\n",
      "[229,     1] loss: 0.0000003875941\n",
      "[230,     1] loss: 0.0000015293428\n",
      "[231,     1] loss: 0.0000019423025\n",
      "[232,     1] loss: 0.0000046923196\n",
      "[233,     1] loss: 0.0000022760069\n",
      "[234,     1] loss: 0.0000065184977\n",
      "[235,     1] loss: 0.0000015932168\n",
      "[236,     1] loss: 0.0000035842404\n",
      "[237,     1] loss: 0.0000007584666\n",
      "[238,     1] loss: 0.0000008722264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239,     1] loss: 0.0000027073131\n",
      "[240,     1] loss: 0.0000032224591\n",
      "[241,     1] loss: 0.0000018366452\n",
      "[242,     1] loss: 0.0000040141666\n",
      "[243,     1] loss: 0.0000010093162\n",
      "[244,     1] loss: 0.0000031478947\n",
      "[245,     1] loss: 0.0000016708542\n",
      "[246,     1] loss: 0.0000013198238\n",
      "[247,     1] loss: 0.0000014687119\n",
      "[248,     1] loss: 0.0000008246149\n",
      "[249,     1] loss: 0.0000007843400\n",
      "[250,     1] loss: 0.0000007132720\n",
      "[251,     1] loss: 0.0000007823342\n",
      "[252,     1] loss: 0.0000006880917\n",
      "[253,     1] loss: 0.0000006212325\n",
      "[254,     1] loss: 0.0000010722009\n",
      "[255,     1] loss: 0.0000004603848\n",
      "[256,     1] loss: 0.0000004736899\n",
      "[257,     1] loss: 0.0000005016547\n",
      "[258,     1] loss: 0.0000006370321\n",
      "[259,     1] loss: 0.0000004221071\n",
      "[260,     1] loss: 0.0000009554643\n",
      "[261,     1] loss: 0.0000006576738\n",
      "[262,     1] loss: 0.0000006774928\n",
      "[263,     1] loss: 0.0000007529762\n",
      "[264,     1] loss: 0.0000004243195\n",
      "[265,     1] loss: 0.0000003973937\n",
      "[266,     1] loss: 0.0000006371646\n",
      "[267,     1] loss: 0.0000008724955\n",
      "[268,     1] loss: 0.0000004214802\n",
      "[269,     1] loss: 0.0000005541468\n",
      "[270,     1] loss: 0.0000002945323\n",
      "[271,     1] loss: 0.0000012888749\n",
      "[272,     1] loss: 0.0000005506663\n",
      "[273,     1] loss: 0.0000003732388\n",
      "[274,     1] loss: 0.0000003639956\n",
      "[275,     1] loss: 0.0000004014616\n",
      "[276,     1] loss: 0.0000003782684\n",
      "[277,     1] loss: 0.0000005238746\n",
      "[278,     1] loss: 0.0000020521637\n",
      "[279,     1] loss: 0.0000007486725\n",
      "[280,     1] loss: 0.0000008819210\n",
      "[281,     1] loss: 0.0000015681990\n",
      "[282,     1] loss: 0.0000004285985\n",
      "[283,     1] loss: 0.0000004694291\n",
      "[284,     1] loss: 0.0000007876056\n",
      "[285,     1] loss: 0.0000002990318\n",
      "[286,     1] loss: 0.0000005741272\n",
      "[287,     1] loss: 0.0000010326939\n",
      "[288,     1] loss: 0.0000004085593\n",
      "[289,     1] loss: 0.0000003079208\n",
      "[290,     1] loss: 0.0000004677078\n",
      "[291,     1] loss: 0.0000005349496\n",
      "[292,     1] loss: 0.0000005471940\n",
      "[293,     1] loss: 0.0000005822792\n",
      "[294,     1] loss: 0.0000003608050\n",
      "[295,     1] loss: 0.0000005648911\n",
      "[296,     1] loss: 0.0000004873797\n",
      "[297,     1] loss: 0.0000008004345\n",
      "[298,     1] loss: 0.0000015393322\n",
      "[299,     1] loss: 0.0000022357015\n",
      "[300,     1] loss: 0.0000004128430\n",
      "[301,     1] loss: 0.0000005368212\n",
      "[302,     1] loss: 0.0000008007391\n",
      "[303,     1] loss: 0.0000004485475\n",
      "[304,     1] loss: 0.0000009479467\n",
      "[305,     1] loss: 0.0000006644581\n",
      "[306,     1] loss: 0.0000007016815\n",
      "[307,     1] loss: 0.0000004094913\n",
      "[308,     1] loss: 0.0000005798464\n",
      "[309,     1] loss: 0.0000004207574\n",
      "[310,     1] loss: 0.0000006168611\n",
      "[311,     1] loss: 0.0000003930392\n",
      "[312,     1] loss: 0.0000005610468\n",
      "[313,     1] loss: 0.0000006119623\n",
      "[314,     1] loss: 0.0000003795478\n",
      "[315,     1] loss: 0.0000006463600\n",
      "[316,     1] loss: 0.0000005751629\n",
      "[317,     1] loss: 0.0000004749952\n",
      "[318,     1] loss: 0.0000004420175\n",
      "[319,     1] loss: 0.0000002830519\n",
      "[320,     1] loss: 0.0000003772729\n",
      "[321,     1] loss: 0.0000003659432\n",
      "[322,     1] loss: 0.0000002970304\n",
      "[323,     1] loss: 0.0000010796452\n",
      "[324,     1] loss: 0.0000002186291\n",
      "[325,     1] loss: 0.0000006200151\n",
      "[326,     1] loss: 0.0000006414884\n",
      "[327,     1] loss: 0.0000003518347\n",
      "[328,     1] loss: 0.0000006094700\n",
      "[329,     1] loss: 0.0000003111878\n",
      "[330,     1] loss: 0.0000002879004\n",
      "[331,     1] loss: 0.0000003268412\n",
      "[332,     1] loss: 0.0000003576321\n",
      "[333,     1] loss: 0.0000003363111\n",
      "[334,     1] loss: 0.0000007485798\n",
      "[335,     1] loss: 0.0000003446470\n",
      "[336,     1] loss: 0.0000002609429\n",
      "[337,     1] loss: 0.0000002379943\n",
      "[338,     1] loss: 0.0000001769371\n",
      "[339,     1] loss: 0.0000002716155\n",
      "[340,     1] loss: 0.0000002983924\n",
      "[341,     1] loss: 0.0000001634183\n",
      "[342,     1] loss: 0.0000007588256\n",
      "[343,     1] loss: 0.0000002053290\n",
      "[344,     1] loss: 0.0000004312220\n",
      "[345,     1] loss: 0.0000008608893\n",
      "[346,     1] loss: 0.0000009380589\n",
      "[347,     1] loss: 0.0000011480943\n",
      "[348,     1] loss: 0.0000011435668\n",
      "[349,     1] loss: 0.0000015757772\n",
      "[350,     1] loss: 0.0000049461058\n",
      "[351,     1] loss: 0.0000036330166\n",
      "[352,     1] loss: 0.0000043748161\n",
      "[353,     1] loss: 0.0000022383538\n",
      "[354,     1] loss: 0.0000133022811\n",
      "[355,     1] loss: 0.0000052357158\n",
      "[356,     1] loss: 0.0000099424418\n",
      "[357,     1] loss: 0.0000168904546\n",
      "[358,     1] loss: 0.0000050736842\n",
      "[359,     1] loss: 0.0000030986950\n",
      "[360,     1] loss: 0.0000019807461\n",
      "[361,     1] loss: 0.0000011701465\n",
      "[362,     1] loss: 0.0000009409618\n",
      "[363,     1] loss: 0.0000014459207\n",
      "[364,     1] loss: 0.0000005771219\n",
      "[365,     1] loss: 0.0000002026688\n",
      "[366,     1] loss: 0.0000003984534\n",
      "[367,     1] loss: 0.0000004894834\n",
      "[368,     1] loss: 0.0000002817260\n",
      "[369,     1] loss: 0.0000002944807\n",
      "[370,     1] loss: 0.0000006562471\n",
      "[371,     1] loss: 0.0000008079374\n",
      "[372,     1] loss: 0.0000001646545\n",
      "[373,     1] loss: 0.0000007092640\n",
      "[374,     1] loss: 0.0000002992170\n",
      "[375,     1] loss: 0.0000001632078\n",
      "[376,     1] loss: 0.0000002090019\n",
      "[377,     1] loss: 0.0000002456707\n",
      "[378,     1] loss: 0.0000001985287\n",
      "[379,     1] loss: 0.0000004591301\n",
      "[380,     1] loss: 0.0000002646294\n",
      "[381,     1] loss: 0.0000003697846\n",
      "[382,     1] loss: 0.0000006183664\n",
      "[383,     1] loss: 0.0000002829218\n",
      "[384,     1] loss: 0.0000001822502\n",
      "[385,     1] loss: 0.0000015485906\n",
      "[386,     1] loss: 0.0000002102135\n",
      "[387,     1] loss: 0.0000007947797\n",
      "[388,     1] loss: 0.0000001202266\n",
      "[389,     1] loss: 0.0000002769180\n",
      "[390,     1] loss: 0.0000003341407\n",
      "[391,     1] loss: 0.0000002295537\n",
      "[392,     1] loss: 0.0000001170590\n",
      "[393,     1] loss: 0.0000001540481\n",
      "[394,     1] loss: 0.0000003172219\n",
      "[395,     1] loss: 0.0000001121604\n",
      "[396,     1] loss: 0.0000001192460\n",
      "[397,     1] loss: 0.0000001281949\n",
      "[398,     1] loss: 0.0000004672490\n",
      "[399,     1] loss: 0.0000004830369\n",
      "[400,     1] loss: 0.0000010604730\n",
      "[401,     1] loss: 0.0000001984229\n",
      "[402,     1] loss: 0.0000001450876\n",
      "[403,     1] loss: 0.0000001355374\n",
      "[404,     1] loss: 0.0000001569833\n",
      "[405,     1] loss: 0.0000003175188\n",
      "[406,     1] loss: 0.0000000832147\n",
      "[407,     1] loss: 0.0000001151022\n",
      "[408,     1] loss: 0.0000003675048\n",
      "[409,     1] loss: 0.0000011692060\n",
      "[410,     1] loss: 0.0000006095211\n",
      "[411,     1] loss: 0.0000016586233\n",
      "[412,     1] loss: 0.0000010462761\n",
      "[413,     1] loss: 0.0000003389095\n",
      "[414,     1] loss: 0.0000002125198\n",
      "[415,     1] loss: 0.0000008822170\n",
      "[416,     1] loss: 0.0000007295052\n",
      "[417,     1] loss: 0.0000001705035\n",
      "[418,     1] loss: 0.0000000828471\n",
      "[419,     1] loss: 0.0000002897156\n",
      "[420,     1] loss: 0.0000001773200\n",
      "[421,     1] loss: 0.0000001609756\n",
      "[422,     1] loss: 0.0000001731384\n",
      "[423,     1] loss: 0.0000004084002\n",
      "[424,     1] loss: 0.0000003427906\n",
      "[425,     1] loss: 0.0000001277094\n",
      "[426,     1] loss: 0.0000002236550\n",
      "[427,     1] loss: 0.0000000964305\n",
      "[428,     1] loss: 0.0000013708116\n",
      "[429,     1] loss: 0.0000005261174\n",
      "[430,     1] loss: 0.0000004955381\n",
      "[431,     1] loss: 0.0000001986900\n",
      "[432,     1] loss: 0.0000001332180\n",
      "[433,     1] loss: 0.0000001007776\n",
      "[434,     1] loss: 0.0000004465805\n",
      "[435,     1] loss: 0.0000002837903\n",
      "[436,     1] loss: 0.0000008126317\n",
      "[437,     1] loss: 0.0000002184661\n",
      "[438,     1] loss: 0.0000008211367\n",
      "[439,     1] loss: 0.0000007864004\n",
      "[440,     1] loss: 0.0000006190493\n",
      "[441,     1] loss: 0.0000002636465\n",
      "[442,     1] loss: 0.0000002667189\n",
      "[443,     1] loss: 0.0000003664701\n",
      "[444,     1] loss: 0.0000001810544\n",
      "[445,     1] loss: 0.0000001943976\n",
      "[446,     1] loss: 0.0000007034846\n",
      "[447,     1] loss: 0.0000002450714\n",
      "[448,     1] loss: 0.0000012097331\n",
      "[449,     1] loss: 0.0000004000692\n",
      "[450,     1] loss: 0.0000006108297\n",
      "[451,     1] loss: 0.0000001425936\n",
      "[452,     1] loss: 0.0000001748158\n",
      "[453,     1] loss: 0.0000003380644\n",
      "[454,     1] loss: 0.0000001414576\n",
      "[455,     1] loss: 0.0000004600353\n",
      "[456,     1] loss: 0.0000004215999\n",
      "[457,     1] loss: 0.0000005562592\n",
      "[458,     1] loss: 0.0000003438978\n",
      "[459,     1] loss: 0.0000001096373\n",
      "[460,     1] loss: 0.0000004608077\n",
      "[461,     1] loss: 0.0000002931188\n",
      "[462,     1] loss: 0.0000001598365\n",
      "[463,     1] loss: 0.0000013080645\n",
      "[464,     1] loss: 0.0000006582098\n",
      "[465,     1] loss: 0.0000023876026\n",
      "[466,     1] loss: 0.0000006402142\n",
      "[467,     1] loss: 0.0000005015986\n",
      "[468,     1] loss: 0.0000006168765\n",
      "[469,     1] loss: 0.0000013280809\n",
      "[470,     1] loss: 0.0000017925015\n",
      "[471,     1] loss: 0.0000018595052\n",
      "[472,     1] loss: 0.0000001788852\n",
      "[473,     1] loss: 0.0000002380905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[474,     1] loss: 0.0000001995920\n",
      "[475,     1] loss: 0.0000011812213\n",
      "[476,     1] loss: 0.0000011527968\n",
      "[477,     1] loss: 0.0000004179148\n",
      "[478,     1] loss: 0.0000007491483\n",
      "[479,     1] loss: 0.0000011534373\n",
      "[480,     1] loss: 0.0000009102007\n",
      "[481,     1] loss: 0.0000001022215\n",
      "[482,     1] loss: 0.0000008962822\n",
      "[483,     1] loss: 0.0000003509020\n",
      "[484,     1] loss: 0.0000002761063\n",
      "[485,     1] loss: 0.0000002374416\n",
      "[486,     1] loss: 0.0000002488137\n",
      "[487,     1] loss: 0.0000002780166\n",
      "[488,     1] loss: 0.0000002912543\n",
      "[489,     1] loss: 0.0000001500119\n",
      "[490,     1] loss: 0.0000009113585\n",
      "[491,     1] loss: 0.0000005395827\n",
      "[492,     1] loss: 0.0000011258518\n",
      "[493,     1] loss: 0.0000013980724\n",
      "[494,     1] loss: 0.0000012729591\n",
      "[495,     1] loss: 0.0000007563548\n",
      "[496,     1] loss: 0.0000001839694\n",
      "[497,     1] loss: 0.0000003437569\n",
      "[498,     1] loss: 0.0000007831440\n",
      "[499,     1] loss: 0.0000006129460\n",
      "[500,     1] loss: 0.0000005562765\n",
      "[501,     1] loss: 0.0000005275852\n",
      "[502,     1] loss: 0.0000021368465\n",
      "[503,     1] loss: 0.0000011351110\n",
      "[504,     1] loss: 0.0000002786832\n",
      "[505,     1] loss: 0.0000003896950\n",
      "[506,     1] loss: 0.0000002463342\n",
      "[507,     1] loss: 0.0000004285203\n",
      "[508,     1] loss: 0.0000001722899\n",
      "[509,     1] loss: 0.0000001173892\n",
      "[510,     1] loss: 0.0000007109275\n",
      "[511,     1] loss: 0.0000007054665\n",
      "[512,     1] loss: 0.0000005187229\n",
      "[513,     1] loss: 0.0000001378452\n",
      "[514,     1] loss: 0.0000002070015\n",
      "[515,     1] loss: 0.0000001807097\n",
      "[516,     1] loss: 0.0000000763817\n",
      "[517,     1] loss: 0.0000005387896\n",
      "[518,     1] loss: 0.0000001303952\n",
      "[519,     1] loss: 0.0000001049420\n",
      "[520,     1] loss: 0.0000002360591\n",
      "[521,     1] loss: 0.0000000671839\n",
      "[522,     1] loss: 0.0000002852632\n",
      "[523,     1] loss: 0.0000000578834\n",
      "[524,     1] loss: 0.0000000613292\n",
      "[525,     1] loss: 0.0000003926039\n",
      "[526,     1] loss: 0.0000001331250\n",
      "[527,     1] loss: 0.0000008615715\n",
      "[528,     1] loss: 0.0000004132393\n",
      "[529,     1] loss: 0.0000000915344\n",
      "[530,     1] loss: 0.0000007793853\n",
      "[531,     1] loss: 0.0000008600173\n",
      "[532,     1] loss: 0.0000009148801\n",
      "[533,     1] loss: 0.0000003100679\n",
      "[534,     1] loss: 0.0000002976725\n",
      "[535,     1] loss: 0.0000002370369\n",
      "[536,     1] loss: 0.0000026477333\n",
      "[537,     1] loss: 0.0000027296905\n",
      "[538,     1] loss: 0.0000096290831\n",
      "[539,     1] loss: 0.0000015821641\n",
      "[540,     1] loss: 0.0000007445497\n",
      "[541,     1] loss: 0.0000017709082\n",
      "[542,     1] loss: 0.0000012472098\n",
      "[543,     1] loss: 0.0000008662905\n",
      "[544,     1] loss: 0.0000015139226\n",
      "[545,     1] loss: 0.0000021849499\n",
      "[546,     1] loss: 0.0000004613917\n",
      "[547,     1] loss: 0.0000002996486\n",
      "[548,     1] loss: 0.0000002877369\n",
      "[549,     1] loss: 0.0000001199490\n",
      "[550,     1] loss: 0.0000000677844\n",
      "[551,     1] loss: 0.0000001854099\n",
      "[552,     1] loss: 0.0000001356276\n",
      "[553,     1] loss: 0.0000005587888\n",
      "[554,     1] loss: 0.0000007958809\n",
      "[555,     1] loss: 0.0000003494596\n",
      "[556,     1] loss: 0.0000002037699\n",
      "[557,     1] loss: 0.0000004859642\n",
      "[558,     1] loss: 0.0000003993521\n",
      "[559,     1] loss: 0.0000001456870\n",
      "[560,     1] loss: 0.0000001381686\n",
      "[561,     1] loss: 0.0000005244816\n",
      "[562,     1] loss: 0.0000001537577\n",
      "[563,     1] loss: 0.0000005337574\n",
      "[564,     1] loss: 0.0000003236139\n",
      "[565,     1] loss: 0.0000031884469\n",
      "[566,     1] loss: 0.0000049327598\n",
      "[567,     1] loss: 0.0000047422163\n",
      "[568,     1] loss: 0.0000032219858\n",
      "[569,     1] loss: 0.0000028723642\n",
      "[570,     1] loss: 0.0000004783721\n",
      "[571,     1] loss: 0.0000005321665\n",
      "[572,     1] loss: 0.0000002485297\n",
      "[573,     1] loss: 0.0000003618175\n",
      "[574,     1] loss: 0.0000001580163\n",
      "[575,     1] loss: 0.0000000998848\n",
      "[576,     1] loss: 0.0000001864064\n",
      "[577,     1] loss: 0.0000000976578\n",
      "[578,     1] loss: 0.0000003886252\n",
      "[579,     1] loss: 0.0000002462452\n",
      "[580,     1] loss: 0.0000013671737\n",
      "[581,     1] loss: 0.0000006410540\n",
      "[582,     1] loss: 0.0000005100043\n",
      "[583,     1] loss: 0.0000006489944\n",
      "[584,     1] loss: 0.0000001876016\n",
      "[585,     1] loss: 0.0000003797783\n",
      "[586,     1] loss: 0.0000003391572\n",
      "[587,     1] loss: 0.0000004710118\n",
      "[588,     1] loss: 0.0000012110728\n",
      "[589,     1] loss: 0.0000003978249\n",
      "[590,     1] loss: 0.0000005296396\n",
      "[591,     1] loss: 0.0000005182414\n",
      "[592,     1] loss: 0.0000002360302\n",
      "[593,     1] loss: 0.0000001418620\n",
      "[594,     1] loss: 0.0000001340004\n",
      "[595,     1] loss: 0.0000004658472\n",
      "[596,     1] loss: 0.0000001448431\n",
      "[597,     1] loss: 0.0000004292701\n",
      "[598,     1] loss: 0.0000001681040\n",
      "[599,     1] loss: 0.0000001058068\n",
      "[600,     1] loss: 0.0000003417059\n",
      "[601,     1] loss: 0.0000003342774\n",
      "[602,     1] loss: 0.0000002318971\n",
      "[603,     1] loss: 0.0000005106093\n",
      "[604,     1] loss: 0.0000020184265\n",
      "[605,     1] loss: 0.0000017023915\n",
      "[606,     1] loss: 0.0000003762606\n",
      "[607,     1] loss: 0.0000002492426\n",
      "[608,     1] loss: 0.0000004060063\n",
      "[609,     1] loss: 0.0000000622581\n",
      "[610,     1] loss: 0.0000001765582\n",
      "[611,     1] loss: 0.0000001247987\n",
      "[612,     1] loss: 0.0000001419297\n",
      "[613,     1] loss: 0.0000006646863\n",
      "[614,     1] loss: 0.0000001648811\n",
      "[615,     1] loss: 0.0000001385395\n",
      "[616,     1] loss: 0.0000006240382\n",
      "[617,     1] loss: 0.0000020643174\n",
      "[618,     1] loss: 0.0000015629095\n",
      "[619,     1] loss: 0.0000019007930\n",
      "[620,     1] loss: 0.0000004111038\n",
      "[621,     1] loss: 0.0000011525845\n",
      "[622,     1] loss: 0.0000007381950\n",
      "[623,     1] loss: 0.0000013968463\n",
      "[624,     1] loss: 0.0000007732465\n",
      "[625,     1] loss: 0.0000011614427\n",
      "[626,     1] loss: 0.0000019999754\n",
      "[627,     1] loss: 0.0000041114628\n",
      "[628,     1] loss: 0.0000006058677\n",
      "[629,     1] loss: 0.0000002893475\n",
      "[630,     1] loss: 0.0000005306474\n",
      "[631,     1] loss: 0.0000008807185\n",
      "[632,     1] loss: 0.0000010341599\n",
      "[633,     1] loss: 0.0000016831234\n",
      "[634,     1] loss: 0.0000007554238\n",
      "[635,     1] loss: 0.0000004575608\n",
      "[636,     1] loss: 0.0000003332732\n",
      "[637,     1] loss: 0.0000001876609\n",
      "[638,     1] loss: 0.0000002866283\n",
      "[639,     1] loss: 0.0000000717903\n",
      "[640,     1] loss: 0.0000001281822\n",
      "[641,     1] loss: 0.0000000702498\n",
      "[642,     1] loss: 0.0000003125521\n",
      "[643,     1] loss: 0.0000006206496\n",
      "[644,     1] loss: 0.0000001682906\n",
      "[645,     1] loss: 0.0000001041657\n",
      "[646,     1] loss: 0.0000006204463\n",
      "[647,     1] loss: 0.0000002689543\n",
      "[648,     1] loss: 0.0000008862598\n",
      "[649,     1] loss: 0.0000012431644\n",
      "[650,     1] loss: 0.0000004485340\n",
      "[651,     1] loss: 0.0000002748196\n",
      "[652,     1] loss: 0.0000004156606\n",
      "[653,     1] loss: 0.0000002633588\n",
      "[654,     1] loss: 0.0000001498376\n",
      "[655,     1] loss: 0.0000003275778\n",
      "[656,     1] loss: 0.0000001091225\n",
      "[657,     1] loss: 0.0000000792362\n",
      "[658,     1] loss: 0.0000000524763\n",
      "[659,     1] loss: 0.0000000473666\n",
      "[660,     1] loss: 0.0000001896849\n",
      "[661,     1] loss: 0.0000003122284\n",
      "[662,     1] loss: 0.0000006060384\n",
      "[663,     1] loss: 0.0000005822647\n",
      "[664,     1] loss: 0.0000005632646\n",
      "[665,     1] loss: 0.0000002931364\n",
      "[666,     1] loss: 0.0000001657374\n",
      "[667,     1] loss: 0.0000001805957\n",
      "[668,     1] loss: 0.0000001482065\n",
      "[669,     1] loss: 0.0000020733469\n",
      "[670,     1] loss: 0.0000002750565\n",
      "[671,     1] loss: 0.0000005108764\n",
      "[672,     1] loss: 0.0000004635326\n",
      "[673,     1] loss: 0.0000004492556\n",
      "[674,     1] loss: 0.0000004007894\n",
      "[675,     1] loss: 0.0000012780053\n",
      "[676,     1] loss: 0.0000004609120\n",
      "[677,     1] loss: 0.0000005979049\n",
      "[678,     1] loss: 0.0000008744296\n",
      "[679,     1] loss: 0.0000002128036\n",
      "[680,     1] loss: 0.0000002716539\n",
      "[681,     1] loss: 0.0000023447275\n",
      "[682,     1] loss: 0.0000005445714\n",
      "[683,     1] loss: 0.0000001448132\n",
      "[684,     1] loss: 0.0000015060930\n",
      "[685,     1] loss: 0.0000002892332\n",
      "[686,     1] loss: 0.0000004003377\n",
      "[687,     1] loss: 0.0000004391771\n",
      "[688,     1] loss: 0.0000000993199\n",
      "[689,     1] loss: 0.0000000656920\n",
      "[690,     1] loss: 0.0000000530707\n",
      "[691,     1] loss: 0.0000007149662\n",
      "[692,     1] loss: 0.0000002769958\n",
      "[693,     1] loss: 0.0000003292251\n",
      "[694,     1] loss: 0.0000001648866\n",
      "[695,     1] loss: 0.0000001850570\n",
      "[696,     1] loss: 0.0000005915018\n",
      "[697,     1] loss: 0.0000001382202\n",
      "[698,     1] loss: 0.0000002915544\n",
      "[699,     1] loss: 0.0000002564672\n",
      "[700,     1] loss: 0.0000001786025\n",
      "[701,     1] loss: 0.0000003520878\n",
      "[702,     1] loss: 0.0000009294469\n",
      "[703,     1] loss: 0.0000005361539\n",
      "[704,     1] loss: 0.0000004717111\n",
      "[705,     1] loss: 0.0000004867522\n",
      "[706,     1] loss: 0.0000001768332\n",
      "[707,     1] loss: 0.0000000978623\n",
      "[708,     1] loss: 0.0000003106707\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[709,     1] loss: 0.0000000435034\n",
      "[710,     1] loss: 0.0000000493770\n",
      "[711,     1] loss: 0.0000001115826\n",
      "[712,     1] loss: 0.0000014044347\n",
      "[713,     1] loss: 0.0000001399318\n",
      "[714,     1] loss: 0.0000004183578\n",
      "[715,     1] loss: 0.0000003339834\n",
      "[716,     1] loss: 0.0000001693842\n",
      "[717,     1] loss: 0.0000003435195\n",
      "[718,     1] loss: 0.0000000989316\n",
      "[719,     1] loss: 0.0000000387715\n",
      "[720,     1] loss: 0.0000001339952\n",
      "[721,     1] loss: 0.0000009070302\n",
      "[722,     1] loss: 0.0000015314368\n",
      "[723,     1] loss: 0.0000007922211\n",
      "[724,     1] loss: 0.0000006732620\n",
      "[725,     1] loss: 0.0000006210568\n",
      "[726,     1] loss: 0.0000001494149\n",
      "[727,     1] loss: 0.0000001351534\n",
      "[728,     1] loss: 0.0000000575129\n",
      "[729,     1] loss: 0.0000000352454\n",
      "[730,     1] loss: 0.0000000979907\n",
      "[731,     1] loss: 0.0000002057230\n",
      "[732,     1] loss: 0.0000033756671\n",
      "[733,     1] loss: 0.0000031394015\n",
      "[734,     1] loss: 0.0000013186154\n",
      "[735,     1] loss: 0.0000009156101\n",
      "[736,     1] loss: 0.0000007564621\n",
      "[737,     1] loss: 0.0000008181390\n",
      "[738,     1] loss: 0.0000002699103\n",
      "[739,     1] loss: 0.0000004333326\n",
      "[740,     1] loss: 0.0000001358840\n",
      "[741,     1] loss: 0.0000006012512\n",
      "[742,     1] loss: 0.0000002479026\n",
      "[743,     1] loss: 0.0000005419787\n",
      "[744,     1] loss: 0.0000001695870\n",
      "[745,     1] loss: 0.0000001249390\n",
      "[746,     1] loss: 0.0000004296193\n",
      "[747,     1] loss: 0.0000010655357\n",
      "[748,     1] loss: 0.0000001755587\n",
      "[749,     1] loss: 0.0000002083431\n",
      "[750,     1] loss: 0.0000011713436\n",
      "[751,     1] loss: 0.0000008353814\n",
      "[752,     1] loss: 0.0000022944132\n",
      "[753,     1] loss: 0.0000005457796\n",
      "[754,     1] loss: 0.0000001569496\n",
      "[755,     1] loss: 0.0000005844025\n",
      "[756,     1] loss: 0.0000007784709\n",
      "[757,     1] loss: 0.0000003473876\n",
      "[758,     1] loss: 0.0000002068431\n",
      "[759,     1] loss: 0.0000002738903\n",
      "[760,     1] loss: 0.0000001017336\n",
      "[761,     1] loss: 0.0000001907641\n",
      "[762,     1] loss: 0.0000001314993\n",
      "[763,     1] loss: 0.0000003410508\n",
      "[764,     1] loss: 0.0000001311402\n",
      "[765,     1] loss: 0.0000000703386\n",
      "[766,     1] loss: 0.0000000403931\n",
      "[767,     1] loss: 0.0000000593471\n",
      "[768,     1] loss: 0.0000000851479\n",
      "[769,     1] loss: 0.0000000930803\n",
      "[770,     1] loss: 0.0000005285345\n",
      "[771,     1] loss: 0.0000000692006\n",
      "[772,     1] loss: 0.0000001409463\n",
      "[773,     1] loss: 0.0000002003868\n",
      "[774,     1] loss: 0.0000003445638\n",
      "[775,     1] loss: 0.0000006902487\n",
      "[776,     1] loss: 0.0000002117210\n",
      "[777,     1] loss: 0.0000007386639\n",
      "[778,     1] loss: 0.0000016937156\n",
      "[779,     1] loss: 0.0000019249401\n",
      "[780,     1] loss: 0.0000001922622\n",
      "[781,     1] loss: 0.0000001090431\n",
      "[782,     1] loss: 0.0000006262544\n",
      "[783,     1] loss: 0.0000012669963\n",
      "[784,     1] loss: 0.0000011184564\n",
      "[785,     1] loss: 0.0000005984767\n",
      "[786,     1] loss: 0.0000008160940\n",
      "[787,     1] loss: 0.0000002411067\n",
      "[788,     1] loss: 0.0000010581041\n",
      "[789,     1] loss: 0.0000009034711\n",
      "[790,     1] loss: 0.0000001907475\n",
      "[791,     1] loss: 0.0000003548393\n",
      "[792,     1] loss: 0.0000003110794\n",
      "[793,     1] loss: 0.0000008548418\n",
      "[794,     1] loss: 0.0000064901054\n",
      "[795,     1] loss: 0.0000018566365\n",
      "[796,     1] loss: 0.0000051541840\n",
      "[797,     1] loss: 0.0000017494851\n",
      "[798,     1] loss: 0.0000004745668\n",
      "[799,     1] loss: 0.0000011173554\n",
      "[800,     1] loss: 0.0000010234528\n",
      "[801,     1] loss: 0.0000013786131\n",
      "[802,     1] loss: 0.0000010671326\n",
      "[803,     1] loss: 0.0000009511165\n",
      "[804,     1] loss: 0.0000004689291\n",
      "[805,     1] loss: 0.0000009711763\n",
      "[806,     1] loss: 0.0000008862460\n",
      "[807,     1] loss: 0.0000005937874\n",
      "[808,     1] loss: 0.0000001844534\n",
      "[809,     1] loss: 0.0000003107793\n",
      "[810,     1] loss: 0.0000001594555\n",
      "[811,     1] loss: 0.0000001953436\n",
      "[812,     1] loss: 0.0000000998707\n",
      "[813,     1] loss: 0.0000001813487\n",
      "[814,     1] loss: 0.0000001338482\n",
      "[815,     1] loss: 0.0000000900756\n",
      "[816,     1] loss: 0.0000003526576\n",
      "[817,     1] loss: 0.0000001763597\n",
      "[818,     1] loss: 0.0000000848514\n",
      "[819,     1] loss: 0.0000001211789\n",
      "[820,     1] loss: 0.0000001729693\n",
      "[821,     1] loss: 0.0000000578294\n",
      "[822,     1] loss: 0.0000000398646\n",
      "[823,     1] loss: 0.0000001911478\n",
      "[824,     1] loss: 0.0000001443763\n",
      "[825,     1] loss: 0.0000000956025\n",
      "[826,     1] loss: 0.0000001198109\n",
      "[827,     1] loss: 0.0000000449884\n",
      "[828,     1] loss: 0.0000003451617\n",
      "[829,     1] loss: 0.0000004321227\n",
      "[830,     1] loss: 0.0000003229544\n",
      "[831,     1] loss: 0.0000004491646\n",
      "[832,     1] loss: 0.0000003323338\n",
      "[833,     1] loss: 0.0000001050322\n",
      "[834,     1] loss: 0.0000003224649\n",
      "[835,     1] loss: 0.0000000797722\n",
      "[836,     1] loss: 0.0000000650675\n",
      "[837,     1] loss: 0.0000000577252\n",
      "[838,     1] loss: 0.0000000532280\n",
      "[839,     1] loss: 0.0000000489010\n",
      "[840,     1] loss: 0.0000000914356\n",
      "[841,     1] loss: 0.0000002528000\n",
      "[842,     1] loss: 0.0000001373651\n",
      "[843,     1] loss: 0.0000000997258\n",
      "[844,     1] loss: 0.0000001008584\n",
      "[845,     1] loss: 0.0000006937208\n",
      "[846,     1] loss: 0.0000001984510\n",
      "[847,     1] loss: 0.0000000669972\n",
      "[848,     1] loss: 0.0000002787139\n",
      "[849,     1] loss: 0.0000000238132\n",
      "[850,     1] loss: 0.0000002828158\n",
      "[851,     1] loss: 0.0000002969071\n",
      "[852,     1] loss: 0.0000003579933\n",
      "[853,     1] loss: 0.0000028200049\n",
      "[854,     1] loss: 0.0000002439610\n",
      "[855,     1] loss: 0.0000002860690\n",
      "[856,     1] loss: 0.0000008327653\n",
      "[857,     1] loss: 0.0000003046661\n",
      "[858,     1] loss: 0.0000006313619\n",
      "[859,     1] loss: 0.0000003292473\n",
      "[860,     1] loss: 0.0000011960763\n",
      "[861,     1] loss: 0.0000011228693\n",
      "[862,     1] loss: 0.0000001663449\n",
      "[863,     1] loss: 0.0000008793011\n",
      "[864,     1] loss: 0.0000008354808\n",
      "[865,     1] loss: 0.0000007313371\n",
      "[866,     1] loss: 0.0000001306447\n",
      "[867,     1] loss: 0.0000002846184\n",
      "[868,     1] loss: 0.0000004172126\n",
      "[869,     1] loss: 0.0000002278605\n",
      "[870,     1] loss: 0.0000001072560\n",
      "[871,     1] loss: 0.0000001232177\n",
      "[872,     1] loss: 0.0000000953862\n",
      "[873,     1] loss: 0.0000000944892\n",
      "[874,     1] loss: 0.0000000277970\n",
      "[875,     1] loss: 0.0000000282531\n",
      "[876,     1] loss: 0.0000000640755\n",
      "[877,     1] loss: 0.0000015173495\n",
      "[878,     1] loss: 0.0000000957645\n",
      "[879,     1] loss: 0.0000000929757\n",
      "[880,     1] loss: 0.0000000720538\n",
      "[881,     1] loss: 0.0000000831474\n",
      "[882,     1] loss: 0.0000001320956\n",
      "[883,     1] loss: 0.0000000583131\n",
      "[884,     1] loss: 0.0000000447533\n",
      "[885,     1] loss: 0.0000001347640\n",
      "[886,     1] loss: 0.0000008684056\n",
      "[887,     1] loss: 0.0000007237314\n",
      "[888,     1] loss: 0.0000010934586\n",
      "[889,     1] loss: 0.0000011152527\n",
      "[890,     1] loss: 0.0000001478877\n",
      "[891,     1] loss: 0.0000002218012\n",
      "[892,     1] loss: 0.0000001159937\n",
      "[893,     1] loss: 0.0000001207663\n",
      "[894,     1] loss: 0.0000000348921\n",
      "[895,     1] loss: 0.0000000533514\n",
      "[896,     1] loss: 0.0000001169991\n",
      "[897,     1] loss: 0.0000000372676\n",
      "[898,     1] loss: 0.0000001937045\n",
      "[899,     1] loss: 0.0000004659453\n",
      "[900,     1] loss: 0.0000008609632\n",
      "[901,     1] loss: 0.0000080201222\n",
      "[902,     1] loss: 0.0000058861100\n",
      "[903,     1] loss: 0.0000006701439\n",
      "[904,     1] loss: 0.0000009416702\n",
      "[905,     1] loss: 0.0000002902171\n",
      "[906,     1] loss: 0.0000002473767\n",
      "[907,     1] loss: 0.0000003927038\n",
      "[908,     1] loss: 0.0000003246942\n",
      "[909,     1] loss: 0.0000000499158\n",
      "[910,     1] loss: 0.0000000784019\n",
      "[911,     1] loss: 0.0000001094759\n",
      "[912,     1] loss: 0.0000001368707\n",
      "[913,     1] loss: 0.0000001120861\n",
      "[914,     1] loss: 0.0000000755227\n",
      "[915,     1] loss: 0.0000013877917\n",
      "[916,     1] loss: 0.0000002135744\n",
      "[917,     1] loss: 0.0000003112485\n",
      "[918,     1] loss: 0.0000001196281\n",
      "[919,     1] loss: 0.0000004082282\n",
      "[920,     1] loss: 0.0000002911905\n",
      "[921,     1] loss: 0.0000006286386\n",
      "[922,     1] loss: 0.0000003920295\n",
      "[923,     1] loss: 0.0000003378198\n",
      "[924,     1] loss: 0.0000007126396\n",
      "[925,     1] loss: 0.0000006419109\n",
      "[926,     1] loss: 0.0000003867718\n",
      "[927,     1] loss: 0.0000003924286\n",
      "[928,     1] loss: 0.0000002537309\n",
      "[929,     1] loss: 0.0000003248728\n",
      "[930,     1] loss: 0.0000001967785\n",
      "[931,     1] loss: 0.0000000367914\n",
      "[932,     1] loss: 0.0000004122620\n",
      "[933,     1] loss: 0.0000000469603\n",
      "[934,     1] loss: 0.0000000648568\n",
      "[935,     1] loss: 0.0000000715728\n",
      "[936,     1] loss: 0.0000004248482\n",
      "[937,     1] loss: 0.0000002024754\n",
      "[938,     1] loss: 0.0000000488631\n",
      "[939,     1] loss: 0.0000009052883\n",
      "[940,     1] loss: 0.0000008079884\n",
      "[941,     1] loss: 0.0000004240749\n",
      "[942,     1] loss: 0.0000009045356\n",
      "[943,     1] loss: 0.0000002470749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944,     1] loss: 0.0000001275507\n",
      "[945,     1] loss: 0.0000001786988\n",
      "[946,     1] loss: 0.0000000910236\n",
      "[947,     1] loss: 0.0000000589273\n",
      "[948,     1] loss: 0.0000008844758\n",
      "[949,     1] loss: 0.0000005892321\n",
      "[950,     1] loss: 0.0000002514893\n",
      "[951,     1] loss: 0.0000006408386\n",
      "[952,     1] loss: 0.0000008537796\n",
      "[953,     1] loss: 0.0000001030762\n",
      "[954,     1] loss: 0.0000003501929\n",
      "[955,     1] loss: 0.0000004873779\n",
      "[956,     1] loss: 0.0000007160974\n",
      "[957,     1] loss: 0.0000002213242\n",
      "[958,     1] loss: 0.0000001620498\n",
      "[959,     1] loss: 0.0000002565412\n",
      "[960,     1] loss: 0.0000001501761\n",
      "[961,     1] loss: 0.0000000736271\n",
      "[962,     1] loss: 0.0000002065859\n",
      "[963,     1] loss: 0.0000004883199\n",
      "[964,     1] loss: 0.0000006096674\n",
      "[965,     1] loss: 0.0000002414169\n",
      "[966,     1] loss: 0.0000006830131\n",
      "[967,     1] loss: 0.0000004178166\n",
      "[968,     1] loss: 0.0000001571749\n",
      "[969,     1] loss: 0.0000004395072\n",
      "[970,     1] loss: 0.0000002352149\n",
      "[971,     1] loss: 0.0000004566890\n",
      "[972,     1] loss: 0.0000001905091\n",
      "[973,     1] loss: 0.0000002388688\n",
      "[974,     1] loss: 0.0000002118714\n",
      "[975,     1] loss: 0.0000006609926\n",
      "[976,     1] loss: 0.0000005159336\n",
      "[977,     1] loss: 0.0000003545759\n",
      "[978,     1] loss: 0.0000006920043\n",
      "[979,     1] loss: 0.0000059891427\n",
      "[980,     1] loss: 0.0000006400280\n",
      "[981,     1] loss: 0.0000015557320\n",
      "[982,     1] loss: 0.0000003810995\n",
      "[983,     1] loss: 0.0000005136954\n",
      "[984,     1] loss: 0.0000003356778\n",
      "[985,     1] loss: 0.0000001283082\n",
      "[986,     1] loss: 0.0000002871194\n",
      "[987,     1] loss: 0.0000002044589\n",
      "[988,     1] loss: 0.0000005042817\n",
      "[989,     1] loss: 0.0000008107279\n",
      "[990,     1] loss: 0.0000002671688\n",
      "[991,     1] loss: 0.0000002317336\n",
      "[992,     1] loss: 0.0000003478190\n",
      "[993,     1] loss: 0.0000005195096\n",
      "[994,     1] loss: 0.0000005941591\n",
      "[995,     1] loss: 0.0000001214938\n",
      "[996,     1] loss: 0.0000003731214\n",
      "[997,     1] loss: 0.0000004511265\n",
      "[998,     1] loss: 0.0000002335688\n",
      "[999,     1] loss: 0.0000003569272\n",
      "[1000,     1] loss: 0.0000001938643\n",
      "[1001,     1] loss: 0.0000006987319\n",
      "[1002,     1] loss: 0.0000002470098\n",
      "[1003,     1] loss: 0.0000002062201\n",
      "[1004,     1] loss: 0.0000002070921\n",
      "[1005,     1] loss: 0.0000001298524\n",
      "[1006,     1] loss: 0.0000009819644\n",
      "[1007,     1] loss: 0.0000014614637\n",
      "[1008,     1] loss: 0.0000001932359\n",
      "[1009,     1] loss: 0.0000001479942\n",
      "[1010,     1] loss: 0.0000000679841\n",
      "[1011,     1] loss: 0.0000000863107\n",
      "[1012,     1] loss: 0.0000000628802\n",
      "[1013,     1] loss: 0.0000004984499\n",
      "[1014,     1] loss: 0.0000000644392\n",
      "[1015,     1] loss: 0.0000000930708\n",
      "[1016,     1] loss: 0.0000001452449\n",
      "[1017,     1] loss: 0.0000000435614\n",
      "[1018,     1] loss: 0.0000001753454\n",
      "[1019,     1] loss: 0.0000006036041\n",
      "[1020,     1] loss: 0.0000000781501\n",
      "[1021,     1] loss: 0.0000000898020\n",
      "[1022,     1] loss: 0.0000002731348\n",
      "[1023,     1] loss: 0.0000007458400\n",
      "[1024,     1] loss: 0.0000012353691\n",
      "[1025,     1] loss: 0.0000003101923\n",
      "[1026,     1] loss: 0.0000000639002\n",
      "[1027,     1] loss: 0.0000001046021\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)    #0.001) #0.0001)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[k][0],test_set[k][1])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch  \n",
    "#        print(i % 10)\n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.13f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[:80],train_loss[:80],'blue',label = 'Training loss')\n",
    "# plt.plot(x[:80],test_loss[:80],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch for $H_2$ training and test set',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.ylim([0,0.0001])\n",
    "\n",
    "plt.savefig('loss_graph_H2',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x1, x2 = test_set[10]\n",
    "x1 = x1\n",
    "print(x1)\n",
    "\n",
    "output = net(x1, x2)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(50)\n",
    "for i in range(50):\n",
    "    x1,x2 = test_set[i]\n",
    "    prediction[i] = net(x1, x2)#[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(prediction*var_lab+mean_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels*var_lab+mean_lab,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(net.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heta)\n",
    "print(Rs)\n",
    "print(zeta)\n",
    "print(lambdaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Training with xyz coordinates as features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N                        = 2       # number of atoms per molecule\n",
    "number_of_features_xyz   = 3       # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "\n",
    "training_set_size    = data_size - 100\n",
    "\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "coordinates_h2_norm = np.zeros((len(coordinates), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates)[0]):\n",
    "    for j in range(1):  # omit second and third column since for our dataset they are always zero\n",
    "        coordinates_h2_norm[i,j] = (coordinates[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "        \n",
    "data_set_xyz = np.vsplit(coordinates_h2_norm,data_size)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "data_set_xyz = data_set_xyz[:,:,0] # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "print(data_set_xyz)\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=100,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subnets_xyz(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets_xyz, self).__init__()\n",
    "        num_hid_feat = 20 #int(number_of_features/2)\n",
    "        self.fc1 = nn.Linear(number_of_features,num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat,num_hid_feat)        \n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_xyz(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN_xyz, self).__init__()\n",
    "        self.network1 = Subnets_xyz(number_of_features)\n",
    "        self.network2 = Subnets_xyz(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = torch.reshape(x1,[1])\n",
    "        x2 = torch.reshape(x2,[1])\n",
    "\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN_xyz(1)\n",
    "#print(np.shape(training_set_xyz[0]))\n",
    "x1, x2 = training_set_xyz[0]\n",
    "\n",
    "print(np.shape(x1))\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_xyz = BPNN_xyz(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_xyz.parameters(), lr=0.0001) #0.0001)\n",
    "#torch.optim.LBFGS(model_xyz.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(model_xyz.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 100000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = model_xyz(inputs[j][0],inputs[j][1])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = model_xyz(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                net_test_set[k] = model_xyz(test_set_xyz[k][0],test_set_xyz[k][1])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch  \n",
    "#        print(i % 10)\n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.13f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[:80],train_loss[:80],'blue',label = 'Training loss')\n",
    "# plt.plot(x[:80],test_loss[:80],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch for $H_2$ training and test set',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim([0,0.00001])\n",
    "\n",
    "plt.savefig('loss_graph_H2',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.zeros(100)\n",
    "for i in range(100):\n",
    "    x1,x2 = test_set_xyz[i]\n",
    "    prediction[i] = model_xyz(x1, x2)#[0]\n",
    "\n",
    "print(prediction*var_lab+mean_lab)\n",
    "\n",
    "\n",
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels*var_lab+mean_lab,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
