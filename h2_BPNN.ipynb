{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Computing Water Potential Energy Surface Using Behler and Parinello Symmetry Functions** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing relevant packages from PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Sat May 15 17:34:55 2021\n",
    "@author: Katerina Karoni\n",
    "\"\"\"\n",
    "import torch                        # Torch is an open-source machine learning library, a scientific computing framework,\n",
    "                                       #and a script language\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F     # Convolution Functions\n",
    "import torch.optim as optim         # Package implementing various optimization algorithms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms  #The torchvision package consists of popular datasets, model \n",
    "                                              #architectures, and common image transformations for computer vision\n",
    "from torch.utils.data import DataLoader, TensorDataset       #Data loading utility class\n",
    "from torch import Tensor\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import ase\n",
    "from ase import Atoms\n",
    "from ase.io import read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data (energies and geometries) for our 500 H2 molecular configurations in .xyz form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Energies file has (500,) entries\n"
     ]
    }
   ],
   "source": [
    "energies = np.genfromtxt('./h2/energies.txt')\n",
    "print(\"Energies file has\",np.shape(energies),\"entries\")\n",
    "#geometry_data =  read('./water/structures.xyz',index=':')\n",
    "#print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "#print(geometry_data[0])\n",
    "#geometry_data = np.array(geometry_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-943f3a945352>:5: ConversionWarning: Some errors were detected !\n",
      "    Line #5 (got 1 columns instead of 5)\n",
      "    Line #9 (got 1 columns instead of 5)\n",
      "    Line #13 (got 1 columns instead of 5)\n",
      "    Line #17 (got 1 columns instead of 5)\n",
      "    Line #21 (got 1 columns instead of 5)\n",
      "    Line #25 (got 1 columns instead of 5)\n",
      "    Line #29 (got 1 columns instead of 5)\n",
      "    Line #33 (got 1 columns instead of 5)\n",
      "    Line #37 (got 1 columns instead of 5)\n",
      "    Line #41 (got 1 columns instead of 5)\n",
      "    Line #45 (got 1 columns instead of 5)\n",
      "    Line #49 (got 1 columns instead of 5)\n",
      "    Line #53 (got 1 columns instead of 5)\n",
      "    Line #57 (got 1 columns instead of 5)\n",
      "    Line #61 (got 1 columns instead of 5)\n",
      "    Line #65 (got 1 columns instead of 5)\n",
      "    Line #69 (got 1 columns instead of 5)\n",
      "    Line #73 (got 1 columns instead of 5)\n",
      "    Line #77 (got 1 columns instead of 5)\n",
      "    Line #81 (got 1 columns instead of 5)\n",
      "    Line #85 (got 1 columns instead of 5)\n",
      "    Line #89 (got 1 columns instead of 5)\n",
      "    Line #93 (got 1 columns instead of 5)\n",
      "    Line #97 (got 1 columns instead of 5)\n",
      "    Line #101 (got 1 columns instead of 5)\n",
      "    Line #105 (got 1 columns instead of 5)\n",
      "    Line #109 (got 1 columns instead of 5)\n",
      "    Line #113 (got 1 columns instead of 5)\n",
      "    Line #117 (got 1 columns instead of 5)\n",
      "    Line #121 (got 1 columns instead of 5)\n",
      "    Line #125 (got 1 columns instead of 5)\n",
      "    Line #129 (got 1 columns instead of 5)\n",
      "    Line #133 (got 1 columns instead of 5)\n",
      "    Line #137 (got 1 columns instead of 5)\n",
      "    Line #141 (got 1 columns instead of 5)\n",
      "    Line #145 (got 1 columns instead of 5)\n",
      "    Line #149 (got 1 columns instead of 5)\n",
      "    Line #153 (got 1 columns instead of 5)\n",
      "    Line #157 (got 1 columns instead of 5)\n",
      "    Line #161 (got 1 columns instead of 5)\n",
      "    Line #165 (got 1 columns instead of 5)\n",
      "    Line #169 (got 1 columns instead of 5)\n",
      "    Line #173 (got 1 columns instead of 5)\n",
      "    Line #177 (got 1 columns instead of 5)\n",
      "    Line #181 (got 1 columns instead of 5)\n",
      "    Line #185 (got 1 columns instead of 5)\n",
      "    Line #189 (got 1 columns instead of 5)\n",
      "    Line #193 (got 1 columns instead of 5)\n",
      "    Line #197 (got 1 columns instead of 5)\n",
      "    Line #201 (got 1 columns instead of 5)\n",
      "    Line #205 (got 1 columns instead of 5)\n",
      "    Line #209 (got 1 columns instead of 5)\n",
      "    Line #213 (got 1 columns instead of 5)\n",
      "    Line #217 (got 1 columns instead of 5)\n",
      "    Line #221 (got 1 columns instead of 5)\n",
      "    Line #225 (got 1 columns instead of 5)\n",
      "    Line #229 (got 1 columns instead of 5)\n",
      "    Line #233 (got 1 columns instead of 5)\n",
      "    Line #237 (got 1 columns instead of 5)\n",
      "    Line #241 (got 1 columns instead of 5)\n",
      "    Line #245 (got 1 columns instead of 5)\n",
      "    Line #249 (got 1 columns instead of 5)\n",
      "    Line #253 (got 1 columns instead of 5)\n",
      "    Line #257 (got 1 columns instead of 5)\n",
      "    Line #261 (got 1 columns instead of 5)\n",
      "    Line #265 (got 1 columns instead of 5)\n",
      "    Line #269 (got 1 columns instead of 5)\n",
      "    Line #273 (got 1 columns instead of 5)\n",
      "    Line #277 (got 1 columns instead of 5)\n",
      "    Line #281 (got 1 columns instead of 5)\n",
      "    Line #285 (got 1 columns instead of 5)\n",
      "    Line #289 (got 1 columns instead of 5)\n",
      "    Line #293 (got 1 columns instead of 5)\n",
      "    Line #297 (got 1 columns instead of 5)\n",
      "    Line #301 (got 1 columns instead of 5)\n",
      "    Line #305 (got 1 columns instead of 5)\n",
      "    Line #309 (got 1 columns instead of 5)\n",
      "    Line #313 (got 1 columns instead of 5)\n",
      "    Line #317 (got 1 columns instead of 5)\n",
      "    Line #321 (got 1 columns instead of 5)\n",
      "    Line #325 (got 1 columns instead of 5)\n",
      "    Line #329 (got 1 columns instead of 5)\n",
      "    Line #333 (got 1 columns instead of 5)\n",
      "    Line #337 (got 1 columns instead of 5)\n",
      "    Line #341 (got 1 columns instead of 5)\n",
      "    Line #345 (got 1 columns instead of 5)\n",
      "    Line #349 (got 1 columns instead of 5)\n",
      "    Line #353 (got 1 columns instead of 5)\n",
      "    Line #357 (got 1 columns instead of 5)\n",
      "    Line #361 (got 1 columns instead of 5)\n",
      "    Line #365 (got 1 columns instead of 5)\n",
      "    Line #369 (got 1 columns instead of 5)\n",
      "    Line #373 (got 1 columns instead of 5)\n",
      "    Line #377 (got 1 columns instead of 5)\n",
      "    Line #381 (got 1 columns instead of 5)\n",
      "    Line #385 (got 1 columns instead of 5)\n",
      "    Line #389 (got 1 columns instead of 5)\n",
      "    Line #393 (got 1 columns instead of 5)\n",
      "    Line #397 (got 1 columns instead of 5)\n",
      "    Line #401 (got 1 columns instead of 5)\n",
      "    Line #405 (got 1 columns instead of 5)\n",
      "    Line #409 (got 1 columns instead of 5)\n",
      "    Line #413 (got 1 columns instead of 5)\n",
      "    Line #417 (got 1 columns instead of 5)\n",
      "    Line #421 (got 1 columns instead of 5)\n",
      "    Line #425 (got 1 columns instead of 5)\n",
      "    Line #429 (got 1 columns instead of 5)\n",
      "    Line #433 (got 1 columns instead of 5)\n",
      "    Line #437 (got 1 columns instead of 5)\n",
      "    Line #441 (got 1 columns instead of 5)\n",
      "    Line #445 (got 1 columns instead of 5)\n",
      "    Line #449 (got 1 columns instead of 5)\n",
      "    Line #453 (got 1 columns instead of 5)\n",
      "    Line #457 (got 1 columns instead of 5)\n",
      "    Line #461 (got 1 columns instead of 5)\n",
      "    Line #465 (got 1 columns instead of 5)\n",
      "    Line #469 (got 1 columns instead of 5)\n",
      "    Line #473 (got 1 columns instead of 5)\n",
      "    Line #477 (got 1 columns instead of 5)\n",
      "    Line #481 (got 1 columns instead of 5)\n",
      "    Line #485 (got 1 columns instead of 5)\n",
      "    Line #489 (got 1 columns instead of 5)\n",
      "    Line #493 (got 1 columns instead of 5)\n",
      "    Line #497 (got 1 columns instead of 5)\n",
      "    Line #501 (got 1 columns instead of 5)\n",
      "    Line #505 (got 1 columns instead of 5)\n",
      "    Line #509 (got 1 columns instead of 5)\n",
      "    Line #513 (got 1 columns instead of 5)\n",
      "    Line #517 (got 1 columns instead of 5)\n",
      "    Line #521 (got 1 columns instead of 5)\n",
      "    Line #525 (got 1 columns instead of 5)\n",
      "    Line #529 (got 1 columns instead of 5)\n",
      "    Line #533 (got 1 columns instead of 5)\n",
      "    Line #537 (got 1 columns instead of 5)\n",
      "    Line #541 (got 1 columns instead of 5)\n",
      "    Line #545 (got 1 columns instead of 5)\n",
      "    Line #549 (got 1 columns instead of 5)\n",
      "    Line #553 (got 1 columns instead of 5)\n",
      "    Line #557 (got 1 columns instead of 5)\n",
      "    Line #561 (got 1 columns instead of 5)\n",
      "    Line #565 (got 1 columns instead of 5)\n",
      "    Line #569 (got 1 columns instead of 5)\n",
      "    Line #573 (got 1 columns instead of 5)\n",
      "    Line #577 (got 1 columns instead of 5)\n",
      "    Line #581 (got 1 columns instead of 5)\n",
      "    Line #585 (got 1 columns instead of 5)\n",
      "    Line #589 (got 1 columns instead of 5)\n",
      "    Line #593 (got 1 columns instead of 5)\n",
      "    Line #597 (got 1 columns instead of 5)\n",
      "    Line #601 (got 1 columns instead of 5)\n",
      "    Line #605 (got 1 columns instead of 5)\n",
      "    Line #609 (got 1 columns instead of 5)\n",
      "    Line #613 (got 1 columns instead of 5)\n",
      "    Line #617 (got 1 columns instead of 5)\n",
      "    Line #621 (got 1 columns instead of 5)\n",
      "    Line #625 (got 1 columns instead of 5)\n",
      "    Line #629 (got 1 columns instead of 5)\n",
      "    Line #633 (got 1 columns instead of 5)\n",
      "    Line #637 (got 1 columns instead of 5)\n",
      "    Line #641 (got 1 columns instead of 5)\n",
      "    Line #645 (got 1 columns instead of 5)\n",
      "    Line #649 (got 1 columns instead of 5)\n",
      "    Line #653 (got 1 columns instead of 5)\n",
      "    Line #657 (got 1 columns instead of 5)\n",
      "    Line #661 (got 1 columns instead of 5)\n",
      "    Line #665 (got 1 columns instead of 5)\n",
      "    Line #669 (got 1 columns instead of 5)\n",
      "    Line #673 (got 1 columns instead of 5)\n",
      "    Line #677 (got 1 columns instead of 5)\n",
      "    Line #681 (got 1 columns instead of 5)\n",
      "    Line #685 (got 1 columns instead of 5)\n",
      "    Line #689 (got 1 columns instead of 5)\n",
      "    Line #693 (got 1 columns instead of 5)\n",
      "    Line #697 (got 1 columns instead of 5)\n",
      "    Line #701 (got 1 columns instead of 5)\n",
      "    Line #705 (got 1 columns instead of 5)\n",
      "    Line #709 (got 1 columns instead of 5)\n",
      "    Line #713 (got 1 columns instead of 5)\n",
      "    Line #717 (got 1 columns instead of 5)\n",
      "    Line #721 (got 1 columns instead of 5)\n",
      "    Line #725 (got 1 columns instead of 5)\n",
      "    Line #729 (got 1 columns instead of 5)\n",
      "    Line #733 (got 1 columns instead of 5)\n",
      "    Line #737 (got 1 columns instead of 5)\n",
      "    Line #741 (got 1 columns instead of 5)\n",
      "    Line #745 (got 1 columns instead of 5)\n",
      "    Line #749 (got 1 columns instead of 5)\n",
      "    Line #753 (got 1 columns instead of 5)\n",
      "    Line #757 (got 1 columns instead of 5)\n",
      "    Line #761 (got 1 columns instead of 5)\n",
      "    Line #765 (got 1 columns instead of 5)\n",
      "    Line #769 (got 1 columns instead of 5)\n",
      "    Line #773 (got 1 columns instead of 5)\n",
      "    Line #777 (got 1 columns instead of 5)\n",
      "    Line #781 (got 1 columns instead of 5)\n",
      "    Line #785 (got 1 columns instead of 5)\n",
      "    Line #789 (got 1 columns instead of 5)\n",
      "    Line #793 (got 1 columns instead of 5)\n",
      "    Line #797 (got 1 columns instead of 5)\n",
      "    Line #801 (got 1 columns instead of 5)\n",
      "    Line #805 (got 1 columns instead of 5)\n",
      "    Line #809 (got 1 columns instead of 5)\n",
      "    Line #813 (got 1 columns instead of 5)\n",
      "    Line #817 (got 1 columns instead of 5)\n",
      "    Line #821 (got 1 columns instead of 5)\n",
      "    Line #825 (got 1 columns instead of 5)\n",
      "    Line #829 (got 1 columns instead of 5)\n",
      "    Line #833 (got 1 columns instead of 5)\n",
      "    Line #837 (got 1 columns instead of 5)\n",
      "    Line #841 (got 1 columns instead of 5)\n",
      "    Line #845 (got 1 columns instead of 5)\n",
      "    Line #849 (got 1 columns instead of 5)\n",
      "    Line #853 (got 1 columns instead of 5)\n",
      "    Line #857 (got 1 columns instead of 5)\n",
      "    Line #861 (got 1 columns instead of 5)\n",
      "    Line #865 (got 1 columns instead of 5)\n",
      "    Line #869 (got 1 columns instead of 5)\n",
      "    Line #873 (got 1 columns instead of 5)\n",
      "    Line #877 (got 1 columns instead of 5)\n",
      "    Line #881 (got 1 columns instead of 5)\n",
      "    Line #885 (got 1 columns instead of 5)\n",
      "    Line #889 (got 1 columns instead of 5)\n",
      "    Line #893 (got 1 columns instead of 5)\n",
      "    Line #897 (got 1 columns instead of 5)\n",
      "    Line #901 (got 1 columns instead of 5)\n",
      "    Line #905 (got 1 columns instead of 5)\n",
      "    Line #909 (got 1 columns instead of 5)\n",
      "    Line #913 (got 1 columns instead of 5)\n",
      "    Line #917 (got 1 columns instead of 5)\n",
      "    Line #921 (got 1 columns instead of 5)\n",
      "    Line #925 (got 1 columns instead of 5)\n",
      "    Line #929 (got 1 columns instead of 5)\n",
      "    Line #933 (got 1 columns instead of 5)\n",
      "    Line #937 (got 1 columns instead of 5)\n",
      "    Line #941 (got 1 columns instead of 5)\n",
      "    Line #945 (got 1 columns instead of 5)\n",
      "    Line #949 (got 1 columns instead of 5)\n",
      "    Line #953 (got 1 columns instead of 5)\n",
      "    Line #957 (got 1 columns instead of 5)\n",
      "    Line #961 (got 1 columns instead of 5)\n",
      "    Line #965 (got 1 columns instead of 5)\n",
      "    Line #969 (got 1 columns instead of 5)\n",
      "    Line #973 (got 1 columns instead of 5)\n",
      "    Line #977 (got 1 columns instead of 5)\n",
      "    Line #981 (got 1 columns instead of 5)\n",
      "    Line #985 (got 1 columns instead of 5)\n",
      "    Line #989 (got 1 columns instead of 5)\n",
      "    Line #993 (got 1 columns instead of 5)\n",
      "    Line #997 (got 1 columns instead of 5)\n",
      "    Line #1001 (got 1 columns instead of 5)\n",
      "    Line #1005 (got 1 columns instead of 5)\n",
      "    Line #1009 (got 1 columns instead of 5)\n",
      "    Line #1013 (got 1 columns instead of 5)\n",
      "    Line #1017 (got 1 columns instead of 5)\n",
      "    Line #1021 (got 1 columns instead of 5)\n",
      "    Line #1025 (got 1 columns instead of 5)\n",
      "    Line #1029 (got 1 columns instead of 5)\n",
      "    Line #1033 (got 1 columns instead of 5)\n",
      "    Line #1037 (got 1 columns instead of 5)\n",
      "    Line #1041 (got 1 columns instead of 5)\n",
      "    Line #1045 (got 1 columns instead of 5)\n",
      "    Line #1049 (got 1 columns instead of 5)\n",
      "    Line #1053 (got 1 columns instead of 5)\n",
      "    Line #1057 (got 1 columns instead of 5)\n",
      "    Line #1061 (got 1 columns instead of 5)\n",
      "    Line #1065 (got 1 columns instead of 5)\n",
      "    Line #1069 (got 1 columns instead of 5)\n",
      "    Line #1073 (got 1 columns instead of 5)\n",
      "    Line #1077 (got 1 columns instead of 5)\n",
      "    Line #1081 (got 1 columns instead of 5)\n",
      "    Line #1085 (got 1 columns instead of 5)\n",
      "    Line #1089 (got 1 columns instead of 5)\n",
      "    Line #1093 (got 1 columns instead of 5)\n",
      "    Line #1097 (got 1 columns instead of 5)\n",
      "    Line #1101 (got 1 columns instead of 5)\n",
      "    Line #1105 (got 1 columns instead of 5)\n",
      "    Line #1109 (got 1 columns instead of 5)\n",
      "    Line #1113 (got 1 columns instead of 5)\n",
      "    Line #1117 (got 1 columns instead of 5)\n",
      "    Line #1121 (got 1 columns instead of 5)\n",
      "    Line #1125 (got 1 columns instead of 5)\n",
      "    Line #1129 (got 1 columns instead of 5)\n",
      "    Line #1133 (got 1 columns instead of 5)\n",
      "    Line #1137 (got 1 columns instead of 5)\n",
      "    Line #1141 (got 1 columns instead of 5)\n",
      "    Line #1145 (got 1 columns instead of 5)\n",
      "    Line #1149 (got 1 columns instead of 5)\n",
      "    Line #1153 (got 1 columns instead of 5)\n",
      "    Line #1157 (got 1 columns instead of 5)\n",
      "    Line #1161 (got 1 columns instead of 5)\n",
      "    Line #1165 (got 1 columns instead of 5)\n",
      "    Line #1169 (got 1 columns instead of 5)\n",
      "    Line #1173 (got 1 columns instead of 5)\n",
      "    Line #1177 (got 1 columns instead of 5)\n",
      "    Line #1181 (got 1 columns instead of 5)\n",
      "    Line #1185 (got 1 columns instead of 5)\n",
      "    Line #1189 (got 1 columns instead of 5)\n",
      "    Line #1193 (got 1 columns instead of 5)\n",
      "    Line #1197 (got 1 columns instead of 5)\n",
      "    Line #1201 (got 1 columns instead of 5)\n",
      "    Line #1205 (got 1 columns instead of 5)\n",
      "    Line #1209 (got 1 columns instead of 5)\n",
      "    Line #1213 (got 1 columns instead of 5)\n",
      "    Line #1217 (got 1 columns instead of 5)\n",
      "    Line #1221 (got 1 columns instead of 5)\n",
      "    Line #1225 (got 1 columns instead of 5)\n",
      "    Line #1229 (got 1 columns instead of 5)\n",
      "    Line #1233 (got 1 columns instead of 5)\n",
      "    Line #1237 (got 1 columns instead of 5)\n",
      "    Line #1241 (got 1 columns instead of 5)\n",
      "    Line #1245 (got 1 columns instead of 5)\n",
      "    Line #1249 (got 1 columns instead of 5)\n",
      "    Line #1253 (got 1 columns instead of 5)\n",
      "    Line #1257 (got 1 columns instead of 5)\n",
      "    Line #1261 (got 1 columns instead of 5)\n",
      "    Line #1265 (got 1 columns instead of 5)\n",
      "    Line #1269 (got 1 columns instead of 5)\n",
      "    Line #1273 (got 1 columns instead of 5)\n",
      "    Line #1277 (got 1 columns instead of 5)\n",
      "    Line #1281 (got 1 columns instead of 5)\n",
      "    Line #1285 (got 1 columns instead of 5)\n",
      "    Line #1289 (got 1 columns instead of 5)\n",
      "    Line #1293 (got 1 columns instead of 5)\n",
      "    Line #1297 (got 1 columns instead of 5)\n",
      "    Line #1301 (got 1 columns instead of 5)\n",
      "    Line #1305 (got 1 columns instead of 5)\n",
      "    Line #1309 (got 1 columns instead of 5)\n",
      "    Line #1313 (got 1 columns instead of 5)\n",
      "    Line #1317 (got 1 columns instead of 5)\n",
      "    Line #1321 (got 1 columns instead of 5)\n",
      "    Line #1325 (got 1 columns instead of 5)\n",
      "    Line #1329 (got 1 columns instead of 5)\n",
      "    Line #1333 (got 1 columns instead of 5)\n",
      "    Line #1337 (got 1 columns instead of 5)\n",
      "    Line #1341 (got 1 columns instead of 5)\n",
      "    Line #1345 (got 1 columns instead of 5)\n",
      "    Line #1349 (got 1 columns instead of 5)\n",
      "    Line #1353 (got 1 columns instead of 5)\n",
      "    Line #1357 (got 1 columns instead of 5)\n",
      "    Line #1361 (got 1 columns instead of 5)\n",
      "    Line #1365 (got 1 columns instead of 5)\n",
      "    Line #1369 (got 1 columns instead of 5)\n",
      "    Line #1373 (got 1 columns instead of 5)\n",
      "    Line #1377 (got 1 columns instead of 5)\n",
      "    Line #1381 (got 1 columns instead of 5)\n",
      "    Line #1385 (got 1 columns instead of 5)\n",
      "    Line #1389 (got 1 columns instead of 5)\n",
      "    Line #1393 (got 1 columns instead of 5)\n",
      "    Line #1397 (got 1 columns instead of 5)\n",
      "    Line #1401 (got 1 columns instead of 5)\n",
      "    Line #1405 (got 1 columns instead of 5)\n",
      "    Line #1409 (got 1 columns instead of 5)\n",
      "    Line #1413 (got 1 columns instead of 5)\n",
      "    Line #1417 (got 1 columns instead of 5)\n",
      "    Line #1421 (got 1 columns instead of 5)\n",
      "    Line #1425 (got 1 columns instead of 5)\n",
      "    Line #1429 (got 1 columns instead of 5)\n",
      "    Line #1433 (got 1 columns instead of 5)\n",
      "    Line #1437 (got 1 columns instead of 5)\n",
      "    Line #1441 (got 1 columns instead of 5)\n",
      "    Line #1445 (got 1 columns instead of 5)\n",
      "    Line #1449 (got 1 columns instead of 5)\n",
      "    Line #1453 (got 1 columns instead of 5)\n",
      "    Line #1457 (got 1 columns instead of 5)\n",
      "    Line #1461 (got 1 columns instead of 5)\n",
      "    Line #1465 (got 1 columns instead of 5)\n",
      "    Line #1469 (got 1 columns instead of 5)\n",
      "    Line #1473 (got 1 columns instead of 5)\n",
      "    Line #1477 (got 1 columns instead of 5)\n",
      "    Line #1481 (got 1 columns instead of 5)\n",
      "    Line #1485 (got 1 columns instead of 5)\n",
      "    Line #1489 (got 1 columns instead of 5)\n",
      "    Line #1493 (got 1 columns instead of 5)\n",
      "    Line #1497 (got 1 columns instead of 5)\n",
      "    Line #1501 (got 1 columns instead of 5)\n",
      "    Line #1505 (got 1 columns instead of 5)\n",
      "    Line #1509 (got 1 columns instead of 5)\n",
      "    Line #1513 (got 1 columns instead of 5)\n",
      "    Line #1517 (got 1 columns instead of 5)\n",
      "    Line #1521 (got 1 columns instead of 5)\n",
      "    Line #1525 (got 1 columns instead of 5)\n",
      "    Line #1529 (got 1 columns instead of 5)\n",
      "    Line #1533 (got 1 columns instead of 5)\n",
      "    Line #1537 (got 1 columns instead of 5)\n",
      "    Line #1541 (got 1 columns instead of 5)\n",
      "    Line #1545 (got 1 columns instead of 5)\n",
      "    Line #1549 (got 1 columns instead of 5)\n",
      "    Line #1553 (got 1 columns instead of 5)\n",
      "    Line #1557 (got 1 columns instead of 5)\n",
      "    Line #1561 (got 1 columns instead of 5)\n",
      "    Line #1565 (got 1 columns instead of 5)\n",
      "    Line #1569 (got 1 columns instead of 5)\n",
      "    Line #1573 (got 1 columns instead of 5)\n",
      "    Line #1577 (got 1 columns instead of 5)\n",
      "    Line #1581 (got 1 columns instead of 5)\n",
      "    Line #1585 (got 1 columns instead of 5)\n",
      "    Line #1589 (got 1 columns instead of 5)\n",
      "    Line #1593 (got 1 columns instead of 5)\n",
      "    Line #1597 (got 1 columns instead of 5)\n",
      "    Line #1601 (got 1 columns instead of 5)\n",
      "    Line #1605 (got 1 columns instead of 5)\n",
      "    Line #1609 (got 1 columns instead of 5)\n",
      "    Line #1613 (got 1 columns instead of 5)\n",
      "    Line #1617 (got 1 columns instead of 5)\n",
      "    Line #1621 (got 1 columns instead of 5)\n",
      "    Line #1625 (got 1 columns instead of 5)\n",
      "    Line #1629 (got 1 columns instead of 5)\n",
      "    Line #1633 (got 1 columns instead of 5)\n",
      "    Line #1637 (got 1 columns instead of 5)\n",
      "    Line #1641 (got 1 columns instead of 5)\n",
      "    Line #1645 (got 1 columns instead of 5)\n",
      "    Line #1649 (got 1 columns instead of 5)\n",
      "    Line #1653 (got 1 columns instead of 5)\n",
      "    Line #1657 (got 1 columns instead of 5)\n",
      "    Line #1661 (got 1 columns instead of 5)\n",
      "    Line #1665 (got 1 columns instead of 5)\n",
      "    Line #1669 (got 1 columns instead of 5)\n",
      "    Line #1673 (got 1 columns instead of 5)\n",
      "    Line #1677 (got 1 columns instead of 5)\n",
      "    Line #1681 (got 1 columns instead of 5)\n",
      "    Line #1685 (got 1 columns instead of 5)\n",
      "    Line #1689 (got 1 columns instead of 5)\n",
      "    Line #1693 (got 1 columns instead of 5)\n",
      "    Line #1697 (got 1 columns instead of 5)\n",
      "    Line #1701 (got 1 columns instead of 5)\n",
      "    Line #1705 (got 1 columns instead of 5)\n",
      "    Line #1709 (got 1 columns instead of 5)\n",
      "    Line #1713 (got 1 columns instead of 5)\n",
      "    Line #1717 (got 1 columns instead of 5)\n",
      "    Line #1721 (got 1 columns instead of 5)\n",
      "    Line #1725 (got 1 columns instead of 5)\n",
      "    Line #1729 (got 1 columns instead of 5)\n",
      "    Line #1733 (got 1 columns instead of 5)\n",
      "    Line #1737 (got 1 columns instead of 5)\n",
      "    Line #1741 (got 1 columns instead of 5)\n",
      "    Line #1745 (got 1 columns instead of 5)\n",
      "    Line #1749 (got 1 columns instead of 5)\n",
      "    Line #1753 (got 1 columns instead of 5)\n",
      "    Line #1757 (got 1 columns instead of 5)\n",
      "    Line #1761 (got 1 columns instead of 5)\n",
      "    Line #1765 (got 1 columns instead of 5)\n",
      "    Line #1769 (got 1 columns instead of 5)\n",
      "    Line #1773 (got 1 columns instead of 5)\n",
      "    Line #1777 (got 1 columns instead of 5)\n",
      "    Line #1781 (got 1 columns instead of 5)\n",
      "    Line #1785 (got 1 columns instead of 5)\n",
      "    Line #1789 (got 1 columns instead of 5)\n",
      "    Line #1793 (got 1 columns instead of 5)\n",
      "    Line #1797 (got 1 columns instead of 5)\n",
      "    Line #1801 (got 1 columns instead of 5)\n",
      "    Line #1805 (got 1 columns instead of 5)\n",
      "    Line #1809 (got 1 columns instead of 5)\n",
      "    Line #1813 (got 1 columns instead of 5)\n",
      "    Line #1817 (got 1 columns instead of 5)\n",
      "    Line #1821 (got 1 columns instead of 5)\n",
      "    Line #1825 (got 1 columns instead of 5)\n",
      "    Line #1829 (got 1 columns instead of 5)\n",
      "    Line #1833 (got 1 columns instead of 5)\n",
      "    Line #1837 (got 1 columns instead of 5)\n",
      "    Line #1841 (got 1 columns instead of 5)\n",
      "    Line #1845 (got 1 columns instead of 5)\n",
      "    Line #1849 (got 1 columns instead of 5)\n",
      "    Line #1853 (got 1 columns instead of 5)\n",
      "    Line #1857 (got 1 columns instead of 5)\n",
      "    Line #1861 (got 1 columns instead of 5)\n",
      "    Line #1865 (got 1 columns instead of 5)\n",
      "    Line #1869 (got 1 columns instead of 5)\n",
      "    Line #1873 (got 1 columns instead of 5)\n",
      "    Line #1877 (got 1 columns instead of 5)\n",
      "    Line #1881 (got 1 columns instead of 5)\n",
      "    Line #1885 (got 1 columns instead of 5)\n",
      "    Line #1889 (got 1 columns instead of 5)\n",
      "    Line #1893 (got 1 columns instead of 5)\n",
      "    Line #1897 (got 1 columns instead of 5)\n",
      "    Line #1901 (got 1 columns instead of 5)\n",
      "    Line #1905 (got 1 columns instead of 5)\n",
      "    Line #1909 (got 1 columns instead of 5)\n",
      "    Line #1913 (got 1 columns instead of 5)\n",
      "    Line #1917 (got 1 columns instead of 5)\n",
      "    Line #1921 (got 1 columns instead of 5)\n",
      "    Line #1925 (got 1 columns instead of 5)\n",
      "    Line #1929 (got 1 columns instead of 5)\n",
      "    Line #1933 (got 1 columns instead of 5)\n",
      "    Line #1937 (got 1 columns instead of 5)\n",
      "    Line #1941 (got 1 columns instead of 5)\n",
      "    Line #1945 (got 1 columns instead of 5)\n",
      "    Line #1949 (got 1 columns instead of 5)\n",
      "    Line #1953 (got 1 columns instead of 5)\n",
      "    Line #1957 (got 1 columns instead of 5)\n",
      "    Line #1961 (got 1 columns instead of 5)\n",
      "    Line #1965 (got 1 columns instead of 5)\n",
      "    Line #1969 (got 1 columns instead of 5)\n",
      "    Line #1973 (got 1 columns instead of 5)\n",
      "    Line #1977 (got 1 columns instead of 5)\n",
      "    Line #1981 (got 1 columns instead of 5)\n",
      "    Line #1985 (got 1 columns instead of 5)\n",
      "    Line #1989 (got 1 columns instead of 5)\n",
      "    Line #1993 (got 1 columns instead of 5)\n",
      "    Line #1997 (got 1 columns instead of 5)\n",
      "  xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n"
     ]
    }
   ],
   "source": [
    "# see https://education.molssi.org/python-data-analysis/01-numpy-arrays/index.html\n",
    "#https://stackoverflow.com/questions/23353585/got-1-columns-instead-of-error-in-numpy\n",
    "\n",
    "file_location = os.path.join('h2', 'structures.xyz')\n",
    "xyz_file = np.genfromtxt(fname=file_location, skip_header=2, dtype='unicode',invalid_raise = False)\n",
    "# where invalid_raise = False was used to skip all lines in the xyz file that only have one column\n",
    "symbols = xyz_file[:,0]\n",
    "coordinates = (xyz_file[:,1:-1])\n",
    "coordinates = coordinates.astype(np.float)\n",
    "\n",
    "#print(symbols)\n",
    "#print(coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternatively loading the data in .npy form\n",
    "# # The data was downloaded from http://www.quantum-machine.org/datasets/ (Densities dataset--> water.zip)\n",
    "# # The data includes energies, densities and structure for water molecules\n",
    "# #For each dataset, structures are given in with positions in Bohr and the energies are given in kcal/mol \n",
    "# energy_data = np.load('./water_102/dft_energies.npy')\n",
    "# print(\"Energies file has\",np.shape(energy_data),\"entries\")\n",
    "# geometry_data =  np.load('./water-2/water_102/structures.npy')\n",
    "# print(\"Geometry file has\",np.shape(geometry_data),\"entries\")\n",
    "# print(type(energy_data))\n",
    "# print(type(geometry_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n",
      "<class 'numpy.ndarray'>\n",
      "[-0.12741596  0.          0.        ]\n",
      "[0.12741596 0.         0.        ]\n",
      "[-0.13270776  0.          0.        ]\n",
      "coord\n",
      "[0.13270776 0.         0.        ]\n",
      "[-0.13988344  0.          0.        ]\n",
      "[0.13988344 0.         0.        ]\n",
      "-274.25385732225\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(coordinates))\n",
    "print(type(coordinates))\n",
    "print(coordinates[0])\n",
    "print(coordinates[1])\n",
    "print(coordinates[2])\n",
    "\n",
    "print('coord')\n",
    "print(coordinates[3])\n",
    "print(coordinates[4])\n",
    "print(coordinates[5])\n",
    "\n",
    "print(energies[0])\n",
    "# There is 500 h2 molecules and each of them consists of 2 atoms, so we have 1000 atoms in total and each\n",
    "# of them has 3 coordinates.\n",
    "# Thus the coordinates array has 1000 lines, each of them corresponding to one atom (the first 2 lines\n",
    "# correspopnd to the first H2 molecule) and 3 columns corresponding to the x, y and z coordinates respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12741596  0.          0.        ]\n",
      " [ 0.12741596  0.          0.        ]\n",
      " [-0.13270776  0.          0.        ]\n",
      " ...\n",
      " [ 1.49673271  0.          0.        ]\n",
      " [-1.49918811  0.          0.        ]\n",
      " [ 1.49918811  0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Cutoff Function** \n",
    "    $$f_c(R_{ij}) = \n",
    "    \\begin{cases}\n",
    "        0.5 \\times \\big[\\cos\\big(\\frac{\\pi R_{ij}}{R_c}\\big)+1\\big]  & \\text{for } R_{ij} \\leq R_c\\\\\n",
    "        0  & \\text{for } R_{ij} > R_c\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    \n",
    "In the Behler and Parinello paper the Cutoff radius $R_c$ was taken to be $6$  Ångströms, or 11.3384 Bohr radii. (Remember, 1 Ångström is $10^{-10}$m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def fc(R,Rc):\n",
    "    if R <= Rc:\n",
    "        fcutoff = 0.5 * (np.cos(np.pi*R/Rc)+1)\n",
    "    else:\n",
    "        fcutoff = 0\n",
    "    return fcutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting fc as a function of interatomic distance Rij\n",
    "\n",
    "Rc  = 11.3384 # Bohr\n",
    "\n",
    "Rij     = np.linspace(0,Rc)\n",
    "fcutoff = np.zeros(np.size(Rij))\n",
    "\n",
    "for i in range(np.size(Rij)):\n",
    "    fcutoff[i] = fc(Rij[i],Rc)\n",
    "\n",
    "plt.plot(Rij,fcutoff)\n",
    "plt.title('Cut-off function vs Interatomic distance', fontsize=16)\n",
    "plt.xlabel('Interatomic Distance $R_{ij}$', fontsize=16)\n",
    "plt.ylabel('$f_c(R_{ij})$', fontsize=16)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Pairwise Distances**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$Dp = \\begin{bmatrix} R_{00} & R_{01} & R_{02} \\\\ R_{10} & R_{11} & R_{12} \\\\ R_{20} & R_{21} & R_{22} \\end{bmatrix} = \\begin{bmatrix} 0 & R_{01} & R_{02} \\\\ R_{01} & 0 & R_{12} \\\\ R_{02} & R_{12} & 0 \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n",
      "[[0.         0.25483192]\n",
      " [0.25483192 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "i = 0                                # i-th water molecule\n",
    "N = 2 # 2 atoms per molecule\n",
    "coord = coordinates[N*i:N*(i+1),:]   # Let's take the coordinates of the ith water molecule in our dataset and compute\n",
    "                                     # pairwise distances between all of its 3 atom\n",
    "print(np.shape(coord))\n",
    "def pairwise_distances(coord):                       # we pass in the coordinates of the 3 atoms in the water molecule\n",
    "    N = len(coord)\n",
    "    pairwise_dist_matrix = np.zeros((N,N))       # Initialise the matrix\n",
    "    for i in range(0,N-1):\n",
    "#        print('i=',i)\n",
    "        for j in range(i+1,N):\n",
    "#            print(j)\n",
    "#            pairwise_dist_matrix[i][j] = \\\n",
    "#            np.sqrt(  (coord[i][0] - coord[j][0] )**2 + (coord[i][1] - coord[j][1] )**2 +(coord[i][2] - coord[j][2] )**2   )\n",
    "            pairwise_dist_matrix[i][j] =  np.sqrt(sum( (coord[i,:] - coord[j,:])**2 ))\n",
    "            pairwise_dist_matrix[j][i] = pairwise_dist_matrix[i][j]\n",
    "\n",
    "    return pairwise_dist_matrix\n",
    "\n",
    "Dp = pairwise_distances(coord)\n",
    "print(Dp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>From Cartesian to Generalised Coordinates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Radial Symmetry Functions**\n",
    "    \n",
    "<h3>$$G_i^1 = \\sum_{j \\neq i}^{\\text{all}} e^{-\\eta (R_{ij}-R_s)^2} f_c (R_{ij})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.99228933 0.99228933]\n"
     ]
    }
   ],
   "source": [
    "heta = 0.1\n",
    "Rs   = 0\n",
    "N    = len(coord)\n",
    "\n",
    "\n",
    "def radial_BP_symm_func(Dp,N,heta,Rs):\n",
    "    G_mu1 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(0,i):\n",
    "            G_mu1[i] = G_mu1[i] + np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "        for j in range(i+1,N):\n",
    "            G_mu1[i] = G_mu1[i] +  np.exp(-heta*(Dp[i][j]-Rs)**2)* fc(Dp[i][j],Rc) \n",
    "    return G_mu1\n",
    "\n",
    "Gmu1 = radial_BP_symm_func(Dp,N,heta,Rs)\n",
    "print(Gmu1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3> Angular Symmetry Functions**\n",
    "\n",
    "$$G_i^2 = 2^{1-\\zeta} \\sum_{j,k \\neq i}^{\\text{all}} (1+\\lambda \\cos \\theta_{ijk})^\\zeta \\times e^{-\\eta (R_{ij}^2+R_{ik}^2+R_{jk}^2 )} f_c (R_{ij})f_c (R_{ik})f_c (R_{jk})$$\n",
    "    \n",
    "with parameters $\\lambda = +1, -1$, $\\eta$ and $\\zeta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.96927623 1.96927623]\n"
     ]
    }
   ],
   "source": [
    "lambdaa = 1     #1\n",
    "zeta    = 0.2\n",
    "heta    = 0.1\n",
    "\n",
    "N = len(coord)\n",
    "\n",
    "def angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta):\n",
    "    G_mu2 = np.zeros(len(Dp))           # since we are dealing with water molecules the dimension of G will be 3 (H,H,O)\n",
    "\n",
    "    for i in range(N):                 # to avoid using an if statement (if i not equal j), break the sum in two\n",
    "        for j in range(N):           \n",
    "            for k in range(N):\n",
    "                if j != i and k !=i:\n",
    "                    R_vec_ij = coord[i,:] - coord[j,:]\n",
    "                    R_vec_jk = coord[i,:] - coord[k,:]\n",
    "                    cos_theta_ijk  = np.dot(R_vec_ij, R_vec_jk)/(Dp[i][j]*Dp[i][k])\n",
    "                    G_mu2[i]   = G_mu2[i] + (  1 + lambdaa * cos_theta_ijk )**zeta  \\\n",
    "                                * np.exp( -heta * (Dp[i][j]**2 + Dp[i][k]**2 + Dp[j][k]**2) ) \\\n",
    "                                * fc(Dp[i][j],Rc) * fc(Dp[i][k],Rc) * fc(Dp[j][k],Rc)            \n",
    "        G_mu2[i]   = 2**(1-zeta) * G_mu2[i] \n",
    "    return G_mu2\n",
    "\n",
    "Gmu2 = angular_BP_symm_func(coord,Dp,N,heta,Rs,lambdaa,zeta)\n",
    "print(Gmu2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5984600690578581"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.cos(180)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training and Test Set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "torch.Size([500, 2, 6])\n",
      "(500,)\n",
      "torch.Size([500, 2, 6])\n",
      "-646.6184348965379\n",
      "torch.Size([450, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "N                    = 2           # number of atoms per molecule\n",
    "number_of_features   = 6           # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "# heta   = np.linspace(0.01, 2, num=number_of_features)\n",
    "# random.shuffle(heta)\n",
    "\n",
    "# Rs     = np.linspace(0, 1, num=number_of_features)\n",
    "# random.shuffle(Rs)\n",
    "\n",
    "# lambdaa = np.ones(number_of_features)\n",
    "# random.shuffle(lambdaa)\n",
    "\n",
    "# zeta    = np.linspace(0, 8, num=number_of_features)\n",
    "# random.shuffle(zeta)\n",
    "\n",
    "\n",
    "# heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "# zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "# Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "# lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "heta    = [0.3,  4.,    3.202, 1.606, 2.404, 0.808] \n",
    "zeta    = [8.,  0, 3.2, 4.8 , 6.4, 8 ]\n",
    "Rs      = [0.8, 0.4, 2, 1. ,  0.,  0.6]\n",
    "lambdaa = [1., 1., 1., 1. , 1., 1.]\n",
    "\n",
    "\n",
    "# print(np.shape(energies))\n",
    "# print(np.shape(coordinates))\n",
    "# shuffler = np.random.permutation(len(energies))\n",
    "\n",
    "# coordinates = coordinates[shuffler]\n",
    "\n",
    "# energies = energies[shuffler]\n",
    "\n",
    "# print(np.shape(energies))\n",
    "# print(np.shape(coordinates))\n",
    "\n",
    "\n",
    "data_size            = np.shape(energies)[0]        # We have 500 H2 molecule conformations\n",
    "training_set_size    = data_size - 50\n",
    "\n",
    "    \n",
    "G = np.zeros((len(coordinates), number_of_features))  # we have 3000x2 features (2 symm funcs for ech of the 3000 atoms in the dataset)\n",
    "\n",
    "for i in range(data_size):\n",
    "    coord = coordinates[N*i:N*(i+1),:]\n",
    "    Dp    = pairwise_distances(coord)\n",
    "    for j in range(0,number_of_features):#,2):\n",
    "        G[N*i:N*(i+1),j]   = radial_BP_symm_func(Dp,N,heta[j],Rs[j])     \n",
    "#        G[N*i:N*(i+1),j+1] = angular_BP_symm_func(coord,Dp,N,heta[j],Rs[j],lambdaa[j],zeta[j])\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "G_train = G[:training_set_size,:]\n",
    "var  = np.var(G_train,axis=0)\n",
    "mean = np.mean(G_train,axis=0)\n",
    "\n",
    "G_norm = np.zeros((len(coordinates), number_of_features))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(G)[0]):\n",
    "    for j in range(np.shape(G)[1]):\n",
    "        G_norm[i,j] = (G[i,j]-mean[j])/var[j]   \n",
    "\n",
    "\n",
    "data_set = np.vsplit(G_norm,data_size)     # Going from a (3000,2) np.array to a (1000,3,2) list\n",
    "#data_set = np.random.permutation(data_set)\n",
    "data_set = torch.FloatTensor(data_set)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "#data_set=data_set[torch.randperm(data_set.size()[0])]\n",
    "\n",
    "\n",
    "# print(data_set[0])\n",
    "# print(data_set[0][1][1])\n",
    "\n",
    "labels = energies          # turning energies into a (1000) tensor\n",
    "\n",
    "\n",
    "\n",
    "print(np.shape(labels))\n",
    "print(np.shape(data_set))\n",
    "shuffler = np.random.permutation(len(labels))\n",
    "\n",
    "data_set = data_set[shuffler]\n",
    "\n",
    "labels = labels[shuffler]\n",
    "\n",
    "print(np.shape(labels))\n",
    "print(np.shape(data_set))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Computing variance and mean on the training data only!\n",
    "lab_train = labels[:training_set_size]\n",
    "var_lab  = np.var(lab_train,axis=0)\n",
    "mean_lab = np.mean(lab_train,axis=0)\n",
    "print(mean_lab)\n",
    "\n",
    "labels_norm = np.zeros((np.shape(labels)))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(labels)[0]):\n",
    "    labels_norm[i] = (labels[i]-mean_lab)/var_lab  \n",
    "    \n",
    "    \n",
    "labels_norm = torch.FloatTensor(labels_norm)      \n",
    "   \n",
    "     \n",
    "# Splitting the dataset into training and test set\n",
    "training_set         = data_set[:training_set_size]\n",
    "test_set             = data_set[training_set_size:]\n",
    "\n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "\n",
    "# Dataset\n",
    "dataset = TensorDataset(training_set, train_labels)\n",
    "#print(dataset[0])\n",
    "\n",
    "# Creating the batches\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=50,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) # ?????\n",
    "\n",
    "print(np.shape(training_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-3.0634e+02, -3.4165e+00,  7.2102e-01, -3.0591e+01, -3.9754e+00,\n",
      "          -3.2715e+01],\n",
      "         [-3.0634e+02, -3.4165e+00,  7.2102e-01, -3.0591e+01, -3.9754e+00,\n",
      "          -3.2715e+01]],\n",
      "\n",
      "        [[-1.7484e+02, -3.4164e+00,  4.8883e+01, -2.5861e+01, -3.9751e+00,\n",
      "          -2.7632e+01],\n",
      "         [-1.7484e+02, -3.4164e+00,  4.8883e+01, -2.5861e+01, -3.9751e+00,\n",
      "          -2.7632e+01]],\n",
      "\n",
      "        [[-2.7093e+02, -3.4165e+00,  1.1482e+01, -3.0206e+01, -3.9754e+00,\n",
      "          -3.1989e+01],\n",
      "         [-2.7093e+02, -3.4165e+00,  1.1482e+01, -3.0206e+01, -3.9754e+00,\n",
      "          -3.1989e+01]],\n",
      "\n",
      "        [[ 6.4108e-01, -2.5509e+00, -1.9157e-02,  6.0993e+00, -3.2679e+00,\n",
      "          -2.6019e+00],\n",
      "         [ 6.4108e-01, -2.5509e+00, -1.9157e-02,  6.0993e+00, -3.2679e+00,\n",
      "          -2.6019e+00]],\n",
      "\n",
      "        [[-3.1344e+02, -3.4165e+00, -6.9502e-01, -3.0632e+01, -3.9754e+00,\n",
      "          -3.2819e+01],\n",
      "         [-3.1344e+02, -3.4165e+00, -6.9502e-01, -3.0632e+01, -3.9754e+00,\n",
      "          -3.2819e+01]],\n",
      "\n",
      "        [[ 1.4181e+01, -1.2799e+00, -3.4015e+00,  7.1147e+00, -2.2612e+00,\n",
      "           1.7040e+00],\n",
      "         [ 1.4181e+01, -1.2799e+00, -3.4015e+00,  7.1147e+00, -2.2612e+00,\n",
      "           1.7040e+00]],\n",
      "\n",
      "        [[-2.9693e+02, -3.4165e+00,  2.9865e+00, -3.0522e+01, -3.9754e+00,\n",
      "          -3.2558e+01],\n",
      "         [-2.9693e+02, -3.4165e+00,  2.9865e+00, -3.0522e+01, -3.9754e+00,\n",
      "          -3.2558e+01]],\n",
      "\n",
      "        [[ 1.9170e+01,  3.5530e-01, -4.5406e+00,  5.0616e+00, -7.5715e-01,\n",
      "           4.2696e+00],\n",
      "         [ 1.9170e+01,  3.5530e-01, -4.5406e+00,  5.0616e+00, -7.5715e-01,\n",
      "           4.2696e+00]],\n",
      "\n",
      "        [[-3.5889e+01, -3.2934e+00,  1.3783e+01, -2.0879e+00, -3.8483e+00,\n",
      "          -1.0891e+01],\n",
      "         [-3.5889e+01, -3.2934e+00,  1.3783e+01, -2.0879e+00, -3.8483e+00,\n",
      "          -1.0891e+01]],\n",
      "\n",
      "        [[-6.9192e+01, -3.3939e+00,  2.9430e+01, -9.9135e+00, -3.9439e+00,\n",
      "          -1.6537e+01],\n",
      "         [-6.9192e+01, -3.3939e+00,  2.9430e+01, -9.9135e+00, -3.9439e+00,\n",
      "          -1.6537e+01]],\n",
      "\n",
      "        [[-5.4381e+01, -3.3684e+00,  2.2408e+01, -6.5408e+00, -3.9171e+00,\n",
      "          -1.4196e+01],\n",
      "         [-5.4381e+01, -3.3684e+00,  2.2408e+01, -6.5408e+00, -3.9171e+00,\n",
      "          -1.4196e+01]],\n",
      "\n",
      "        [[-2.1490e+00, -2.6803e+00,  8.0539e-01,  5.6151e+00, -3.3666e+00,\n",
      "          -3.3566e+00],\n",
      "         [-2.1490e+00, -2.6803e+00,  8.0539e-01,  5.6151e+00, -3.3666e+00,\n",
      "          -3.3566e+00]],\n",
      "\n",
      "        [[ 1.5042e+01, -1.1187e+00, -3.5868e+00,  7.0138e+00, -2.1257e+00,\n",
      "           2.0479e+00],\n",
      "         [ 1.5042e+01, -1.1187e+00, -3.5868e+00,  7.0138e+00, -2.1257e+00,\n",
      "           2.0479e+00]],\n",
      "\n",
      "        [[-9.7199e+01, -3.4113e+00,  4.1368e+01, -1.5582e+01, -3.9655e+00,\n",
      "          -2.0346e+01],\n",
      "         [-9.7199e+01, -3.4113e+00,  4.1368e+01, -1.5582e+01, -3.9655e+00,\n",
      "          -2.0346e+01]],\n",
      "\n",
      "        [[-1.5991e+02, -3.4163e+00,  5.1095e+01, -2.4468e+01, -3.9748e+00,\n",
      "          -2.6544e+01],\n",
      "         [-1.5991e+02, -3.4163e+00,  5.1095e+01, -2.4468e+01, -3.9748e+00,\n",
      "          -2.6544e+01]],\n",
      "\n",
      "        [[-1.5709e+02, -3.4163e+00,  5.1316e+01, -2.4178e+01, -3.9747e+00,\n",
      "          -2.6324e+01],\n",
      "         [-1.5709e+02, -3.4163e+00,  5.1316e+01, -2.4178e+01, -3.9747e+00,\n",
      "          -2.6324e+01]],\n",
      "\n",
      "        [[-7.3912e+01, -3.3988e+00,  3.1612e+01, -1.0938e+01, -3.9495e+00,\n",
      "          -1.7233e+01],\n",
      "         [-7.3912e+01, -3.3988e+00,  3.1612e+01, -1.0938e+01, -3.9495e+00,\n",
      "          -1.7233e+01]],\n",
      "\n",
      "        [[-2.6247e+02, -3.4165e+00,  1.4857e+01, -3.0052e+01, -3.9754e+00,\n",
      "          -3.1758e+01],\n",
      "         [-2.6247e+02, -3.4165e+00,  1.4857e+01, -3.0052e+01, -3.9754e+00,\n",
      "          -3.1758e+01]],\n",
      "\n",
      "        [[-1.4643e+02, -3.4161e+00,  5.1571e+01, -2.2989e+01, -3.9742e+00,\n",
      "          -2.5445e+01],\n",
      "         [-1.4643e+02, -3.4161e+00,  5.1571e+01, -2.2989e+01, -3.9742e+00,\n",
      "          -2.5445e+01]],\n",
      "\n",
      "        [[ 1.7528e+01,  2.3351e+00, -4.9819e+00,  1.9307e-01,  1.7325e+00,\n",
      "           5.6278e+00],\n",
      "         [ 1.7528e+01,  2.3351e+00, -4.9819e+00,  1.9307e-01,  1.7325e+00,\n",
      "           5.6278e+00]],\n",
      "\n",
      "        [[-1.6233e+02, -3.4163e+00,  5.0854e+01, -2.4711e+01, -3.9749e+00,\n",
      "          -2.6730e+01],\n",
      "         [-1.6233e+02, -3.4163e+00,  5.0854e+01, -2.4711e+01, -3.9749e+00,\n",
      "          -2.6730e+01]],\n",
      "\n",
      "        [[-1.3875e+02, -3.4160e+00,  5.1180e+01, -2.2046e+01, -3.9738e+00,\n",
      "          -2.4767e+01],\n",
      "         [-1.3875e+02, -3.4160e+00,  5.1180e+01, -2.2046e+01, -3.9738e+00,\n",
      "          -2.4767e+01]],\n",
      "\n",
      "        [[ 9.9523e+00,  3.5351e+00, -5.0950e+00, -5.4385e+00,  4.4839e+00,\n",
      "           5.3949e+00],\n",
      "         [ 9.9523e+00,  3.5351e+00, -5.0950e+00, -5.4385e+00,  4.4839e+00,\n",
      "           5.3949e+00]],\n",
      "\n",
      "        [[-7.1595e+01, -3.3965e+00,  3.0546e+01, -1.0439e+01, -3.9468e+00,\n",
      "          -1.6894e+01],\n",
      "         [-7.1595e+01, -3.3965e+00,  3.0546e+01, -1.0439e+01, -3.9468e+00,\n",
      "          -1.6894e+01]],\n",
      "\n",
      "        [[-4.1629e+01, -3.3246e+00,  1.6400e+01, -3.4878e+00, -3.8759e+00,\n",
      "          -1.1967e+01],\n",
      "         [-4.1629e+01, -3.3246e+00,  1.6400e+01, -3.4878e+00, -3.8759e+00,\n",
      "          -1.1967e+01]],\n",
      "\n",
      "        [[-2.1281e+02, -3.4165e+00,  3.6683e+01, -2.8388e+01, -3.9754e+00,\n",
      "          -2.9850e+01],\n",
      "         [-2.1281e+02, -3.4165e+00,  3.6683e+01, -2.8388e+01, -3.9754e+00,\n",
      "          -2.9850e+01]],\n",
      "\n",
      "        [[-1.7515e+02, -3.4164e+00,  4.8819e+01, -2.5887e+01, -3.9751e+00,\n",
      "          -2.7653e+01],\n",
      "         [-1.7515e+02, -3.4164e+00,  4.8819e+01, -2.5887e+01, -3.9751e+00,\n",
      "          -2.7653e+01]],\n",
      "\n",
      "        [[-2.6261e+02, -3.4165e+00,  1.4799e+01, -3.0054e+01, -3.9754e+00,\n",
      "          -3.1762e+01],\n",
      "         [-2.6261e+02, -3.4165e+00,  1.4799e+01, -3.0054e+01, -3.9754e+00,\n",
      "          -3.1762e+01]],\n",
      "\n",
      "        [[-4.5878e+01, -3.3424e+00,  1.8379e+01, -4.5163e+00, -3.8923e+00,\n",
      "          -1.2733e+01],\n",
      "         [-4.5878e+01, -3.3424e+00,  1.8379e+01, -4.5163e+00, -3.8923e+00,\n",
      "          -1.2733e+01]],\n",
      "\n",
      "        [[-3.0767e+00, -2.7183e+00,  1.0892e+00,  5.4436e+00, -3.3956e+00,\n",
      "          -3.6013e+00],\n",
      "         [-3.0767e+00, -2.7183e+00,  1.0892e+00,  5.4436e+00, -3.3956e+00,\n",
      "          -3.6013e+00]],\n",
      "\n",
      "        [[ 1.9427e+01,  7.0381e-01, -4.6585e+00,  4.3923e+00, -3.8936e-01,\n",
      "           4.6223e+00],\n",
      "         [ 1.9427e+01,  7.0381e-01, -4.6585e+00,  4.3923e+00, -3.8936e-01,\n",
      "           4.6223e+00]],\n",
      "\n",
      "        [[ 1.5267e+01, -1.0731e+00, -3.6349e+00,  6.9800e+00, -2.0869e+00,\n",
      "           2.1404e+00],\n",
      "         [ 1.5267e+01, -1.0731e+00, -3.6349e+00,  6.9800e+00, -2.0869e+00,\n",
      "           2.1404e+00]],\n",
      "\n",
      "        [[-2.1812e+02, -3.4165e+00,  3.4450e+01, -2.8642e+01, -3.9754e+00,\n",
      "          -3.0104e+01],\n",
      "         [-2.1812e+02, -3.4165e+00,  3.4450e+01, -2.8642e+01, -3.9754e+00,\n",
      "          -3.0104e+01]],\n",
      "\n",
      "        [[ 1.3561e+01,  3.1723e+00, -5.0664e+00, -3.1625e+00,  3.3733e+00,\n",
      "           5.6452e+00],\n",
      "         [ 1.3561e+01,  3.1723e+00, -5.0664e+00, -3.1625e+00,  3.3733e+00,\n",
      "           5.6452e+00]],\n",
      "\n",
      "        [[-1.7422e+02, -3.4164e+00,  4.9009e+01, -2.5807e+01, -3.9751e+00,\n",
      "          -2.7589e+01],\n",
      "         [-1.7422e+02, -3.4164e+00,  4.9009e+01, -2.5807e+01, -3.9751e+00,\n",
      "          -2.7589e+01]],\n",
      "\n",
      "        [[-3.2248e+02, -3.4165e+00, -2.1474e+00, -3.0671e+01, -3.9754e+00,\n",
      "          -3.2934e+01],\n",
      "         [-3.2248e+02, -3.4165e+00, -2.1474e+00, -3.0671e+01, -3.9754e+00,\n",
      "          -3.2934e+01]],\n",
      "\n",
      "        [[ 2.9882e+00,  3.7670e+00, -5.1181e+00, -8.9386e+00,  6.1811e+00,\n",
      "           4.6459e+00],\n",
      "         [ 2.9882e+00,  3.7670e+00, -5.1181e+00, -8.9386e+00,  6.1811e+00,\n",
      "           4.6459e+00]],\n",
      "\n",
      "        [[-1.7191e+02, -3.4164e+00,  4.9451e+01, -2.5607e+01, -3.9751e+00,\n",
      "          -2.7429e+01],\n",
      "         [-1.7191e+02, -3.4164e+00,  4.9451e+01, -2.5607e+01, -3.9751e+00,\n",
      "          -2.7429e+01]],\n",
      "\n",
      "        [[-2.4529e+02, -3.4165e+00,  2.2308e+01, -2.9639e+01, -3.9754e+00,\n",
      "          -3.1209e+01],\n",
      "         [-2.4529e+02, -3.4165e+00,  2.2308e+01, -2.9639e+01, -3.9754e+00,\n",
      "          -3.1209e+01]],\n",
      "\n",
      "        [[-3.0077e+02, -3.4165e+00,  2.0088e+00, -3.0553e+01, -3.9754e+00,\n",
      "          -3.2625e+01],\n",
      "         [-3.0077e+02, -3.4165e+00,  2.0088e+00, -3.0553e+01, -3.9754e+00,\n",
      "          -3.2625e+01]],\n",
      "\n",
      "        [[-2.3425e+02, -3.4165e+00,  2.7297e+01, -2.9291e+01, -3.9754e+00,\n",
      "          -3.0797e+01],\n",
      "         [-2.3425e+02, -3.4165e+00,  2.7297e+01, -2.9291e+01, -3.9754e+00,\n",
      "          -3.0797e+01]],\n",
      "\n",
      "        [[ 5.5668e+00,  3.7330e+00, -5.1121e+00, -7.7428e+00,  5.6042e+00,\n",
      "           4.9505e+00],\n",
      "         [ 5.5668e+00,  3.7330e+00, -5.1121e+00, -7.7428e+00,  5.6042e+00,\n",
      "           4.9505e+00]],\n",
      "\n",
      "        [[-2.2660e+02, -3.4165e+00,  3.0737e+01, -2.9005e+01, -3.9754e+00,\n",
      "          -3.0482e+01],\n",
      "         [-2.2660e+02, -3.4165e+00,  3.0737e+01, -2.9005e+01, -3.9754e+00,\n",
      "          -3.0482e+01]],\n",
      "\n",
      "        [[-1.5911e+02, -3.4163e+00,  5.1164e+01, -2.4387e+01, -3.9748e+00,\n",
      "          -2.6482e+01],\n",
      "         [-1.5911e+02, -3.4163e+00,  5.1164e+01, -2.4387e+01, -3.9748e+00,\n",
      "          -2.6482e+01]],\n",
      "\n",
      "        [[-2.5514e+02, -3.4165e+00,  1.7959e+01, -2.9893e+01, -3.9754e+00,\n",
      "          -3.1537e+01],\n",
      "         [-2.5514e+02, -3.4165e+00,  1.7959e+01, -2.9893e+01, -3.9754e+00,\n",
      "          -3.1537e+01]],\n",
      "\n",
      "        [[-6.3257e+01, -3.3859e+00,  2.6636e+01, -8.5890e+00, -3.9351e+00,\n",
      "          -1.5628e+01],\n",
      "         [-6.3257e+01, -3.3859e+00,  2.6636e+01, -8.5890e+00, -3.9351e+00,\n",
      "          -1.5628e+01]],\n",
      "\n",
      "        [[-1.3546e+01,  3.1992e+00, -5.1321e+00, -1.4905e+01,  8.9152e+00,\n",
      "           2.3141e+00],\n",
      "         [-1.3546e+01,  3.1992e+00, -5.1321e+00, -1.4905e+01,  8.9152e+00,\n",
      "           2.3141e+00]],\n",
      "\n",
      "        [[ 5.2398e+00,  3.7402e+00, -5.1130e+00, -7.9001e+00,  5.6803e+00,\n",
      "           4.9134e+00],\n",
      "         [ 5.2398e+00,  3.7402e+00, -5.1130e+00, -7.9001e+00,  5.6803e+00,\n",
      "           4.9134e+00]],\n",
      "\n",
      "        [[-1.3386e+02, -3.4158e+00,  5.0681e+01, -2.1405e+01, -3.9734e+00,\n",
      "          -2.4313e+01],\n",
      "         [-1.3386e+02, -3.4158e+00,  5.0681e+01, -2.1405e+01, -3.9734e+00,\n",
      "          -2.4313e+01]],\n",
      "\n",
      "        [[-2.6398e+02, -3.4165e+00,  1.4239e+01, -3.0081e+01, -3.9754e+00,\n",
      "          -3.1801e+01],\n",
      "         [-2.6398e+02, -3.4165e+00,  1.4239e+01, -3.0081e+01, -3.9754e+00,\n",
      "          -3.1801e+01]]])\n"
     ]
    }
   ],
   "source": [
    "print(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "<class 'numpy.float64'>\n",
      "[[0.33333333 2.        ]\n",
      " [3.         4.        ]\n",
      " [5.         6.        ]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1.0,2.0],[3.0,4.0],[5.0,6.0]])\n",
    "# for i in range(3):\n",
    "#     a[i,0] = a[i,0]/(sum(a[:,0])/3)\n",
    "# print(a)\n",
    "\n",
    "b = (sum(a[:,0])/3)\n",
    "print(b)\n",
    "print(type(b))\n",
    "a[0,0] = a[0,0]/b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Building Neural Network Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6])\n",
      "x1 tensor([-156.5109,   -3.4163,   51.3541,  -24.1163,   -3.9747,  -26.2782])\n",
      "x2 tensor([-156.5109,   -3.4163,   51.3541,  -24.1163,   -3.9747,  -26.2782])\n",
      "output\n",
      "tensor([-0.0168], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets, self).__init__()\n",
    "        num_hid_feat = int(number_of_features/2)\n",
    "        self.fc1 = nn.Linear(number_of_features,num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat,num_hid_feat)        \n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN, self).__init__()\n",
    "        self.network1 = Subnets(number_of_features)\n",
    "        self.network2 = Subnets(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN(number_of_features)\n",
    "print(np.shape(training_set[0]))\n",
    "x1, x2 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n",
    "# print(model)\n",
    "\n",
    "# print('Network1')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network1.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network1.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network1.fc2.bias)\n",
    "\n",
    "# print('Network2')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network2.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network2.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network2.fc2.bias)\n",
    "\n",
    "# print('Network3')\n",
    "\n",
    "# print('layer 1')\n",
    "# print('weights')\n",
    "# print(model.network3.fc1.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc1.bias)\n",
    "\n",
    "# print('layer 2')\n",
    "# print('weights')\n",
    "# print(model.network3.fc2.weight)\n",
    "# print('biases')\n",
    "# print(model.network3.fc2.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class simplenn(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(simplenn, self).__init__()\n",
    "#         self.fc1 = nn.Linear(2, 3)        # where fc stands for fully connected \n",
    "#         self.fc2 = nn.Linear(3, 1)        \n",
    "   \n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "#         x = self.fc2(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "#         return x\n",
    "\n",
    "# mod = simplenn()\n",
    "\n",
    "# print(mod.fc1.weight)\n",
    "# print(mod.fc1.bias)\n",
    "\n",
    "# print(mod.fc2.weight)\n",
    "# print(mod.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x1, x2, x3 = training_set[0]\n",
    "# x1 = x1[:2]\n",
    "# x2 = x2[:2]\n",
    "\n",
    "# x1[0] = -18650\n",
    "# x1[1] = 109075\n",
    "# print('x1',x1)\n",
    "\n",
    "# x2[0] = -6\n",
    "# x2[1] = 7\n",
    "# print('x2',x2)\n",
    "\n",
    "# output1 = mod(x1)\n",
    "# print('output1')\n",
    "# print(output1)\n",
    "\n",
    "# output2 = mod(x2)\n",
    "# print('output2')\n",
    "# print(output2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1 tensor([-156.5109,   -3.4163,   51.3541,  -24.1163,   -3.9747,  -26.2782])\n",
      "x2 tensor([-156.5109,   -3.4163,   51.3541,  -24.1163,   -3.9747,  -26.2782])\n",
      "output\n",
      "tensor([-0.0168], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = training_set[0]\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Manually compute neural Network output\n",
    "\n",
    "# w11 = model.network1.fc1.weight\n",
    "# b11 = model.network1.fc1.bias\n",
    "# print(np.shape(w11))\n",
    "# x1 = np.reshape(x1,(2,1))\n",
    "# x1 = np.array(x1)\n",
    "# print(np.shape(x1))\n",
    "\n",
    "# w11 = w11.cpu().detach().numpy()\n",
    "# b11 = b11.cpu().detach().numpy()\n",
    "# #b11 = np.transpose(b11)\n",
    "# b11 = np.reshape(b11,(3,1))\n",
    "# print(np.shape(b11))\n",
    "# print(type(x1))\n",
    "# print(type(w11))\n",
    "\n",
    "# a11 = np.matmul(w11,x1) + b11\n",
    "# a11 = np.tanh(a11)\n",
    "# print(a11)\n",
    "# #print(torch.tensordot(w11,x1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.1992, -0.3847, -0.2133,  0.3786,  0.1332, -0.3160],\n",
      "        [-0.2592, -0.2056,  0.1949,  0.2793,  0.2133, -0.3306],\n",
      "        [ 0.2274,  0.0470, -0.1217, -0.0236,  0.1102, -0.2871]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([ 0.3366, -0.1313, -0.0264], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.3474,  0.1690, -0.1109],\n",
      "        [-0.0808,  0.4236, -0.5736],\n",
      "        [ 0.0981, -0.2192, -0.4103]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.3453, -0.5098,  0.0853], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(model.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(model.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(model.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h2>Training the Neural Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = BPNN(number_of_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.0066977053881\n",
      "[2,     1] loss: 0.0005946470425\n",
      "[3,     1] loss: 0.0001828949200\n",
      "[4,     1] loss: 0.0000247089629\n",
      "[5,     1] loss: 0.0000325801084\n",
      "[6,     1] loss: 0.0000183394062\n",
      "[7,     1] loss: 0.0000177821057\n",
      "[8,     1] loss: 0.0000119532095\n",
      "[9,     1] loss: 0.0000220197893\n",
      "[10,     1] loss: 0.0000144976875\n",
      "[11,     1] loss: 0.0000097647302\n",
      "[12,     1] loss: 0.0000081356338\n",
      "[13,     1] loss: 0.0000097812117\n",
      "[14,     1] loss: 0.0000146747640\n",
      "[15,     1] loss: 0.0000054857592\n",
      "[16,     1] loss: 0.0000073912481\n",
      "[17,     1] loss: 0.0000103931241\n",
      "[18,     1] loss: 0.0000134747126\n",
      "[19,     1] loss: 0.0000103955557\n",
      "[20,     1] loss: 0.0000101795165\n",
      "[21,     1] loss: 0.0000103867169\n",
      "[22,     1] loss: 0.0000087950975\n",
      "[23,     1] loss: 0.0000157304035\n",
      "[24,     1] loss: 0.0000065159045\n",
      "[25,     1] loss: 0.0000058684003\n",
      "[26,     1] loss: 0.0000045582539\n",
      "[27,     1] loss: 0.0000098786673\n",
      "[28,     1] loss: 0.0000082469182\n",
      "[29,     1] loss: 0.0000110779678\n",
      "[30,     1] loss: 0.0000071201677\n",
      "[31,     1] loss: 0.0000074704229\n",
      "[32,     1] loss: 0.0000053066222\n",
      "[33,     1] loss: 0.0000129972672\n",
      "[34,     1] loss: 0.0000092582653\n",
      "[35,     1] loss: 0.0000147700121\n",
      "[36,     1] loss: 0.0000126036190\n",
      "[37,     1] loss: 0.0000088341862\n",
      "[38,     1] loss: 0.0000144063539\n",
      "[39,     1] loss: 0.0000049588569\n",
      "[40,     1] loss: 0.0000070476264\n",
      "[41,     1] loss: 0.0000109119865\n",
      "[42,     1] loss: 0.0000039667560\n",
      "[43,     1] loss: 0.0000089457513\n",
      "[44,     1] loss: 0.0000033934946\n",
      "[45,     1] loss: 0.0000035847643\n",
      "[46,     1] loss: 0.0000045428824\n",
      "[47,     1] loss: 0.0000094082243\n",
      "[48,     1] loss: 0.0000047873578\n",
      "[49,     1] loss: 0.0000058426689\n",
      "[50,     1] loss: 0.0000063982989\n",
      "[51,     1] loss: 0.0000045415989\n",
      "[52,     1] loss: 0.0000057042249\n",
      "[53,     1] loss: 0.0000044862358\n",
      "[54,     1] loss: 0.0000046759327\n",
      "[55,     1] loss: 0.0000053044361\n",
      "[56,     1] loss: 0.0000056309254\n",
      "[57,     1] loss: 0.0000056798701\n",
      "[58,     1] loss: 0.0000042890093\n",
      "[59,     1] loss: 0.0000044694534\n",
      "[60,     1] loss: 0.0000056973171\n",
      "[61,     1] loss: 0.0000033824716\n",
      "[62,     1] loss: 0.0000055077275\n",
      "[63,     1] loss: 0.0000038647653\n",
      "[64,     1] loss: 0.0000034848894\n",
      "[65,     1] loss: 0.0000036002170\n",
      "[66,     1] loss: 0.0000040389532\n",
      "[67,     1] loss: 0.0000033212771\n",
      "[68,     1] loss: 0.0000032465337\n",
      "[69,     1] loss: 0.0000033206175\n",
      "[70,     1] loss: 0.0000028056915\n",
      "[71,     1] loss: 0.0000032791053\n",
      "[72,     1] loss: 0.0000048863702\n",
      "[73,     1] loss: 0.0000054759585\n",
      "[74,     1] loss: 0.0000059710736\n",
      "[75,     1] loss: 0.0000043285218\n",
      "[76,     1] loss: 0.0000036796089\n",
      "[77,     1] loss: 0.0000035835270\n",
      "[78,     1] loss: 0.0000028742304\n",
      "[79,     1] loss: 0.0000040169936\n",
      "[80,     1] loss: 0.0000052798689\n",
      "[81,     1] loss: 0.0000043259846\n",
      "[82,     1] loss: 0.0000035019712\n",
      "[83,     1] loss: 0.0000042452848\n",
      "[84,     1] loss: 0.0000027498665\n",
      "[85,     1] loss: 0.0000040851544\n",
      "[86,     1] loss: 0.0000042798380\n",
      "[87,     1] loss: 0.0000034316490\n",
      "[88,     1] loss: 0.0000026472662\n",
      "[89,     1] loss: 0.0000048489441\n",
      "[90,     1] loss: 0.0000040920640\n",
      "[91,     1] loss: 0.0000042852280\n",
      "[92,     1] loss: 0.0000042523803\n",
      "[93,     1] loss: 0.0000030257221\n",
      "[94,     1] loss: 0.0000040249361\n",
      "[95,     1] loss: 0.0000047127178\n",
      "[96,     1] loss: 0.0000033801025\n",
      "[97,     1] loss: 0.0000035151850\n",
      "[98,     1] loss: 0.0000032526637\n",
      "[99,     1] loss: 0.0000037361136\n",
      "[100,     1] loss: 0.0000025302515\n",
      "[101,     1] loss: 0.0000047571793\n",
      "[102,     1] loss: 0.0000039885414\n",
      "[103,     1] loss: 0.0000063177809\n",
      "[104,     1] loss: 0.0000040422176\n",
      "[105,     1] loss: 0.0000038316517\n",
      "[106,     1] loss: 0.0000033008848\n",
      "[107,     1] loss: 0.0000031357002\n",
      "[108,     1] loss: 0.0000042641714\n",
      "[109,     1] loss: 0.0000033662011\n",
      "[110,     1] loss: 0.0000044820961\n",
      "[111,     1] loss: 0.0000031562686\n",
      "[112,     1] loss: 0.0000033026769\n",
      "[113,     1] loss: 0.0000026114669\n",
      "[114,     1] loss: 0.0000035899062\n",
      "[115,     1] loss: 0.0000034459030\n",
      "[116,     1] loss: 0.0000042905154\n",
      "[117,     1] loss: 0.0000048434911\n",
      "[118,     1] loss: 0.0000042450432\n",
      "[119,     1] loss: 0.0000029273780\n",
      "[120,     1] loss: 0.0000039530882\n",
      "[121,     1] loss: 0.0000035954890\n",
      "[122,     1] loss: 0.0000030430763\n",
      "[123,     1] loss: 0.0000026820117\n",
      "[124,     1] loss: 0.0000050750165\n",
      "[125,     1] loss: 0.0000027305163\n",
      "[126,     1] loss: 0.0000040160576\n",
      "[127,     1] loss: 0.0000027389058\n",
      "[128,     1] loss: 0.0000047673733\n",
      "[129,     1] loss: 0.0000042457516\n",
      "[130,     1] loss: 0.0000042616226\n",
      "[131,     1] loss: 0.0000029266930\n",
      "[132,     1] loss: 0.0000057807720\n",
      "[133,     1] loss: 0.0000042544081\n",
      "[134,     1] loss: 0.0000038393609\n",
      "[135,     1] loss: 0.0000026788557\n",
      "[136,     1] loss: 0.0000046510435\n",
      "[137,     1] loss: 0.0000028174827\n",
      "[138,     1] loss: 0.0000033559070\n",
      "[139,     1] loss: 0.0000041888426\n",
      "[140,     1] loss: 0.0000025513080\n",
      "[141,     1] loss: 0.0000039133105\n",
      "[142,     1] loss: 0.0000026925567\n",
      "[143,     1] loss: 0.0000028843333\n",
      "[144,     1] loss: 0.0000033731245\n",
      "[145,     1] loss: 0.0000040064901\n",
      "[146,     1] loss: 0.0000033577442\n",
      "[147,     1] loss: 0.0000032780026\n",
      "[148,     1] loss: 0.0000037257581\n",
      "[149,     1] loss: 0.0000039293012\n",
      "[150,     1] loss: 0.0000068282970\n",
      "[151,     1] loss: 0.0000041856281\n",
      "[152,     1] loss: 0.0000032746299\n",
      "[153,     1] loss: 0.0000035447985\n",
      "[154,     1] loss: 0.0000031450927\n",
      "[155,     1] loss: 0.0000038699676\n",
      "[156,     1] loss: 0.0000048462894\n",
      "[157,     1] loss: 0.0000039659724\n",
      "[158,     1] loss: 0.0000038273967\n",
      "[159,     1] loss: 0.0000036311034\n",
      "[160,     1] loss: 0.0000037353642\n",
      "[161,     1] loss: 0.0000044488599\n",
      "[162,     1] loss: 0.0000032702184\n",
      "[163,     1] loss: 0.0000037471251\n",
      "[164,     1] loss: 0.0000035615180\n",
      "[165,     1] loss: 0.0000084389074\n",
      "[166,     1] loss: 0.0000032167351\n",
      "[167,     1] loss: 0.0000041540745\n",
      "[168,     1] loss: 0.0000039635721\n",
      "[169,     1] loss: 0.0000030003632\n",
      "[170,     1] loss: 0.0000038612965\n",
      "[171,     1] loss: 0.0000041018127\n",
      "[172,     1] loss: 0.0000042531014\n",
      "[173,     1] loss: 0.0000042211144\n",
      "[174,     1] loss: 0.0000024208339\n",
      "[175,     1] loss: 0.0000046739671\n",
      "[176,     1] loss: 0.0000045175431\n",
      "[177,     1] loss: 0.0000043627992\n",
      "[178,     1] loss: 0.0000039400849\n",
      "[179,     1] loss: 0.0000032242475\n",
      "[180,     1] loss: 0.0000033343982\n",
      "[181,     1] loss: 0.0000026176202\n",
      "[182,     1] loss: 0.0000048392347\n",
      "[183,     1] loss: 0.0000026057160\n",
      "[184,     1] loss: 0.0000040206753\n",
      "[185,     1] loss: 0.0000051611303\n",
      "[186,     1] loss: 0.0000041227882\n",
      "[187,     1] loss: 0.0000029109440\n",
      "[188,     1] loss: 0.0000040557763\n",
      "[189,     1] loss: 0.0000028722739\n",
      "[190,     1] loss: 0.0000022426741\n",
      "[191,     1] loss: 0.0000041202977\n",
      "[192,     1] loss: 0.0000051982879\n",
      "[193,     1] loss: 0.0000045731991\n",
      "[194,     1] loss: 0.0000043935644\n",
      "[195,     1] loss: 0.0000040985942\n",
      "[196,     1] loss: 0.0000026157259\n",
      "[197,     1] loss: 0.0000041627918\n",
      "[198,     1] loss: 0.0000034515211\n",
      "[199,     1] loss: 0.0000053497624\n",
      "[200,     1] loss: 0.0000042276537\n",
      "[201,     1] loss: 0.0000037557889\n",
      "[202,     1] loss: 0.0000032320120\n",
      "[203,     1] loss: 0.0000029054323\n",
      "[204,     1] loss: 0.0000029364033\n",
      "[205,     1] loss: 0.0000034947923\n",
      "[206,     1] loss: 0.0000042317930\n",
      "[207,     1] loss: 0.0000029540744\n",
      "[208,     1] loss: 0.0000029399964\n",
      "[209,     1] loss: 0.0000027647538\n",
      "[210,     1] loss: 0.0000038717179\n",
      "[211,     1] loss: 0.0000047161262\n",
      "[212,     1] loss: 0.0000032964374\n",
      "[213,     1] loss: 0.0000047116620\n",
      "[214,     1] loss: 0.0000057257592\n",
      "[215,     1] loss: 0.0000040871368\n",
      "[216,     1] loss: 0.0000041769556\n",
      "[217,     1] loss: 0.0000037812002\n",
      "[218,     1] loss: 0.0000046367357\n",
      "[219,     1] loss: 0.0000028899936\n",
      "[220,     1] loss: 0.0000026963104\n",
      "[221,     1] loss: 0.0000044568260\n",
      "[222,     1] loss: 0.0000050209430\n",
      "[223,     1] loss: 0.0000042209191\n",
      "[224,     1] loss: 0.0000024294739\n",
      "[225,     1] loss: 0.0000032642103\n",
      "[226,     1] loss: 0.0000029002887\n",
      "[227,     1] loss: 0.0000048543970\n",
      "[228,     1] loss: 0.0000039881248\n",
      "[229,     1] loss: 0.0000042396765\n",
      "[230,     1] loss: 0.0000032614680\n",
      "[231,     1] loss: 0.0000034425288\n",
      "[232,     1] loss: 0.0000027848619\n",
      "[233,     1] loss: 0.0000036930349\n",
      "[234,     1] loss: 0.0000020775173\n",
      "[235,     1] loss: 0.0000013410223\n",
      "[236,     1] loss: 0.0000040396571\n",
      "[237,     1] loss: 0.0000058859569\n",
      "[238,     1] loss: 0.0000044953245\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239,     1] loss: 0.0000055675591\n",
      "[240,     1] loss: 0.0000024243580\n",
      "[241,     1] loss: 0.0000045627505\n",
      "[242,     1] loss: 0.0000033444125\n",
      "[243,     1] loss: 0.0000033838613\n",
      "[244,     1] loss: 0.0000038810245\n",
      "[245,     1] loss: 0.0000056973844\n",
      "[246,     1] loss: 0.0000036504563\n",
      "[247,     1] loss: 0.0000033907716\n",
      "[248,     1] loss: 0.0000095473348\n",
      "[249,     1] loss: 0.0000034874913\n",
      "[250,     1] loss: 0.0000035022473\n",
      "[251,     1] loss: 0.0000067954992\n",
      "[252,     1] loss: 0.0000047290261\n",
      "[253,     1] loss: 0.0000048054604\n",
      "[254,     1] loss: 0.0000055809407\n",
      "[255,     1] loss: 0.0000048516315\n",
      "[256,     1] loss: 0.0000053328044\n",
      "[257,     1] loss: 0.0000066390443\n",
      "[258,     1] loss: 0.0000047237281\n",
      "[259,     1] loss: 0.0000036476642\n",
      "[260,     1] loss: 0.0000080616286\n",
      "[261,     1] loss: 0.0000057574838\n",
      "[262,     1] loss: 0.0000027288101\n",
      "[263,     1] loss: 0.0000040875504\n",
      "[264,     1] loss: 0.0000037196674\n",
      "[265,     1] loss: 0.0000033737564\n",
      "[266,     1] loss: 0.0000035471156\n",
      "[267,     1] loss: 0.0000042971122\n",
      "[268,     1] loss: 0.0000031841559\n",
      "[269,     1] loss: 0.0000032164608\n",
      "[270,     1] loss: 0.0000032823078\n",
      "[271,     1] loss: 0.0000030523355\n",
      "[272,     1] loss: 0.0000023389686\n",
      "[273,     1] loss: 0.0000041193965\n",
      "[274,     1] loss: 0.0000035080782\n",
      "[275,     1] loss: 0.0000031210569\n",
      "[276,     1] loss: 0.0000042349064\n",
      "[277,     1] loss: 0.0000021171350\n",
      "[278,     1] loss: 0.0000039926763\n",
      "[279,     1] loss: 0.0000032976906\n",
      "[280,     1] loss: 0.0000035577941\n",
      "[281,     1] loss: 0.0000027695187\n",
      "[282,     1] loss: 0.0000035977486\n",
      "[283,     1] loss: 0.0000021501917\n",
      "[284,     1] loss: 0.0000030879710\n",
      "[285,     1] loss: 0.0000041384046\n",
      "[286,     1] loss: 0.0000051461324\n",
      "[287,     1] loss: 0.0000052856263\n",
      "[288,     1] loss: 0.0000088213274\n",
      "[289,     1] loss: 0.0000046990783\n",
      "[290,     1] loss: 0.0000033341286\n",
      "[291,     1] loss: 0.0000031653861\n",
      "[292,     1] loss: 0.0000030710238\n",
      "[293,     1] loss: 0.0000034767076\n",
      "[294,     1] loss: 0.0000032444575\n",
      "[295,     1] loss: 0.0000029129173\n",
      "[296,     1] loss: 0.0000030880590\n",
      "[297,     1] loss: 0.0000032258326\n",
      "[298,     1] loss: 0.0000049421025\n",
      "[299,     1] loss: 0.0000031700252\n",
      "[300,     1] loss: 0.0000033927015\n",
      "[301,     1] loss: 0.0000030824274\n",
      "[302,     1] loss: 0.0000041433992\n",
      "[303,     1] loss: 0.0000041706309\n",
      "[304,     1] loss: 0.0000044662407\n",
      "[305,     1] loss: 0.0000036744314\n",
      "[306,     1] loss: 0.0000055194861\n",
      "[307,     1] loss: 0.0000029915511\n",
      "[308,     1] loss: 0.0000042272713\n",
      "[309,     1] loss: 0.0000050325190\n",
      "[310,     1] loss: 0.0000040975287\n",
      "[311,     1] loss: 0.0000029955707\n",
      "[312,     1] loss: 0.0000034107448\n",
      "[313,     1] loss: 0.0000045866920\n",
      "[314,     1] loss: 0.0000035264409\n",
      "[315,     1] loss: 0.0000025452786\n",
      "[316,     1] loss: 0.0000046696481\n",
      "[317,     1] loss: 0.0000036649271\n",
      "[318,     1] loss: 0.0000025710015\n",
      "[319,     1] loss: 0.0000037382641\n",
      "[320,     1] loss: 0.0000027611177\n",
      "[321,     1] loss: 0.0000036791127\n",
      "[322,     1] loss: 0.0000030974308\n",
      "[323,     1] loss: 0.0000037588583\n",
      "[324,     1] loss: 0.0000056267727\n",
      "[325,     1] loss: 0.0000031502455\n",
      "[326,     1] loss: 0.0000027319278\n",
      "[327,     1] loss: 0.0000032241052\n",
      "[328,     1] loss: 0.0000025293908\n",
      "[329,     1] loss: 0.0000027893258\n",
      "[330,     1] loss: 0.0000033752010\n",
      "[331,     1] loss: 0.0000039100050\n",
      "[332,     1] loss: 0.0000037969850\n",
      "[333,     1] loss: 0.0000037393871\n",
      "[334,     1] loss: 0.0000035131197\n",
      "[335,     1] loss: 0.0000046332269\n",
      "[336,     1] loss: 0.0000028932023\n",
      "[337,     1] loss: 0.0000023889328\n",
      "[338,     1] loss: 0.0000025587780\n",
      "[339,     1] loss: 0.0000037762456\n",
      "[340,     1] loss: 0.0000032955610\n",
      "[341,     1] loss: 0.0000050673007\n",
      "[342,     1] loss: 0.0000048024147\n",
      "[343,     1] loss: 0.0000035954843\n",
      "[344,     1] loss: 0.0000042637799\n",
      "[345,     1] loss: 0.0000025421987\n",
      "[346,     1] loss: 0.0000016396905\n",
      "[347,     1] loss: 0.0000025610947\n",
      "[348,     1] loss: 0.0000031506705\n",
      "[349,     1] loss: 0.0000042391952\n",
      "[350,     1] loss: 0.0000021461516\n",
      "[351,     1] loss: 0.0000032085394\n",
      "[352,     1] loss: 0.0000042276810\n",
      "[353,     1] loss: 0.0000039767619\n",
      "[354,     1] loss: 0.0000027436319\n",
      "[355,     1] loss: 0.0000027848622\n",
      "[356,     1] loss: 0.0000039805669\n",
      "[357,     1] loss: 0.0000023450451\n",
      "[358,     1] loss: 0.0000016922144\n",
      "[359,     1] loss: 0.0000022550921\n",
      "[360,     1] loss: 0.0000042186672\n",
      "[361,     1] loss: 0.0000043701300\n",
      "[362,     1] loss: 0.0000031985794\n",
      "[363,     1] loss: 0.0000030468307\n",
      "[364,     1] loss: 0.0000038237606\n",
      "[365,     1] loss: 0.0000031981988\n",
      "[366,     1] loss: 0.0000031954041\n",
      "[367,     1] loss: 0.0000021170374\n",
      "[368,     1] loss: 0.0000022690494\n",
      "[369,     1] loss: 0.0000027600845\n",
      "[370,     1] loss: 0.0000029254723\n",
      "[371,     1] loss: 0.0000025862744\n",
      "[372,     1] loss: 0.0000027113871\n",
      "[373,     1] loss: 0.0000022802469\n",
      "[374,     1] loss: 0.0000030222600\n",
      "[375,     1] loss: 0.0000014177855\n",
      "[376,     1] loss: 0.0000038361519\n",
      "[377,     1] loss: 0.0000022859671\n",
      "[378,     1] loss: 0.0000018773308\n",
      "[379,     1] loss: 0.0000017544990\n",
      "[380,     1] loss: 0.0000025497075\n",
      "[381,     1] loss: 0.0000016073016\n",
      "[382,     1] loss: 0.0000006352547\n",
      "[383,     1] loss: 0.0000013720436\n",
      "[384,     1] loss: 0.0000019169313\n",
      "[385,     1] loss: 0.0000022112154\n",
      "[386,     1] loss: 0.0000026982891\n",
      "[387,     1] loss: 0.0000006881380\n",
      "[388,     1] loss: 0.0000012322611\n",
      "[389,     1] loss: 0.0000005658971\n",
      "[390,     1] loss: 0.0000009002950\n",
      "[391,     1] loss: 0.0000005661025\n",
      "[392,     1] loss: 0.0000005049358\n",
      "[393,     1] loss: 0.0000006091669\n",
      "[394,     1] loss: 0.0000004457042\n",
      "[395,     1] loss: 0.0000004277801\n",
      "[396,     1] loss: 0.0000004024811\n",
      "[397,     1] loss: 0.0000004056284\n",
      "[398,     1] loss: 0.0000004625680\n",
      "[399,     1] loss: 0.0000002622457\n",
      "[400,     1] loss: 0.0000002109708\n",
      "[401,     1] loss: 0.0000002466962\n",
      "[402,     1] loss: 0.0000003080062\n",
      "[403,     1] loss: 0.0000005021685\n",
      "[404,     1] loss: 0.0000009414393\n",
      "[405,     1] loss: 0.0000003877049\n",
      "[406,     1] loss: 0.0000013024612\n",
      "[407,     1] loss: 0.0000008152141\n",
      "[408,     1] loss: 0.0000005419011\n",
      "[409,     1] loss: 0.0000003532748\n",
      "[410,     1] loss: 0.0000002414595\n",
      "[411,     1] loss: 0.0000007114641\n",
      "[412,     1] loss: 0.0000006928876\n",
      "[413,     1] loss: 0.0000011105748\n",
      "[414,     1] loss: 0.0000006389477\n",
      "[415,     1] loss: 0.0000006031332\n",
      "[416,     1] loss: 0.0000004505970\n",
      "[417,     1] loss: 0.0000008614556\n",
      "[418,     1] loss: 0.0000003604569\n",
      "[419,     1] loss: 0.0000003431948\n",
      "[420,     1] loss: 0.0000011195913\n",
      "[421,     1] loss: 0.0000007748572\n",
      "[422,     1] loss: 0.0000006050236\n",
      "[423,     1] loss: 0.0000008670939\n",
      "[424,     1] loss: 0.0000002664417\n",
      "[425,     1] loss: 0.0000003606424\n",
      "[426,     1] loss: 0.0000002936639\n",
      "[427,     1] loss: 0.0000002017555\n",
      "[428,     1] loss: 0.0000002590616\n",
      "[429,     1] loss: 0.0000004299091\n",
      "[430,     1] loss: 0.0000002777730\n",
      "[431,     1] loss: 0.0000004497151\n",
      "[432,     1] loss: 0.0000007550186\n",
      "[433,     1] loss: 0.0000003331711\n",
      "[434,     1] loss: 0.0000005596116\n",
      "[435,     1] loss: 0.0000007279014\n",
      "[436,     1] loss: 0.0000002151668\n",
      "[437,     1] loss: 0.0000005108674\n",
      "[438,     1] loss: 0.0000004913205\n",
      "[439,     1] loss: 0.0000002527523\n",
      "[440,     1] loss: 0.0000004706249\n",
      "[441,     1] loss: 0.0000007119134\n",
      "[442,     1] loss: 0.0000004746764\n",
      "[443,     1] loss: 0.0000004837801\n",
      "[444,     1] loss: 0.0000003201049\n",
      "[445,     1] loss: 0.0000005653679\n",
      "[446,     1] loss: 0.0000008679338\n",
      "[447,     1] loss: 0.0000004087851\n",
      "[448,     1] loss: 0.0000006998637\n",
      "[449,     1] loss: 0.0000005855263\n",
      "[450,     1] loss: 0.0000001644811\n",
      "[451,     1] loss: 0.0000003120549\n",
      "[452,     1] loss: 0.0000003221154\n",
      "[453,     1] loss: 0.0000004728757\n",
      "[454,     1] loss: 0.0000002643137\n",
      "[455,     1] loss: 0.0000024284891\n",
      "[456,     1] loss: 0.0000008612223\n",
      "[457,     1] loss: 0.0000006239816\n",
      "[458,     1] loss: 0.0000005250281\n",
      "[459,     1] loss: 0.0000008762359\n",
      "[460,     1] loss: 0.0000004118273\n",
      "[461,     1] loss: 0.0000003254150\n",
      "[462,     1] loss: 0.0000002009174\n",
      "[463,     1] loss: 0.0000003283742\n",
      "[464,     1] loss: 0.0000003543499\n",
      "[465,     1] loss: 0.0000002831348\n",
      "[466,     1] loss: 0.0000005927567\n",
      "[467,     1] loss: 0.0000005087188\n",
      "[468,     1] loss: 0.0000002876448\n",
      "[469,     1] loss: 0.0000003112106\n",
      "[470,     1] loss: 0.0000005800572\n",
      "[471,     1] loss: 0.0000003643033\n",
      "[472,     1] loss: 0.0000001804437\n",
      "[473,     1] loss: 0.0000001715824\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[474,     1] loss: 0.0000002031124\n",
      "[475,     1] loss: 0.0000002566631\n",
      "[476,     1] loss: 0.0000001513831\n",
      "[477,     1] loss: 0.0000001423545\n",
      "[478,     1] loss: 0.0000001818757\n",
      "[479,     1] loss: 0.0000002447721\n",
      "[480,     1] loss: 0.0000005229674\n",
      "[481,     1] loss: 0.0000003958229\n",
      "[482,     1] loss: 0.0000005055914\n",
      "[483,     1] loss: 0.0000006697438\n",
      "[484,     1] loss: 0.0000002861411\n",
      "[485,     1] loss: 0.0000003253393\n",
      "[486,     1] loss: 0.0000006163708\n",
      "[487,     1] loss: 0.0000003681060\n",
      "[488,     1] loss: 0.0000003934222\n",
      "[489,     1] loss: 0.0000003202514\n",
      "[490,     1] loss: 0.0000005376339\n",
      "[491,     1] loss: 0.0000006372199\n",
      "[492,     1] loss: 0.0000003622705\n",
      "[493,     1] loss: 0.0000003359422\n",
      "[494,     1] loss: 0.0000001369376\n",
      "[495,     1] loss: 0.0000000886068\n",
      "[496,     1] loss: 0.0000001358992\n",
      "[497,     1] loss: 0.0000001592349\n",
      "[498,     1] loss: 0.0000001749192\n",
      "[499,     1] loss: 0.0000002442130\n",
      "[500,     1] loss: 0.0000001551968\n",
      "[501,     1] loss: 0.0000001794802\n",
      "[502,     1] loss: 0.0000000952327\n",
      "[503,     1] loss: 0.0000000575210\n",
      "[504,     1] loss: 0.0000001405674\n",
      "[505,     1] loss: 0.0000000819068\n",
      "[506,     1] loss: 0.0000000902994\n",
      "[507,     1] loss: 0.0000002010860\n",
      "[508,     1] loss: 0.0000001902529\n",
      "[509,     1] loss: 0.0000000838264\n",
      "[510,     1] loss: 0.0000001229031\n",
      "[511,     1] loss: 0.0000001959172\n",
      "[512,     1] loss: 0.0000000683758\n",
      "[513,     1] loss: 0.0000000849567\n",
      "[514,     1] loss: 0.0000000854984\n",
      "[515,     1] loss: 0.0000000799693\n",
      "[516,     1] loss: 0.0000001451798\n",
      "[517,     1] loss: 0.0000002190986\n",
      "[518,     1] loss: 0.0000002922805\n",
      "[519,     1] loss: 0.0000002549361\n",
      "[520,     1] loss: 0.0000003009450\n",
      "[521,     1] loss: 0.0000000893844\n",
      "[522,     1] loss: 0.0000003172533\n",
      "[523,     1] loss: 0.0000001257672\n",
      "[524,     1] loss: 0.0000001097600\n",
      "[525,     1] loss: 0.0000001679094\n",
      "[526,     1] loss: 0.0000002174115\n",
      "[527,     1] loss: 0.0000001004129\n",
      "[528,     1] loss: 0.0000002053015\n",
      "[529,     1] loss: 0.0000005731916\n",
      "[530,     1] loss: 0.0000002538252\n",
      "[531,     1] loss: 0.0000004976088\n",
      "[532,     1] loss: 0.0000001268364\n",
      "[533,     1] loss: 0.0000000580829\n",
      "[534,     1] loss: 0.0000001443866\n",
      "[535,     1] loss: 0.0000001126913\n",
      "[536,     1] loss: 0.0000000723577\n",
      "[537,     1] loss: 0.0000001459893\n",
      "[538,     1] loss: 0.0000001403198\n",
      "[539,     1] loss: 0.0000001351777\n",
      "[540,     1] loss: 0.0000000635605\n",
      "[541,     1] loss: 0.0000000881187\n",
      "[542,     1] loss: 0.0000002922398\n",
      "[543,     1] loss: 0.0000001131705\n",
      "[544,     1] loss: 0.0000003401285\n",
      "[545,     1] loss: 0.0000000739019\n",
      "[546,     1] loss: 0.0000000490497\n",
      "[547,     1] loss: 0.0000000961640\n",
      "[548,     1] loss: 0.0000000544572\n",
      "[549,     1] loss: 0.0000000752648\n",
      "[550,     1] loss: 0.0000000833380\n",
      "[551,     1] loss: 0.0000000482775\n",
      "[552,     1] loss: 0.0000001235624\n",
      "[553,     1] loss: 0.0000001319026\n",
      "[554,     1] loss: 0.0000000492617\n",
      "[555,     1] loss: 0.0000000356744\n",
      "[556,     1] loss: 0.0000000538422\n",
      "[557,     1] loss: 0.0000000837182\n",
      "[558,     1] loss: 0.0000002520200\n",
      "[559,     1] loss: 0.0000003490012\n",
      "[560,     1] loss: 0.0000002507060\n",
      "[561,     1] loss: 0.0000002535815\n",
      "[562,     1] loss: 0.0000002317648\n",
      "[563,     1] loss: 0.0000000735322\n",
      "[564,     1] loss: 0.0000000602693\n",
      "[565,     1] loss: 0.0000001968062\n",
      "[566,     1] loss: 0.0000001238266\n",
      "[567,     1] loss: 0.0000000472317\n",
      "[568,     1] loss: 0.0000001075553\n",
      "[569,     1] loss: 0.0000001275710\n",
      "[570,     1] loss: 0.0000000934180\n",
      "[571,     1] loss: 0.0000002591442\n",
      "[572,     1] loss: 0.0000000879831\n",
      "[573,     1] loss: 0.0000001586785\n",
      "[574,     1] loss: 0.0000000649403\n",
      "[575,     1] loss: 0.0000002755658\n",
      "[576,     1] loss: 0.0000002093475\n",
      "[577,     1] loss: 0.0000001862361\n",
      "[578,     1] loss: 0.0000000732068\n",
      "[579,     1] loss: 0.0000001457763\n",
      "[580,     1] loss: 0.0000003015919\n",
      "[581,     1] loss: 0.0000003250979\n",
      "[582,     1] loss: 0.0000003890029\n",
      "[583,     1] loss: 0.0000004808908\n",
      "[584,     1] loss: 0.0000000886970\n",
      "[585,     1] loss: 0.0000001764905\n",
      "[586,     1] loss: 0.0000001739163\n",
      "[587,     1] loss: 0.0000001924534\n",
      "[588,     1] loss: 0.0000001231796\n",
      "[589,     1] loss: 0.0000000404057\n",
      "[590,     1] loss: 0.0000000771688\n",
      "[591,     1] loss: 0.0000001533432\n",
      "[592,     1] loss: 0.0000001534260\n",
      "[593,     1] loss: 0.0000001052108\n",
      "[594,     1] loss: 0.0000001446633\n",
      "[595,     1] loss: 0.0000000771608\n",
      "[596,     1] loss: 0.0000001474898\n",
      "[597,     1] loss: 0.0000001550492\n",
      "[598,     1] loss: 0.0000000975344\n",
      "[599,     1] loss: 0.0000000905839\n",
      "[600,     1] loss: 0.0000000932043\n",
      "[601,     1] loss: 0.0000001028864\n",
      "[602,     1] loss: 0.0000001297450\n",
      "[603,     1] loss: 0.0000004836581\n",
      "[604,     1] loss: 0.0000001838520\n",
      "[605,     1] loss: 0.0000006191352\n",
      "[606,     1] loss: 0.0000003460990\n",
      "[607,     1] loss: 0.0000001571364\n",
      "[608,     1] loss: 0.0000000508863\n",
      "[609,     1] loss: 0.0000000782375\n",
      "[610,     1] loss: 0.0000002067944\n",
      "[611,     1] loss: 0.0000000638555\n",
      "[612,     1] loss: 0.0000001188391\n",
      "[613,     1] loss: 0.0000001478432\n",
      "[614,     1] loss: 0.0000002050461\n",
      "[615,     1] loss: 0.0000000563533\n",
      "[616,     1] loss: 0.0000000776228\n",
      "[617,     1] loss: 0.0000001278983\n",
      "[618,     1] loss: 0.0000001181636\n",
      "[619,     1] loss: 0.0000001068462\n",
      "[620,     1] loss: 0.0000002161212\n",
      "[621,     1] loss: 0.0000000633260\n",
      "[622,     1] loss: 0.0000001007533\n",
      "[623,     1] loss: 0.0000001007966\n",
      "[624,     1] loss: 0.0000001795918\n",
      "[625,     1] loss: 0.0000002706892\n",
      "[626,     1] loss: 0.0000000871509\n",
      "[627,     1] loss: 0.0000003475109\n",
      "[628,     1] loss: 0.0000001352859\n",
      "[629,     1] loss: 0.0000000956959\n",
      "[630,     1] loss: 0.0000002093924\n",
      "[631,     1] loss: 0.0000002911626\n",
      "[632,     1] loss: 0.0000001886953\n",
      "[633,     1] loss: 0.0000002201724\n",
      "[634,     1] loss: 0.0000002575828\n",
      "[635,     1] loss: 0.0000003699694\n",
      "[636,     1] loss: 0.0000006464117\n",
      "[637,     1] loss: 0.0000002140599\n",
      "[638,     1] loss: 0.0000003907002\n",
      "[639,     1] loss: 0.0000003732032\n",
      "[640,     1] loss: 0.0000002051100\n",
      "[641,     1] loss: 0.0000001129959\n",
      "[642,     1] loss: 0.0000003120787\n",
      "[643,     1] loss: 0.0000002140594\n",
      "[644,     1] loss: 0.0000001377306\n",
      "[645,     1] loss: 0.0000004022575\n",
      "[646,     1] loss: 0.0000001034841\n",
      "[647,     1] loss: 0.0000001438539\n",
      "[648,     1] loss: 0.0000001730445\n",
      "[649,     1] loss: 0.0000002238029\n",
      "[650,     1] loss: 0.0000000697002\n",
      "[651,     1] loss: 0.0000002629882\n",
      "[652,     1] loss: 0.0000001742612\n",
      "[653,     1] loss: 0.0000002542361\n",
      "[654,     1] loss: 0.0000000725454\n",
      "[655,     1] loss: 0.0000001646520\n",
      "[656,     1] loss: 0.0000001114572\n",
      "[657,     1] loss: 0.0000003981319\n",
      "[658,     1] loss: 0.0000000717445\n",
      "[659,     1] loss: 0.0000001509071\n",
      "[660,     1] loss: 0.0000001594356\n",
      "[661,     1] loss: 0.0000004539588\n",
      "[662,     1] loss: 0.0000001748484\n",
      "[663,     1] loss: 0.0000014819740\n",
      "[664,     1] loss: 0.0000025726024\n",
      "[665,     1] loss: 0.0000017666849\n",
      "[666,     1] loss: 0.0000006607896\n",
      "[667,     1] loss: 0.0000006265476\n",
      "[668,     1] loss: 0.0000005819540\n",
      "[669,     1] loss: 0.0000008791873\n",
      "[670,     1] loss: 0.0000005709534\n",
      "[671,     1] loss: 0.0000004118354\n",
      "[672,     1] loss: 0.0000001770636\n",
      "[673,     1] loss: 0.0000003232322\n",
      "[674,     1] loss: 0.0000001671597\n",
      "[675,     1] loss: 0.0000002338801\n",
      "[676,     1] loss: 0.0000002976743\n",
      "[677,     1] loss: 0.0000002034444\n",
      "[678,     1] loss: 0.0000003034143\n",
      "[679,     1] loss: 0.0000002889867\n",
      "[680,     1] loss: 0.0000004928677\n",
      "[681,     1] loss: 0.0000006376663\n",
      "[682,     1] loss: 0.0000001467494\n",
      "[683,     1] loss: 0.0000001159694\n",
      "[684,     1] loss: 0.0000000817861\n",
      "[685,     1] loss: 0.0000001278163\n",
      "[686,     1] loss: 0.0000001575550\n",
      "[687,     1] loss: 0.0000001990232\n",
      "[688,     1] loss: 0.0000000782155\n",
      "[689,     1] loss: 0.0000000837259\n",
      "[690,     1] loss: 0.0000002097213\n",
      "[691,     1] loss: 0.0000001785979\n",
      "[692,     1] loss: 0.0000002254095\n",
      "[693,     1] loss: 0.0000003989765\n",
      "[694,     1] loss: 0.0000001334169\n",
      "[695,     1] loss: 0.0000002780044\n",
      "[696,     1] loss: 0.0000002299729\n",
      "[697,     1] loss: 0.0000004219595\n",
      "[698,     1] loss: 0.0000001844334\n",
      "[699,     1] loss: 0.0000003104836\n",
      "[700,     1] loss: 0.0000003490227\n",
      "[701,     1] loss: 0.0000006067658\n",
      "[702,     1] loss: 0.0000017589588\n",
      "[703,     1] loss: 0.0000006496762\n",
      "[704,     1] loss: 0.0000009978686\n",
      "[705,     1] loss: 0.0000009972687\n",
      "[706,     1] loss: 0.0000007682537\n",
      "[707,     1] loss: 0.0000006883359\n",
      "[708,     1] loss: 0.0000005714890\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[709,     1] loss: 0.0000014825509\n",
      "[710,     1] loss: 0.0000004856417\n",
      "[711,     1] loss: 0.0000003239219\n",
      "[712,     1] loss: 0.0000016149446\n",
      "[713,     1] loss: 0.0000009833065\n",
      "[714,     1] loss: 0.0000003193144\n",
      "[715,     1] loss: 0.0000008193924\n",
      "[716,     1] loss: 0.0000001785016\n",
      "[717,     1] loss: 0.0000006079784\n",
      "[718,     1] loss: 0.0000004140023\n",
      "[719,     1] loss: 0.0000010084510\n",
      "[720,     1] loss: 0.0000005291758\n",
      "[721,     1] loss: 0.0000004953641\n",
      "[722,     1] loss: 0.0000003396550\n",
      "[723,     1] loss: 0.0000005289179\n",
      "[724,     1] loss: 0.0000011230424\n",
      "[725,     1] loss: 0.0000006883585\n",
      "[726,     1] loss: 0.0000009183029\n",
      "[727,     1] loss: 0.0000002728559\n",
      "[728,     1] loss: 0.0000002614137\n",
      "[729,     1] loss: 0.0000002110100\n",
      "[730,     1] loss: 0.0000001629445\n",
      "[731,     1] loss: 0.0000001837879\n",
      "[732,     1] loss: 0.0000000761181\n",
      "[733,     1] loss: 0.0000002062570\n",
      "[734,     1] loss: 0.0000001098702\n",
      "[735,     1] loss: 0.0000003233974\n",
      "[736,     1] loss: 0.0000003961888\n",
      "[737,     1] loss: 0.0000001497846\n",
      "[738,     1] loss: 0.0000000765359\n",
      "[739,     1] loss: 0.0000002288921\n",
      "[740,     1] loss: 0.0000001189494\n",
      "[741,     1] loss: 0.0000002178150\n",
      "[742,     1] loss: 0.0000001983516\n",
      "[743,     1] loss: 0.0000001852135\n",
      "[744,     1] loss: 0.0000000994443\n",
      "[745,     1] loss: 0.0000001118511\n",
      "[746,     1] loss: 0.0000000507089\n",
      "[747,     1] loss: 0.0000000763740\n",
      "[748,     1] loss: 0.0000000920676\n",
      "[749,     1] loss: 0.0000001150440\n",
      "[750,     1] loss: 0.0000000879879\n",
      "[751,     1] loss: 0.0000001795473\n",
      "[752,     1] loss: 0.0000001187650\n",
      "[753,     1] loss: 0.0000001178278\n",
      "[754,     1] loss: 0.0000001558796\n",
      "[755,     1] loss: 0.0000003701601\n",
      "[756,     1] loss: 0.0000001744169\n",
      "[757,     1] loss: 0.0000003447153\n",
      "[758,     1] loss: 0.0000001126177\n",
      "[759,     1] loss: 0.0000001478885\n",
      "[760,     1] loss: 0.0000002416803\n",
      "[761,     1] loss: 0.0000002954131\n",
      "[762,     1] loss: 0.0000003990972\n",
      "[763,     1] loss: 0.0000001697156\n",
      "[764,     1] loss: 0.0000000779141\n",
      "[765,     1] loss: 0.0000002162556\n",
      "[766,     1] loss: 0.0000001123352\n",
      "[767,     1] loss: 0.0000001921736\n",
      "[768,     1] loss: 0.0000002792282\n",
      "[769,     1] loss: 0.0000007741471\n",
      "[770,     1] loss: 0.0000001955152\n",
      "[771,     1] loss: 0.0000006465582\n",
      "[772,     1] loss: 0.0000002685473\n",
      "[773,     1] loss: 0.0000000828785\n",
      "[774,     1] loss: 0.0000001539206\n",
      "[775,     1] loss: 0.0000002768419\n",
      "[776,     1] loss: 0.0000000541367\n",
      "[777,     1] loss: 0.0000003682572\n",
      "[778,     1] loss: 0.0000001212685\n",
      "[779,     1] loss: 0.0000001297941\n",
      "[780,     1] loss: 0.0000002240733\n",
      "[781,     1] loss: 0.0000003741002\n",
      "[782,     1] loss: 0.0000002010014\n",
      "[783,     1] loss: 0.0000002032435\n",
      "[784,     1] loss: 0.0000003553773\n",
      "[785,     1] loss: 0.0000002582702\n",
      "[786,     1] loss: 0.0000005544514\n",
      "[787,     1] loss: 0.0000004069370\n",
      "[788,     1] loss: 0.0000000640805\n",
      "[789,     1] loss: 0.0000001486521\n",
      "[790,     1] loss: 0.0000000582799\n",
      "[791,     1] loss: 0.0000000677615\n",
      "[792,     1] loss: 0.0000000623476\n",
      "[793,     1] loss: 0.0000001621059\n",
      "[794,     1] loss: 0.0000002096074\n",
      "[795,     1] loss: 0.0000001848691\n",
      "[796,     1] loss: 0.0000001417004\n",
      "[797,     1] loss: 0.0000003459889\n",
      "[798,     1] loss: 0.0000003956200\n",
      "[799,     1] loss: 0.0000004019550\n",
      "[800,     1] loss: 0.0000005678664\n",
      "[801,     1] loss: 0.0000006219062\n",
      "[802,     1] loss: 0.0000015471058\n",
      "[803,     1] loss: 0.0000004715738\n",
      "[804,     1] loss: 0.0000010707514\n",
      "[805,     1] loss: 0.0000007097778\n",
      "[806,     1] loss: 0.0000006009276\n",
      "[807,     1] loss: 0.0000001793570\n",
      "[808,     1] loss: 0.0000015045232\n",
      "[809,     1] loss: 0.0000042205575\n",
      "[810,     1] loss: 0.0000012483201\n",
      "[811,     1] loss: 0.0000017959788\n",
      "[812,     1] loss: 0.0000010867132\n",
      "[813,     1] loss: 0.0000015948208\n",
      "[814,     1] loss: 0.0000012096779\n",
      "[815,     1] loss: 0.0000008403098\n",
      "[816,     1] loss: 0.0000003442462\n",
      "[817,     1] loss: 0.0000002536241\n",
      "[818,     1] loss: 0.0000002774113\n",
      "[819,     1] loss: 0.0000153238812\n",
      "[820,     1] loss: 0.0000092747658\n",
      "[821,     1] loss: 0.0000010116964\n",
      "[822,     1] loss: 0.0000028129443\n",
      "[823,     1] loss: 0.0000016328599\n",
      "[824,     1] loss: 0.0000015830130\n",
      "[825,     1] loss: 0.0000019521942\n",
      "[826,     1] loss: 0.0000017724928\n",
      "[827,     1] loss: 0.0000007074476\n",
      "[828,     1] loss: 0.0000002809098\n",
      "[829,     1] loss: 0.0000002167262\n",
      "[830,     1] loss: 0.0000003088538\n",
      "[831,     1] loss: 0.0000002423802\n",
      "[832,     1] loss: 0.0000004106975\n",
      "[833,     1] loss: 0.0000002608966\n",
      "[834,     1] loss: 0.0000001843633\n",
      "[835,     1] loss: 0.0000001628566\n",
      "[836,     1] loss: 0.0000002280511\n",
      "[837,     1] loss: 0.0000002088387\n",
      "[838,     1] loss: 0.0000001380167\n",
      "[839,     1] loss: 0.0000001604972\n",
      "[840,     1] loss: 0.0000007695931\n",
      "[841,     1] loss: 0.0000002077865\n",
      "[842,     1] loss: 0.0000002497274\n",
      "[843,     1] loss: 0.0000002028402\n",
      "[844,     1] loss: 0.0000002611681\n",
      "[845,     1] loss: 0.0000008364985\n",
      "[846,     1] loss: 0.0000005304514\n",
      "[847,     1] loss: 0.0000002367554\n",
      "[848,     1] loss: 0.0000003776496\n",
      "[849,     1] loss: 0.0000002268883\n",
      "[850,     1] loss: 0.0000002149956\n",
      "[851,     1] loss: 0.0000001660157\n",
      "[852,     1] loss: 0.0000002816215\n",
      "[853,     1] loss: 0.0000002975967\n",
      "[854,     1] loss: 0.0000003534592\n",
      "[855,     1] loss: 0.0000002302939\n",
      "[856,     1] loss: 0.0000003581638\n",
      "[857,     1] loss: 0.0000001606042\n",
      "[858,     1] loss: 0.0000001628386\n",
      "[859,     1] loss: 0.0000001410257\n",
      "[860,     1] loss: 0.0000002535451\n",
      "[861,     1] loss: 0.0000002248759\n",
      "[862,     1] loss: 0.0000003566271\n",
      "[863,     1] loss: 0.0000002100875\n",
      "[864,     1] loss: 0.0000001950420\n",
      "[865,     1] loss: 0.0000003544131\n",
      "[866,     1] loss: 0.0000004583078\n",
      "[867,     1] loss: 0.0000001474323\n",
      "[868,     1] loss: 0.0000001932762\n",
      "[869,     1] loss: 0.0000003039883\n",
      "[870,     1] loss: 0.0000001755282\n",
      "[871,     1] loss: 0.0000002697313\n",
      "[872,     1] loss: 0.0000003048044\n",
      "[873,     1] loss: 0.0000007009403\n",
      "[874,     1] loss: 0.0000001702146\n",
      "[875,     1] loss: 0.0000001737109\n",
      "[876,     1] loss: 0.0000003801302\n",
      "[877,     1] loss: 0.0000001809205\n",
      "[878,     1] loss: 0.0000002583213\n",
      "[879,     1] loss: 0.0000002126249\n",
      "[880,     1] loss: 0.0000003001391\n",
      "[881,     1] loss: 0.0000002246790\n",
      "[882,     1] loss: 0.0000002978459\n",
      "[883,     1] loss: 0.0000002151198\n",
      "[884,     1] loss: 0.0000001675992\n",
      "[885,     1] loss: 0.0000000968737\n",
      "[886,     1] loss: 0.0000011134956\n",
      "[887,     1] loss: 0.0000008378967\n",
      "[888,     1] loss: 0.0000003187840\n",
      "[889,     1] loss: 0.0000002327332\n",
      "[890,     1] loss: 0.0000006468470\n",
      "[891,     1] loss: 0.0000006202688\n",
      "[892,     1] loss: 0.0000029655110\n",
      "[893,     1] loss: 0.0000007876957\n",
      "[894,     1] loss: 0.0000019095258\n",
      "[895,     1] loss: 0.0000014835025\n",
      "[896,     1] loss: 0.0000018905608\n",
      "[897,     1] loss: 0.0000019909721\n",
      "[898,     1] loss: 0.0000011631047\n",
      "[899,     1] loss: 0.0000004288546\n",
      "[900,     1] loss: 0.0000001956616\n",
      "[901,     1] loss: 0.0000001292720\n",
      "[902,     1] loss: 0.0000001090631\n",
      "[903,     1] loss: 0.0000002370496\n",
      "[904,     1] loss: 0.0000001771151\n",
      "[905,     1] loss: 0.0000001493353\n",
      "[906,     1] loss: 0.0000005462726\n",
      "[907,     1] loss: 0.0000002237117\n",
      "[908,     1] loss: 0.0000002477986\n",
      "[909,     1] loss: 0.0000003938881\n",
      "[910,     1] loss: 0.0000004711491\n",
      "[911,     1] loss: 0.0000004299417\n",
      "[912,     1] loss: 0.0000004962234\n",
      "[913,     1] loss: 0.0000001075357\n",
      "[914,     1] loss: 0.0000001390680\n",
      "[915,     1] loss: 0.0000002738235\n",
      "[916,     1] loss: 0.0000001526020\n",
      "[917,     1] loss: 0.0000001238051\n",
      "[918,     1] loss: 0.0000000956548\n",
      "[919,     1] loss: 0.0000001822241\n",
      "[920,     1] loss: 0.0000002799785\n",
      "[921,     1] loss: 0.0000002421686\n",
      "[922,     1] loss: 0.0000003304770\n",
      "[923,     1] loss: 0.0000002369903\n",
      "[924,     1] loss: 0.0000002563221\n",
      "[925,     1] loss: 0.0000001372370\n",
      "[926,     1] loss: 0.0000001634975\n",
      "[927,     1] loss: 0.0000001529443\n",
      "[928,     1] loss: 0.0000002911891\n",
      "[929,     1] loss: 0.0000006298835\n",
      "[930,     1] loss: 0.0000004502885\n",
      "[931,     1] loss: 0.0000001967638\n",
      "[932,     1] loss: 0.0000002125404\n",
      "[933,     1] loss: 0.0000002210410\n",
      "[934,     1] loss: 0.0000001733559\n",
      "[935,     1] loss: 0.0000001320029\n",
      "[936,     1] loss: 0.0000001802353\n",
      "[937,     1] loss: 0.0000005159110\n",
      "[938,     1] loss: 0.0000003500902\n",
      "[939,     1] loss: 0.0000003059279\n",
      "[940,     1] loss: 0.0000001744359\n",
      "[941,     1] loss: 0.0000002153414\n",
      "[942,     1] loss: 0.0000002732201\n",
      "[943,     1] loss: 0.0000001384608\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944,     1] loss: 0.0000000798104\n",
      "[945,     1] loss: 0.0000001272288\n",
      "[946,     1] loss: 0.0000001009252\n",
      "[947,     1] loss: 0.0000001106773\n",
      "[948,     1] loss: 0.0000001078025\n",
      "[949,     1] loss: 0.0000000656212\n",
      "[950,     1] loss: 0.0000000702829\n",
      "[951,     1] loss: 0.0000004840428\n",
      "[952,     1] loss: 0.0000003004954\n",
      "[953,     1] loss: 0.0000000739338\n",
      "[954,     1] loss: 0.0000000762632\n",
      "[955,     1] loss: 0.0000004667601\n",
      "[956,     1] loss: 0.0000003750092\n",
      "[957,     1] loss: 0.0000001789573\n",
      "[958,     1] loss: 0.0000001381591\n",
      "[959,     1] loss: 0.0000003369028\n",
      "[960,     1] loss: 0.0000002526491\n",
      "[961,     1] loss: 0.0000005621515\n",
      "[962,     1] loss: 0.0000001717372\n",
      "[963,     1] loss: 0.0000003648489\n",
      "[964,     1] loss: 0.0000001232106\n",
      "[965,     1] loss: 0.0000002546984\n",
      "[966,     1] loss: 0.0000001020453\n",
      "[967,     1] loss: 0.0000000818237\n",
      "[968,     1] loss: 0.0000001122797\n",
      "[969,     1] loss: 0.0000002670211\n",
      "[970,     1] loss: 0.0000001770358\n",
      "[971,     1] loss: 0.0000001384714\n",
      "[972,     1] loss: 0.0000001131965\n",
      "[973,     1] loss: 0.0000002172427\n",
      "[974,     1] loss: 0.0000002770065\n",
      "[975,     1] loss: 0.0000001493205\n",
      "[976,     1] loss: 0.0000001099298\n",
      "[977,     1] loss: 0.0000001832639\n",
      "[978,     1] loss: 0.0000006434121\n",
      "[979,     1] loss: 0.0000005880620\n",
      "[980,     1] loss: 0.0000005092232\n",
      "[981,     1] loss: 0.0000001278704\n",
      "[982,     1] loss: 0.0000004664416\n",
      "[983,     1] loss: 0.0000002322757\n",
      "[984,     1] loss: 0.0000001150138\n",
      "[985,     1] loss: 0.0000002385098\n",
      "[986,     1] loss: 0.0000004247200\n",
      "[987,     1] loss: 0.0000001071559\n",
      "[988,     1] loss: 0.0000000833457\n",
      "[989,     1] loss: 0.0000001106639\n",
      "[990,     1] loss: 0.0000003377003\n",
      "[991,     1] loss: 0.0000003800895\n",
      "[992,     1] loss: 0.0000001032190\n",
      "[993,     1] loss: 0.0000002733089\n",
      "[994,     1] loss: 0.0000001932897\n",
      "[995,     1] loss: 0.0000003467473\n",
      "[996,     1] loss: 0.0000003596757\n",
      "[997,     1] loss: 0.0000003585924\n",
      "[998,     1] loss: 0.0000005341054\n",
      "[999,     1] loss: 0.0000004885100\n",
      "[1000,     1] loss: 0.0000004761427\n",
      "[1001,     1] loss: 0.0000001403924\n",
      "[1002,     1] loss: 0.0000003634648\n",
      "[1003,     1] loss: 0.0000003378830\n",
      "[1004,     1] loss: 0.0000002138907\n",
      "[1005,     1] loss: 0.0000001382027\n",
      "[1006,     1] loss: 0.0000001141157\n",
      "[1007,     1] loss: 0.0000003354121\n",
      "[1008,     1] loss: 0.0000001988473\n",
      "[1009,     1] loss: 0.0000003517522\n",
      "[1010,     1] loss: 0.0000002291115\n",
      "[1011,     1] loss: 0.0000004388681\n",
      "[1012,     1] loss: 0.0000001906558\n",
      "[1013,     1] loss: 0.0000000949622\n",
      "[1014,     1] loss: 0.0000001947574\n",
      "[1015,     1] loss: 0.0000001528785\n",
      "[1016,     1] loss: 0.0000002100136\n",
      "[1017,     1] loss: 0.0000001911428\n",
      "[1018,     1] loss: 0.0000002412175\n",
      "[1019,     1] loss: 0.0000002204875\n",
      "[1020,     1] loss: 0.0000005414209\n",
      "[1021,     1] loss: 0.0000005064458\n",
      "[1022,     1] loss: 0.0000005044638\n",
      "[1023,     1] loss: 0.0000002946269\n",
      "[1024,     1] loss: 0.0000003120334\n",
      "[1025,     1] loss: 0.0000003904195\n",
      "[1026,     1] loss: 0.0000007641625\n",
      "[1027,     1] loss: 0.0000002994648\n",
      "[1028,     1] loss: 0.0000003849056\n",
      "[1029,     1] loss: 0.0000001284836\n",
      "[1030,     1] loss: 0.0000005287648\n",
      "[1031,     1] loss: 0.0000003253850\n",
      "[1032,     1] loss: 0.0000002218208\n",
      "[1033,     1] loss: 0.0000001752408\n",
      "[1034,     1] loss: 0.0000004154597\n",
      "[1035,     1] loss: 0.0000008013125\n",
      "[1036,     1] loss: 0.0000005646237\n",
      "[1037,     1] loss: 0.0000005379924\n",
      "[1038,     1] loss: 0.0000006466777\n",
      "[1039,     1] loss: 0.0000002525780\n",
      "[1040,     1] loss: 0.0000001340629\n",
      "[1041,     1] loss: 0.0000001433039\n",
      "[1042,     1] loss: 0.0000001787697\n",
      "[1043,     1] loss: 0.0000003864770\n",
      "[1044,     1] loss: 0.0000004230456\n",
      "[1045,     1] loss: 0.0000001299078\n",
      "[1046,     1] loss: 0.0000001296592\n",
      "[1047,     1] loss: 0.0000001819518\n",
      "[1048,     1] loss: 0.0000001370488\n",
      "[1049,     1] loss: 0.0000002918448\n",
      "[1050,     1] loss: 0.0000003560220\n",
      "[1051,     1] loss: 0.0000001206379\n",
      "[1052,     1] loss: 0.0000016552727\n",
      "[1053,     1] loss: 0.0000009622761\n",
      "[1054,     1] loss: 0.0000003285312\n",
      "[1055,     1] loss: 0.0000002140356\n",
      "[1056,     1] loss: 0.0000001802299\n",
      "[1057,     1] loss: 0.0000003875225\n",
      "[1058,     1] loss: 0.0000005558187\n",
      "[1059,     1] loss: 0.0000002790506\n",
      "[1060,     1] loss: 0.0000002430274\n",
      "[1061,     1] loss: 0.0000003589471\n",
      "[1062,     1] loss: 0.0000011011934\n",
      "[1063,     1] loss: 0.0000002891464\n",
      "[1064,     1] loss: 0.0000001835617\n",
      "[1065,     1] loss: 0.0000001726121\n",
      "[1066,     1] loss: 0.0000002212449\n",
      "[1067,     1] loss: 0.0000003747958\n",
      "[1068,     1] loss: 0.0000006251049\n",
      "[1069,     1] loss: 0.0000006713724\n",
      "[1070,     1] loss: 0.0000001753313\n",
      "[1071,     1] loss: 0.0000001464979\n",
      "[1072,     1] loss: 0.0000001864381\n",
      "[1073,     1] loss: 0.0000007446562\n",
      "[1074,     1] loss: 0.0000005880167\n",
      "[1075,     1] loss: 0.0000003853817\n",
      "[1076,     1] loss: 0.0000005182582\n",
      "[1077,     1] loss: 0.0000005479229\n",
      "[1078,     1] loss: 0.0000003669014\n",
      "[1079,     1] loss: 0.0000002239303\n",
      "[1080,     1] loss: 0.0000004781234\n",
      "[1081,     1] loss: 0.0000003141168\n",
      "[1082,     1] loss: 0.0000003478136\n",
      "[1083,     1] loss: 0.0000001186569\n",
      "[1084,     1] loss: 0.0000001038842\n",
      "[1085,     1] loss: 0.0000001827481\n",
      "[1086,     1] loss: 0.0000002405478\n",
      "[1087,     1] loss: 0.0000003652147\n",
      "[1088,     1] loss: 0.0000022459008\n",
      "[1089,     1] loss: 0.0000006511581\n",
      "[1090,     1] loss: 0.0000003992320\n",
      "[1091,     1] loss: 0.0000009587949\n",
      "[1092,     1] loss: 0.0000007213923\n",
      "[1093,     1] loss: 0.0000003908214\n",
      "[1094,     1] loss: 0.0000003243369\n",
      "[1095,     1] loss: 0.0000003960145\n",
      "[1096,     1] loss: 0.0000004302646\n",
      "[1097,     1] loss: 0.0000003633606\n",
      "[1098,     1] loss: 0.0000008333296\n",
      "[1099,     1] loss: 0.0000004147294\n",
      "[1100,     1] loss: 0.0000001419682\n",
      "[1101,     1] loss: 0.0000004157484\n",
      "[1102,     1] loss: 0.0000002375785\n",
      "[1103,     1] loss: 0.0000001544565\n",
      "[1104,     1] loss: 0.0000001508079\n",
      "[1105,     1] loss: 0.0000001637918\n",
      "[1106,     1] loss: 0.0000001387859\n",
      "[1107,     1] loss: 0.0000001895441\n",
      "[1108,     1] loss: 0.0000003979797\n",
      "[1109,     1] loss: 0.0000016805838\n",
      "[1110,     1] loss: 0.0000023443121\n",
      "[1111,     1] loss: 0.0000004978106\n",
      "[1112,     1] loss: 0.0000002607326\n",
      "[1113,     1] loss: 0.0000001429681\n",
      "[1114,     1] loss: 0.0000001372850\n",
      "[1115,     1] loss: 0.0000003338470\n",
      "[1116,     1] loss: 0.0000001750599\n",
      "[1117,     1] loss: 0.0000002526379\n",
      "[1118,     1] loss: 0.0000002563239\n",
      "[1119,     1] loss: 0.0000000539862\n",
      "[1120,     1] loss: 0.0000001083897\n",
      "[1121,     1] loss: 0.0000000909568\n",
      "[1122,     1] loss: 0.0000001340926\n",
      "[1123,     1] loss: 0.0000001340340\n",
      "[1124,     1] loss: 0.0000001573865\n",
      "[1125,     1] loss: 0.0000002102654\n",
      "[1126,     1] loss: 0.0000000685842\n",
      "[1127,     1] loss: 0.0000001045877\n",
      "[1128,     1] loss: 0.0000001539985\n",
      "[1129,     1] loss: 0.0000001033792\n",
      "[1130,     1] loss: 0.0000000485908\n",
      "[1131,     1] loss: 0.0000001243872\n",
      "[1132,     1] loss: 0.0000000868746\n",
      "[1133,     1] loss: 0.0000002839793\n",
      "[1134,     1] loss: 0.0000001381064\n",
      "[1135,     1] loss: 0.0000002319247\n",
      "[1136,     1] loss: 0.0000001050312\n",
      "[1137,     1] loss: 0.0000001164444\n",
      "[1138,     1] loss: 0.0000008280469\n",
      "[1139,     1] loss: 0.0000002929408\n",
      "[1140,     1] loss: 0.0000004804849\n",
      "[1141,     1] loss: 0.0000001135032\n",
      "[1142,     1] loss: 0.0000001818051\n",
      "[1143,     1] loss: 0.0000001024602\n",
      "[1144,     1] loss: 0.0000006143415\n",
      "[1145,     1] loss: 0.0000002358678\n",
      "[1146,     1] loss: 0.0000001441133\n",
      "[1147,     1] loss: 0.0000001100988\n",
      "[1148,     1] loss: 0.0000000419200\n",
      "[1149,     1] loss: 0.0000001267079\n",
      "[1150,     1] loss: 0.0000000994734\n",
      "[1151,     1] loss: 0.0000003237471\n",
      "[1152,     1] loss: 0.0000003013339\n",
      "[1153,     1] loss: 0.0000016644512\n",
      "[1154,     1] loss: 0.0000011371467\n",
      "[1155,     1] loss: 0.0000008040487\n",
      "[1156,     1] loss: 0.0000037190715\n",
      "[1157,     1] loss: 0.0000009436032\n",
      "[1158,     1] loss: 0.0000006421467\n",
      "[1159,     1] loss: 0.0000003601043\n",
      "[1160,     1] loss: 0.0000004676031\n",
      "[1161,     1] loss: 0.0000011586706\n",
      "[1162,     1] loss: 0.0000013588795\n",
      "[1163,     1] loss: 0.0000004921203\n",
      "[1164,     1] loss: 0.0000008195064\n",
      "[1165,     1] loss: 0.0000004202345\n",
      "[1166,     1] loss: 0.0000000660819\n",
      "[1167,     1] loss: 0.0000012284570\n",
      "[1168,     1] loss: 0.0000012752875\n",
      "[1169,     1] loss: 0.0000005117763\n",
      "[1170,     1] loss: 0.0000002457006\n",
      "[1171,     1] loss: 0.0000001111366\n",
      "[1172,     1] loss: 0.0000001874445\n",
      "[1173,     1] loss: 0.0000005125189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1174,     1] loss: 0.0000001294143\n",
      "[1175,     1] loss: 0.0000002566186\n",
      "[1176,     1] loss: 0.0000003136206\n",
      "[1177,     1] loss: 0.0000002314538\n",
      "[1178,     1] loss: 0.0000001421102\n",
      "[1179,     1] loss: 0.0000002391181\n",
      "[1180,     1] loss: 0.0000000411549\n",
      "[1181,     1] loss: 0.0000001355403\n",
      "[1182,     1] loss: 0.0000000397870\n",
      "[1183,     1] loss: 0.0000000592721\n",
      "[1184,     1] loss: 0.0000001082548\n",
      "[1185,     1] loss: 0.0000002998054\n",
      "[1186,     1] loss: 0.0000005613223\n",
      "[1187,     1] loss: 0.0000002834449\n",
      "[1188,     1] loss: 0.0000002492621\n",
      "[1189,     1] loss: 0.0000005029837\n",
      "[1190,     1] loss: 0.0000001162686\n",
      "[1191,     1] loss: 0.0000002641554\n",
      "[1192,     1] loss: 0.0000001596465\n",
      "[1193,     1] loss: 0.0000000933412\n",
      "[1194,     1] loss: 0.0000000984115\n",
      "[1195,     1] loss: 0.0000001032200\n",
      "[1196,     1] loss: 0.0000001235024\n",
      "[1197,     1] loss: 0.0000002169872\n",
      "[1198,     1] loss: 0.0000005923653\n",
      "[1199,     1] loss: 0.0000003840627\n",
      "[1200,     1] loss: 0.0000002804140\n",
      "[1201,     1] loss: 0.0000002270177\n",
      "[1202,     1] loss: 0.0000005080120\n",
      "[1203,     1] loss: 0.0000003298523\n",
      "[1204,     1] loss: 0.0000001410850\n",
      "[1205,     1] loss: 0.0000001253243\n",
      "[1206,     1] loss: 0.0000005276726\n",
      "[1207,     1] loss: 0.0000004129945\n",
      "[1208,     1] loss: 0.0000003498504\n",
      "[1209,     1] loss: 0.0000001143609\n",
      "[1210,     1] loss: 0.0000002065490\n",
      "[1211,     1] loss: 0.0000001889074\n",
      "[1212,     1] loss: 0.0000001442933\n",
      "[1213,     1] loss: 0.0000001523443\n",
      "[1214,     1] loss: 0.0000002441545\n",
      "[1215,     1] loss: 0.0000000646702\n",
      "[1216,     1] loss: 0.0000000576242\n",
      "[1217,     1] loss: 0.0000001214712\n",
      "[1218,     1] loss: 0.0000000820899\n",
      "[1219,     1] loss: 0.0000000347891\n",
      "[1220,     1] loss: 0.0000001167286\n",
      "[1221,     1] loss: 0.0000000492398\n",
      "[1222,     1] loss: 0.0000000549202\n",
      "[1223,     1] loss: 0.0000001229986\n",
      "[1224,     1] loss: 0.0000000975138\n",
      "[1225,     1] loss: 0.0000001295782\n",
      "[1226,     1] loss: 0.0000004520542\n",
      "[1227,     1] loss: 0.0000001725359\n",
      "[1228,     1] loss: 0.0000002438707\n",
      "[1229,     1] loss: 0.0000001512100\n",
      "[1230,     1] loss: 0.0000000500615\n",
      "[1231,     1] loss: 0.0000000674273\n",
      "[1232,     1] loss: 0.0000001537475\n",
      "[1233,     1] loss: 0.0000000555274\n",
      "[1234,     1] loss: 0.0000001191398\n",
      "[1235,     1] loss: 0.0000001231204\n",
      "[1236,     1] loss: 0.0000002769305\n",
      "[1237,     1] loss: 0.0000000700809\n",
      "[1238,     1] loss: 0.0000004099345\n",
      "[1239,     1] loss: 0.0000005061675\n",
      "[1240,     1] loss: 0.0000007036691\n",
      "[1241,     1] loss: 0.0000008137615\n",
      "[1242,     1] loss: 0.0000001959239\n",
      "[1243,     1] loss: 0.0000008097480\n",
      "[1244,     1] loss: 0.0000002749061\n",
      "[1245,     1] loss: 0.0000001683103\n",
      "[1246,     1] loss: 0.0000001893037\n",
      "[1247,     1] loss: 0.0000001261833\n",
      "[1248,     1] loss: 0.0000001854385\n",
      "[1249,     1] loss: 0.0000003192713\n",
      "[1250,     1] loss: 0.0000005132904\n",
      "[1251,     1] loss: 0.0000009452529\n",
      "[1252,     1] loss: 0.0000002444234\n",
      "[1253,     1] loss: 0.0000001103831\n",
      "[1254,     1] loss: 0.0000000875604\n",
      "[1255,     1] loss: 0.0000000619362\n",
      "[1256,     1] loss: 0.0000000616915\n",
      "[1257,     1] loss: 0.0000001295837\n",
      "[1258,     1] loss: 0.0000001180591\n",
      "[1259,     1] loss: 0.0000000833356\n",
      "[1260,     1] loss: 0.0000001330612\n",
      "[1261,     1] loss: 0.0000000903869\n",
      "[1262,     1] loss: 0.0000002523520\n",
      "[1263,     1] loss: 0.0000001832616\n",
      "[1264,     1] loss: 0.0000002869418\n",
      "[1265,     1] loss: 0.0000000784248\n",
      "[1266,     1] loss: 0.0000003392349\n",
      "[1267,     1] loss: 0.0000002789342\n",
      "[1268,     1] loss: 0.0000003722050\n",
      "[1269,     1] loss: 0.0000003728260\n",
      "[1270,     1] loss: 0.0000003243174\n",
      "[1271,     1] loss: 0.0000006618858\n",
      "[1272,     1] loss: 0.0000006449137\n",
      "[1273,     1] loss: 0.0000003795766\n",
      "[1274,     1] loss: 0.0000004840330\n",
      "[1275,     1] loss: 0.0000009433053\n",
      "[1276,     1] loss: 0.0000007016658\n",
      "[1277,     1] loss: 0.0000003052335\n",
      "[1278,     1] loss: 0.0000003090487\n",
      "[1279,     1] loss: 0.0000002162390\n",
      "[1280,     1] loss: 0.0000001526098\n",
      "[1281,     1] loss: 0.0000003943553\n",
      "[1282,     1] loss: 0.0000002083616\n",
      "[1283,     1] loss: 0.0000000985744\n",
      "[1284,     1] loss: 0.0000002932310\n",
      "[1285,     1] loss: 0.0000002952983\n",
      "[1286,     1] loss: 0.0000001388294\n",
      "[1287,     1] loss: 0.0000001828859\n",
      "[1288,     1] loss: 0.0000004872432\n",
      "[1289,     1] loss: 0.0000002560210\n",
      "[1290,     1] loss: 0.0000004308806\n",
      "[1291,     1] loss: 0.0000002165170\n",
      "[1292,     1] loss: 0.0000003396830\n",
      "[1293,     1] loss: 0.0000001818316\n",
      "[1294,     1] loss: 0.0000002026798\n",
      "[1295,     1] loss: 0.0000002187085\n",
      "[1296,     1] loss: 0.0000001033724\n",
      "[1297,     1] loss: 0.0000001067319\n",
      "[1298,     1] loss: 0.0000002906287\n",
      "[1299,     1] loss: 0.0000001258070\n",
      "[1300,     1] loss: 0.0000007156561\n",
      "[1301,     1] loss: 0.0000006484026\n",
      "[1302,     1] loss: 0.0000010988744\n",
      "[1303,     1] loss: 0.0000006356533\n",
      "[1304,     1] loss: 0.0000004131908\n",
      "[1305,     1] loss: 0.0000008531155\n",
      "[1306,     1] loss: 0.0000004392713\n",
      "[1307,     1] loss: 0.0000002443875\n",
      "[1308,     1] loss: 0.0000001354407\n",
      "[1309,     1] loss: 0.0000001568248\n",
      "[1310,     1] loss: 0.0000001398551\n",
      "[1311,     1] loss: 0.0000001508727\n",
      "[1312,     1] loss: 0.0000002057997\n",
      "[1313,     1] loss: 0.0000003843068\n",
      "[1314,     1] loss: 0.0000005604236\n",
      "[1315,     1] loss: 0.0000016228003\n",
      "[1316,     1] loss: 0.0000007861218\n",
      "[1317,     1] loss: 0.0000008435210\n",
      "[1318,     1] loss: 0.0000003862143\n",
      "[1319,     1] loss: 0.0000002669688\n",
      "[1320,     1] loss: 0.0000006198243\n",
      "[1321,     1] loss: 0.0000003197857\n",
      "[1322,     1] loss: 0.0000007112305\n",
      "[1323,     1] loss: 0.0000005247071\n",
      "[1324,     1] loss: 0.0000010089987\n",
      "[1325,     1] loss: 0.0000011525648\n",
      "[1326,     1] loss: 0.0000003023111\n",
      "[1327,     1] loss: 0.0000001055470\n",
      "[1328,     1] loss: 0.0000007846209\n",
      "[1329,     1] loss: 0.0000005156044\n",
      "[1330,     1] loss: 0.0000006775540\n",
      "[1331,     1] loss: 0.0000001976587\n",
      "[1332,     1] loss: 0.0000002682471\n",
      "[1333,     1] loss: 0.0000002317777\n",
      "[1334,     1] loss: 0.0000002814252\n",
      "[1335,     1] loss: 0.0000001333236\n",
      "[1336,     1] loss: 0.0000001007209\n",
      "[1337,     1] loss: 0.0000001765713\n",
      "[1338,     1] loss: 0.0000001044278\n",
      "[1339,     1] loss: 0.0000001231291\n",
      "[1340,     1] loss: 0.0000001226378\n",
      "[1341,     1] loss: 0.0000000468736\n",
      "[1342,     1] loss: 0.0000000496432\n",
      "[1343,     1] loss: 0.0000001485295\n",
      "[1344,     1] loss: 0.0000000696692\n",
      "[1345,     1] loss: 0.0000000795773\n",
      "[1346,     1] loss: 0.0000000772448\n",
      "[1347,     1] loss: 0.0000000650576\n",
      "[1348,     1] loss: 0.0000000614028\n",
      "[1349,     1] loss: 0.0000004801419\n",
      "[1350,     1] loss: 0.0000001944733\n",
      "[1351,     1] loss: 0.0000003183889\n",
      "[1352,     1] loss: 0.0000001476352\n",
      "[1353,     1] loss: 0.0000004378078\n",
      "[1354,     1] loss: 0.0000002754625\n",
      "[1355,     1] loss: 0.0000001158330\n",
      "[1356,     1] loss: 0.0000001064688\n",
      "[1357,     1] loss: 0.0000003175184\n",
      "[1358,     1] loss: 0.0000005360400\n",
      "[1359,     1] loss: 0.0000001025074\n",
      "[1360,     1] loss: 0.0000001938158\n",
      "[1361,     1] loss: 0.0000001702580\n",
      "[1362,     1] loss: 0.0000005607043\n",
      "[1363,     1] loss: 0.0000004702204\n",
      "[1364,     1] loss: 0.0000005231507\n",
      "[1365,     1] loss: 0.0000000993208\n",
      "[1366,     1] loss: 0.0000006544848\n",
      "[1367,     1] loss: 0.0000001100815\n",
      "[1368,     1] loss: 0.0000003130887\n",
      "[1369,     1] loss: 0.0000004122150\n",
      "[1370,     1] loss: 0.0000001179247\n",
      "[1371,     1] loss: 0.0000001386213\n",
      "[1372,     1] loss: 0.0000001566286\n",
      "[1373,     1] loss: 0.0000004118613\n",
      "[1374,     1] loss: 0.0000002458329\n",
      "[1375,     1] loss: 0.0000001826367\n",
      "[1376,     1] loss: 0.0000005430737\n",
      "[1377,     1] loss: 0.0000014117352\n",
      "[1378,     1] loss: 0.0000005601696\n",
      "[1379,     1] loss: 0.0000005668696\n",
      "[1380,     1] loss: 0.0000005068039\n",
      "[1381,     1] loss: 0.0000003366156\n",
      "[1382,     1] loss: 0.0000007927007\n",
      "[1383,     1] loss: 0.0000003372150\n",
      "[1384,     1] loss: 0.0000009959451\n",
      "[1385,     1] loss: 0.0000003503328\n",
      "[1386,     1] loss: 0.0000006322038\n",
      "[1387,     1] loss: 0.0000004363750\n",
      "[1388,     1] loss: 0.0000006612814\n",
      "[1389,     1] loss: 0.0000005645199\n",
      "[1390,     1] loss: 0.0000004771031\n",
      "[1391,     1] loss: 0.0000005411091\n",
      "[1392,     1] loss: 0.0000002706376\n",
      "[1393,     1] loss: 0.0000002303178\n",
      "[1394,     1] loss: 0.0000003720734\n",
      "[1395,     1] loss: 0.0000001601487\n",
      "[1396,     1] loss: 0.0000002138467\n",
      "[1397,     1] loss: 0.0000002029276\n",
      "[1398,     1] loss: 0.0000001437902\n",
      "[1399,     1] loss: 0.0000000544092\n",
      "[1400,     1] loss: 0.0000001204286\n",
      "[1401,     1] loss: 0.0000001922630\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1402,     1] loss: 0.0000001054182\n",
      "[1403,     1] loss: 0.0000001598547\n",
      "[1404,     1] loss: 0.0000000818014\n",
      "[1405,     1] loss: 0.0000000609964\n",
      "[1406,     1] loss: 0.0000000607641\n",
      "[1407,     1] loss: 0.0000002864513\n",
      "[1408,     1] loss: 0.0000000924344\n",
      "[1409,     1] loss: 0.0000002155628\n",
      "[1410,     1] loss: 0.0000004558068\n",
      "[1411,     1] loss: 0.0000001371235\n",
      "[1412,     1] loss: 0.0000001461289\n",
      "[1413,     1] loss: 0.0000002057633\n",
      "[1414,     1] loss: 0.0000000819600\n",
      "[1415,     1] loss: 0.0000002654395\n",
      "[1416,     1] loss: 0.0000001118428\n",
      "[1417,     1] loss: 0.0000000410705\n",
      "[1418,     1] loss: 0.0000001035513\n",
      "[1419,     1] loss: 0.0000000925266\n",
      "[1420,     1] loss: 0.0000002791196\n",
      "[1421,     1] loss: 0.0000001498808\n",
      "[1422,     1] loss: 0.0000002108013\n",
      "[1423,     1] loss: 0.0000001526054\n",
      "[1424,     1] loss: 0.0000000758150\n",
      "[1425,     1] loss: 0.0000000745541\n",
      "[1426,     1] loss: 0.0000000271582\n",
      "[1427,     1] loss: 0.0000000649608\n",
      "[1428,     1] loss: 0.0000000997601\n",
      "[1429,     1] loss: 0.0000000623848\n",
      "[1430,     1] loss: 0.0000000916346\n",
      "[1431,     1] loss: 0.0000001471944\n",
      "[1432,     1] loss: 0.0000004396922\n",
      "[1433,     1] loss: 0.0000005871706\n",
      "[1434,     1] loss: 0.0000002478693\n",
      "[1435,     1] loss: 0.0000000729659\n",
      "[1436,     1] loss: 0.0000008609030\n",
      "[1437,     1] loss: 0.0000010082124\n",
      "[1438,     1] loss: 0.0000011274613\n",
      "[1439,     1] loss: 0.0000008858835\n",
      "[1440,     1] loss: 0.0000003009833\n",
      "[1441,     1] loss: 0.0000011885234\n",
      "[1442,     1] loss: 0.0000008137376\n",
      "[1443,     1] loss: 0.0000006007601\n",
      "[1444,     1] loss: 0.0000004596720\n",
      "[1445,     1] loss: 0.0000003735750\n",
      "[1446,     1] loss: 0.0000006845390\n",
      "[1447,     1] loss: 0.0000002183751\n",
      "[1448,     1] loss: 0.0000003423703\n",
      "[1449,     1] loss: 0.0000002328515\n",
      "[1450,     1] loss: 0.0000001073508\n",
      "[1451,     1] loss: 0.0000001072551\n",
      "[1452,     1] loss: 0.0000012338476\n",
      "[1453,     1] loss: 0.0000008300779\n",
      "[1454,     1] loss: 0.0000005552515\n",
      "[1455,     1] loss: 0.0000001756276\n",
      "[1456,     1] loss: 0.0000003146371\n",
      "[1457,     1] loss: 0.0000004208050\n",
      "[1458,     1] loss: 0.0000001989688\n",
      "[1459,     1] loss: 0.0000001380869\n",
      "[1460,     1] loss: 0.0000001425172\n",
      "[1461,     1] loss: 0.0000001756273\n",
      "[1462,     1] loss: 0.0000000737254\n",
      "[1463,     1] loss: 0.0000000780095\n",
      "[1464,     1] loss: 0.0000001472520\n",
      "[1465,     1] loss: 0.0000001570112\n",
      "[1466,     1] loss: 0.0000000875479\n",
      "[1467,     1] loss: 0.0000000965911\n",
      "[1468,     1] loss: 0.0000002174703\n",
      "[1469,     1] loss: 0.0000000791083\n",
      "[1470,     1] loss: 0.0000002126103\n",
      "[1471,     1] loss: 0.0000001743950\n",
      "[1472,     1] loss: 0.0000000820425\n",
      "[1473,     1] loss: 0.0000000645429\n",
      "[1474,     1] loss: 0.0000001190610\n",
      "[1475,     1] loss: 0.0000000543943\n",
      "[1476,     1] loss: 0.0000003721278\n",
      "[1477,     1] loss: 0.0000000512212\n",
      "[1478,     1] loss: 0.0000003361301\n",
      "[1479,     1] loss: 0.0000000792122\n",
      "[1480,     1] loss: 0.0000001227379\n",
      "[1481,     1] loss: 0.0000000986976\n",
      "[1482,     1] loss: 0.0000001046259\n",
      "[1483,     1] loss: 0.0000001367868\n",
      "[1484,     1] loss: 0.0000005947898\n",
      "[1485,     1] loss: 0.0000003542213\n",
      "[1486,     1] loss: 0.0000001275697\n",
      "[1487,     1] loss: 0.0000000462317\n",
      "[1488,     1] loss: 0.0000000895516\n",
      "[1489,     1] loss: 0.0000000696654\n",
      "[1490,     1] loss: 0.0000000746751\n",
      "[1491,     1] loss: 0.0000004921023\n",
      "[1492,     1] loss: 0.0000001305935\n",
      "[1493,     1] loss: 0.0000001477951\n",
      "[1494,     1] loss: 0.0000000882786\n",
      "[1495,     1] loss: 0.0000001486750\n",
      "[1496,     1] loss: 0.0000001106847\n",
      "[1497,     1] loss: 0.0000001300879\n",
      "[1498,     1] loss: 0.0000001793700\n",
      "[1499,     1] loss: 0.0000002580712\n",
      "[1500,     1] loss: 0.0000000747518\n",
      "[1501,     1] loss: 0.0000001201973\n",
      "[1502,     1] loss: 0.0000004030297\n",
      "[1503,     1] loss: 0.0000002361574\n",
      "[1504,     1] loss: 0.0000009141332\n",
      "[1505,     1] loss: 0.0000001066277\n",
      "[1506,     1] loss: 0.0000002172362\n",
      "[1507,     1] loss: 0.0000002395443\n",
      "[1508,     1] loss: 0.0000003276472\n",
      "[1509,     1] loss: 0.0000001759939\n",
      "[1510,     1] loss: 0.0000000978249\n",
      "[1511,     1] loss: 0.0000001497912\n",
      "[1512,     1] loss: 0.0000003688557\n",
      "[1513,     1] loss: 0.0000003092968\n",
      "[1514,     1] loss: 0.0000012755290\n",
      "[1515,     1] loss: 0.0000003318856\n",
      "[1516,     1] loss: 0.0000006966711\n",
      "[1517,     1] loss: 0.0000006200966\n",
      "[1518,     1] loss: 0.0000003869238\n",
      "[1519,     1] loss: 0.0000005904260\n",
      "[1520,     1] loss: 0.0000002320498\n",
      "[1521,     1] loss: 0.0000002109408\n",
      "[1522,     1] loss: 0.0000004614498\n",
      "[1523,     1] loss: 0.0000006817325\n",
      "[1524,     1] loss: 0.0000015676324\n",
      "[1525,     1] loss: 0.0000005307599\n",
      "[1526,     1] loss: 0.0000003785199\n",
      "[1527,     1] loss: 0.0000006028807\n",
      "[1528,     1] loss: 0.0000009779965\n",
      "[1529,     1] loss: 0.0000002143108\n",
      "[1530,     1] loss: 0.0000002059375\n",
      "[1531,     1] loss: 0.0000002389963\n",
      "[1532,     1] loss: 0.0000001046082\n",
      "[1533,     1] loss: 0.0000002251590\n",
      "[1534,     1] loss: 0.0000000876696\n",
      "[1535,     1] loss: 0.0000001987267\n",
      "[1536,     1] loss: 0.0000002293478\n",
      "[1537,     1] loss: 0.0000003538018\n",
      "[1538,     1] loss: 0.0000004464311\n",
      "[1539,     1] loss: 0.0000004361345\n",
      "[1540,     1] loss: 0.0000005185430\n",
      "[1541,     1] loss: 0.0000002836932\n",
      "[1542,     1] loss: 0.0000008375125\n",
      "[1543,     1] loss: 0.0000006105934\n",
      "[1544,     1] loss: 0.0000004116538\n",
      "[1545,     1] loss: 0.0000000689297\n",
      "[1546,     1] loss: 0.0000001297822\n",
      "[1547,     1] loss: 0.0000001327179\n",
      "[1548,     1] loss: 0.0000003350789\n",
      "[1549,     1] loss: 0.0000001536610\n",
      "[1550,     1] loss: 0.0000005767382\n",
      "[1551,     1] loss: 0.0000003112328\n",
      "[1552,     1] loss: 0.0000000840181\n",
      "[1553,     1] loss: 0.0000000585524\n",
      "[1554,     1] loss: 0.0000001310819\n",
      "[1555,     1] loss: 0.0000003183020\n",
      "[1556,     1] loss: 0.0000001140051\n",
      "[1557,     1] loss: 0.0000001166855\n",
      "[1558,     1] loss: 0.0000004555881\n",
      "[1559,     1] loss: 0.0000001996281\n",
      "[1560,     1] loss: 0.0000005015132\n",
      "[1561,     1] loss: 0.0000005451649\n",
      "[1562,     1] loss: 0.0000003661617\n",
      "[1563,     1] loss: 0.0000001391289\n",
      "[1564,     1] loss: 0.0000002714019\n",
      "[1565,     1] loss: 0.0000000561109\n",
      "[1566,     1] loss: 0.0000001076457\n",
      "[1567,     1] loss: 0.0000001623475\n",
      "[1568,     1] loss: 0.0000001127378\n",
      "[1569,     1] loss: 0.0000002766277\n",
      "[1570,     1] loss: 0.0000002944850\n",
      "[1571,     1] loss: 0.0000001851708\n",
      "[1572,     1] loss: 0.0000002369606\n",
      "[1573,     1] loss: 0.0000001290972\n",
      "[1574,     1] loss: 0.0000000888095\n",
      "[1575,     1] loss: 0.0000000437332\n",
      "[1576,     1] loss: 0.0000001196941\n",
      "[1577,     1] loss: 0.0000001179636\n",
      "[1578,     1] loss: 0.0000001330285\n",
      "[1579,     1] loss: 0.0000003611003\n",
      "[1580,     1] loss: 0.0000002651705\n",
      "[1581,     1] loss: 0.0000003941916\n",
      "[1582,     1] loss: 0.0000009291497\n",
      "[1583,     1] loss: 0.0000003563661\n",
      "[1584,     1] loss: 0.0000004876356\n",
      "[1585,     1] loss: 0.0000004856418\n",
      "[1586,     1] loss: 0.0000005435862\n",
      "[1587,     1] loss: 0.0000003405451\n",
      "[1588,     1] loss: 0.0000005072596\n",
      "[1589,     1] loss: 0.0000002732085\n",
      "[1590,     1] loss: 0.0000011171924\n",
      "[1591,     1] loss: 0.0000002474266\n",
      "[1592,     1] loss: 0.0000004121516\n",
      "[1593,     1] loss: 0.0000006897574\n",
      "[1594,     1] loss: 0.0000010141493\n",
      "[1595,     1] loss: 0.0000007223589\n",
      "[1596,     1] loss: 0.0000001334059\n",
      "[1597,     1] loss: 0.0000001362803\n",
      "[1598,     1] loss: 0.0000002041439\n",
      "[1599,     1] loss: 0.0000001759998\n",
      "[1600,     1] loss: 0.0000000655816\n",
      "[1601,     1] loss: 0.0000001309110\n",
      "[1602,     1] loss: 0.0000000447454\n",
      "[1603,     1] loss: 0.0000000456959\n",
      "[1604,     1] loss: 0.0000000385562\n",
      "[1605,     1] loss: 0.0000002309602\n",
      "[1606,     1] loss: 0.0000000514955\n",
      "[1607,     1] loss: 0.0000002297250\n",
      "[1608,     1] loss: 0.0000003609320\n",
      "[1609,     1] loss: 0.0000000777658\n",
      "[1610,     1] loss: 0.0000002358545\n",
      "[1611,     1] loss: 0.0000000891684\n",
      "[1612,     1] loss: 0.0000000632256\n",
      "[1613,     1] loss: 0.0000001621174\n",
      "[1614,     1] loss: 0.0000004140501\n",
      "[1615,     1] loss: 0.0000009416497\n",
      "[1616,     1] loss: 0.0000005397591\n",
      "[1617,     1] loss: 0.0000001354490\n",
      "[1618,     1] loss: 0.0000006813969\n",
      "[1619,     1] loss: 0.0000003029653\n",
      "[1620,     1] loss: 0.0000001519980\n",
      "[1621,     1] loss: 0.0000005194333\n",
      "[1622,     1] loss: 0.0000005036604\n",
      "[1623,     1] loss: 0.0000003963577\n",
      "[1624,     1] loss: 0.0000002809760\n",
      "[1625,     1] loss: 0.0000003961619\n",
      "[1626,     1] loss: 0.0000003218511\n",
      "[1627,     1] loss: 0.0000007516684\n",
      "[1628,     1] loss: 0.0000001839184\n",
      "[1629,     1] loss: 0.0000003320567\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1630,     1] loss: 0.0000009542466\n",
      "[1631,     1] loss: 0.0000001555833\n",
      "[1632,     1] loss: 0.0000005268030\n",
      "[1633,     1] loss: 0.0000004297527\n",
      "[1634,     1] loss: 0.0000003960815\n",
      "[1635,     1] loss: 0.0000002953919\n",
      "[1636,     1] loss: 0.0000012091365\n",
      "[1637,     1] loss: 0.0000005183413\n",
      "[1638,     1] loss: 0.0000003388301\n",
      "[1639,     1] loss: 0.0000000461576\n",
      "[1640,     1] loss: 0.0000006087900\n",
      "[1641,     1] loss: 0.0000001300261\n",
      "[1642,     1] loss: 0.0000005101167\n",
      "[1643,     1] loss: 0.0000003416214\n",
      "[1644,     1] loss: 0.0000000823519\n",
      "[1645,     1] loss: 0.0000001268146\n",
      "[1646,     1] loss: 0.0000002574354\n",
      "[1647,     1] loss: 0.0000000841573\n",
      "[1648,     1] loss: 0.0000001368797\n",
      "[1649,     1] loss: 0.0000002306942\n",
      "[1650,     1] loss: 0.0000004508515\n",
      "[1651,     1] loss: 0.0000002137428\n",
      "[1652,     1] loss: 0.0000002201524\n",
      "[1653,     1] loss: 0.0000000269855\n",
      "[1654,     1] loss: 0.0000000563203\n",
      "[1655,     1] loss: 0.0000001560834\n",
      "[1656,     1] loss: 0.0000000634332\n",
      "[1657,     1] loss: 0.0000000952001\n",
      "[1658,     1] loss: 0.0000000673389\n",
      "[1659,     1] loss: 0.0000002157113\n",
      "[1660,     1] loss: 0.0000000960705\n",
      "[1661,     1] loss: 0.0000006791563\n",
      "[1662,     1] loss: 0.0000015112240\n",
      "[1663,     1] loss: 0.0000052732332\n",
      "[1664,     1] loss: 0.0000008030820\n",
      "[1665,     1] loss: 0.0000005330247\n",
      "[1666,     1] loss: 0.0000010692971\n",
      "[1667,     1] loss: 0.0000007249344\n",
      "[1668,     1] loss: 0.0000006700124\n",
      "[1669,     1] loss: 0.0000004037450\n",
      "[1670,     1] loss: 0.0000006347369\n",
      "[1671,     1] loss: 0.0000009130296\n",
      "[1672,     1] loss: 0.0000003391179\n",
      "[1673,     1] loss: 0.0000008687534\n",
      "[1674,     1] loss: 0.0000014672336\n",
      "[1675,     1] loss: 0.0000007432472\n",
      "[1676,     1] loss: 0.0000003347289\n",
      "[1677,     1] loss: 0.0000010541895\n",
      "[1678,     1] loss: 0.0000003596744\n",
      "[1679,     1] loss: 0.0000009413714\n",
      "[1680,     1] loss: 0.0000009941535\n",
      "[1681,     1] loss: 0.0000001801185\n",
      "[1682,     1] loss: 0.0000002188485\n",
      "[1683,     1] loss: 0.0000000907703\n",
      "[1684,     1] loss: 0.0000001366674\n",
      "[1685,     1] loss: 0.0000001117891\n",
      "[1686,     1] loss: 0.0000002277057\n",
      "[1687,     1] loss: 0.0000001211577\n",
      "[1688,     1] loss: 0.0000003082877\n",
      "[1689,     1] loss: 0.0000002661568\n",
      "[1690,     1] loss: 0.0000007758223\n",
      "[1691,     1] loss: 0.0000011878139\n",
      "[1692,     1] loss: 0.0000003555225\n",
      "[1693,     1] loss: 0.0000003596993\n",
      "[1694,     1] loss: 0.0000007079117\n",
      "[1695,     1] loss: 0.0000002309537\n",
      "[1696,     1] loss: 0.0000001306412\n",
      "[1697,     1] loss: 0.0000006410552\n",
      "[1698,     1] loss: 0.0000003336032\n",
      "[1699,     1] loss: 0.0000001004343\n",
      "[1700,     1] loss: 0.0000001034851\n",
      "[1701,     1] loss: 0.0000000570095\n",
      "[1702,     1] loss: 0.0000000863463\n",
      "[1703,     1] loss: 0.0000000855449\n",
      "[1704,     1] loss: 0.0000001783446\n",
      "[1705,     1] loss: 0.0000000660914\n",
      "[1706,     1] loss: 0.0000004985767\n",
      "[1707,     1] loss: 0.0000002138445\n",
      "[1708,     1] loss: 0.0000000749622\n",
      "[1709,     1] loss: 0.0000000948174\n",
      "[1710,     1] loss: 0.0000003183770\n",
      "[1711,     1] loss: 0.0000006077549\n",
      "[1712,     1] loss: 0.0000003222993\n",
      "[1713,     1] loss: 0.0000000957200\n",
      "[1714,     1] loss: 0.0000000394843\n",
      "[1715,     1] loss: 0.0000000536267\n",
      "[1716,     1] loss: 0.0000000442705\n",
      "[1717,     1] loss: 0.0000000786502\n",
      "[1718,     1] loss: 0.0000000447784\n",
      "[1719,     1] loss: 0.0000000813358\n",
      "[1720,     1] loss: 0.0000002669514\n",
      "[1721,     1] loss: 0.0000003261140\n",
      "[1722,     1] loss: 0.0000001269471\n",
      "[1723,     1] loss: 0.0000000857388\n",
      "[1724,     1] loss: 0.0000000538565\n",
      "[1725,     1] loss: 0.0000000844365\n",
      "[1726,     1] loss: 0.0000001434415\n",
      "[1727,     1] loss: 0.0000001759326\n",
      "[1728,     1] loss: 0.0000001143440\n",
      "[1729,     1] loss: 0.0000000444414\n",
      "[1730,     1] loss: 0.0000000358070\n",
      "[1731,     1] loss: 0.0000000460739\n",
      "[1732,     1] loss: 0.0000000684586\n",
      "[1733,     1] loss: 0.0000000629092\n",
      "[1734,     1] loss: 0.0000000411481\n",
      "[1735,     1] loss: 0.0000003077209\n",
      "[1736,     1] loss: 0.0000000708389\n",
      "[1737,     1] loss: 0.0000001149563\n",
      "[1738,     1] loss: 0.0000001223232\n",
      "[1739,     1] loss: 0.0000002272383\n",
      "[1740,     1] loss: 0.0000001469917\n",
      "[1741,     1] loss: 0.0000001700978\n",
      "[1742,     1] loss: 0.0000000942811\n",
      "[1743,     1] loss: 0.0000001316612\n",
      "[1744,     1] loss: 0.0000000393125\n",
      "[1745,     1] loss: 0.0000000605254\n",
      "[1746,     1] loss: 0.0000000545414\n",
      "[1747,     1] loss: 0.0000000682479\n",
      "[1748,     1] loss: 0.0000001432638\n",
      "[1749,     1] loss: 0.0000001977252\n",
      "[1750,     1] loss: 0.0000003277542\n",
      "[1751,     1] loss: 0.0000001001208\n",
      "[1752,     1] loss: 0.0000000502892\n",
      "[1753,     1] loss: 0.0000002313014\n",
      "[1754,     1] loss: 0.0000001262040\n",
      "[1755,     1] loss: 0.0000001140693\n",
      "[1756,     1] loss: 0.0000000613177\n",
      "[1757,     1] loss: 0.0000003025504\n",
      "[1758,     1] loss: 0.0000001859672\n",
      "[1759,     1] loss: 0.0000001282668\n",
      "[1760,     1] loss: 0.0000007217966\n",
      "[1761,     1] loss: 0.0000001267802\n",
      "[1762,     1] loss: 0.0000003036558\n",
      "[1763,     1] loss: 0.0000004316395\n",
      "[1764,     1] loss: 0.0000001015419\n",
      "[1765,     1] loss: 0.0000000813753\n",
      "[1766,     1] loss: 0.0000003736083\n",
      "[1767,     1] loss: 0.0000001924975\n",
      "[1768,     1] loss: 0.0000000677751\n",
      "[1769,     1] loss: 0.0000006069095\n",
      "[1770,     1] loss: 0.0000004095750\n",
      "[1771,     1] loss: 0.0000002059398\n",
      "[1772,     1] loss: 0.0000001099221\n",
      "[1773,     1] loss: 0.0000001092359\n",
      "[1774,     1] loss: 0.0000000623270\n",
      "[1775,     1] loss: 0.0000001376680\n",
      "[1776,     1] loss: 0.0000001711485\n",
      "[1777,     1] loss: 0.0000001262315\n",
      "[1778,     1] loss: 0.0000000765853\n",
      "[1779,     1] loss: 0.0000004214809\n",
      "[1780,     1] loss: 0.0000000806445\n",
      "[1781,     1] loss: 0.0000001852349\n",
      "[1782,     1] loss: 0.0000003751833\n",
      "[1783,     1] loss: 0.0000001583167\n",
      "[1784,     1] loss: 0.0000004626038\n",
      "[1785,     1] loss: 0.0000000770508\n",
      "[1786,     1] loss: 0.0000000991810\n",
      "[1787,     1] loss: 0.0000001435860\n",
      "[1788,     1] loss: 0.0000002412128\n",
      "[1789,     1] loss: 0.0000000557232\n",
      "[1790,     1] loss: 0.0000001221236\n",
      "[1791,     1] loss: 0.0000000603322\n",
      "[1792,     1] loss: 0.0000001768692\n",
      "[1793,     1] loss: 0.0000006318273\n",
      "[1794,     1] loss: 0.0000016965219\n",
      "[1795,     1] loss: 0.0000021521635\n",
      "[1796,     1] loss: 0.0000008170982\n",
      "[1797,     1] loss: 0.0000016313539\n",
      "[1798,     1] loss: 0.0000013410022\n",
      "[1799,     1] loss: 0.0000006853066\n",
      "[1800,     1] loss: 0.0000003894210\n",
      "[1801,     1] loss: 0.0000002498512\n",
      "[1802,     1] loss: 0.0000001751405\n",
      "[1803,     1] loss: 0.0000001821884\n",
      "[1804,     1] loss: 0.0000000959533\n",
      "[1805,     1] loss: 0.0000000992290\n",
      "[1806,     1] loss: 0.0000001455669\n",
      "[1807,     1] loss: 0.0000001092648\n",
      "[1808,     1] loss: 0.0000001277697\n",
      "[1809,     1] loss: 0.0000001174607\n",
      "[1810,     1] loss: 0.0000000629729\n",
      "[1811,     1] loss: 0.0000003591656\n",
      "[1812,     1] loss: 0.0000001176422\n",
      "[1813,     1] loss: 0.0000001498709\n",
      "[1814,     1] loss: 0.0000001227873\n",
      "[1815,     1] loss: 0.0000002859480\n",
      "[1816,     1] loss: 0.0000003824012\n",
      "[1817,     1] loss: 0.0000001012169\n",
      "[1818,     1] loss: 0.0000000914388\n",
      "[1819,     1] loss: 0.0000000572334\n",
      "[1820,     1] loss: 0.0000000338686\n",
      "[1821,     1] loss: 0.0000000768869\n",
      "[1822,     1] loss: 0.0000000905293\n",
      "[1823,     1] loss: 0.0000001260461\n",
      "[1824,     1] loss: 0.0000001948875\n",
      "[1825,     1] loss: 0.0000002563209\n",
      "[1826,     1] loss: 0.0000001566627\n",
      "[1827,     1] loss: 0.0000000910847\n",
      "[1828,     1] loss: 0.0000001744475\n",
      "[1829,     1] loss: 0.0000001401893\n",
      "[1830,     1] loss: 0.0000003356310\n",
      "[1831,     1] loss: 0.0000002039385\n",
      "[1832,     1] loss: 0.0000001027645\n",
      "[1833,     1] loss: 0.0000003129245\n",
      "[1834,     1] loss: 0.0000007369114\n",
      "[1835,     1] loss: 0.0000002264460\n",
      "[1836,     1] loss: 0.0000001604544\n",
      "[1837,     1] loss: 0.0000000782671\n",
      "[1838,     1] loss: 0.0000002594187\n",
      "[1839,     1] loss: 0.0000002003595\n",
      "[1840,     1] loss: 0.0000002584094\n",
      "[1841,     1] loss: 0.0000003916572\n",
      "[1842,     1] loss: 0.0000001654687\n",
      "[1843,     1] loss: 0.0000002756219\n",
      "[1844,     1] loss: 0.0000000898156\n",
      "[1845,     1] loss: 0.0000002304168\n",
      "[1846,     1] loss: 0.0000001840196\n",
      "[1847,     1] loss: 0.0000003932813\n",
      "[1848,     1] loss: 0.0000002710014\n",
      "[1849,     1] loss: 0.0000005397344\n",
      "[1850,     1] loss: 0.0000004871467\n",
      "[1851,     1] loss: 0.0000001705929\n",
      "[1852,     1] loss: 0.0000002332165\n",
      "[1853,     1] loss: 0.0000001939093\n",
      "[1854,     1] loss: 0.0000003061136\n",
      "[1855,     1] loss: 0.0000005300647\n",
      "[1856,     1] loss: 0.0000003171127\n",
      "[1857,     1] loss: 0.0000013308451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1858,     1] loss: 0.0000009378077\n",
      "[1859,     1] loss: 0.0000007794638\n",
      "[1860,     1] loss: 0.0000007070628\n",
      "[1861,     1] loss: 0.0000004011037\n",
      "[1862,     1] loss: 0.0000005516057\n",
      "[1863,     1] loss: 0.0000005153147\n",
      "[1864,     1] loss: 0.0000010206302\n",
      "[1865,     1] loss: 0.0000015146564\n",
      "[1866,     1] loss: 0.0000006303763\n",
      "[1867,     1] loss: 0.0000004306323\n",
      "[1868,     1] loss: 0.0000002354448\n",
      "[1869,     1] loss: 0.0000002109035\n",
      "[1870,     1] loss: 0.0000002345297\n",
      "[1871,     1] loss: 0.0000003366414\n",
      "[1872,     1] loss: 0.0000002684739\n",
      "[1873,     1] loss: 0.0000005688027\n",
      "[1874,     1] loss: 0.0000005026038\n",
      "[1875,     1] loss: 0.0000000892757\n",
      "[1876,     1] loss: 0.0000001589168\n",
      "[1877,     1] loss: 0.0000002257265\n",
      "[1878,     1] loss: 0.0000001837525\n",
      "[1879,     1] loss: 0.0000001371768\n",
      "[1880,     1] loss: 0.0000000755960\n",
      "[1881,     1] loss: 0.0000002250538\n",
      "[1882,     1] loss: 0.0000000976060\n",
      "[1883,     1] loss: 0.0000004204531\n",
      "[1884,     1] loss: 0.0000004589727\n",
      "[1885,     1] loss: 0.0000008909958\n",
      "[1886,     1] loss: 0.0000005632826\n",
      "[1887,     1] loss: 0.0000001700776\n",
      "[1888,     1] loss: 0.0000003583675\n",
      "[1889,     1] loss: 0.0000002140554\n",
      "[1890,     1] loss: 0.0000001662618\n",
      "[1891,     1] loss: 0.0000001781468\n",
      "[1892,     1] loss: 0.0000000858953\n",
      "[1893,     1] loss: 0.0000002960303\n",
      "[1894,     1] loss: 0.0000003025229\n",
      "[1895,     1] loss: 0.0000002774213\n",
      "[1896,     1] loss: 0.0000003383647\n",
      "[1897,     1] loss: 0.0000001160913\n",
      "[1898,     1] loss: 0.0000003120297\n",
      "[1899,     1] loss: 0.0000003250829\n",
      "[1900,     1] loss: 0.0000004096477\n",
      "[1901,     1] loss: 0.0000003916288\n",
      "[1902,     1] loss: 0.0000001273837\n",
      "[1903,     1] loss: 0.0000003264988\n",
      "[1904,     1] loss: 0.0000003102824\n",
      "[1905,     1] loss: 0.0000001473600\n",
      "[1906,     1] loss: 0.0000001771117\n",
      "[1907,     1] loss: 0.0000005326773\n",
      "[1908,     1] loss: 0.0000002140815\n",
      "[1909,     1] loss: 0.0000001676158\n",
      "[1910,     1] loss: 0.0000001021392\n",
      "[1911,     1] loss: 0.0000001832134\n",
      "[1912,     1] loss: 0.0000002492163\n",
      "[1913,     1] loss: 0.0000005406986\n",
      "[1914,     1] loss: 0.0000007598969\n",
      "[1915,     1] loss: 0.0000009837760\n",
      "[1916,     1] loss: 0.0000008637948\n",
      "[1917,     1] loss: 0.0000001114930\n",
      "[1918,     1] loss: 0.0000002697336\n",
      "[1919,     1] loss: 0.0000002585452\n",
      "[1920,     1] loss: 0.0000002452475\n",
      "[1921,     1] loss: 0.0000003142999\n",
      "[1922,     1] loss: 0.0000002937627\n",
      "[1923,     1] loss: 0.0000004409908\n",
      "[1924,     1] loss: 0.0000004538736\n",
      "[1925,     1] loss: 0.0000006082171\n",
      "[1926,     1] loss: 0.0000001883186\n",
      "[1927,     1] loss: 0.0000004149308\n",
      "[1928,     1] loss: 0.0000002775846\n",
      "[1929,     1] loss: 0.0000001445361\n",
      "[1930,     1] loss: 0.0000000871341\n",
      "[1931,     1] loss: 0.0000002197196\n",
      "[1932,     1] loss: 0.0000002711194\n",
      "[1933,     1] loss: 0.0000001512854\n",
      "[1934,     1] loss: 0.0000001797886\n",
      "[1935,     1] loss: 0.0000003464290\n",
      "[1936,     1] loss: 0.0000006212024\n",
      "[1937,     1] loss: 0.0000006655602\n",
      "[1938,     1] loss: 0.0000003743061\n",
      "[1939,     1] loss: 0.0000003388179\n",
      "[1940,     1] loss: 0.0000002039948\n",
      "[1941,     1] loss: 0.0000001085077\n",
      "[1942,     1] loss: 0.0000001340641\n",
      "[1943,     1] loss: 0.0000001481306\n",
      "[1944,     1] loss: 0.0000001184155\n",
      "[1945,     1] loss: 0.0000001026156\n",
      "[1946,     1] loss: 0.0000003783162\n",
      "[1947,     1] loss: 0.0000002063585\n",
      "[1948,     1] loss: 0.0000003103931\n",
      "[1949,     1] loss: 0.0000001214469\n",
      "[1950,     1] loss: 0.0000004694823\n",
      "[1951,     1] loss: 0.0000001261130\n",
      "[1952,     1] loss: 0.0000002100002\n",
      "[1953,     1] loss: 0.0000000891928\n",
      "[1954,     1] loss: 0.0000002483461\n",
      "[1955,     1] loss: 0.0000007734498\n",
      "[1956,     1] loss: 0.0000001669466\n",
      "[1957,     1] loss: 0.0000002379914\n",
      "[1958,     1] loss: 0.0000001320102\n",
      "[1959,     1] loss: 0.0000001115302\n",
      "[1960,     1] loss: 0.0000000909240\n",
      "[1961,     1] loss: 0.0000001898807\n",
      "[1962,     1] loss: 0.0000001697263\n",
      "[1963,     1] loss: 0.0000004577687\n",
      "[1964,     1] loss: 0.0000001007415\n",
      "[1965,     1] loss: 0.0000000750540\n",
      "[1966,     1] loss: 0.0000000996529\n",
      "[1967,     1] loss: 0.0000000820429\n",
      "[1968,     1] loss: 0.0000002246262\n",
      "[1969,     1] loss: 0.0000002369594\n",
      "[1970,     1] loss: 0.0000002746841\n",
      "[1971,     1] loss: 0.0000002097542\n",
      "[1972,     1] loss: 0.0000000488935\n",
      "[1973,     1] loss: 0.0000003601985\n",
      "[1974,     1] loss: 0.0000000801114\n",
      "[1975,     1] loss: 0.0000003985424\n",
      "[1976,     1] loss: 0.0000002334295\n",
      "[1977,     1] loss: 0.0000003570490\n",
      "[1978,     1] loss: 0.0000002134766\n",
      "[1979,     1] loss: 0.0000002345934\n",
      "[1980,     1] loss: 0.0000000706216\n",
      "[1981,     1] loss: 0.0000002504177\n",
      "[1982,     1] loss: 0.0000002275354\n",
      "[1983,     1] loss: 0.0000003859191\n",
      "[1984,     1] loss: 0.0000004191272\n",
      "[1985,     1] loss: 0.0000000747050\n",
      "[1986,     1] loss: 0.0000002901739\n",
      "[1987,     1] loss: 0.0000002433165\n",
      "[1988,     1] loss: 0.0000001411602\n",
      "[1989,     1] loss: 0.0000003236991\n",
      "[1990,     1] loss: 0.0000003793752\n",
      "[1991,     1] loss: 0.0000007608120\n",
      "[1992,     1] loss: 0.0000006990741\n",
      "[1993,     1] loss: 0.0000024982290\n",
      "[1994,     1] loss: 0.0000012852764\n",
      "[1995,     1] loss: 0.0000007304842\n",
      "[1996,     1] loss: 0.0000003078870\n",
      "[1997,     1] loss: 0.0000002038258\n",
      "[1998,     1] loss: 0.0000004561652\n",
      "[1999,     1] loss: 0.0000002892606\n",
      "[2000,     1] loss: 0.0000003725460\n",
      "[2001,     1] loss: 0.0000000710626\n",
      "[2002,     1] loss: 0.0000001783053\n",
      "[2003,     1] loss: 0.0000002604791\n",
      "[2004,     1] loss: 0.0000001967993\n",
      "[2005,     1] loss: 0.0000002509882\n",
      "[2006,     1] loss: 0.0000004575696\n",
      "[2007,     1] loss: 0.0000001008323\n",
      "[2008,     1] loss: 0.0000004057951\n",
      "[2009,     1] loss: 0.0000001656398\n",
      "[2010,     1] loss: 0.0000002376259\n",
      "[2011,     1] loss: 0.0000002870123\n",
      "[2012,     1] loss: 0.0000005048820\n",
      "[2013,     1] loss: 0.0000001413764\n",
      "[2014,     1] loss: 0.0000001100655\n",
      "[2015,     1] loss: 0.0000002965707\n",
      "[2016,     1] loss: 0.0000002991750\n",
      "[2017,     1] loss: 0.0000002292424\n",
      "[2018,     1] loss: 0.0000002139091\n",
      "[2019,     1] loss: 0.0000002610188\n",
      "[2020,     1] loss: 0.0000002039543\n",
      "[2021,     1] loss: 0.0000003451359\n",
      "[2022,     1] loss: 0.0000000777177\n",
      "[2023,     1] loss: 0.0000000642731\n",
      "[2024,     1] loss: 0.0000001627484\n",
      "[2025,     1] loss: 0.0000001628477\n",
      "[2026,     1] loss: 0.0000003239380\n",
      "[2027,     1] loss: 0.0000000456635\n",
      "[2028,     1] loss: 0.0000001387633\n",
      "[2029,     1] loss: 0.0000000658948\n",
      "[2030,     1] loss: 0.0000000396053\n",
      "[2031,     1] loss: 0.0000001887909\n",
      "[2032,     1] loss: 0.0000000328743\n",
      "[2033,     1] loss: 0.0000000550172\n",
      "[2034,     1] loss: 0.0000001298638\n",
      "[2035,     1] loss: 0.0000002362250\n",
      "[2036,     1] loss: 0.0000004290078\n",
      "[2037,     1] loss: 0.0000004184313\n",
      "[2038,     1] loss: 0.0000003899010\n",
      "[2039,     1] loss: 0.0000000870389\n",
      "[2040,     1] loss: 0.0000001801656\n",
      "[2041,     1] loss: 0.0000000880756\n",
      "[2042,     1] loss: 0.0000002865643\n",
      "[2043,     1] loss: 0.0000002785121\n",
      "[2044,     1] loss: 0.0000000892673\n",
      "[2045,     1] loss: 0.0000013217574\n",
      "[2046,     1] loss: 0.0000008292134\n",
      "[2047,     1] loss: 0.0000000711162\n",
      "[2048,     1] loss: 0.0000001378345\n",
      "[2049,     1] loss: 0.0000002671383\n",
      "[2050,     1] loss: 0.0000002805105\n",
      "[2051,     1] loss: 0.0000000594195\n",
      "[2052,     1] loss: 0.0000000994920\n",
      "[2053,     1] loss: 0.0000003302135\n",
      "[2054,     1] loss: 0.0000002592684\n",
      "[2055,     1] loss: 0.0000001581984\n",
      "[2056,     1] loss: 0.0000001896776\n",
      "[2057,     1] loss: 0.0000000400722\n",
      "[2058,     1] loss: 0.0000001580509\n",
      "[2059,     1] loss: 0.0000005542414\n",
      "[2060,     1] loss: 0.0000000645198\n",
      "[2061,     1] loss: 0.0000002338107\n",
      "[2062,     1] loss: 0.0000003026102\n",
      "[2063,     1] loss: 0.0000001676844\n",
      "[2064,     1] loss: 0.0000009701399\n",
      "[2065,     1] loss: 0.0000002568929\n",
      "[2066,     1] loss: 0.0000006532330\n",
      "[2067,     1] loss: 0.0000015396805\n",
      "[2068,     1] loss: 0.0000004519624\n",
      "[2069,     1] loss: 0.0000003435656\n",
      "[2070,     1] loss: 0.0000001024904\n",
      "[2071,     1] loss: 0.0000001177223\n",
      "[2072,     1] loss: 0.0000003201449\n",
      "[2073,     1] loss: 0.0000002076573\n",
      "[2074,     1] loss: 0.0000001097918\n",
      "[2075,     1] loss: 0.0000001578667\n",
      "[2076,     1] loss: 0.0000001778443\n",
      "[2077,     1] loss: 0.0000004202460\n",
      "[2078,     1] loss: 0.0000002941023\n",
      "[2079,     1] loss: 0.0000005977049\n",
      "[2080,     1] loss: 0.0000004152293\n",
      "[2081,     1] loss: 0.0000001352380\n",
      "[2082,     1] loss: 0.0000001303011\n",
      "[2083,     1] loss: 0.0000000824653\n",
      "[2084,     1] loss: 0.0000002416748\n",
      "[2085,     1] loss: 0.0000000666759\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2086,     1] loss: 0.0000001566673\n",
      "[2087,     1] loss: 0.0000001225435\n",
      "[2088,     1] loss: 0.0000000404870\n",
      "[2089,     1] loss: 0.0000001820883\n",
      "[2090,     1] loss: 0.0000001087013\n",
      "[2091,     1] loss: 0.0000000563562\n",
      "[2092,     1] loss: 0.0000003286812\n",
      "[2093,     1] loss: 0.0000001577519\n",
      "[2094,     1] loss: 0.0000002098422\n",
      "[2095,     1] loss: 0.0000000920379\n",
      "[2096,     1] loss: 0.0000001750255\n",
      "[2097,     1] loss: 0.0000000690249\n",
      "[2098,     1] loss: 0.0000001325870\n",
      "[2099,     1] loss: 0.0000002913896\n",
      "[2100,     1] loss: 0.0000001408151\n",
      "[2101,     1] loss: 0.0000000589077\n",
      "[2102,     1] loss: 0.0000002587638\n",
      "[2103,     1] loss: 0.0000000651240\n",
      "[2104,     1] loss: 0.0000000568128\n",
      "[2105,     1] loss: 0.0000000452201\n",
      "[2106,     1] loss: 0.0000000667416\n",
      "[2107,     1] loss: 0.0000001525673\n",
      "[2108,     1] loss: 0.0000000464179\n",
      "[2109,     1] loss: 0.0000000859870\n",
      "[2110,     1] loss: 0.0000000384403\n",
      "[2111,     1] loss: 0.0000000445033\n",
      "[2112,     1] loss: 0.0000004578857\n",
      "[2113,     1] loss: 0.0000005743516\n",
      "[2114,     1] loss: 0.0000003339247\n",
      "[2115,     1] loss: 0.0000006592823\n",
      "[2116,     1] loss: 0.0000001564304\n",
      "[2117,     1] loss: 0.0000001570327\n",
      "[2118,     1] loss: 0.0000005086224\n",
      "[2119,     1] loss: 0.0000002351897\n",
      "[2120,     1] loss: 0.0000001968311\n",
      "[2121,     1] loss: 0.0000001403527\n",
      "[2122,     1] loss: 0.0000002219158\n",
      "[2123,     1] loss: 0.0000000837934\n",
      "[2124,     1] loss: 0.0000001240632\n",
      "[2125,     1] loss: 0.0000001951873\n",
      "[2126,     1] loss: 0.0000001806723\n",
      "[2127,     1] loss: 0.0000002824702\n",
      "[2128,     1] loss: 0.0000006957097\n",
      "[2129,     1] loss: 0.0000006366616\n",
      "[2130,     1] loss: 0.0000011164245\n",
      "[2131,     1] loss: 0.0000013867472\n",
      "[2132,     1] loss: 0.0000007006839\n",
      "[2133,     1] loss: 0.0000004536301\n",
      "[2134,     1] loss: 0.0000008329482\n",
      "[2135,     1] loss: 0.0000000916607\n",
      "[2136,     1] loss: 0.0000001180239\n",
      "[2137,     1] loss: 0.0000002340246\n",
      "[2138,     1] loss: 0.0000002012546\n",
      "[2139,     1] loss: 0.0000017848926\n",
      "[2140,     1] loss: 0.0000003966232\n",
      "[2141,     1] loss: 0.0000003060629\n",
      "[2142,     1] loss: 0.0000002357027\n",
      "[2143,     1] loss: 0.0000001703062\n",
      "[2144,     1] loss: 0.0000002200055\n",
      "[2145,     1] loss: 0.0000001068911\n",
      "[2146,     1] loss: 0.0000001567649\n",
      "[2147,     1] loss: 0.0000000834375\n",
      "[2148,     1] loss: 0.0000000647233\n",
      "[2149,     1] loss: 0.0000002505285\n",
      "[2150,     1] loss: 0.0000005738257\n",
      "[2151,     1] loss: 0.0000001892810\n",
      "[2152,     1] loss: 0.0000003857953\n",
      "[2153,     1] loss: 0.0000007985436\n",
      "[2154,     1] loss: 0.0000025880099\n",
      "[2155,     1] loss: 0.0000009888886\n",
      "[2156,     1] loss: 0.0000013242770\n",
      "[2157,     1] loss: 0.0000003858632\n",
      "[2158,     1] loss: 0.0000002625432\n",
      "[2159,     1] loss: 0.0000000887270\n",
      "[2160,     1] loss: 0.0000000820022\n",
      "[2161,     1] loss: 0.0000001125372\n",
      "[2162,     1] loss: 0.0000001510376\n",
      "[2163,     1] loss: 0.0000002234765\n",
      "[2164,     1] loss: 0.0000001356114\n",
      "[2165,     1] loss: 0.0000005004522\n",
      "[2166,     1] loss: 0.0000008890855\n",
      "[2167,     1] loss: 0.0000025356447\n",
      "[2168,     1] loss: 0.0000011791088\n",
      "[2169,     1] loss: 0.0000004612386\n",
      "[2170,     1] loss: 0.0000002200882\n",
      "[2171,     1] loss: 0.0000001753627\n",
      "[2172,     1] loss: 0.0000000769775\n",
      "[2173,     1] loss: 0.0000000681642\n",
      "[2174,     1] loss: 0.0000000876532\n",
      "[2175,     1] loss: 0.0000000692223\n",
      "[2176,     1] loss: 0.0000000617667\n",
      "[2177,     1] loss: 0.0000000427524\n",
      "[2178,     1] loss: 0.0000001322987\n",
      "[2179,     1] loss: 0.0000001503783\n",
      "[2180,     1] loss: 0.0000000796547\n",
      "[2181,     1] loss: 0.0000001366181\n",
      "[2182,     1] loss: 0.0000001213519\n",
      "[2183,     1] loss: 0.0000003695910\n",
      "[2184,     1] loss: 0.0000001442578\n",
      "[2185,     1] loss: 0.0000001438144\n",
      "[2186,     1] loss: 0.0000000611942\n",
      "[2187,     1] loss: 0.0000000942658\n",
      "[2188,     1] loss: 0.0000000739050\n",
      "[2189,     1] loss: 0.0000001208759\n",
      "[2190,     1] loss: 0.0000002039097\n",
      "[2191,     1] loss: 0.0000001923743\n",
      "[2192,     1] loss: 0.0000000673511\n",
      "[2193,     1] loss: 0.0000001749651\n",
      "[2194,     1] loss: 0.0000000919318\n",
      "[2195,     1] loss: 0.0000001298344\n",
      "[2196,     1] loss: 0.0000001109711\n",
      "[2197,     1] loss: 0.0000002501860\n",
      "[2198,     1] loss: 0.0000000704307\n",
      "[2199,     1] loss: 0.0000001186200\n",
      "[2200,     1] loss: 0.0000000412619\n",
      "[2201,     1] loss: 0.0000000759997\n",
      "[2202,     1] loss: 0.0000000960662\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-f78d2d14b44c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# a single value, same as loss.item(), which is the mean loss for each mini-batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                   \u001b[0;31m# performs one back-propagation step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m                  \u001b[0;31m# update the network parameters (perform an update step)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)    #0.001) #0.0001)\n",
    "#torch.optim.LBFGS(net.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 20000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = net(inputs[j][0],inputs[j][1])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = net(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set)[0]):\n",
    "                   net_test_set[k] = net(test_set[k][0],test_set[k][1])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch  \n",
    "#        print(i % 10)\n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.13f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loss1 = train_loss\n",
    "test_loss1  = test_loss\n",
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[:80],train_loss1[:80],'blue',label = 'Training loss')\n",
    "# plt.plot(x[:80],test_loss1[:80],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss1[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss1[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch for $H_2$ training and test set',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.ylim([0,0.001])\n",
    "\n",
    "plt.savefig('loss_graph_H2_G_feat',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-306.3376,   -3.4165,    0.7210,  -30.5914,   -3.9754,  -32.7153])\n",
      "output\n",
      "tensor([0.0131], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "x1, x2 = test_set[0]\n",
    "x1 = x1\n",
    "print(x1)\n",
    "\n",
    "output = net(x1, x2)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-597.0193)\n",
      "tensor(-615.7928)\n",
      "tensor(-597.1568)\n",
      "tensor(-698.4772)\n",
      "tensor(-597.0146)\n",
      "tensor(-727.1431)\n",
      "tensor(-597.0345)\n",
      "tensor(-739.4082)\n",
      "tensor(-670.2426)\n",
      "tensor(-651.5956)\n",
      "tensor(-658.2557)\n",
      "tensor(-702.3465)\n",
      "tensor(-727.5401)\n",
      "tensor(-640.6017)\n",
      "tensor(-621.3918)\n",
      "tensor(-622.5107)\n",
      "tensor(-649.6316)\n",
      "tensor(-605.6471)\n",
      "tensor(-626.6551)\n",
      "tensor(-734.7863)\n",
      "tensor(-620.4340)\n",
      "tensor(-629.3580)\n",
      "tensor(-691.1891)\n",
      "tensor(-650.5927)\n",
      "tensor(-665.8497)\n",
      "tensor(-607.7446)\n",
      "tensor(-615.6875)\n",
      "tensor(-605.6397)\n",
      "tensor(-663.0187)\n",
      "tensor(-701.2945)\n",
      "tensor(-739.3383)\n",
      "tensor(-727.7437)\n",
      "tensor(-607.2624)\n",
      "tensor(-713.6720)\n",
      "tensor(-616.0054)\n",
      "tensor(-597.0157)\n",
      "tensor(-627.6926)\n",
      "tensor(-616.8139)\n",
      "tensor(-605.9640)\n",
      "tensor(-597.0270)\n",
      "tensor(-606.3174)\n",
      "tensor(-653.9921)\n",
      "tensor(-606.6846)\n",
      "tensor(-621.7081)\n",
      "tensor(-605.7650)\n",
      "tensor(-654.1271)\n",
      "tensor(-318.3680)\n",
      "tensor(-650.9316)\n",
      "tensor(-630.9034)\n",
      "tensor(-605.2818)\n"
     ]
    }
   ],
   "source": [
    "prediction = torch.zeros(data_size-training_set_size)\n",
    "for i in range(data_size-training_set_size):\n",
    "    x1, x2 = test_set[i]\n",
    "#     print('x1',x1)\n",
    "#     print('x2',x2)\n",
    "    with torch.no_grad():\n",
    "        prediction[i] = net(x1, x2)\n",
    "    print(prediction[i]*var_lab+mean_lab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-597.0193, -615.7928, -597.1568, -698.4772, -597.0146, -727.1431,\n",
      "        -597.0345, -739.4082, -670.2426, -651.5956, -658.2557, -702.3465,\n",
      "        -727.5401, -640.6017, -621.3918, -622.5107, -649.6316, -605.6471,\n",
      "        -626.6551, -734.7863, -620.4340, -629.3580, -691.1891, -650.5927,\n",
      "        -665.8497, -607.7446, -615.6875, -605.6397, -663.0187, -701.2945,\n",
      "        -739.3383, -727.7437, -607.2624, -713.6720, -616.0054, -597.0157,\n",
      "        -627.6926, -616.8139, -605.9640, -597.0270, -606.3174, -653.9921,\n",
      "        -606.6846, -621.7081, -605.7650, -654.1271, -318.3680, -650.9316,\n",
      "        -630.9034, -605.2818])\n"
     ]
    }
   ],
   "source": [
    "print(prediction*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([450])\n",
      "-646.61847\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels*var_lab+mean_lab,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-622.7420)\n",
      "tensor(-666.1892)\n",
      "tensor(-738.7323)\n",
      "tensor(-640.6372)\n",
      "tensor(-613.5397)\n",
      "tensor(-712.3762)\n",
      "tensor(-607.0988)\n",
      "tensor(-597.0161)\n",
      "tensor(-612.7322)\n",
      "tensor(-605.7790)\n",
      "tensor(-579.7433)\n",
      "tensor(-370.1789)\n",
      "tensor(-674.2429)\n",
      "tensor(-645.6144)\n",
      "tensor(-611.7500)\n",
      "tensor(-615.2167)\n",
      "tensor(-664.0031)\n",
      "tensor(-739.3055)\n",
      "tensor(-597.0143)\n",
      "tensor(-721.9595)\n",
      "tensor(-694.3843)\n",
      "tensor(-630.5059)\n",
      "tensor(-597.0184)\n",
      "tensor(-638.2246)\n",
      "tensor(-607.0460)\n",
      "tensor(-687.6047)\n",
      "tensor(-640.3032)\n",
      "tensor(-736.4626)\n",
      "tensor(-635.6883)\n",
      "tensor(-701.6996)\n",
      "tensor(-614.4746)\n",
      "tensor(-605.6935)\n",
      "tensor(-674.7762)\n",
      "tensor(-597.0319)\n",
      "tensor(-597.0179)\n",
      "tensor(-656.0615)\n",
      "tensor(-659.4647)\n",
      "tensor(-699.9034)\n",
      "tensor(-739.1063)\n",
      "tensor(-608.4914)\n",
      "tensor(-739.4136)\n",
      "tensor(-674.5972)\n",
      "tensor(-615.9201)\n",
      "tensor(-597.0303)\n",
      "tensor(-606.0771)\n",
      "tensor(-681.5898)\n",
      "tensor(-726.9847)\n",
      "tensor(-687.3089)\n",
      "tensor(-726.8185)\n",
      "tensor(-614.6493)\n",
      "tensor(-597.0420)\n",
      "tensor(-649.8845)\n",
      "tensor(-608.1406)\n",
      "tensor(-739.2824)\n",
      "tensor(-619.9593)\n",
      "tensor(-696.2150)\n",
      "tensor(-631.2510)\n",
      "tensor(-613.0256)\n",
      "tensor(-597.0153)\n",
      "tensor(-739.3434)\n",
      "tensor(-734.9869)\n",
      "tensor(-676.2380)\n",
      "tensor(-606.7424)\n",
      "tensor(-632.1423)\n",
      "tensor(-606.1288)\n",
      "tensor(-725.9901)\n",
      "tensor(-738.1425)\n",
      "tensor(-617.5721)\n",
      "tensor(-678.4904)\n",
      "tensor(-726.2206)\n",
      "tensor(-622.8505)\n",
      "tensor(-611.2696)\n",
      "tensor(-739.4225)\n",
      "tensor(-597.0184)\n",
      "tensor(-627.0400)\n",
      "tensor(-646.0287)\n",
      "tensor(-714.5616)\n",
      "tensor(-639.1233)\n",
      "tensor(-597.0143)\n",
      "tensor(-700.1744)\n",
      "tensor(-658.6988)\n",
      "tensor(-597.0576)\n",
      "tensor(-639.6191)\n",
      "tensor(-702.2272)\n",
      "tensor(-605.7741)\n",
      "tensor(-619.6218)\n",
      "tensor(-671.1307)\n",
      "tensor(-720.3676)\n",
      "tensor(-609.0302)\n",
      "tensor(-738.7292)\n",
      "tensor(-631.4106)\n",
      "tensor(-559.9898)\n",
      "tensor(-727.8033)\n",
      "tensor(-632.9794)\n",
      "tensor(-727.8212)\n",
      "tensor(-710.7996)\n",
      "tensor(-609.2098)\n",
      "tensor(-605.9060)\n",
      "tensor(-739.4221)\n",
      "tensor(-614.9979)\n",
      "tensor(-659.4299)\n",
      "tensor(-616.0497)\n",
      "tensor(-597.0143)\n",
      "tensor(-638.4782)\n",
      "tensor(-701.7305)\n",
      "tensor(-720.9033)\n",
      "tensor(-597.0333)\n",
      "tensor(-728.1909)\n",
      "tensor(-633.5563)\n",
      "tensor(-597.0164)\n",
      "tensor(-650.2387)\n",
      "tensor(-597.6772)\n",
      "tensor(-597.0254)\n",
      "tensor(-719.6320)\n",
      "tensor(-597.0142)\n",
      "tensor(-717.0554)\n",
      "tensor(-708.8297)\n",
      "tensor(-621.5552)\n",
      "tensor(-726.4969)\n",
      "tensor(-614.1264)\n",
      "tensor(-668.4822)\n",
      "tensor(-605.7241)\n",
      "tensor(-720.5999)\n",
      "tensor(-393.5602)\n",
      "tensor(-611.4957)\n",
      "tensor(-739.0012)\n",
      "tensor(-655.2720)\n",
      "tensor(-632.5808)\n",
      "tensor(-597.0153)\n",
      "tensor(-606.2190)\n",
      "tensor(-667.5176)\n",
      "tensor(-629.9841)\n",
      "tensor(-739.3170)\n",
      "tensor(-646.9521)\n",
      "tensor(-597.0332)\n",
      "tensor(-606.0970)\n",
      "tensor(-738.9812)\n",
      "tensor(-597.0233)\n",
      "tensor(-605.8787)\n",
      "tensor(-702.2705)\n",
      "tensor(-739.4209)\n",
      "tensor(-608.3546)\n",
      "tensor(-614.1264)\n",
      "tensor(-620.5906)\n",
      "tensor(-699.0444)\n",
      "tensor(-597.0142)\n",
      "tensor(-620.7051)\n",
      "tensor(-701.6956)\n",
      "tensor(-698.4139)\n",
      "tensor(-683.2398)\n",
      "tensor(-665.0129)\n",
      "tensor(-605.7771)\n",
      "tensor(-710.8221)\n",
      "tensor(-625.5859)\n",
      "tensor(-738.8516)\n",
      "tensor(-734.5014)\n",
      "tensor(-653.5693)\n",
      "tensor(-739.4230)\n",
      "tensor(-607.0358)\n",
      "tensor(-664.7216)\n",
      "tensor(-719.5732)\n",
      "tensor(-632.8679)\n",
      "tensor(-659.8062)\n",
      "tensor(-728.9906)\n",
      "tensor(-712.8919)\n",
      "tensor(-674.5798)\n",
      "tensor(-488.3557)\n",
      "tensor(-739.4235)\n",
      "tensor(-626.1050)\n",
      "tensor(-739.4211)\n",
      "tensor(-698.5986)\n",
      "tensor(-726.1630)\n",
      "tensor(-632.0726)\n",
      "tensor(-635.1053)\n",
      "tensor(-656.3210)\n",
      "tensor(-632.3254)\n",
      "tensor(-671.1083)\n",
      "tensor(-645.4352)\n",
      "tensor(-597.3054)\n",
      "tensor(-636.8204)\n",
      "tensor(-729.6418)\n",
      "tensor(-606.2281)\n",
      "tensor(-515.6821)\n",
      "tensor(-620.6470)\n",
      "tensor(-635.1522)\n",
      "tensor(-641.6046)\n",
      "tensor(-619.8337)\n",
      "tensor(-727.1220)\n",
      "tensor(-731.1775)\n",
      "tensor(-634.5283)\n",
      "tensor(-739.4037)\n",
      "tensor(-606.4745)\n",
      "tensor(-667.6394)\n",
      "tensor(-719.6991)\n",
      "tensor(-635.0113)\n",
      "tensor(-737.5680)\n",
      "tensor(-597.0216)\n",
      "tensor(-727.7697)\n",
      "tensor(-739.2991)\n",
      "tensor(-613.6251)\n",
      "tensor(-597.0248)\n",
      "tensor(-644.0427)\n",
      "tensor(-597.0510)\n",
      "tensor(-642.3628)\n",
      "tensor(-597.0162)\n",
      "tensor(-597.0184)\n",
      "tensor(-630.3330)\n",
      "tensor(-597.0383)\n",
      "tensor(-608.6265)\n",
      "tensor(-649.7863)\n",
      "tensor(-398.4519)\n",
      "tensor(-597.0178)\n",
      "tensor(-639.8113)\n",
      "tensor(-597.0174)\n",
      "tensor(-652.9240)\n",
      "tensor(-597.0251)\n",
      "tensor(-739.3778)\n",
      "tensor(-640.3755)\n",
      "tensor(-608.3465)\n",
      "tensor(-681.2124)\n",
      "tensor(-723.8081)\n",
      "tensor(-648.6744)\n",
      "tensor(-488.8384)\n",
      "tensor(-609.1642)\n",
      "tensor(-609.1716)\n",
      "tensor(-629.1681)\n",
      "tensor(-653.7708)\n",
      "tensor(-644.5138)\n",
      "tensor(-720.8715)\n",
      "tensor(-619.1078)\n",
      "tensor(-638.0334)\n",
      "tensor(-617.7084)\n",
      "tensor(-712.0612)\n",
      "tensor(-739.0250)\n",
      "tensor(-619.2693)\n",
      "tensor(-677.7021)\n",
      "tensor(-738.9606)\n",
      "tensor(-606.8022)\n",
      "tensor(-737.9986)\n",
      "tensor(-597.0151)\n",
      "tensor(-625.3574)\n",
      "tensor(-632.5616)\n",
      "tensor(-457.7724)\n",
      "tensor(-670.2863)\n",
      "tensor(-605.8108)\n",
      "tensor(-692.1376)\n",
      "tensor(-628.1310)\n",
      "tensor(-597.0182)\n",
      "tensor(-673.2411)\n",
      "tensor(-328.8717)\n",
      "tensor(-730.6705)\n",
      "tensor(-598.1770)\n",
      "tensor(-606.1301)\n",
      "tensor(-732.7213)\n",
      "tensor(-723.9810)\n",
      "tensor(-608.1897)\n",
      "tensor(-597.0206)\n",
      "tensor(-727.1965)\n",
      "tensor(-730.7177)\n",
      "tensor(-726.6360)\n",
      "tensor(-628.9851)\n",
      "tensor(-597.0428)\n",
      "tensor(-672.7629)\n",
      "tensor(-597.0160)\n",
      "tensor(-607.0602)\n",
      "tensor(-735.9766)\n",
      "tensor(-651.0859)\n",
      "tensor(-631.8035)\n",
      "tensor(-606.3543)\n",
      "tensor(-617.1901)\n",
      "tensor(-710.3777)\n",
      "tensor(-739.1710)\n",
      "tensor(-739.2244)\n",
      "tensor(-739.4218)\n",
      "tensor(-663.5818)\n",
      "tensor(-607.6426)\n",
      "tensor(-727.0696)\n",
      "tensor(-616.3567)\n",
      "tensor(-650.3235)\n",
      "tensor(-606.2459)\n",
      "tensor(-671.8366)\n",
      "tensor(-597.0865)\n",
      "tensor(-606.0458)\n",
      "tensor(-605.8840)\n",
      "tensor(-635.2098)\n",
      "tensor(-626.9578)\n",
      "tensor(-695.1240)\n",
      "tensor(-695.1089)\n",
      "tensor(-638.6937)\n",
      "tensor(-738.8963)\n",
      "tensor(-638.6063)\n",
      "tensor(-674.0350)\n",
      "tensor(-682.6263)\n",
      "tensor(-678.6289)\n",
      "tensor(-656.6019)\n",
      "tensor(-650.2889)\n",
      "tensor(-654.1423)\n",
      "tensor(-720.1235)\n",
      "tensor(-605.7800)\n",
      "tensor(-737.8295)\n",
      "tensor(-606.6501)\n",
      "tensor(-597.0214)\n",
      "tensor(-736.1047)\n",
      "tensor(-493.4417)\n",
      "tensor(-688.3079)\n",
      "tensor(-597.0146)\n",
      "tensor(-641.4765)\n",
      "tensor(-551.3948)\n",
      "tensor(-642.2796)\n",
      "tensor(-664.8287)\n",
      "tensor(-733.1785)\n",
      "tensor(-671.7203)\n",
      "tensor(-626.9529)\n",
      "tensor(-739.4233)\n",
      "tensor(-739.3412)\n",
      "tensor(-637.5630)\n",
      "tensor(-613.4548)\n",
      "tensor(-378.2737)\n",
      "tensor(-710.8456)\n",
      "tensor(-508.3605)\n",
      "tensor(-738.8083)\n",
      "tensor(-572.9808)\n",
      "tensor(-665.3969)\n",
      "tensor(-733.6947)\n",
      "tensor(-642.2688)\n",
      "tensor(-597.0197)\n",
      "tensor(-608.3557)\n",
      "tensor(-654.5772)\n",
      "tensor(-597.1236)\n",
      "tensor(-617.6343)\n",
      "tensor(-653.6496)\n",
      "tensor(-700.0469)\n",
      "tensor(-725.1602)\n",
      "tensor(-608.2241)\n",
      "tensor(-710.8406)\n",
      "tensor(-597.0854)\n",
      "tensor(-597.0146)\n",
      "tensor(-698.1964)\n",
      "tensor(-650.8584)\n",
      "tensor(-597.0677)\n",
      "tensor(-631.9319)\n",
      "tensor(-611.6798)\n",
      "tensor(-618.4733)\n",
      "tensor(-585.4118)\n",
      "tensor(-597.0143)\n",
      "tensor(-597.0142)\n",
      "tensor(-633.3145)\n",
      "tensor(-716.5293)\n",
      "tensor(-737.8885)\n",
      "tensor(-667.6057)\n",
      "tensor(-739.0727)\n",
      "tensor(-612.9753)\n",
      "tensor(-677.0159)\n",
      "tensor(-654.8438)\n",
      "tensor(-644.6401)\n",
      "tensor(-636.3193)\n",
      "tensor(-607.9380)\n",
      "tensor(-639.4152)\n",
      "tensor(-607.2359)\n",
      "tensor(-632.5663)\n",
      "tensor(-739.0238)\n",
      "tensor(-597.0197)\n",
      "tensor(-641.8405)\n",
      "tensor(-679.6721)\n",
      "tensor(-607.7381)\n",
      "tensor(-663.0321)\n",
      "tensor(-726.8671)\n",
      "tensor(-607.8025)\n",
      "tensor(-725.2831)\n",
      "tensor(-597.1682)\n",
      "tensor(-611.9753)\n",
      "tensor(-699.6721)\n",
      "tensor(-611.9152)\n",
      "tensor(-647.7784)\n",
      "tensor(-607.6783)\n",
      "tensor(-597.0187)\n",
      "tensor(-701.9830)\n",
      "tensor(-608.8821)\n",
      "tensor(-739.3813)\n",
      "tensor(-700.7966)\n",
      "tensor(-655.0583)\n",
      "tensor(-726.6585)\n",
      "tensor(-726.5737)\n",
      "tensor(-641.4305)\n",
      "tensor(-633.1490)\n",
      "tensor(-739.3243)\n",
      "tensor(-639.8503)\n",
      "tensor(-739.4188)\n",
      "tensor(-726.8820)\n",
      "tensor(-678.8878)\n",
      "tensor(-653.2131)\n",
      "tensor(-605.7260)\n",
      "tensor(-738.9774)\n",
      "tensor(-633.1276)\n",
      "tensor(-607.1999)\n",
      "tensor(-605.6874)\n",
      "tensor(-586.5106)\n",
      "tensor(-681.9819)\n",
      "tensor(-597.0312)\n",
      "tensor(-664.3672)\n",
      "tensor(-725.7576)\n",
      "tensor(-605.3926)\n",
      "tensor(-737.8746)\n",
      "tensor(-646.0720)\n",
      "tensor(-606.2661)\n",
      "tensor(-546.4862)\n",
      "tensor(-726.4368)\n",
      "tensor(-641.4572)\n",
      "tensor(-722.5544)\n",
      "tensor(-682.0471)\n",
      "tensor(-597.0154)\n",
      "tensor(-648.8363)\n",
      "tensor(-612.6465)\n",
      "tensor(-610.8856)\n",
      "tensor(-597.0209)\n",
      "tensor(-617.0245)\n",
      "tensor(-472.2439)\n",
      "tensor(-646.5375)\n",
      "tensor(-654.4172)\n",
      "tensor(-363.2242)\n",
      "tensor(-662.0293)\n",
      "tensor(-583.3735)\n",
      "tensor(-630.8489)\n",
      "tensor(-636.5476)\n",
      "tensor(-607.2467)\n",
      "tensor(-683.6348)\n",
      "tensor(-606.1671)\n",
      "tensor(-738.4974)\n",
      "tensor(-739.3565)\n",
      "tensor(-597.0155)\n",
      "tensor(-606.0524)\n",
      "tensor(-605.7691)\n",
      "tensor(-606.2845)\n",
      "tensor(-597.0698)\n",
      "tensor(-605.8058)\n",
      "tensor(-624.8380)\n",
      "tensor(-650.7505)\n",
      "tensor(-615.5812)\n",
      "tensor(-597.1297)\n",
      "tensor(-614.2706)\n",
      "tensor(-686.3527)\n",
      "tensor(-673.8047)\n",
      "tensor(-608.0189)\n",
      "tensor(-739.4149)\n",
      "tensor(-727.3477)\n",
      "tensor(-715.0508)\n",
      "tensor(-597.0142)\n",
      "tensor(-661.2276)\n",
      "tensor(-597.0168)\n",
      "tensor(-635.9998)\n"
     ]
    }
   ],
   "source": [
    "predictiont = torch.zeros(training_set_size)\n",
    "for i in range(training_set_size):\n",
    "    x1, x2 = training_set[i]\n",
    "#     print('x1',x1)\n",
    "#     print('x2',x2)\n",
    "    with torch.no_grad():\n",
    "        predictiont[i] = net(x1, x2)\n",
    "    print(predictiont[i]*var_lab+mean_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-35-24f531ab5161>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  prediction = torch.tensor(prediction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0245)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAesAAAEiCAYAAADZFPrdAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABLCElEQVR4nO3de7xM5f7A8c/XnShFnE6uSUWIyFEdIXQ93fudFElyTVcVOboeKYmKcglJl126nC6KbmLThXaRO5V7QknEdrf39/fHs4Yxe2bvGWbWzN77+3695rX3PGvNWt951pr5zlrrWc8jqooxxhhjUleRZAdgjDHGmNxZsjbGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNSXEzJWpxVIqIicnKsKxORf4tIp1hfF+M6JojI94lcR4T1PuLVS+CxXkT+JyK1ErjOet66WgaVxfT+471NwsVUmInIQyLyq4hki8iEBK3jGRHZHGHahyIy+wiXLyIyX0RuOpLl5Bd+fE9560n4vpHH+sO+T7+/QxNZD35ty8MhIiNE5MVo5y8W4/LPBmp4/7cDHovx9f8GKgITYnxdfvEXcJH3/0nAAOALETldVXf4FMMAoHQM8xf0bZI0ItIEeBT4D5AO/J6gVdUHFuYy7fMjXP6/gWOB149wOflFwj8TPu4buYn0PmP9DjlsPtRDKn+/PQUsE5EnVHV5XjPHmqyvB3YAi7z/Y03WBd1+VQ0cxcwWkbXAl8AlwNuhM4tIUaCoqu6NVwCquiJeyyrIElH3YZzm/R2hqtuOZEF5xFuf8PvX0UB1IifyaN0BvKqq+45wOQXKEe5Dcds34s3n75CUrYdQ8f7OUNXVIvIV0BO4J5oXRPUAigIbcb+uuwEKNAgz33nAdCATd6SZDjTC/bLRkMcj3mvSgXdCltPSm6ee9/xsYBKwHveDYR7QPsz6JwDf5/I+bgb2AOVDyk/31tfa+/8T4E9vXUuBXnnUzyPAHyFlpb1l3hccG3AlsBjYBzT3pv0TmAHsBDYDY4FyIcu7FfjFi+lDoK23/Ja5vf/D2SbxjCmXOst1+UH11RZY4K3jK+D0WJYTRd3fFvQe3vf2AcXtg5cC2UDNkOXV9Movj/DewtVtS2/av3EJdI+33oFAsWjjDZmvkrfs7mGmnRvttshlG53sLaNRyGcl7Ocj2voKen+XAku8bTcZOM5b53Rv2d8T8j1zhK+NZp+L9D0Vbps8HOv+kce+kU7e34WBOHL9XBzB9/EEcn6H5LrPxhJTNPUQ5bbKNSfk8R5jqefg7d08hvjyzCO4RL0RKJLnZzGGD23gS/hy3IdiL/BEmDe7D/gMuAZ3SngA8C+gFjANmAs08x5VYqi4dkBf3FHq+cCDXgzXh/sg5/I+jvF2uJtDyv8L/Ib7UbIC9+G/BPfFfStwfx718wg5k3Ud7z3cGBTbH8BPQAevTqvgvlT3AG9667wR+DW4ToArvGWNAi4EHsd9aHJN1kewTeIWU4T6imb5E3CnxeYB1+H2vZ9wHxyJdjl51P1VXrwjgAu8/WB14D14+8M6gn7EeMt71NtfikV4f7W8elaglVe3R3vrUOBlb1v08eIfHU28YdbTxlveRUD5kMdd3rQK0X7Owyy/O+6LvkhQWcTPR7T1FbRt5wBXe+9xC/AO7guyO3Cxt+2XBLb3kbw2mn2F3D8T4bZJ9Vj3j0j7RgzfhYH3P48In4sj/OxP4NDvkDz32WhjiqEeotlWueaEPN5jtPUc9jMYZXx55hHgDG+dZ+T5WYzhQzse94Eo4T2fDKwK2Tlm4T4skTbOO0B6mPI8Ky5kmuBO4b8ATAvzJRcxWXvzfAB8ElL2I/A87vqGAvVj/FJ7xNuwxbzHKbhftNuAE4JiU6BhyGu/BKaHlJ0fsuNkAB+HzDOWvJP14W6TuMUUYb3RLH8CsB+oHTTPld48p0W7nDzq/jtgckjZyOD3gLvcc2Bf9/a/1cCQPN5jJ285ZYPKZoeJtw+QRVAyjhRvmHX0JufRQ/BjfSz7cZjljwG+C3qe5+cjmvoK2ra1gsoGe8vuGFR2iVdW50hfG8O+EukzEWkfinn/CLdveOXpRJdEcv1ceGWH+9mfwKHfIXnus9HGFEM9RLWtgqaFzQm5vMdo6znsZzCv+Igyj3gx7we65vVZjKo1uIiUxB2BvKcHz9e/gWts1syb5yjgH8DL6kURTyJyrIgMF5E1uF+L+3Cn4085jMW9CbQWkYresht6y3kTd8riF2C0iFwnIpViWG6FoNh+xDUyu05VNwTN86uqzgt6X2Vwp3PeEpFigQfuFNI+oLF3raQR7kdGsHdzC+Zwt0kiY4p2+UGzr1bVn4OeL/H+VolxOZCz7osCDXGn0oKFPh+PO4Jq6T1v5T1/Ka/3Gsxb35nkvL78Ju7OjLNzizeC+sBaL6bQx1LcKcnA+quKyBcislREFovIYBGRPJb/N9yP0IBoPh/R1tdqPfT6aKCRzbQwZSceyWsPY1+JJNw2icv+EaOInwuI3/dxjPtsrjHFsM6otlWcc0IkObZ3lPFFlUdUdT+wFfc5y1W0t25djDutNkVEyotIedwvkz24hmbgWosKsCHM6+NhAu70ylO40zJn4T4kpQ5jWZNwlXq19/w63CmMr1Q121v+Rm/5G0XkSxFpFMVy//LiaoLbQWuo6sch8/wW8vxY3KnDkRzc4fbh6rY4UBU4HvcLLLSlZF4tJw93myQypmiXH7A15LWBH4ulYlwO5Kz7wHvYFFJ+yHNVXYnb32/2im4GMlR1ccR3GF5FL67QOALPj8sj3nDqA3NVNT30AZzAoY3L9gN9VbUO7ofWPzj4GYikFK4+AYjm8xFDfW0Neb43THnw9j6S18a6r0SSY5vEcf+IxdaQ56H1FK/v41j22bxiila022oC8csJkYT7DOYZX4x5ZE80MUfbGjyQkHO0OAX+LSJ3406RZ+O+IGK1GygRUnZgJxCRUrjGJLep6uig8sPq1EVVM0VkMm5Dj8E1nngr8AtUVZcB14hIcaA58CQwWUSqeBshkv2qmtf9iaG/crd6ZY8AU8LMvx6XPPbjGhMFy+uo/3C3SSJjinb50Yh1OaF1H3gPx4eUhz4HGAeMFZF+uASXd+vNnP7AfahD66iy9/fPkPJcj4i8/b8u7pJU6LRquB/YB5K1d4Zng/f/XhFZQN5J6k9CfvVH+fmIR33F01bis89F2ibxer+5fhfG4Ei+j4PFus/Gw1by2FZxyAnR1nO47Z1nfBBTHilPFPWY5xsTkbK4BglvkPM0W2/cRmul7j7ib4GOuZxa20v4XxDrONiEP6Bt0P8lcb9kDvzCF5FyuEYMh2si0EJELsOdrp4YOoOq7lPVacDTuJ2+/BGsLyyv3mYDp6rq92Ee61U1C9dw44qQl+d6VHS42ySRMUW7/LyWEY/l5PIewu1X7+LqaiLuc5Njf4ki3ixco6j/C5n0b9wX66wYF1kbd8dBuFuz6nt/F4SZhohUwF1T/DSPdfyIa9mcQx6fjyOur3iKcV+J9D2Vm3i937y+C6NyhN/HwcuJ9z6bpyi3VbQ54XBzzpHGFzx/xM+JiBwPlME1YstVNEfWV3gLG6aq3wZPEJGvgf64I++pwP3e349FZAyuufrZuMYKHwHLgCtE5EpcZa333th7wC0i8gzuKKEVrnVx4M3+JSLfAQ+JyDbcTnI/7rTz0VG8h3Am45rcvwCsUtUM7z01AIbgrsmsxJ3y6AvMV9VE/IoE11jjCxHJxjWI2A5Uw/1y7K+qP+FaWr8rIqNw9dWCgx2w5OZwt0kiY4r2PfuxnMB7eB53eeRc77Xg9jMAVHW3iKQBvYA3VHVrlPGFehj4VERewn2h18e1iB2rqutiXFYgIUdK1lm469aH8NqgvAM8q6o5pof4Gve5O15VN0X7+YhjfcVTtPtKpM9ERHF8v7l+F8bocD/7oeK5z0Yrz20VZU44rJxzpPHhfiBEk0ea4I7Sv8lzjZpHCzTgI+CnXKaPxJ1yKek9bwHMxCXCrbgW0Q29aRW9SvqToHvevGn9cBfktwOv4X4hBbfMOxnXeGQHrkFNH8LfLjWBPFqDB837mreOJ4LKKgGvehW8G3fN4Q2gWh7LyhFLmHkixoa7fvgJrvX4DlzjjKeBY4LmuQ23w+3EnX4J3FLRMrd1HME2iUtMudRHrsuP8F5qeMv/V4xx5lb3t4e8h/8jfKvfwG1SbaLcvzoRvqXrdbgEu9dbb8T7rPNY/qPALlxHDaHT0oAlYcqL4r5cno7yPZTA3UMauP0w6s9HbvUVYdvmqK8I2/tIXhvNvhL2M5HXNoll/4i0b3jT8vouDPf+c7zXw/3sR1h+rvtsLDHFUA95fT/kmRMivcfDredo4yPKzwkwjJBW5ZEegVsNjDEeEXkA9+v4OFXdFVQ+GPelVVNzb7uQskRkHC5hd9YoP/wiMgw4WVUvzXPmQ1+X7+srFoXt/Zoj47W0X4O79/q1vOaPtbtRYwoU75pRP9wRx05cQ5C+wIuBRC0ip+IacvUEHs2vX8Qici5wC6674B+8S5njVXV4Hi99CvhRRE7RKC5PFJT6ilZhe78mbv4Pd2YsqvYNdmRtCjUROQZ3eqop7vTVBlyXug+q1xe2iKTjTnlNwp0OTmR/4ilJRNoBG1R1RhTzplOI6quwvV8THyJyPe4+7plRzW/J2hhjjElth3WfsjHGGGP8Y9esD1PFihW1Ro0ayQ7jEDt27OCoo45KdhhJZ/VwkNWFY/VwULLrYs6cOX+oariOh0wuLFkfpho1avD993l1Vuav9PR0WrZsmewwks7q4SCrC8fq4aBk14XXl7eJkZ0GN8YYY1Kcr0fWXs9Jf8d1kbhJVUMHUDDGGGNMiIQfWYtIORHpKSIzcV3BLcfd57lRRH4RkbEiclai4zDGGGPyq4QeWXujcT2A63JtEq6LuvW4G8GPww3S3Rz4XERmA7froeOh5iv79u1j3bp17N69OynrP+aYY1i6NK+unpOrVKlSVKlSheLFiyc7FGOMyTcSfRr8HKCFqi6KMD0DGC8iPXA9K7UA8m2yXrduHeXKlaNGjRpEHugmcbZv3065cuV8X2+0VJXNmzezbt06atYMO5CTMcaYMBKarFU1dFi1SPPtwQ0Ikq/t3r07aYk6PxARKlSowKZN1lTBGGNiYa3B48wSde6sfowxJnaJvmY9Kdp5VTV00HBjjDGpZM8eeOIJuOYaqF8/7/lN3CT6mvXmBC/fBNm8eTPNmzcHYOPGjRQtWpTjj3cdBWVkZFCiRIlcX5+enk6JEiU455xzjiiOrVu38vrrr3Prrbce0XKMMSnk66+ha1dYuhSKFbNk7bNEX7O+OZHLz+/S0qB/f1i7FqpVg4EDoX37w19ehQoVmDdvHgCPPPIIZcuW5d5774369enp6ZQtWzYuyXrkyJGWrI0pCLZtg379YORIqF4dpkyBiy9OdlSFjl2zTpK0NOjWDdasAVX3t1s3Vx5Pc+bMoUWLFjRu3JgLL7yQDRs2ADB8+HDq1q1LgwYNaNeuHatXr2b06NE888wzNGzYkC+//PKQ5cyYMYOGDRvSsGFDGjVqxPbt2wF46qmnOOuss2jQoAEPP/wwAPfffz8rVqygYcOG3HffffF9Q8YY/0yaBHXrwqhRcNddsGiRJeok8bsHs8pAL9xA7QosAUaq6m9+xpEK+veHnTsPLdu505UfydF1MFXl9ttv54MPPuD444/nzTffpH///owfP55BgwaxatUqSpYsydatWylfvjw9evSIeDQ+ZMgQRowYwbnnnktmZialSpXis88+4+effyYjIwNV5fLLL2fmzJkMGjSIRYsWHTjKN8bkMxs3wh13wNtvu9Pd774LTZsmO6pCzbdkLSLnAp8AvwGzvOL2wN0icqGqzor44gJo7drYyg/Hnj17WLRoEW3btgUgKyuLE044AYAGDRrQvn17rrzySq688so8l3XuuefSu3dv2rdvz9VXX02VKlX47LPP+Oyzz2jUqBEAmZmZ/Pzzz1SrVi1+b8IY4x9VGD8e7r0Xdu1y1+buuw+sE6Ok8/PIegjwBtBDVbMBRKQIMBoYiutApdCoVs2d+g5XHi+qyumnn86sWTl/B02ePJmZM2cyadIkBgwYwOLFi3Nd1v3338+ll17KlClTaNasGVOnTkVV6devH927dz9k3tWrV8fvTRhj/PHzz9C9O0yfDi1awJgxcMopyY7KePy8Zt0QGBpI1ADe/08DjXyMIyUMHAhlyhxaVqaMK4+XkiVLsmnTpgPJet++fSxevJjs7Gx++eUXWrVqxeDBg9m6dSuZmZmUK1fuwLXoUCtWrKB+/fr07duXJk2asGzZMi688ELGjx9PZmYmAL/++iu///57rssxxqSYfftg0CBo0ADmznVJeto0S9Qpxs9k/RcQro/JmsBWH+NICe3bu89E9eog4v6OGRO/69UARYoU4Z133qFv376cccYZNGzYkG+++YasrCw6dOhA/fr1adSoEXfffTfly5fnsssu47333gvbwOzZZ5+lXr16nHHGGZQuXZqLL76YCy64gBtuuIGzzz6b+vXrc+2117J9+3YqVKjAueeeS7169ayBmTGp7Pvv4ayzXGvvSy91t2V17QpFrO1xylFVXx7As8CvuOvUNYEaQAdgHfB0HNczAFgAzAM+A/4eNK0fbtSvH4ELg8obAwu9acMByWs9jRs31lBLlizJUeanbdu2JXX90Up0PU2fPj2hy89PrC4cq4eDpk+frpqZqdq7t2qRIqonnKD67ru+rR/4Xn3KOwXp4efPpz7AO8B4LymuBMYBbwP3x3E9T6lqA1VtCHwEPAQgInWBdsDpwEXASBEp6r1mFNANqO09LopjPMYYkzKOzciAevXg6afd/aJLl8JVVyU7LJMH3xqYqepe4E4R6QfUAgRYrqo7c39lzOvZFvT0KNwtYgBXABPVDRqySkSWA01FZDVwtHqt0UXkFeBK4ON4xmWMMUn1xx9w992c8dprcOqpMHMmeD0emtTn933Wf8O1+q6Eu17+z8DADqoat1G3RGQg0BF3nbyVV3wiMDtotnVe2T7v/9DycMvthjsCp3LlyqSnpx8y/Zhjjklqw6qsrKx80bBr9+7dOeounjIzMxO6/PzE6sIp1PWgSuWpU6k1YgTFduxg+XXXsbFzZ7KzsqCw1kk+5Od91h1wp70F2MLBI168/6NO1iIyFfhbmEn9VfUDVe0P9PeO4m8DHvbWG0pzKc9ZqDoGGAPQpEkTbdmy5SHTly5dmtTxpFN9POuAUqVKHbg3OxHS09MJ3TaFldWFU2jrYdUq6NkTPv0U/vEPGDeO9X/8UTjrIp/z88h6IDAY+K+q7j+SBalqmyhnfR2YjEvW64CqQdOqAOu98iphyo0xJn/KyoLhw+GBB1zL7ueec0m7aFE7ms6n/GxgdjQw4UgTdV5EpHbQ08uBZd7/k4B2IlJSRGriGpJlqOoGYLuINBN3Tr4j8EEiYzTGmISZPx+aNYPevaFVK1i8GG67zSVqk2/5mazTgEt9WM8gEVkkIguAC4A7AVR1MfAWrj/yT4BeqprlvaYn7hT9cmAF1rjMGJPf7Nrl7pdu3Nh1jzhxInz4YXy7RTRJ4+dp8N7A+yLSGndP877giar633isRFWvyWXaQNzp+NDy74F68Vi/Mcb4bvp0dxvW8uXQqRMMGQIVKsR9NfEe1tdEz88j6+64+5fPAa4C/i/oca2PcRRYAwYMYNiwYQee9+/fn+HDh+f6mr/++otTTz2VH3/8EYDrr7+esWPHJjROY0ycbNkCXbrA+edDdjZ8/jm89FLCErUfw/qa8Pw8sn4QuEdVn/Fxnclz110Q7yEiGzaEZ5+NOLljx4507NiRO++8k+zsbCZOnMi0adNo2LBh2Plff/116taty/PPP0+nTp2488472bJlC127do1v3MaY+FKF//3PXYv+4w/o0wcefjjngANx5MewviYyP5N1UVwjL5Mg1atXp0KFCvzwww/89ttvNGrUiOrVq+c5rnTbtm15++236dWrF/Pnz/cnWGPM4Vm3Dnr1gkmToFEjmDIFzjwz4av1Y1hfE5mfyfolXL/gcbk2nfJyOQJOpC5dujBhwgQ2btxI586d2b59O80j9FIUOLLOzs5m6dKllC5dmj///JMqVaqEnd8Yk0TZ2TB6NNx/P+zfD4MHw913QzF/vsb9GNbXROZnsi4DdBGRC3EDbYQ2MLvDx1gKrKuuuoqHHnqIffv28frrr1O0aNE8j6yfeeYZ6tSpw+OPP07nzp2ZNWsWxW2weWNSR2A0rK+/hjZt4IUX4KSTfA1h4EB3jTr4VHi8h/U1kfmZrOsAP3j/nxYyLWyPYSZ2JUqUoFWrVpQvX56iUdxX+dNPPzFu3DgyMjIoV64c5513Ho899hiPPvqoD9EaY3K1Z48ba/rxx6FsWZgwATp2dOPq+ixwXdpagyeHnwN5tMp7LnOksrOzmT17Nm+//XZU859yyiksXbr0wPOnn346UaEZY2Ixa5Zr6b1kCVx/vbu0VqlSUkNq396Sc7Ik/NYtERkqIs1FxEYzT7Bly5Zx8skn07p1a2rXrp33C4wxqWfbNtfK+9xzYft2mDwZXn896YnaJJcfR9ZlgDeAkiIyGXgf+FRVd/mw7kLltNNOY+XKlckOwxhzuD76yPXh/euvcPvt8NhjkA8G5zGJl/CjXVXtqapVcF2N/go8BvwhIpNEpLOIHJ/oGPykapffc2P1Y0wYv/0G110Hl10G5cu7U+DDhlmiNgf4dmpaVTNUtb+q1gPOAGYAnYB1IvKViNwrImHHkc4vSpUqxebNmy0hRaCqbN68mVKlSiU7FGNSgyqMHw916sD778OAATBnjhvO0pggfrYGP0BVlwNDgaHekfVluBGyAIYkI6Z4qFKlCuvWrWPTpk1JWf/u3btTPhGWKlXK7uM2Blw/3t27w7Rp0Lw5jBkDp4XeKGOMk5RkHUxVNwHjvUe+Vrx4cWrWrJm09aenp9OoUaOkrd8YE4V9++Dpp+GRR6BECXfPdJcubtxpYyJIaLIWkai7F1XVy/Oeyxhj8rE5c1xinjcPrr4annsO/v73ZEdl8oFEH1lvTvDyjTEm9e3Y4QbaeOYZqFzZDcJx9dXJjsrkIwlN1qp6cyKXb4wxKe+zz6BHD1i1yvXX+eSTrsW3MTHw9Zq1iBQDmgLVgBJBk1RVX/UzFmOMSajNm6F3b3jlFTjlFJgxA847L9lRmXzKt2QtIqcBHwI1AQGyvPXvA/YAlqyNMfmfKrzxBtx5J2zdCg884DrUTvE7NUxq87P54bPAHOAYYCduYI8mwDzgGh/jMMaYxFizBi691HWgXasWzJ3r7p22RG2OkJ/J+izgMVXdAWQDxVR1LtAHd8+1McbkT1lZbqCN00+HmTPd/19/DfXrJzsyU0D4ec1acEfUAJuAE4EfgXXAyT7GYYwx8bNggbsd67vv4JJLYORIqF492VGZAsbPI+tFuG5GATKAviLSAngUWO5jHMYYc+R273bXohs3htWr3chYH31kidokhJ9H1gOBo7z/HwA+AqYDfwD/9jEOY4w5MjNmuNuwfvoJbroJhg6FChWSHZUpwHxL1qr6adD/K4G6InIcsEVt5AtjTH6wZQv06QPjxkHNmu4e6rZtkx2VKQSS2hmtqv5pidoYk/JUXa9jdeu6UbLuuw8WLbJEbXzjW7IWkYEi0iNMeQ8RGeBXHMYYE5Nff4WrroJrr4UTToCMDBg8GMqUSXZkphDx88j6RuCHMOVzgI4+xmGMMXnLzobRo93R9KefugSdkeEalBnjMz8bmFXC3bIVajNQ2cc4jDEmd8uWQdeu8NVXcP75bhjLk+0OU5M8fh5ZrwWahyk/D3evtTHGJNfeva7HsTPOgMWL4aWXYOpUS9Qm6fw8sn4BeEZESgDTvLLWwBPAkz7GYYwxOc2a5Y6mFy+G666DYcPccJbGpAA/b90aKiIVgeEcHHFrLzAMeMqvOIwx5hDbt7vOTZ5/Hk48ET78EP71r2RHZcwhfB0iU1X7ichjQF1c96NLVDXTzxiMMeaAyZOhZ09Ytw569YLHH4dy5ZIdlTE5+HnrVlcAVd2hqt+pakYgUYvIaL/iMMYYfvsNrr/eHUGXK+cG3XjuOUvUJmX52cBssIjkGApTRMYAF/sYhzGmsFKFCROgTh1491149FH44Qc4++xkR2ZMrvxM1tcC40WkdaDAS9QXAa3ivTIRuVdE1LtOHijrJyLLReRHEbkwqLyxiCz0pg0XEYl3PMaYJFuxwvU4dvPN7t7pefPgoYegRIk8X2pMsvmWrFX1C6Az8I6I/ENExgIXAi29vsLjRkSqAm1xt4sFyuoC7YDTcT8QRopIUW/yKKAbUNt7XBTPeIwxSbR/P1XfeMONLf3ddzBqlBtzuk6dZEdmTNT8bmD2P2/wjpnABqCFqq5OwKqeAfoAHwSVXQFMVNU9wCoRWQ40FZHVwNGqOgtARF4BrgQ+TkBcxhg/zZ0LXbpQ64cf4IorYMQI1+LbmHwmoclaRIZHmPQbsBDoHTjjrKp3xGmdlwO/qur8kLPZJwKzg56v88r2cWinLIHycMvuhjsCp3LlyqSnp8cj5LjJzMxMuZiSwerhoMJaF0V276bGhAlUfftt9pYvz8L77yfzggvg55/doxArrPtEfpfoI+v6EcpXAGWDpsc08paITAX+FmZSf+A/wAXhXhamTHMpz1moOgYYA9CkSRNt2bJlNOH6Jj09nVSLKRmsHg4qlHUxdaq7DWvlSujalZJPPknm/PmFrx4iKJT7RAGQ0GStqnFvOOYtt024chGpD9QEAkfVVYC5ItIUd8RcNWj2KsB6r7xKmHJjTH6yeTPccw+8/DLUrg3Tp4MlJVNAJHU863hT1YWqWklVa6hqDVwiPlNVNwKTgHYiUlJEauIakmWo6gZgu4g081qBd+TQa93GmFSmCm+84RqMpaXBf/4D8+dbojYFSqKvWddU1VVRzitAFVX9JRGxqOpiEXkLWALsB3qpapY3uScwASiNa1hmjcuMyQ/WrnU9kE2ZAmed5U6BN2iQ7KiMibtEH1nPEpEXRSRijwMicqyI9MQl0SviuXLvCPuPoOcDVbWWqp6qqh8HlX+vqvW8abepakzX0I0xPsvKguHD3f3SM2bAM8+4gTgsUZsCKtENzE7DNfqaLCJZwBzcLVu7gWNxfYTXATKAu1T10wTHY4zJ7xYudKNjffstXHSRu2+6Ro1kR2VMQiX0yFpVt6rqfbhboXoCy4DyuEZg+4GXgUaqeq4lamNMrnbvhgcfhDPPdL2RpaW509+WqE0h4EunKKq6C3jHexhjTGxmzoRu3eDHH6FjRxg6FCpWzPt1xhQQBao1uDGmgPnrL+jRA1q0gL174dNP3a1ZlqhNIWPJ2hiTmt57z92ONXYs9O7trlVfEK6/I2MKPkvWxpjUsn49XHMNXH01VK7sGpINHQpHHZXsyIxJGkvWxpjUkJ0NY8a427GmTIFBgyAjA5o0SXZkxiSdb8laRN4XkX+JiP1AMMYc6scfoVUr6N7dtfZeuBD69oXixZMdmTEpwc/EuQN4E1gnIo+LSG0f122MSUV798LAga4zk4UL4cUX4Ysv4OSTkx2ZMSnFt2Stqu2BE4ABQBvgRxGZKSIdRaS0X3EYY1LEt99C48bwwANw5ZWwZAl07gwSbiA8Ywo3X09Jq+o2VR2lqk1xw2POAV4ANorICyJSx894jDFJkJkJd94JZ58NW7fCpEnw5pvwt3Cj3hpjIEkNzETk77h+wP+F68nsHdzwlQtE5N5kxGSM8cGUKXD66fDcc27M6cWL4bLLkh2VMSnPzwZmxUXkWhGZAqwBrgQGAyeo6i2qegnQHnjAr5iMMT75/Xe44Qa49FIoWxa++sol7KOPTnZkxuQLvnQ36tkACPA6cL+qLggzz+fAFh9jMsYkkiq88orr1GT7dnj0UdfKu2TJZEdmTL7iZ7K+G3hbVXdHmkFVt+AG+TDG5HcrV7pbsaZOhXPOcT2R1a2b7KiMyZf8bA3+am6J2hhTQOzfD0OGQL16rsX3iBHw5ZeWqI05Ar4dWYvI+AiTFDe+9XLgTVVd71dMxpg4++EH6NIF5s51DcdGjoQqVZIdlTH5np+nwY8HmgPZwCKvrB7uOvYc4GrgvyLSXFXn+RiXMeZI7dzprkcHhq586y249lq7Z9qYOPEzWX8NZAK3qOpOABEpA4wF5gOXAK8AQ4HWPsZljDkSX3zhxppeuRJuuQWeegqOPTbZURlToPh5n/WdwH8DiRrA+38gcLeq7gWeBBr6GJMx5nD9+afrcaxNGyhSBKZNg3HjLFEbkwB+JuuyuO5GQ/3NmwawDX+P9o0xsVKFiRPdWNOvvAL33w8LFriBOIwxCeFnYnwPeFFE+gDf4RqWNcV1jPKuN09T4CcfYzLGxGLtWrj1Vpg82Q1d+dlncMYZyY7KmALPz2TdA3gaeC1ovfuB8UCgi9GlQFcfYzLGRCMrC0aNgn793LjTTz8Nt98OxexEmDF+8O2T5l2f7iEi9wC1cK3Al6vqjqB55vkVjzEmSosXu9uxZs+GCy90Sbum9V1kjJ98uWbt9Qv+rYicqqo7VHWBqs4PTtTGmBSzZw88/DA0agQ//+yuT3/8sSVqY5LAlyNrVd0nIjVx16mNManuq6+ga1dYtgw6dHCnvY8/PtlRGVNo+dka/GXserQxqW3bNteArHlz2LULPvkEXn3VErUxSeZn65CjgPYi0hbXY9khp8BV9Q4fYzHGhPrgA5eoN26Eu++G//7XDWdpjEk6P5N1HWCu9/9JIdPs9LgxybJhA9xxB7zzDjRoAO+/D2edleyojDFB/GwNbj0mGJNKVOHFF+Hee2H3bnjiCbjnHihePNmRGWNC2E2SxhRGP/3k+vOeMQNatoQxY6B27WRHZYyJwM8GZojIxSIyWUSWikhVr6yLiNjAHcb4Yd8+ePxxd7p7/nzXl/e0aZaojUlxviVrEWkPvIXrTrQGEDjXVhTo41ccxhRaGRmui9D+/eHyy2HpUjdKlg1jaUzK8/PIug/QVVXvxnUzGjCbOI60JSKPiMivIjLPe1wSNK2fiCwXkR9F5MKg8sYistCbNlzEvr1MAZKZ6Vp3n302bN7sWn2/9Rb87W/JjswYEyU/k3VtYFaY8kzg6Div6xlVbeg9pgCISF2gHXA6cBEwUkSKevOPArp5Mdb2phuT7x2XkQH16sGwYdCjByxZ4o6qjTH5ip/Jej1wSpjy84AVPqz/CmCiqu5R1VXAcqCpiJwAHK2qs1RVgVeAK32Ix5jE2bQJ2renQd++UKYMfPkljBgBR8f7d7Exxg9+tgYfAwwXkS7e86oi0hw3ROYjcV7XbSLSEfgeuEdVtwAn4k65B6zzyvZ5/4eW5yAi3XBH4FSuXJn09PQ4h31kMjMzUy6mZCjU9aBK5c8/5+QRIyi6cyfLr7+eDZ06ofv2QWGtEwr5PhHC6iJ/8vM+68EicgzwOVAKmA7sAYao6ohYliUiU4FwF9z6405pD8B1tDIAGAp0xo3ylSOsXMrDvYcxuB8dNGnSRFu2bBlL2AmXnp5OqsWUDIW2Hlatgu7d4fPP3fXpsWNZv2lT4ayLEIV2nwjD6iJ/8vU+a1XtLyIDgbq4U/BLVDXzMJbTJpr5RGQs8JH3dB1QNWhyFdyp+XXe/6HlxuQP+/fD8OHw4INQpIg73d2jh/vfjqCMKRB8vc8a3LjWqvq9qmYcTqLOi3cNOuAqYJH3/ySgnYiU9EYAqw1kqOoGYLuINPNagXcEPoh3XMYkxLx50KyZ63ns/PNdA7Jbb3WJ2hhTYPh6ZC0i1wGtgUqE/FBQ1Xg1UR0sIg1xp7JXA9295S8WkbeAJbhbx3qpapb3mp7ABKA08LH3MCZ17doFjz4KQ4ZAhQowcSL8+992z7QxBZRvyVpEngLuwl2rXk+CBu9Q1RtzmTYQGBim/HugXiLiMSbupk1zXYWuWAGdO8NTT8FxxyU7KmNMAvl5ZN0RuF5V3/FxncYUHH/+CffdB+PHQ61a8MUX7tS3MabA8/PCVhFgno/rM6ZgUHU9jtWpAy+/DH37wsKFlqiNKUT8TNZjgA4+rs+Y/O+XX+CKK+C666BqVfjuOxg0CEqXTnZkxhgf+XkavDxwg4i0BRbgOiM5QFXv8DEWY1JbdjaMGgX33w9ZWa4h2Z13QjEb1daYwsjPT35dDp4GPy1kWkIamxmTLy1eDF27wqxZ0LYtjB4NJ52U7KiMMUnkZw9mrfxalzH50p498MQTbrzpcuXglVegQwe7HcsY42+nKCJysYh8JCJLRKSqV9ZFRFr7GYcxKefrr6FRI3fv9P/9nxtr+sYbLVEbYwAfk7WItAfeAn4GagLFvUlFcWNdG1P4bNsGvXrBP/8JO3bAlCmQlgaVKiU7MmNMCvHzyLoP0FVV78b1IBYwG2joYxzGpIZJk6BuXdeQ7K673LXqiy9OdlTGmBTkZ7KuDcwKU54J2CC7pvDYuNF1DXrFFa7nsdmz4ZlnoGzZZEdmjElRfibr9cApYcrPA1b4GIcxyaEKL77oOjeZNAkGDoQ5c6Bp02RHZoxJcX7eujUGGC4iXbznVUWkOTAYeMTHOIzx388/u7Gmp0+HFi1gzBg4JdxvV2OMycnPW7cGi8gxwOdAKdyAHnuAIao6wq84jPHVvn0wdKhr5V2ypEvSt9xiQ1gaY2Lia3dIqtpfRAbiOkgpAixJxJjWxqSE775znZvMnw/XXAPPPQcnnJD364wxJoTvP+9Vdaeqfq+qGZaoTYG0Ywfccw80awabNsH778M771iiNsYcNuto2Jh4+vRT6NEDVq+Gnj1dj2THHJPsqIwx+ZxdODMmHv74Azp2hIsuglKl4MsvYeRIS9TGmLiwZG3MkVCF116D006DiRPhoYdg3jzXI5kxxsSJnQY35nCtXu1OeX/6qbs+PW4cnH56sqMyxhRASTmyFpFSIlIlTLl905nUl5Xlehw7/XQ3AMdzz8FXX1miNsYkjO/JWkSuAn4CJovIYhH5R9DkV/2Ox5iYzJ/vjqJ794ZWrWDJErjtNihaNNmRGWMKsGQcWT8ENFbVM4CbgPEicoM3zcYDNKlp1y7o1w8aN4Y1a9z16Q8/hKpVkx2ZMaYQSMY16xKquglAVb8XkfOAd0XkZECTEI8xuZs+Hbp1g+XL4eabYcgQNwCHMcb4JBlH1r+LSIPAE1XdDLQF6gANIr7KGL9t2QJdusD550N2Nnz+OYwfb4naGOO7ZCTrG4HfgwtUda+qXg+0SEI8xhxKFd5+242ONWEC9OkDCxdCmzbJjswYU0gl/DS4iJRT1e2B56q6LtK8qvp1ouMxJlfr1kGvXm4IyzPPhI8/hkaNkh2VMaaQ8+Oa9VYR+RmYE/SYG5zAjUm67GwYPRruvx/274ennoK77oJi1hWBMSb5/PgmugFoDDQBHgSOBlRElhOUwFV1hg+xGJPTkiVudKxvvnGnul94AU46KdlRGWPMAQlP1qr6JvAmgIgIkIVL2scAZwL/AY4F7EZV4689e2DQIBg4EMqVc9enO3YEsTsIjTGpxe/xrNXlaz5S1QWBchGp5mccxvDNN+5oeskSaNcOhg2DSpWSHZUxxoSVEgN5qOraZMdgColt21yPY//8J2zfDpMnwxtvWKI2xqS0lEjWxvjiww9d/90jR8Ltt8PixXDJJcmOyhhj8pTwZC0ir4rInSJyjoiU8YqtpzITd2lpUKMGnH9+C8qWhSJF3OXnyvIbb8p1cPnlLNlQnk8enuVOe5crl+yQjTEmKn5csz4RuBQoj2tcBjBQRGZw8DaubT7EYQqwtDTXI+jOnQDCjh0Ays28xBDu5Sh20J/HeCrrPuTxEow/Gdq3T27MxhgTrYQfWavq+ap6HHAy7jauwUBpoB8wDdgiIj/Fc50icruI/OiN6jU4qLyfiCz3pl0YVN5YRBZ604Z7rdZNPtK/fyBRO7VYzlTaMJ5bWEh9GrCAx+nPPkqwd6+b3xhj8gvfWoOr6kpgJfB2oExEauDuvz4zXusRkVbAFUADVd0jIpW88rpAO+B04O/AVBE5RVWzgFFAN2A2MAW4CPg4XjGZxFvrNVEsxj568zSP8Ah7KUE3XmAcXdCQ36VrrUmjMSYfSWoDM1VdrarvqOp/4rjYnsAgVd3jrSPQD/kVwERV3aOqq4DlQFMROQE4WlVnqaoCrwBXxjEekwCB69Mi7tq0KpzJHDJoypPczxQuoQ5LGUu3HIkaoJrdLGiMyUcKYl+KpwDNRWQgsBu4V1W/w107nx003zqvbJ/3f2h5DiLSDXcETuXKlUlPT4978EciMzMz5WKKp6lTK/HccyezbVtxr8RdrSitO3iUh7mbZ/iNylzN/3iPqyMup1ixbDp0WEZ6+u8R5ykoCvo+ES2rh4OsLvKnfJmsRWQq8Lcwk/rj3tOxQDPgLOAtETmJwDf7oTSX8pyFqmOAMQBNmjTRli1bxhx7IqWnp5NqMcVLWhoMHgz79h1a3pbPGE0PTmIVo+nO/QziL8pHXE6FCjBsWBHat68L1E1ozKmgIO8TsbB6OMjqIn/Kl8laVSOOVSgiPYF3vVPaGSKSDVTEHTFXDZq1CrDeK68SptykkP79D03Ux7GZp+nNTbzCMk7lPGbwJefleJ3aTYLGmAKgIHaK8j5wPoCInAKUAP4AJgHtRKSkiNQEagMZqroB2C4izbxW4B2BD5ISuQHcUXS5cu56dOCa9Jo1ganK9bzOMk7jBl5nAA/QkHlhE7UxxhQUCT2yFpHx0c6rqp3jtNrxwHgRWQTsBW7yjrIXi8hbwBJgP9DLawkOrlHaBNwtZR9jLcGTJi0Nbrzx0CPiwP/VWMNoenAxnzCbf9CVsSyifsRltW6d4GCNMcYniT4NfnzI8/OAbGCh97we7uh+ZrxWqKp7gQ4Rpg0EBoYp/96LxSRRuEQNUIQsbuN5BuJujr6DYYygF9kRB2pTWrcWpk5NbLzGGOOXhJ4GV9XLAg/gG+BToIqqnqeq5+GuIX8CfJvIOEzqu/VW6NAhZ6KuzwK+4RyGcRczaME/yy/mOe44kKgrVIDXXnOvCzymT59hidoYU6D42cDsDqC1qu4IFKjqDhEZAHxBmCNeUzikpcGoUYeWlWQ3DzKAPgxmC8dyPa/zdpF27N9incsZYwofPxuYlcX1HBbqBKBMmHJTCKSlwU03HVp2HjNYQAP68zhptKcOS5nI9XTrLgc6QylSxP1NS0tG1MYY4y8/k/X/gJdEpJ2I1PAe7YAXgXd9jMOkiFtvddeos7xmfuXZwhi6MoOWFGM/bfmMm5nAn1SgdWs491w3WMeaNe5095o17rklbGNMQefnafCewFBcq+tAF1T7ccn6Xh/jMCng1luDT30rV/Muz3MblfidwdzHIzzCLu+Ey2uvuRGyatQ4dLAOcM/797cRtIwxBZufA3nsAm4VkfuAWriew5YHX8M2hUNaGowe7f7/O7/yPLdxFe8zl0b8i4+YS+MD8/bseTARRxp8wwblMMYUdL53iqKqO1R1garOt0RdOPXvD2g23RnNEupyEZ/QhydpSsaBRF2kiEvUI0cefF2kwTdsUA5jTEHna7IWkYtFZLKILBGRql5ZFxGx7isKkdJrljGDFoymJ99xFvVYxFP0IYtiiLjT3llZhyZqgIEDoUxIU8QyZVy5McYUZL4laxFpD7wF/ATU5OB166JAH7/iMP4I22p7714YMIB5nMHpLKYTL9GWz1lJrQOv69Ej8vXn9u1hzBioXt11Q1q9untu16uNMQWdn0fWfYCuqno3rmFZwGygoY9xmARLS8vZanvsLbPZWutMeOghNjS7msall/IynQgMeiaS87R3OO3bw+rVkJ3t/lqiNsYUBn4m69rArDDlmcDRPsZhEqx//4OttsuynWHcwbQ957Bzw1/w4YfUmPUGj42tfMgR8quv5p2ojTGmsPLz1q31wCnAmpDy84AVPsZhEizQOvsSJjOKnlRhHc9zGw9kDWTbv8oB7ojYjoqNMSY6fh5ZjwGGi8i53vOqInITMBgYFfllJhWlpUHFigeHsaxY8WDnJGee+Buvcz2T+RfbOJpz+IY7GU6JCuWs9zFjjDkMft5nPVhEjgE+B0oB04E9wBBVHeFXHObIpaVBp06wP6jlwebNcHMn5aQZE/h6yz3ADh5gAIPpwz5KULw4bN/u5oODvY+BHWEbY0xe/GwNXg14EKgINAWa4YbQfMibZvKBtDTo2PHQRA1wEiuYsr8tZ4/tTMlGp/PZ4Pm8Vv0B9ksJKlRwt2Lt3XvoawK9jxljjMmdn6fBVwEVVXWnqn6vqhmqmgkc500zKSpwG5aI68s7O/vgtKLs5z4Gs5D6nMV39GA0zJjBZfedxurVruHYrl2HviaY9T5mjDF587OBmQAaprwssNvHOEwMArdhBVp3B4833Yi5jKMLZ/ID73Elt/E8xaufeMhPwOCW4eFY72PGGJO3hCdrERnu/avAEyIS/NVdFHdKfF6i4zCxSUtziXZNaNt9oDQ7eZSH6c3T/E4lruZ/vMfVALwW0ptYbkfO1vuYMcZEx48j6/reXwHqAMFXLvcCc4EhPsRhohR6NB2sNVN5ge7UYiVj6EofBvMX5YFDB90IqFYtfMIvWtR6HzPGmGglPFmraisAEXkJuFNVtyV6nebwpaXBTTcdHGM64Dg2M5R76MTL/ERtWpDOTFoAUKECDBsWPvEOHJgz8ZcpY4naGGNi4VsDM1W92RJ1agscUR+aqJV2vMFS6tCeNAbyH85gATNpQfXqbtCNP/6w/ryNMSaRfGtgJiIDgV9UdXRIeQ/gRFV90K9YTE7hjqirspZR9ORSppDBWbRhKtuqN2DcwNiSrfVWZowxR8bPW7duBH4IUz4H6OhjHCZE6BF1EbK4neEsoS4tSedunqZ16Vn0fa2BDZ5hjDFJ4OetW5WATWHKNwOVfYzDhAi+vaoeCxlLV5rxLR9zET0ZxbqiNXh5rCVpY4xJFj+PrNcCzcOUnwes8zEOE2LtWijJbgbwAHM5k1qs4AbSuIQpbCpTg5dftkRtjDHJ5OeR9QvAMyJSApjmlbUGngCe9DEOE+LaSjP572/dOI0feZmO3MNQNlPRbq8yxpgU4edAHkNFpCIwHCjhFe8FhqnqYL/iMEG2boW+fXnrtzGslhpcoJ/yORcAdnuVMcakEj9Pg6Oq/XADeTQDzgaOV9X7/YzBeN59F+rWhXHj4J57mD12ET9Vv8BurzLGmBTk52lwAFR1B/Cd3+s1nvXr4bbb4L33oGFDmDQJmjShHdDulmQHZ4wxJpyEJmsRmQR0UNVt3v8RqerliYyl0MvOhrFjoU8f2LuXH64bxP/N6s3KpsWpVs31NGZH0sYYk5oSfWS9mYMjbW1O8LpMJMuWuRupv/wSWrXig0vHcMNDJx+4XWvNGjcZLGEbY0wqSmiyVtWbw/1vfLJ3LwweDAMGwFFHwfjx0KkTd9aUHIN07Nzp7re2ZG2MManH92vWxifffgtdusCiRXDddW6kjcqu75lIw1bmNpylMcaY5En0Nevx0c6rqp3jtM43gVO9p+WBrara0JvWD7gFyALuUNVPvfLGwASgNDAFNzqYksIC402vXcuBa84nnghs3w4PPADPPecKJk2Cyy475LWRhq2sVs2f2I0xxsQm0UfWx4c8Pw/IBhZ6z+vhbh+bGa8Vqup1gf9FZCjwl/d/XaAdcDrwd2CqiJyiqlnAKKAbMBuXrC8CPo5XTPEWOt70mjXQoQNczA5qFalHFf0FufVWePxxOProHK+PNGzlwIE+vQFjjDExSeh91qp6WeABfAN8ClRR1fNU9TygKvAJ8G281y0iAvwbeMMrugKYqKp7VHUVsBxoKiInAEer6izvaPoV4Mp4xxNPwX15AxzP76RxA1P4F9uyy9Ki6Neknf182EQNNmylMcbkN35es74DaO3dZw24e65FZADwBRDv47rmwG+q+rP3/ETckXPAOq9sH4f2TR4oz0FEuuGOwKlcuTLp6elxDjk6a9e2AARQOvIKT9ObsmTyEI/yJH3Zu78kP92zmxNPnB1xGSeeCBMmHFqWpLcTd5mZmUnbNqnG6sKxejjI6iJ/8jNZl8Wdfl4SUn4CUCaWBYnIVOBvYSb1V9UPvP+v5+BRNbjsFkpzKc9ZqDoGGAPQpEkTbdmyZbQhx1W1alBkzUpeoDttmcpXnEtXxrKMOgfm+f33UiQrvmRLT08vtO89lNWFY/VwkNVF/uRnsv4f8JKI3MfBI9xmuEE83o1lQaraJrfpIlIMuBpoHFS8DnfaPaAKsN4rrxKmPDXt38+75zzLaWseYj/FuJURjKYHGnJFwxqLGWNMweFn3+A9gQ9xra5XAiuAl4HJwK1xXlcbYJmqBp/engS0E5GSIlITqA1kqOoGYLuINPOuc3cEPsi5yBTwww/wj39w5hv38XONttSTJYzi1hyJukQJayxmjDEFiW/JWlV3qeqtQAWgIXAmcJyq3qqqO3N9cezacegpcFR1MfAW7jT8J0AvryU4uB8S43CNzlaQai3Bd+6Evn3hrLNg/Xpm3vE25/z2Pr9olZAZlQoVXN8n1ljMGGMKDl87RRGRi4FewEnAhV4Dsy7AKlX9Il7rUdVOEcoHEqYhm6p+j7uNLPV88QV07w4rVrhOTgYPpmOjY9m5K+eslSvvYePGUv7HaIwxJqF8O7IWkfa4I9ufgZpAcW9SUaCPX3HkG3/+CTffDG3auPurpk1zA3Ece2zEnsZ+/72kvzEaY4zxhZ/XrPsAXVX1bmB/UPls3GlxA6AKEydCnTrw6qvQrx8sWACtWh2YJVLjsUqV9vgUpDHGGD/5maxrA7PClGcC4XvvKGzWrnVdg15/vcvIc+a4XshKlz5ktoEDXY9jwcqUgS5dVvoYrDHGGL/4mazXA6eEKT8P16ir0OrVI4s7ijzH9uqns2PydPqWeJrX75gNZ5wRdv5IPZC1afO7z5EbY4zxg58NzMYAw70GZQBVRaQ5MBh4xMc4UsqA6xbR4a2unM1sPuUCejCa1XtrUqQTaJHIrbrbt885zTolMsaYgsnPW7cG4zo/+Rw4CpgOjAZGq+oIv+JIGXv2wEMP0fetM6nNz3TgVS7iE1ZTE4DsbNcHuDHGGOPrrVuq2l9EBgJ1cT8Ulqhqpp8xpISvvoKuXWHZMt6kA715mj9yDFBm40sbY4xxfDmyFpHiIvKtiJyqqjtV9XtVzSiUiXrcOGjeHHbtYlqfT+jIq2ETNViXocYYYxxfkrWq7sPdWx12gIzCIi0N6vW5hEH0peyaRbR56sKI8xYpYl2GGmOMcfxsDf4y0NXH9aWUtDTo3BkWb/k7/RjEDsqiufx0eeUV6zLUGGOM4+c166OA9iLSFpgD7AieqKp3+BiL7/r3h717o5u3enVL1MYYYw7yM1nXAeZ6/58UMq3Anx6PtrFYmTJ2+tsYY8yhfEvWqtoq77kKrmrVYM2a8NOKFnW3alWr5hK1HVUbY4wJlvBr1iJSRkRGiMivIvK7iLwuIhUTvd5UM3CgG2c6VPHi8PLLLlmvXm2J2hhjTE5+NDB7FOgETAYmAm2BUT6sN6W0b+/Gma5Q4WBZhQrw0kuWoI0xxuTOj9PgVwO3qOpEABF5DfhaRIqqapYP608Z4boINcYYY/Lix5F1VeDLwBNVzcANkfl3H9ZtjDHG5Ht+JOuiQOhNS/vxuatTY4wxJr/yI2EK8JqI7AkqKwWMFZGdgQJVvdyHWIwxxph8x49k/XKYstd8WK8xxhhTICQ8WavqzYlehzHGGFOQiebWQbWJSEQ2ARG6OUmaisAfyQ4iBVg9HGR14Vg9HJTsuqiuquGHGjQRWbIuQETke1Vtkuw4ks3q4SCrC8fq4SCri/zJz1G3jDHGGHMYLFkbY4wxKc6SdcEyJtkBpAirh4OsLhyrh4OsLvIhu2ZtjDHGpDg7sjbGGGNSnCVrY4wxJsVZss6HRORNEZnnPVaLyLygaf1EZLmI/CgiFwaVNxaRhd604SIiSQk+zkTkdu+9LhaRwUHlha0eHvHGjA/sF5cETStUdQEgIveKiIpIxaCyQlUPIjJARBZ4+8NnIvL3oGmFqi4KBFW1Rz5+AEOBh7z/6wLzgZJATWAFUNSblgGcjeur/WPg4mTHHof33gqYCpT0nlcqjPXgva9HgHvDlBfGuqgKfIrrtKhiIa6Ho4P+vwMYXVjroiA87Mg6H/N+9f4beMMrugKYqKp7VHUVsBxoKiIn4D64s9R9Il8BrkxGzHHWExikqnsAVPV3r7yw1UNuCmNdPAP0AYJbzxa6elDVbUFPj+JgfRS6uigILFnnb82B31T1Z+/5icAvQdPXeWUnev+Hlud3pwDNReRbEZkhImd55YWtHgJu8057jheRY72yQlUXInI58Kuqzg+ZVKjqIUBEBorIL0B74CGvuFDWRX5nY0qnKBGZCvwtzKT+qvqB9//1HDyqBnfqKpTmUp7ycqsH3P57LNAMOAt4S0ROogDWA+RZF6OAAbj3MwB3eaQzBbAu8qiH/wAXhHtZmLJ8XQ+Q9/eEqvYH+otIP+A24GEKaF0UdJasU5SqtsltuogUA64GGgcVr8NdrwuoAqz3yquEKU95udWDiPQE3vVO2WWISDZukIICVw+Q9z4RICJjgY+8pwWuLiLVg4jUx12Dne+1i6oCzBWRphTAeoDo9wngdWAyLlkXyLoo6Ow0eP7VBlimqsGnrSYB7USkpIjUBGoDGaq6AdguIs2869wdgQ9yLjLfeR84H0BETgFK4EYTKmz1gHe9MeAqYJH3f6GpC1VdqKqVVLWGqtbAJZ8zVXUjhageAkSkdtDTy4Fl3v+Fri4KAjuyzr/acegpcFR1sYi8BSwB9gO9VDXLm9wTmACUxrXy/Ni/UBNmPDBeRBYBe4GbvKPswlYPAINFpCHutOVqoDsUyn0irEJaD4NE5FQgG9cyvgcU2rrI96y7UWOMMSbF2WlwY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjaFkohMEJGP8p7TRENEjhWR30SkVlBZuog8n8B1Jm0bhq5bRN4Rkd7JiMUUDpasTUKJSCMRyRKRrw/jtQn9so9i/dNEJC1M+XUiki0ixyQjrhT1H2CKqq5IdiAiMlpEnvF5tY8CD9g+YRLFkrVJtK7ASKCeiNRJdjAxagR8H6a8CbBcVf/yOZ7DJiIlErjsMkAX4MVErSOGWAS4DJ/7tFbVhcBKoIOf6zWFhyVrkzAiUhq4ARgLvAPcEjJdROQeEflZRPaIyDoRecKbNgFoAfQSEfUeNcIdbYc5JXmRiHwpIltE5E8R+TTWHwre6dzyRE7Wc2JZXtByRUT6iMgKEdklIgtFpEPQ9HQRGSkij4vIHyLyu4gMEZEi0S4jaDmjvNduAr72yo8SkVdEJNM7bd1PRD7y6rCjiGwWkZIhy0oTkUm5vK1LcP1P53r2RERai8hWEeke9D4ibf/D3YZnAaWAr0LqYai3nE0icqe4QSxGePGsFZEbg+IsKSLPevWzW0Rmi8g/o1j3JNywtcbEnSVrk0jXAmtUdQHwKtBRRIoHTX8ceBB4Ajgd+D/gF2/ancAs4CXgBO/xC9E5CngWaAq0BP4CPozx6LIxLgH9EFzoHbk14jCTNfAY7kdLL6Au7r2/ICKXBs3THjfAwjm4MYjvAq6LcRngjvIEaI4bQQncONctcCNznQ+c4U0HeBv3nXBF0Ps9xps3t6Pm5sAczWWgARG5BngP6KaqL3jFuW3/w92GVwKTVXV/UFl7YDvwD2CQt9z3gZ9wP7xeBsaJyN+9+Qfj6rszblsvBD6RQ0c2CycDaOr9SDUmvlTVHvZIyAOYAdzr/S+40aCu8Z6XBXYDPXJ5fTrwfBRlE4CPclnOUUAW8M8YXvMkbgSrSI9W3nxVvZiWAPOBq/OIYxfQPKT8Wdz13sD7mxUy/XNgXLTLCFrOgpB5yuJGJ2sXEtMWYIL3/Hngk6DpPYGNQLFc3tf7wMuRth/QDZdsLwiJJdftfzjbEFgcvA1C69PbDzcBk4LKinv1cq23nr1Ax6DpRYEVwGN5rLuBt2/USvZnzx4F72FDZJqEEJGTgXPxTguqqnqNtboA/8MdEZYEvkjAumsBA3BHUsfjjhaLANViWExjYDLwQEj5pd6y53rP9wN3qeo8EakEzBGRT1R1Z5hl1sWdov1ERIKPQovjfsgELAh53XqgUozLgJxH/7W8+TICBaq6Q9wQowFjgbkiUkXdWOmdcYl4P5GVBn6LMO0K3HCd56nqrKDyXLf/4WxDb587Cfg0ZNKB+vT2w99xR8uBsn0isgVXx4E6+jpoepaIzPJizs0u768dWZu4s2RtEqUL7ohkrTtzDLijGkSkauD/w5Ad5rXFQ55/CPyKSxK/4hLqEiCW0+CNgEGqOi+4UERuIKhxmapuADZ4///ufelXBNaGWWbgstNlYabvi/A/uKO1wGujXQbAjpDngXqLeLpaVeeLyFygk4i8jztNnFejqT+AYyNMW+Ct7xYRma2qgXXntf0PZxteCXyhqqHvO1x9Rqrj3Ooor/GEj/P+bspjPmNiZtesTdyJSDHgJqAf0DDocQbuy/tm3BfvHqB1Lovai0v4wTbhrl8HOyNo3RWAOsDjqjpVVZcC5Yjhh6mI1MR98Ya7Ln1mhHJEpAnuh0Oka+uB91xdVZeHPNZEGd6RLGM5Lkk1DYq5DFAvZL6xQCfcD66vVfXHPJb7A5GPOlfhrjlfAIyRg7/cIm7/I9iGV+BOyR+J5bj97kCDMhEpCpztxZybesB6VY10lsGYw2ZH1iYRLsUdXY5V1c3BE0RkIu466GPAMOAJEdkDzAQqAI1VdZQ3+2pcg50aQCbwJzANeFZELgd+xB15VeXgKeAtuCO9riLyC3Ai8BTuyCxajb2/c8NMa4RrEHUIL8G8AtwSdPR4CFXdLiJDgCFe0pqJu3bbDMhW1TF5BXYky1DVTBEZDzwpIn/gzgg8gPvRHhzzG8DTuO3UI6+YcKednxSRCqHb21vvShFphbt+PEZEunnvI+z2B14gxm0oIsd7dXBtFPFG5F0WGAUM8upoFXA3UBl3C2JumgOfHMn6jYnEjqxNItwCTA/3xY1rcVwdaIM78n4S1yJ4Ke5adpWgeYfgjnKW4I6oqwHjgx5f45L4e4EXqGo2riVvA2ARMMJb/p4Y4m8MrFTVrcGFIlKdMEfc3q1O7wFPqOo3eSz7QeAR4F5cY6jPgWtwSSFaR7KMe4EvcbcZTced6fge19gLcD8IgLdwdf9WXgtUd49xBtAul3lW4I6wL8K1XBcibP/D3IaXAd/F6ai2L+59vwTM8+K4yLvkEZaIlMK1mh8bh/Ubk4NEOAgwxkTBSzqvAz+q6iNJDidm3g+NNcBTqjo0qPxjYJ2qdo1yORfhzpTUVdWshASb+/o/wJ2yH+z3ur319wKuUNULkrF+U/DZaXBjjsy5uKPABSJypVd2o3e0mXJEpBHuenAG7jpwX+/vm97043BnPS4gqC1AXlT1ExEZgTszEu3193j6Gnf6Pln2Abcncf2mgLMja2MKES9ZjwVOxV0Dnoe7F36ON3017lT/QFV9MklhGmNCWLI2xhhjUpw1MDPGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNSnCVrY4wxJsVZsjbGGGNS3P8DC1OzlAo18ewAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "#plt.plot(train_labels*var_lab+mean_lab,predictiont*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy for $H_2$ (symmetry function features)',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2_G_feat',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer 1\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[ 0.1144,  0.3912,  0.2673,  0.1923, -0.0608, -0.0858],\n",
      "        [ 0.4741, -0.0474, -0.6173, -0.2730, -0.2365,  0.0102],\n",
      "        [-0.0073, -0.2544,  0.0384,  0.2952, -0.0694, -0.0316]],\n",
      "       requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([ 0.9993, -0.2053,  2.0866], requires_grad=True)\n",
      "layer 2\n",
      "weights\n",
      "Parameter containing:\n",
      "tensor([[-0.1123, -0.2351, -0.0664],\n",
      "        [ 0.0015, -0.0010, -0.0011],\n",
      "        [ 0.0036, -0.0023, -0.0027]], requires_grad=True)\n",
      "biases\n",
      "Parameter containing:\n",
      "tensor([-0.0650,  0.0004,  0.0009], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print('layer 1')\n",
    "print('weights')\n",
    "print(net.network1.fc1.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc1.bias)\n",
    "\n",
    "print('layer 2')\n",
    "print('weights')\n",
    "print(net.network1.fc2.weight)\n",
    "print('biases')\n",
    "print(net.network1.fc2.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(heta)\n",
    "print(Rs)\n",
    "print(zeta)\n",
    "print(lambdaa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<h3>Training with xyz coordinates as features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5661,  0.5661],\n",
      "        [-0.5896,  0.5896],\n",
      "        [-0.6215,  0.6215],\n",
      "        [-0.6259,  0.6259],\n",
      "        [-0.6309,  0.6309],\n",
      "        [-0.6398,  0.6398],\n",
      "        [-0.6427,  0.6427],\n",
      "        [-0.6867,  0.6867],\n",
      "        [-0.7024,  0.7024],\n",
      "        [-0.7211,  0.7211],\n",
      "        [-0.7217,  0.7217],\n",
      "        [-0.7271,  0.7271],\n",
      "        [-0.7440,  0.7440],\n",
      "        [-0.7521,  0.7521],\n",
      "        [-0.7853,  0.7853],\n",
      "        [-0.7906,  0.7906],\n",
      "        [-0.8000,  0.8000],\n",
      "        [-0.8149,  0.8149],\n",
      "        [-0.8230,  0.8230],\n",
      "        [-0.8275,  0.8275],\n",
      "        [-0.8301,  0.8301],\n",
      "        [-0.8315,  0.8315],\n",
      "        [-0.8703,  0.8703],\n",
      "        [-0.8977,  0.8977],\n",
      "        [-0.9004,  0.9004],\n",
      "        [-0.9043,  0.9043],\n",
      "        [-0.9081,  0.9081],\n",
      "        [-0.9137,  0.9137],\n",
      "        [-0.9516,  0.9516],\n",
      "        [-0.9533,  0.9533],\n",
      "        [-0.9596,  0.9596],\n",
      "        [-0.9606,  0.9606],\n",
      "        [-0.9617,  0.9617],\n",
      "        [-0.9772,  0.9772],\n",
      "        [-1.0100,  1.0100],\n",
      "        [-1.0195,  1.0195],\n",
      "        [-1.0213,  1.0213],\n",
      "        [-1.0221,  1.0221],\n",
      "        [-1.0239,  1.0239],\n",
      "        [-1.0411,  1.0411],\n",
      "        [-1.0695,  1.0695],\n",
      "        [-1.0706,  1.0706],\n",
      "        [-1.0735,  1.0735],\n",
      "        [-1.0855,  1.0855],\n",
      "        [-1.1276,  1.1276],\n",
      "        [-1.1316,  1.1316],\n",
      "        [-1.1367,  1.1367],\n",
      "        [-1.1787,  1.1787],\n",
      "        [-1.1886,  1.1886],\n",
      "        [-1.2103,  1.2103],\n",
      "        [-1.2517,  1.2517],\n",
      "        [-1.2553,  1.2553],\n",
      "        [-1.2571,  1.2571],\n",
      "        [-1.2608,  1.2608],\n",
      "        [-1.2610,  1.2610],\n",
      "        [-1.2735,  1.2735],\n",
      "        [-1.2847,  1.2847],\n",
      "        [-1.2993,  1.2993],\n",
      "        [-1.3052,  1.3052],\n",
      "        [-1.3092,  1.3092],\n",
      "        [-1.3173,  1.3173],\n",
      "        [-1.3213,  1.3213],\n",
      "        [-1.3472,  1.3472],\n",
      "        [-1.3531,  1.3531],\n",
      "        [-1.3731,  1.3731],\n",
      "        [-1.3797,  1.3797],\n",
      "        [-1.3877,  1.3877],\n",
      "        [-1.4015,  1.4015],\n",
      "        [-1.4069,  1.4069],\n",
      "        [-1.4108,  1.4108],\n",
      "        [-1.4364,  1.4364],\n",
      "        [-1.4462,  1.4462],\n",
      "        [-1.4860,  1.4860],\n",
      "        [-1.4988,  1.4988],\n",
      "        [-1.5012,  1.5012],\n",
      "        [-1.5019,  1.5019],\n",
      "        [-1.5081,  1.5081],\n",
      "        [-1.5169,  1.5169],\n",
      "        [-1.5436,  1.5436],\n",
      "        [-1.5669,  1.5669],\n",
      "        [-1.5765,  1.5765],\n",
      "        [-1.5823,  1.5823],\n",
      "        [-1.5887,  1.5887],\n",
      "        [-1.6016,  1.6016],\n",
      "        [-1.6022,  1.6022],\n",
      "        [-1.6058,  1.6058],\n",
      "        [-1.6100,  1.6100],\n",
      "        [-1.6102,  1.6102],\n",
      "        [-1.6197,  1.6197],\n",
      "        [-1.6272,  1.6272],\n",
      "        [-1.6435,  1.6435],\n",
      "        [-1.6598,  1.6598],\n",
      "        [-1.6894,  1.6894],\n",
      "        [-1.6925,  1.6925],\n",
      "        [-1.7023,  1.7023],\n",
      "        [-1.7105,  1.7105],\n",
      "        [-1.7123,  1.7123],\n",
      "        [-1.7137,  1.7137],\n",
      "        [-1.7393,  1.7393],\n",
      "        [-1.7425,  1.7425],\n",
      "        [-1.7673,  1.7673],\n",
      "        [-1.7737,  1.7737],\n",
      "        [-1.7851,  1.7851],\n",
      "        [-1.7988,  1.7988],\n",
      "        [-1.7994,  1.7994],\n",
      "        [-1.8015,  1.8015],\n",
      "        [-1.8040,  1.8040],\n",
      "        [-1.8412,  1.8412],\n",
      "        [-1.8417,  1.8417],\n",
      "        [-1.8426,  1.8426],\n",
      "        [-1.8451,  1.8451],\n",
      "        [-1.8515,  1.8515],\n",
      "        [-1.8585,  1.8585],\n",
      "        [-1.8882,  1.8882],\n",
      "        [-1.8973,  1.8973],\n",
      "        [-1.9031,  1.9031],\n",
      "        [-1.9286,  1.9286],\n",
      "        [-1.9375,  1.9375],\n",
      "        [-1.9769,  1.9769],\n",
      "        [-2.0133,  2.0133],\n",
      "        [-2.0226,  2.0226],\n",
      "        [-2.0300,  2.0300],\n",
      "        [-2.0541,  2.0541],\n",
      "        [-2.0547,  2.0547],\n",
      "        [-2.0571,  2.0571],\n",
      "        [-2.0673,  2.0673],\n",
      "        [-2.0820,  2.0820],\n",
      "        [-2.0981,  2.0981],\n",
      "        [-2.1045,  2.1045],\n",
      "        [-2.1070,  2.1070],\n",
      "        [-2.1133,  2.1133],\n",
      "        [-2.1232,  2.1232],\n",
      "        [-2.1359,  2.1359],\n",
      "        [-2.1408,  2.1408],\n",
      "        [-2.1551,  2.1551],\n",
      "        [-2.1569,  2.1569],\n",
      "        [-2.1618,  2.1618],\n",
      "        [-2.1675,  2.1675],\n",
      "        [-2.1850,  2.1850],\n",
      "        [-2.1882,  2.1882],\n",
      "        [-2.1970,  2.1970],\n",
      "        [-2.2251,  2.2251],\n",
      "        [-2.2290,  2.2290],\n",
      "        [-2.2624,  2.2624],\n",
      "        [-2.2925,  2.2925],\n",
      "        [-2.3092,  2.3092],\n",
      "        [-2.3188,  2.3188],\n",
      "        [-2.3201,  2.3201],\n",
      "        [-2.3431,  2.3431],\n",
      "        [-2.3475,  2.3475],\n",
      "        [-2.3597,  2.3597],\n",
      "        [-2.3637,  2.3637],\n",
      "        [-2.3784,  2.3784],\n",
      "        [-2.3835,  2.3835],\n",
      "        [-2.3869,  2.3869],\n",
      "        [-2.4117,  2.4117],\n",
      "        [-2.4231,  2.4231],\n",
      "        [-2.4248,  2.4248],\n",
      "        [-2.4253,  2.4253],\n",
      "        [-2.4786,  2.4786],\n",
      "        [-2.4961,  2.4961],\n",
      "        [-2.5013,  2.5013],\n",
      "        [-2.5105,  2.5105],\n",
      "        [-2.5177,  2.5177],\n",
      "        [-2.5373,  2.5373],\n",
      "        [-2.5427,  2.5427],\n",
      "        [-2.5567,  2.5567],\n",
      "        [-2.5604,  2.5604],\n",
      "        [-2.5704,  2.5704],\n",
      "        [-2.5760,  2.5760],\n",
      "        [-2.5896,  2.5896],\n",
      "        [-2.5953,  2.5953],\n",
      "        [-2.6077,  2.6077],\n",
      "        [-2.6126,  2.6126],\n",
      "        [-2.6397,  2.6397],\n",
      "        [-2.6583,  2.6583],\n",
      "        [-2.6585,  2.6585],\n",
      "        [-2.6728,  2.6728],\n",
      "        [-2.7253,  2.7253],\n",
      "        [-2.8517,  2.8517],\n",
      "        [-2.9006,  2.9006],\n",
      "        [-2.9073,  2.9073],\n",
      "        [-2.9177,  2.9177],\n",
      "        [-2.9274,  2.9274],\n",
      "        [-2.9285,  2.9285],\n",
      "        [-2.9350,  2.9350],\n",
      "        [-2.9412,  2.9412],\n",
      "        [-2.9787,  2.9787],\n",
      "        [-2.9829,  2.9829],\n",
      "        [-2.9851,  2.9851],\n",
      "        [-2.9976,  2.9976],\n",
      "        [-3.0086,  3.0086],\n",
      "        [-3.0209,  3.0209],\n",
      "        [-3.0442,  3.0442],\n",
      "        [-3.0471,  3.0471],\n",
      "        [-3.0528,  3.0528],\n",
      "        [-3.0768,  3.0768],\n",
      "        [-3.0920,  3.0920],\n",
      "        [-3.0939,  3.0939],\n",
      "        [-3.1038,  3.1038],\n",
      "        [-3.1041,  3.1041],\n",
      "        [-3.1188,  3.1188],\n",
      "        [-3.1493,  3.1493],\n",
      "        [-3.1644,  3.1644],\n",
      "        [-3.1650,  3.1650],\n",
      "        [-3.1666,  3.1666],\n",
      "        [-3.1912,  3.1912],\n",
      "        [-3.1977,  3.1977],\n",
      "        [-3.2064,  3.2064],\n",
      "        [-3.2138,  3.2138],\n",
      "        [-3.2175,  3.2175],\n",
      "        [-3.2196,  3.2196],\n",
      "        [-3.2266,  3.2266],\n",
      "        [-3.2340,  3.2340],\n",
      "        [-3.2426,  3.2426],\n",
      "        [-3.2540,  3.2540],\n",
      "        [-3.2543,  3.2543],\n",
      "        [-3.2754,  3.2754],\n",
      "        [-3.2932,  3.2932],\n",
      "        [-3.3259,  3.3259],\n",
      "        [-3.3341,  3.3341],\n",
      "        [-3.3527,  3.3527],\n",
      "        [-3.3637,  3.3637],\n",
      "        [-3.4062,  3.4062],\n",
      "        [-3.4136,  3.4136],\n",
      "        [-3.4205,  3.4205],\n",
      "        [-3.4418,  3.4418],\n",
      "        [-3.4477,  3.4477],\n",
      "        [-3.4536,  3.4536],\n",
      "        [-3.4609,  3.4609],\n",
      "        [-3.4653,  3.4653],\n",
      "        [-3.4729,  3.4729],\n",
      "        [-3.4733,  3.4733],\n",
      "        [-3.4833,  3.4833],\n",
      "        [-3.4867,  3.4867],\n",
      "        [-3.5070,  3.5070],\n",
      "        [-3.5445,  3.5445],\n",
      "        [-3.5590,  3.5590],\n",
      "        [-3.5654,  3.5654],\n",
      "        [-3.5685,  3.5685],\n",
      "        [-3.5729,  3.5729],\n",
      "        [-3.5806,  3.5806],\n",
      "        [-3.5829,  3.5829],\n",
      "        [-3.5930,  3.5930],\n",
      "        [-3.5957,  3.5957],\n",
      "        [-3.6001,  3.6001],\n",
      "        [-3.6225,  3.6225],\n",
      "        [-3.6271,  3.6271],\n",
      "        [-3.6523,  3.6523],\n",
      "        [-3.6755,  3.6755],\n",
      "        [-3.6872,  3.6872],\n",
      "        [-3.7003,  3.7003],\n",
      "        [-3.7015,  3.7015],\n",
      "        [-3.7132,  3.7132],\n",
      "        [-3.7183,  3.7183],\n",
      "        [-3.7410,  3.7410],\n",
      "        [-3.7446,  3.7446],\n",
      "        [-3.7582,  3.7582],\n",
      "        [-3.8080,  3.8080],\n",
      "        [-3.8105,  3.8105],\n",
      "        [-3.8109,  3.8109],\n",
      "        [-3.8241,  3.8241],\n",
      "        [-3.8314,  3.8314],\n",
      "        [-3.8355,  3.8355],\n",
      "        [-3.8361,  3.8361],\n",
      "        [-3.8369,  3.8369],\n",
      "        [-3.8627,  3.8627],\n",
      "        [-3.8638,  3.8638],\n",
      "        [-3.8714,  3.8714],\n",
      "        [-3.8739,  3.8739],\n",
      "        [-3.8894,  3.8894],\n",
      "        [-3.8908,  3.8908],\n",
      "        [-3.8976,  3.8976],\n",
      "        [-3.9048,  3.9048],\n",
      "        [-3.9155,  3.9155],\n",
      "        [-3.9316,  3.9316],\n",
      "        [-3.9349,  3.9349],\n",
      "        [-3.9398,  3.9398],\n",
      "        [-3.9497,  3.9497],\n",
      "        [-3.9574,  3.9574],\n",
      "        [-3.9766,  3.9766],\n",
      "        [-4.0083,  4.0083],\n",
      "        [-4.0204,  4.0204],\n",
      "        [-4.0306,  4.0306],\n",
      "        [-4.0450,  4.0450],\n",
      "        [-4.0592,  4.0592],\n",
      "        [-4.0811,  4.0811],\n",
      "        [-4.0838,  4.0838],\n",
      "        [-4.0902,  4.0902],\n",
      "        [-4.1121,  4.1121],\n",
      "        [-4.1548,  4.1548],\n",
      "        [-4.1651,  4.1651],\n",
      "        [-4.1720,  4.1720],\n",
      "        [-4.1729,  4.1729],\n",
      "        [-4.1790,  4.1790],\n",
      "        [-4.1836,  4.1836],\n",
      "        [-4.1957,  4.1957],\n",
      "        [-4.1959,  4.1959],\n",
      "        [-4.2052,  4.2052],\n",
      "        [-4.2123,  4.2123],\n",
      "        [-4.2150,  4.2150],\n",
      "        [-4.2203,  4.2203],\n",
      "        [-4.2252,  4.2252],\n",
      "        [-4.2397,  4.2397],\n",
      "        [-4.2455,  4.2455],\n",
      "        [-4.2579,  4.2579],\n",
      "        [-4.2716,  4.2716],\n",
      "        [-4.2775,  4.2775],\n",
      "        [-4.2892,  4.2892],\n",
      "        [-4.3095,  4.3095],\n",
      "        [-4.3155,  4.3155],\n",
      "        [-4.3473,  4.3473],\n",
      "        [-4.3793,  4.3793],\n",
      "        [-4.3816,  4.3816],\n",
      "        [-4.3818,  4.3818],\n",
      "        [-4.3902,  4.3902],\n",
      "        [-4.4057,  4.4057],\n",
      "        [-4.4200,  4.4200],\n",
      "        [-4.4262,  4.4262],\n",
      "        [-4.4404,  4.4404],\n",
      "        [-4.4933,  4.4933],\n",
      "        [-4.4962,  4.4962],\n",
      "        [-4.5023,  4.5023],\n",
      "        [-4.5235,  4.5235],\n",
      "        [-4.5276,  4.5276],\n",
      "        [-4.5319,  4.5319],\n",
      "        [-4.5501,  4.5501],\n",
      "        [-4.5517,  4.5517],\n",
      "        [-4.5532,  4.5532],\n",
      "        [-4.5574,  4.5574],\n",
      "        [-4.5701,  4.5701],\n",
      "        [-4.5735,  4.5735],\n",
      "        [-4.5792,  4.5792],\n",
      "        [-4.5888,  4.5888],\n",
      "        [-4.5933,  4.5933],\n",
      "        [-4.6108,  4.6108],\n",
      "        [-4.6324,  4.6324],\n",
      "        [-4.6345,  4.6345],\n",
      "        [-4.6363,  4.6363],\n",
      "        [-4.6473,  4.6473],\n",
      "        [-4.6522,  4.6522],\n",
      "        [-4.6584,  4.6584],\n",
      "        [-4.6721,  4.6721],\n",
      "        [-4.6814,  4.6814],\n",
      "        [-4.6828,  4.6828],\n",
      "        [-4.6854,  4.6854],\n",
      "        [-4.6894,  4.6894],\n",
      "        [-4.6927,  4.6927],\n",
      "        [-4.6960,  4.6960],\n",
      "        [-4.7076,  4.7076],\n",
      "        [-4.7147,  4.7147],\n",
      "        [-4.7262,  4.7262],\n",
      "        [-4.7321,  4.7321],\n",
      "        [-4.7391,  4.7391],\n",
      "        [-4.7441,  4.7441],\n",
      "        [-4.7441,  4.7441],\n",
      "        [-4.7618,  4.7618],\n",
      "        [-4.7649,  4.7649],\n",
      "        [-4.7680,  4.7680],\n",
      "        [-4.7841,  4.7841],\n",
      "        [-4.7955,  4.7955],\n",
      "        [-4.7989,  4.7989],\n",
      "        [-4.8265,  4.8265],\n",
      "        [-4.8291,  4.8291],\n",
      "        [-4.8363,  4.8363],\n",
      "        [-4.8394,  4.8394],\n",
      "        [-4.8477,  4.8477],\n",
      "        [-4.8581,  4.8581],\n",
      "        [-4.8766,  4.8766],\n",
      "        [-4.9737,  4.9737],\n",
      "        [-4.9763,  4.9763],\n",
      "        [-4.9768,  4.9768],\n",
      "        [-4.9863,  4.9863],\n",
      "        [-4.9971,  4.9971],\n",
      "        [-5.0168,  5.0168],\n",
      "        [-5.0279,  5.0279],\n",
      "        [-5.0394,  5.0394],\n",
      "        [-5.0395,  5.0395],\n",
      "        [-5.0402,  5.0402],\n",
      "        [-5.0511,  5.0511],\n",
      "        [-5.0543,  5.0543],\n",
      "        [-5.0589,  5.0589],\n",
      "        [-5.0705,  5.0705],\n",
      "        [-5.0786,  5.0786],\n",
      "        [-5.0926,  5.0926],\n",
      "        [-5.0989,  5.0989],\n",
      "        [-5.0996,  5.0996],\n",
      "        [-5.1062,  5.1062],\n",
      "        [-5.1102,  5.1102],\n",
      "        [-5.1578,  5.1578],\n",
      "        [-5.1599,  5.1599],\n",
      "        [-5.1614,  5.1614],\n",
      "        [-5.1665,  5.1665],\n",
      "        [-5.1813,  5.1813],\n",
      "        [-5.1872,  5.1872],\n",
      "        [-5.1894,  5.1894],\n",
      "        [-5.1910,  5.1910],\n",
      "        [-5.2306,  5.2306],\n",
      "        [-5.2419,  5.2419],\n",
      "        [-5.2532,  5.2532],\n",
      "        [-5.2602,  5.2602],\n",
      "        [-5.2995,  5.2995],\n",
      "        [-5.3304,  5.3304],\n",
      "        [-5.3408,  5.3408],\n",
      "        [-5.3504,  5.3504],\n",
      "        [-5.3559,  5.3559],\n",
      "        [-5.3621,  5.3621],\n",
      "        [-5.3677,  5.3677],\n",
      "        [-5.3706,  5.3706],\n",
      "        [-5.3879,  5.3879],\n",
      "        [-5.4010,  5.4010],\n",
      "        [-5.4014,  5.4014],\n",
      "        [-5.4133,  5.4133],\n",
      "        [-5.4210,  5.4210],\n",
      "        [-5.4309,  5.4309],\n",
      "        [-5.4336,  5.4336],\n",
      "        [-5.4699,  5.4699],\n",
      "        [-5.4992,  5.4992],\n",
      "        [-5.5113,  5.5113],\n",
      "        [-5.5143,  5.5143],\n",
      "        [-5.5560,  5.5560],\n",
      "        [-5.5594,  5.5594],\n",
      "        [-5.5774,  5.5774],\n",
      "        [-5.5781,  5.5781],\n",
      "        [-5.5795,  5.5795],\n",
      "        [-5.5817,  5.5817],\n",
      "        [-5.5854,  5.5854],\n",
      "        [-5.5885,  5.5885],\n",
      "        [-5.6197,  5.6197],\n",
      "        [-5.6214,  5.6214],\n",
      "        [-5.6489,  5.6489],\n",
      "        [-5.6545,  5.6545],\n",
      "        [-5.6791,  5.6791],\n",
      "        [-5.6809,  5.6809],\n",
      "        [-5.6958,  5.6958],\n",
      "        [-5.6980,  5.6980],\n",
      "        [-5.7280,  5.7280],\n",
      "        [-5.7327,  5.7327],\n",
      "        [-5.7418,  5.7418],\n",
      "        [-5.7708,  5.7708],\n",
      "        [-5.7866,  5.7866],\n",
      "        [-5.8334,  5.8334],\n",
      "        [-5.8452,  5.8452],\n",
      "        [-5.9307,  5.9307],\n",
      "        [-5.9341,  5.9341],\n",
      "        [-5.9812,  5.9812],\n",
      "        [-5.9882,  5.9882],\n",
      "        [-6.0254,  6.0254],\n",
      "        [-6.0529,  6.0529],\n",
      "        [-6.0918,  6.0918],\n",
      "        [-6.0965,  6.0965],\n",
      "        [-6.1173,  6.1173],\n",
      "        [-6.1405,  6.1405],\n",
      "        [-6.1484,  6.1484],\n",
      "        [-6.1490,  6.1490],\n",
      "        [-6.1577,  6.1577],\n",
      "        [-6.1631,  6.1631],\n",
      "        [-6.1699,  6.1699],\n",
      "        [-6.1965,  6.1965],\n",
      "        [-6.2106,  6.2106],\n",
      "        [-6.2131,  6.2131],\n",
      "        [-6.2159,  6.2159],\n",
      "        [-6.2312,  6.2312],\n",
      "        [-6.2532,  6.2532],\n",
      "        [-6.2800,  6.2800],\n",
      "        [-6.2903,  6.2903],\n",
      "        [-6.2938,  6.2938],\n",
      "        [-6.2948,  6.2948],\n",
      "        [-6.2981,  6.2981],\n",
      "        [-6.3034,  6.3034],\n",
      "        [-6.3039,  6.3039],\n",
      "        [-6.3558,  6.3558],\n",
      "        [-6.3876,  6.3876],\n",
      "        [-6.3904,  6.3904],\n",
      "        [-6.3923,  6.3923],\n",
      "        [-6.4082,  6.4082],\n",
      "        [-6.4083,  6.4083],\n",
      "        [-6.4173,  6.4173],\n",
      "        [-6.4310,  6.4310],\n",
      "        [-6.4601,  6.4601],\n",
      "        [-6.4626,  6.4626],\n",
      "        [-6.4710,  6.4710],\n",
      "        [-6.4734,  6.4734],\n",
      "        [-6.5170,  6.5170],\n",
      "        [-6.5214,  6.5214],\n",
      "        [-6.5239,  6.5239],\n",
      "        [-6.5277,  6.5277],\n",
      "        [-6.5383,  6.5383],\n",
      "        [-6.5453,  6.5453],\n",
      "        [-6.5474,  6.5474],\n",
      "        [-6.5533,  6.5533],\n",
      "        [-6.5585,  6.5585],\n",
      "        [-6.5673,  6.5673],\n",
      "        [-6.5828,  6.5828],\n",
      "        [-6.6020,  6.6020],\n",
      "        [-6.6276,  6.6276],\n",
      "        [-6.6279,  6.6279],\n",
      "        [-6.6448,  6.6448],\n",
      "        [-6.6495,  6.6495],\n",
      "        [-6.6604,  6.6604]])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 2])\n",
      "torch.Size([500])\n",
      "torch.Size([500, 2])\n"
     ]
    }
   ],
   "source": [
    "N                        = 2       # number of atoms per molecule\n",
    "number_of_features_xyz   = 3       # number of features (symmetry functions) for each atom (we create one radial)\n",
    "                                   # and one angular, but can create more by vaying the parameters η, λ, ζ, Rs etc.\n",
    "\n",
    "\n",
    "training_set_size    = data_size -50\n",
    "\n",
    "    \n",
    "# Computing variance and mean on the training data only!\n",
    "coord_train = coordinates[:training_set_size,:]\n",
    "var_train_xyz  = np.var(coord_train,axis=0)\n",
    "mean_train_xyz = np.mean(coord_train,axis=0)\n",
    "\n",
    "coordinates_h2_norm = np.zeros((len(coordinates), number_of_features_xyz))\n",
    "# normalize all data (training and test), using training set mean and variance\n",
    "for i in range(np.shape(coordinates)[0]):\n",
    "    for j in range(1):  # omit second and third column since for our dataset they are always zero\n",
    "        coordinates_h2_norm[i,j] = (coordinates[i,j]-mean_train_xyz[j])/var_train_xyz[j]\n",
    "\n",
    "        \n",
    "data_set_xyz = np.vsplit(coordinates_h2_norm,data_size)     # !!!!!!!!!!!!  change to coordinates_water_norm if you are normalising\n",
    "data_set_xyz = torch.FloatTensor(data_set_xyz)          # Going from a (1000,3,2) list to a a (1000,3,2) tensor\n",
    "\n",
    "\n",
    "data_set_xyz = data_set_xyz[:,:,0] # !!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "print(data_set_xyz)\n",
    "\n",
    "\n",
    "print(np.shape(labels_norm))\n",
    "print(np.shape(data_set_xyz))\n",
    "shuffler = np.random.permutation(len(labels_norm))\n",
    "\n",
    "data_set_xyz = data_set_xyz[shuffler]\n",
    "\n",
    "labels_norm = labels_norm[shuffler]\n",
    "\n",
    "print(np.shape(labels_norm))\n",
    "print(np.shape(data_set_xyz))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# labels same as before   \n",
    "train_labels         = labels_norm[:training_set_size]\n",
    "train_labels         = torch.FloatTensor(train_labels)\n",
    "test_labels          = labels_norm[training_set_size:]\n",
    "test_labels          = torch.FloatTensor(test_labels)\n",
    "\n",
    "    \n",
    "# Splitting the dataset into training and test set\n",
    "training_set_xyz         = data_set_xyz[:training_set_size]\n",
    "test_set_xyz             = data_set_xyz[training_set_size:]\n",
    "#train and test labels same as before\n",
    "\n",
    "\n",
    "#Dataset\n",
    "dataset_xyz = TensorDataset(training_set_xyz, train_labels)\n",
    "\n",
    "# Creating the batches\n",
    "dataloader_xyz = torch.utils.data.DataLoader(dataset_xyz, batch_size=225,\n",
    "                                           shuffle=True, num_workers=2, drop_last=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "x1 tensor(-3.7015)\n",
      "x2 tensor(3.7015)\n",
      "output\n",
      "tensor([0.1154], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class Subnets_xyz(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(Subnets_xyz, self).__init__()\n",
    "        num_hid_feat = 20 #int(number_of_features/2)\n",
    "        self.fc1 = nn.Linear(number_of_features,num_hid_feat)        # where fc stands for fully connected \n",
    "        self.fc2 = nn.Linear(num_hid_feat,num_hid_feat)        \n",
    "        self.fc3 = nn.Linear(num_hid_feat, 1)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.fc1(x))           # Apply a tanh activation on fully connected layer 1 \n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)                   # Using a linear function (identity function) for the subnet output layers\n",
    "        return x\n",
    "\n",
    "class BPNN_xyz(nn.Module):\n",
    "    def __init__(self,number_of_features):\n",
    "        super(BPNN_xyz, self).__init__()\n",
    "        self.network1 = Subnets_xyz(number_of_features)\n",
    "        self.network2 = Subnets_xyz(number_of_features)\n",
    "        \n",
    "#        self.fc_out = nn.Linear(3, 1)      # should this be defined here, given that we are not trying to optimise\n",
    "        \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = torch.reshape(x1,[1])\n",
    "        x2 = torch.reshape(x2,[1])\n",
    "\n",
    "        x1 = self.network1(x1)\n",
    "        x2 = self.network2(x2)\n",
    "        \n",
    "#         print(x1)\n",
    "#         print(x2)\n",
    "        \n",
    "        x = torch.cat((x1, x2), 0) \n",
    "        x = torch.sum(x)                   #??????????????????????????? try average pooling?\n",
    "        x = torch.reshape(x,[1])\n",
    "        return x\n",
    "\n",
    "    \n",
    "model = BPNN_xyz(1)\n",
    "#print(np.shape(training_set_xyz[0]))\n",
    "x1, x2 = training_set_xyz[0]\n",
    "\n",
    "print(np.shape(x1))\n",
    "print('x1',x1)\n",
    "print('x2',x2)\n",
    "\n",
    "\n",
    "output = model(x1, x2)\n",
    "print('output')\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_xyz = BPNN_xyz(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 0.0084571890533\n",
      "[2,     1] loss: 0.0047205906361\n",
      "[3,     1] loss: 0.0022410558537\n",
      "[4,     1] loss: 0.0007663301192\n",
      "[5,     1] loss: 0.0002487762133\n",
      "[6,     1] loss: 0.0003433441045\n",
      "[7,     1] loss: 0.0006946383510\n",
      "[8,     1] loss: 0.0009920204058\n",
      "[9,     1] loss: 0.0010240420699\n",
      "[10,     1] loss: 0.0008761198260\n",
      "[11,     1] loss: 0.0006023976021\n",
      "[12,     1] loss: 0.0003212953685\n",
      "[13,     1] loss: 0.0001843536040\n",
      "[14,     1] loss: 0.0001331446692\n",
      "[15,     1] loss: 0.0001710883342\n",
      "[16,     1] loss: 0.0002154972637\n",
      "[17,     1] loss: 0.0002323134104\n",
      "[18,     1] loss: 0.0002225665841\n",
      "[19,     1] loss: 0.0001967158169\n",
      "[20,     1] loss: 0.0001380494214\n",
      "[21,     1] loss: 0.0000859649328\n",
      "[22,     1] loss: 0.0000751986110\n",
      "[23,     1] loss: 0.0000685831998\n",
      "[24,     1] loss: 0.0000795627246\n",
      "[25,     1] loss: 0.0000824003306\n",
      "[26,     1] loss: 0.0000863435969\n",
      "[27,     1] loss: 0.0000679532473\n",
      "[28,     1] loss: 0.0000542405003\n",
      "[29,     1] loss: 0.0000542605820\n",
      "[30,     1] loss: 0.0000417668343\n",
      "[31,     1] loss: 0.0000501426635\n",
      "[32,     1] loss: 0.0000512829458\n",
      "[33,     1] loss: 0.0000514345593\n",
      "[34,     1] loss: 0.0000443782308\n",
      "[35,     1] loss: 0.0000468300976\n",
      "[36,     1] loss: 0.0000459382951\n",
      "[37,     1] loss: 0.0000423474994\n",
      "[38,     1] loss: 0.0000450904772\n",
      "[39,     1] loss: 0.0000412791938\n",
      "[40,     1] loss: 0.0000403139595\n",
      "[41,     1] loss: 0.0000426030223\n",
      "[42,     1] loss: 0.0000391778885\n",
      "[43,     1] loss: 0.0000374594470\n",
      "[44,     1] loss: 0.0000416248135\n",
      "[45,     1] loss: 0.0000344001164\n",
      "[46,     1] loss: 0.0000386208121\n",
      "[47,     1] loss: 0.0000434787798\n",
      "[48,     1] loss: 0.0000332869851\n",
      "[49,     1] loss: 0.0000443115161\n",
      "[50,     1] loss: 0.0000358425168\n",
      "[51,     1] loss: 0.0000419703632\n",
      "[52,     1] loss: 0.0000336104509\n",
      "[53,     1] loss: 0.0000423307414\n",
      "[54,     1] loss: 0.0000396393414\n",
      "[55,     1] loss: 0.0000418493262\n",
      "[56,     1] loss: 0.0000358267222\n",
      "[57,     1] loss: 0.0000402900012\n",
      "[58,     1] loss: 0.0000373008515\n",
      "[59,     1] loss: 0.0000376757933\n",
      "[60,     1] loss: 0.0000360028498\n",
      "[61,     1] loss: 0.0000397263735\n",
      "[62,     1] loss: 0.0000330542098\n",
      "[63,     1] loss: 0.0000406957668\n",
      "[64,     1] loss: 0.0000396516873\n",
      "[65,     1] loss: 0.0000412067922\n",
      "[66,     1] loss: 0.0000340072060\n",
      "[67,     1] loss: 0.0000405803119\n",
      "[68,     1] loss: 0.0000406033418\n",
      "[69,     1] loss: 0.0000420846860\n",
      "[70,     1] loss: 0.0000404837629\n",
      "[71,     1] loss: 0.0000372937851\n",
      "[72,     1] loss: 0.0000396743824\n",
      "[73,     1] loss: 0.0000479890266\n",
      "[74,     1] loss: 0.0000332959753\n",
      "[75,     1] loss: 0.0000367696659\n",
      "[76,     1] loss: 0.0000478367350\n",
      "[77,     1] loss: 0.0000342860818\n",
      "[78,     1] loss: 0.0000393416674\n",
      "[79,     1] loss: 0.0000468957354\n",
      "[80,     1] loss: 0.0000391578680\n",
      "[81,     1] loss: 0.0000335538789\n",
      "[82,     1] loss: 0.0000376655982\n",
      "[83,     1] loss: 0.0000373866351\n",
      "[84,     1] loss: 0.0000414693233\n",
      "[85,     1] loss: 0.0000386990607\n",
      "[86,     1] loss: 0.0000433419278\n",
      "[87,     1] loss: 0.0000410360517\n",
      "[88,     1] loss: 0.0000343865977\n",
      "[89,     1] loss: 0.0000398028700\n",
      "[90,     1] loss: 0.0000401476223\n",
      "[91,     1] loss: 0.0000391200185\n",
      "[92,     1] loss: 0.0000329665141\n",
      "[93,     1] loss: 0.0000384416635\n",
      "[94,     1] loss: 0.0000392215705\n",
      "[95,     1] loss: 0.0000357899495\n",
      "[96,     1] loss: 0.0000399384298\n",
      "[97,     1] loss: 0.0000438454706\n",
      "[98,     1] loss: 0.0000342447107\n",
      "[99,     1] loss: 0.0000418459473\n",
      "[100,     1] loss: 0.0000390769797\n",
      "[101,     1] loss: 0.0000417409086\n",
      "[102,     1] loss: 0.0000351244205\n",
      "[103,     1] loss: 0.0000399648474\n",
      "[104,     1] loss: 0.0000350940332\n",
      "[105,     1] loss: 0.0000359876605\n",
      "[106,     1] loss: 0.0000374033028\n",
      "[107,     1] loss: 0.0000371871254\n",
      "[108,     1] loss: 0.0000337737030\n",
      "[109,     1] loss: 0.0000411684654\n",
      "[110,     1] loss: 0.0000351556839\n",
      "[111,     1] loss: 0.0000387839711\n",
      "[112,     1] loss: 0.0000348151865\n",
      "[113,     1] loss: 0.0000399786513\n",
      "[114,     1] loss: 0.0000430610293\n",
      "[115,     1] loss: 0.0000306518748\n",
      "[116,     1] loss: 0.0000318425999\n",
      "[117,     1] loss: 0.0000343199994\n",
      "[118,     1] loss: 0.0000312307966\n",
      "[119,     1] loss: 0.0000319222716\n",
      "[120,     1] loss: 0.0000334931799\n",
      "[121,     1] loss: 0.0000372711715\n",
      "[122,     1] loss: 0.0000284364069\n",
      "[123,     1] loss: 0.0000340398110\n",
      "[124,     1] loss: 0.0000307431619\n",
      "[125,     1] loss: 0.0000398844277\n",
      "[126,     1] loss: 0.0000348218338\n",
      "[127,     1] loss: 0.0000276213716\n",
      "[128,     1] loss: 0.0000319950428\n",
      "[129,     1] loss: 0.0000338538230\n",
      "[130,     1] loss: 0.0000415512070\n",
      "[131,     1] loss: 0.0000364556850\n",
      "[132,     1] loss: 0.0000392300950\n",
      "[133,     1] loss: 0.0000302122557\n",
      "[134,     1] loss: 0.0000354239222\n",
      "[135,     1] loss: 0.0000318141945\n",
      "[136,     1] loss: 0.0000418915472\n",
      "[137,     1] loss: 0.0000382522179\n",
      "[138,     1] loss: 0.0000316269201\n",
      "[139,     1] loss: 0.0000384077110\n",
      "[140,     1] loss: 0.0000381123973\n",
      "[141,     1] loss: 0.0000409675384\n",
      "[142,     1] loss: 0.0000325680536\n",
      "[143,     1] loss: 0.0000324217865\n",
      "[144,     1] loss: 0.0000408654538\n",
      "[145,     1] loss: 0.0000366296474\n",
      "[146,     1] loss: 0.0000427304563\n",
      "[147,     1] loss: 0.0000340034574\n",
      "[148,     1] loss: 0.0000373825664\n",
      "[149,     1] loss: 0.0000362030405\n",
      "[150,     1] loss: 0.0000295933278\n",
      "[151,     1] loss: 0.0000357509241\n",
      "[152,     1] loss: 0.0000391305744\n",
      "[153,     1] loss: 0.0000330030103\n",
      "[154,     1] loss: 0.0000341816049\n",
      "[155,     1] loss: 0.0000290187221\n",
      "[156,     1] loss: 0.0000299804698\n",
      "[157,     1] loss: 0.0000331986434\n",
      "[158,     1] loss: 0.0000330194773\n",
      "[159,     1] loss: 0.0000340394356\n",
      "[160,     1] loss: 0.0000283074914\n",
      "[161,     1] loss: 0.0000329626404\n",
      "[162,     1] loss: 0.0000299833890\n",
      "[163,     1] loss: 0.0000313214376\n",
      "[164,     1] loss: 0.0000326916866\n",
      "[165,     1] loss: 0.0000271434808\n",
      "[166,     1] loss: 0.0000400408870\n",
      "[167,     1] loss: 0.0000313872792\n",
      "[168,     1] loss: 0.0000382926286\n",
      "[169,     1] loss: 0.0000336491939\n",
      "[170,     1] loss: 0.0000246531214\n",
      "[171,     1] loss: 0.0000355946540\n",
      "[172,     1] loss: 0.0000356730161\n",
      "[173,     1] loss: 0.0000320232357\n",
      "[174,     1] loss: 0.0000397177791\n",
      "[175,     1] loss: 0.0000308647781\n",
      "[176,     1] loss: 0.0000371772709\n",
      "[177,     1] loss: 0.0000295290229\n",
      "[178,     1] loss: 0.0000314934645\n",
      "[179,     1] loss: 0.0000319554994\n",
      "[180,     1] loss: 0.0000365169544\n",
      "[181,     1] loss: 0.0000392428337\n",
      "[182,     1] loss: 0.0000271655415\n",
      "[183,     1] loss: 0.0000373184273\n",
      "[184,     1] loss: 0.0000309069379\n",
      "[185,     1] loss: 0.0000362900959\n",
      "[186,     1] loss: 0.0000308608607\n",
      "[187,     1] loss: 0.0000367144617\n",
      "[188,     1] loss: 0.0000326229754\n",
      "[189,     1] loss: 0.0000311228447\n",
      "[190,     1] loss: 0.0000384747575\n",
      "[191,     1] loss: 0.0000326114474\n",
      "[192,     1] loss: 0.0000385238614\n",
      "[193,     1] loss: 0.0000362389663\n",
      "[194,     1] loss: 0.0000337131583\n",
      "[195,     1] loss: 0.0000317449274\n",
      "[196,     1] loss: 0.0000248958968\n",
      "[197,     1] loss: 0.0000355078431\n",
      "[198,     1] loss: 0.0000345786219\n",
      "[199,     1] loss: 0.0000287205359\n",
      "[200,     1] loss: 0.0000389520486\n",
      "[201,     1] loss: 0.0000342760031\n",
      "[202,     1] loss: 0.0000331967400\n",
      "[203,     1] loss: 0.0000339041551\n",
      "[204,     1] loss: 0.0000349545822\n",
      "[205,     1] loss: 0.0000312955672\n",
      "[206,     1] loss: 0.0000374970434\n",
      "[207,     1] loss: 0.0000307562761\n",
      "[208,     1] loss: 0.0000340088474\n",
      "[209,     1] loss: 0.0000302978791\n",
      "[210,     1] loss: 0.0000291342934\n",
      "[211,     1] loss: 0.0000310899544\n",
      "[212,     1] loss: 0.0000333236501\n",
      "[213,     1] loss: 0.0000319918647\n",
      "[214,     1] loss: 0.0000412309309\n",
      "[215,     1] loss: 0.0000328746974\n",
      "[216,     1] loss: 0.0000387337961\n",
      "[217,     1] loss: 0.0000397911557\n",
      "[218,     1] loss: 0.0000271765777\n",
      "[219,     1] loss: 0.0000355424389\n",
      "[220,     1] loss: 0.0000286159950\n",
      "[221,     1] loss: 0.0000351945229\n",
      "[222,     1] loss: 0.0000328166818\n",
      "[223,     1] loss: 0.0000327029877\n",
      "[224,     1] loss: 0.0000363240251\n",
      "[225,     1] loss: 0.0000383854727\n",
      "[226,     1] loss: 0.0000358559191\n",
      "[227,     1] loss: 0.0000324141554\n",
      "[228,     1] loss: 0.0000347601628\n",
      "[229,     1] loss: 0.0000265025999\n",
      "[230,     1] loss: 0.0000346165092\n",
      "[231,     1] loss: 0.0000275900209\n",
      "[232,     1] loss: 0.0000341161212\n",
      "[233,     1] loss: 0.0000286136259\n",
      "[234,     1] loss: 0.0000365198997\n",
      "[235,     1] loss: 0.0000367601111\n",
      "[236,     1] loss: 0.0000350607937\n",
      "[237,     1] loss: 0.0000284841779\n",
      "[238,     1] loss: 0.0000348072121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239,     1] loss: 0.0000303403649\n",
      "[240,     1] loss: 0.0000277040963\n",
      "[241,     1] loss: 0.0000312774791\n",
      "[242,     1] loss: 0.0000408379448\n",
      "[243,     1] loss: 0.0000310958392\n",
      "[244,     1] loss: 0.0000405488885\n",
      "[245,     1] loss: 0.0000308577233\n",
      "[246,     1] loss: 0.0000345523760\n",
      "[247,     1] loss: 0.0000305342750\n",
      "[248,     1] loss: 0.0000314088102\n",
      "[249,     1] loss: 0.0000303987705\n",
      "[250,     1] loss: 0.0000306870264\n",
      "[251,     1] loss: 0.0000287242234\n",
      "[252,     1] loss: 0.0000290677912\n",
      "[253,     1] loss: 0.0000260719069\n",
      "[254,     1] loss: 0.0000317769445\n",
      "[255,     1] loss: 0.0000280842622\n",
      "[256,     1] loss: 0.0000265992130\n",
      "[257,     1] loss: 0.0000307178067\n",
      "[258,     1] loss: 0.0000275049330\n",
      "[259,     1] loss: 0.0000287309143\n",
      "[260,     1] loss: 0.0000242900729\n",
      "[261,     1] loss: 0.0000327547139\n",
      "[262,     1] loss: 0.0000334753422\n",
      "[263,     1] loss: 0.0000261760142\n",
      "[264,     1] loss: 0.0000306234811\n",
      "[265,     1] loss: 0.0000314967678\n",
      "[266,     1] loss: 0.0000327356305\n",
      "[267,     1] loss: 0.0000308316608\n",
      "[268,     1] loss: 0.0000381912978\n",
      "[269,     1] loss: 0.0000243382703\n",
      "[270,     1] loss: 0.0000308727205\n",
      "[271,     1] loss: 0.0000316873280\n",
      "[272,     1] loss: 0.0000301589433\n",
      "[273,     1] loss: 0.0000373148883\n",
      "[274,     1] loss: 0.0000304929214\n",
      "[275,     1] loss: 0.0000312067132\n",
      "[276,     1] loss: 0.0000307591457\n",
      "[277,     1] loss: 0.0000362531602\n",
      "[278,     1] loss: 0.0000320836814\n",
      "[279,     1] loss: 0.0000302430097\n",
      "[280,     1] loss: 0.0000318107195\n",
      "[281,     1] loss: 0.0000292317563\n",
      "[282,     1] loss: 0.0000307074079\n",
      "[283,     1] loss: 0.0000338132842\n",
      "[284,     1] loss: 0.0000379232806\n",
      "[285,     1] loss: 0.0000250893238\n",
      "[286,     1] loss: 0.0000258355343\n",
      "[287,     1] loss: 0.0000248545635\n",
      "[288,     1] loss: 0.0000265350594\n",
      "[289,     1] loss: 0.0000261031993\n",
      "[290,     1] loss: 0.0000299085252\n",
      "[291,     1] loss: 0.0000281948858\n",
      "[292,     1] loss: 0.0000284712965\n",
      "[293,     1] loss: 0.0000330377574\n",
      "[294,     1] loss: 0.0000304971589\n",
      "[295,     1] loss: 0.0000316842634\n",
      "[296,     1] loss: 0.0000280450186\n",
      "[297,     1] loss: 0.0000307921873\n",
      "[298,     1] loss: 0.0000302613800\n",
      "[299,     1] loss: 0.0000301108754\n",
      "[300,     1] loss: 0.0000297894818\n",
      "[301,     1] loss: 0.0000393346680\n",
      "[302,     1] loss: 0.0000355418102\n",
      "[303,     1] loss: 0.0000346790737\n",
      "[304,     1] loss: 0.0000269582175\n",
      "[305,     1] loss: 0.0000253787526\n",
      "[306,     1] loss: 0.0000287657604\n",
      "[307,     1] loss: 0.0000262665737\n",
      "[308,     1] loss: 0.0000296708633\n",
      "[309,     1] loss: 0.0000297341147\n",
      "[310,     1] loss: 0.0000297082559\n",
      "[311,     1] loss: 0.0000262236281\n",
      "[312,     1] loss: 0.0000288693758\n",
      "[313,     1] loss: 0.0000220518326\n",
      "[314,     1] loss: 0.0000250724494\n",
      "[315,     1] loss: 0.0000294975122\n",
      "[316,     1] loss: 0.0000301269611\n",
      "[317,     1] loss: 0.0000319244980\n",
      "[318,     1] loss: 0.0000255099294\n",
      "[319,     1] loss: 0.0000323653279\n",
      "[320,     1] loss: 0.0000240509573\n",
      "[321,     1] loss: 0.0000358783407\n",
      "[322,     1] loss: 0.0000306173053\n",
      "[323,     1] loss: 0.0000294931378\n",
      "[324,     1] loss: 0.0000240285750\n",
      "[325,     1] loss: 0.0000315517391\n",
      "[326,     1] loss: 0.0000377454096\n",
      "[327,     1] loss: 0.0000246178388\n",
      "[328,     1] loss: 0.0000292001991\n",
      "[329,     1] loss: 0.0000312882476\n",
      "[330,     1] loss: 0.0000295178819\n",
      "[331,     1] loss: 0.0000289611053\n",
      "[332,     1] loss: 0.0000379011937\n",
      "[333,     1] loss: 0.0000307377166\n",
      "[334,     1] loss: 0.0000367243832\n",
      "[335,     1] loss: 0.0000286543713\n",
      "[336,     1] loss: 0.0000321439642\n",
      "[337,     1] loss: 0.0000258410204\n",
      "[338,     1] loss: 0.0000220334099\n",
      "[339,     1] loss: 0.0000318877108\n",
      "[340,     1] loss: 0.0000285435759\n",
      "[341,     1] loss: 0.0000297617749\n",
      "[342,     1] loss: 0.0000329120812\n",
      "[343,     1] loss: 0.0000296106678\n",
      "[344,     1] loss: 0.0000359030470\n",
      "[345,     1] loss: 0.0000301488646\n",
      "[346,     1] loss: 0.0000274352962\n",
      "[347,     1] loss: 0.0000311963493\n",
      "[348,     1] loss: 0.0000280015694\n",
      "[349,     1] loss: 0.0000264881470\n",
      "[350,     1] loss: 0.0000288754964\n",
      "[351,     1] loss: 0.0000265489740\n",
      "[352,     1] loss: 0.0000222382529\n",
      "[353,     1] loss: 0.0000352535833\n",
      "[354,     1] loss: 0.0000269112526\n",
      "[355,     1] loss: 0.0000345105393\n",
      "[356,     1] loss: 0.0000298853149\n",
      "[357,     1] loss: 0.0000290846685\n",
      "[358,     1] loss: 0.0000366197433\n",
      "[359,     1] loss: 0.0000293014280\n",
      "[360,     1] loss: 0.0000276417530\n",
      "[361,     1] loss: 0.0000278684631\n",
      "[362,     1] loss: 0.0000339322753\n",
      "[363,     1] loss: 0.0000324956811\n",
      "[364,     1] loss: 0.0000366795517\n",
      "[365,     1] loss: 0.0000297268358\n",
      "[366,     1] loss: 0.0000290487340\n",
      "[367,     1] loss: 0.0000272423611\n",
      "[368,     1] loss: 0.0000303707609\n",
      "[369,     1] loss: 0.0000309759780\n",
      "[370,     1] loss: 0.0000396272691\n",
      "[371,     1] loss: 0.0000321274361\n",
      "[372,     1] loss: 0.0000265955809\n",
      "[373,     1] loss: 0.0000256909465\n",
      "[374,     1] loss: 0.0000301355321\n",
      "[375,     1] loss: 0.0000219385329\n",
      "[376,     1] loss: 0.0000331960531\n",
      "[377,     1] loss: 0.0000293259160\n",
      "[378,     1] loss: 0.0000232239181\n",
      "[379,     1] loss: 0.0000281464774\n",
      "[380,     1] loss: 0.0000280641689\n",
      "[381,     1] loss: 0.0000303267327\n",
      "[382,     1] loss: 0.0000375708711\n",
      "[383,     1] loss: 0.0000310113799\n",
      "[384,     1] loss: 0.0000344241911\n",
      "[385,     1] loss: 0.0000233271188\n",
      "[386,     1] loss: 0.0000278422958\n",
      "[387,     1] loss: 0.0000319755665\n",
      "[388,     1] loss: 0.0000307174545\n",
      "[389,     1] loss: 0.0000289112329\n",
      "[390,     1] loss: 0.0000298294559\n",
      "[391,     1] loss: 0.0000361467653\n",
      "[392,     1] loss: 0.0000321604894\n",
      "[393,     1] loss: 0.0000315165089\n",
      "[394,     1] loss: 0.0000198890048\n",
      "[395,     1] loss: 0.0000311212701\n",
      "[396,     1] loss: 0.0000341587875\n",
      "[397,     1] loss: 0.0000239376459\n",
      "[398,     1] loss: 0.0000300403015\n",
      "[399,     1] loss: 0.0000278143823\n",
      "[400,     1] loss: 0.0000265443086\n",
      "[401,     1] loss: 0.0000339278806\n",
      "[402,     1] loss: 0.0000310404401\n",
      "[403,     1] loss: 0.0000312791672\n",
      "[404,     1] loss: 0.0000261560053\n",
      "[405,     1] loss: 0.0000257891050\n",
      "[406,     1] loss: 0.0000249736040\n",
      "[407,     1] loss: 0.0000268088421\n",
      "[408,     1] loss: 0.0000263344438\n",
      "[409,     1] loss: 0.0000239013534\n",
      "[410,     1] loss: 0.0000301943597\n",
      "[411,     1] loss: 0.0000348114903\n",
      "[412,     1] loss: 0.0000302744651\n",
      "[413,     1] loss: 0.0000322191510\n",
      "[414,     1] loss: 0.0000268792850\n",
      "[415,     1] loss: 0.0000296362006\n",
      "[416,     1] loss: 0.0000311975309\n",
      "[417,     1] loss: 0.0000275986618\n",
      "[418,     1] loss: 0.0000275847851\n",
      "[419,     1] loss: 0.0000313602242\n",
      "[420,     1] loss: 0.0000263098249\n",
      "[421,     1] loss: 0.0000264371309\n",
      "[422,     1] loss: 0.0000279020052\n",
      "[423,     1] loss: 0.0000245314033\n",
      "[424,     1] loss: 0.0000269875425\n",
      "[425,     1] loss: 0.0000330755720\n",
      "[426,     1] loss: 0.0000305575202\n",
      "[427,     1] loss: 0.0000312083430\n",
      "[428,     1] loss: 0.0000321575848\n",
      "[429,     1] loss: 0.0000246831478\n",
      "[430,     1] loss: 0.0000301663298\n",
      "[431,     1] loss: 0.0000294561411\n",
      "[432,     1] loss: 0.0000281316286\n",
      "[433,     1] loss: 0.0000234485909\n",
      "[434,     1] loss: 0.0000281958637\n",
      "[435,     1] loss: 0.0000337450183\n",
      "[436,     1] loss: 0.0000283704139\n",
      "[437,     1] loss: 0.0000252140278\n",
      "[438,     1] loss: 0.0000339761318\n",
      "[439,     1] loss: 0.0000237823828\n",
      "[440,     1] loss: 0.0000280526117\n",
      "[441,     1] loss: 0.0000256233732\n",
      "[442,     1] loss: 0.0000288486452\n",
      "[443,     1] loss: 0.0000286553957\n",
      "[444,     1] loss: 0.0000310438831\n",
      "[445,     1] loss: 0.0000278283085\n",
      "[446,     1] loss: 0.0000257491192\n",
      "[447,     1] loss: 0.0000274494116\n",
      "[448,     1] loss: 0.0000292709097\n",
      "[449,     1] loss: 0.0000279675063\n",
      "[450,     1] loss: 0.0000326012349\n",
      "[451,     1] loss: 0.0000295666920\n",
      "[452,     1] loss: 0.0000271276542\n",
      "[453,     1] loss: 0.0000258565327\n",
      "[454,     1] loss: 0.0000276400213\n",
      "[455,     1] loss: 0.0000280632637\n",
      "[456,     1] loss: 0.0000307956128\n",
      "[457,     1] loss: 0.0000294885540\n",
      "[458,     1] loss: 0.0000339669816\n",
      "[459,     1] loss: 0.0000297746068\n",
      "[460,     1] loss: 0.0000316278863\n",
      "[461,     1] loss: 0.0000247113756\n",
      "[462,     1] loss: 0.0000255656720\n",
      "[463,     1] loss: 0.0000301150169\n",
      "[464,     1] loss: 0.0000348881673\n",
      "[465,     1] loss: 0.0000326893292\n",
      "[466,     1] loss: 0.0000281184242\n",
      "[467,     1] loss: 0.0000248539553\n",
      "[468,     1] loss: 0.0000258812244\n",
      "[469,     1] loss: 0.0000347661116\n",
      "[470,     1] loss: 0.0000286593771\n",
      "[471,     1] loss: 0.0000318851846\n",
      "[472,     1] loss: 0.0000263281487\n",
      "[473,     1] loss: 0.0000288841256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[474,     1] loss: 0.0000309779018\n",
      "[475,     1] loss: 0.0000284027919\n",
      "[476,     1] loss: 0.0000315133162\n",
      "[477,     1] loss: 0.0000280740933\n",
      "[478,     1] loss: 0.0000242681228\n",
      "[479,     1] loss: 0.0000229628844\n",
      "[480,     1] loss: 0.0000290699740\n",
      "[481,     1] loss: 0.0000248912140\n",
      "[482,     1] loss: 0.0000291358039\n",
      "[483,     1] loss: 0.0000223800031\n",
      "[484,     1] loss: 0.0000270116958\n",
      "[485,     1] loss: 0.0000238106222\n",
      "[486,     1] loss: 0.0000313605356\n",
      "[487,     1] loss: 0.0000213442763\n",
      "[488,     1] loss: 0.0000280827691\n",
      "[489,     1] loss: 0.0000343799620\n",
      "[490,     1] loss: 0.0000282091554\n",
      "[491,     1] loss: 0.0000205605669\n",
      "[492,     1] loss: 0.0000278769236\n",
      "[493,     1] loss: 0.0000339688209\n",
      "[494,     1] loss: 0.0000315614045\n",
      "[495,     1] loss: 0.0000268492993\n",
      "[496,     1] loss: 0.0000242387949\n",
      "[497,     1] loss: 0.0000306898059\n",
      "[498,     1] loss: 0.0000249425968\n",
      "[499,     1] loss: 0.0000272361212\n",
      "[500,     1] loss: 0.0000321538217\n",
      "[501,     1] loss: 0.0000246900890\n",
      "[502,     1] loss: 0.0000294409430\n",
      "[503,     1] loss: 0.0000316538702\n",
      "[504,     1] loss: 0.0000262221060\n",
      "[505,     1] loss: 0.0000285705260\n",
      "[506,     1] loss: 0.0000240831461\n",
      "[507,     1] loss: 0.0000308815419\n",
      "[508,     1] loss: 0.0000292885001\n",
      "[509,     1] loss: 0.0000302067900\n",
      "[510,     1] loss: 0.0000274575228\n",
      "[511,     1] loss: 0.0000284005655\n",
      "[512,     1] loss: 0.0000245850591\n",
      "[513,     1] loss: 0.0000319901912\n",
      "[514,     1] loss: 0.0000308695075\n",
      "[515,     1] loss: 0.0000275343511\n",
      "[516,     1] loss: 0.0000243969902\n",
      "[517,     1] loss: 0.0000256138650\n",
      "[518,     1] loss: 0.0000292665791\n",
      "[519,     1] loss: 0.0000270760851\n",
      "[520,     1] loss: 0.0000310414704\n",
      "[521,     1] loss: 0.0000317621889\n",
      "[522,     1] loss: 0.0000263102353\n",
      "[523,     1] loss: 0.0000291058270\n",
      "[524,     1] loss: 0.0000303917652\n",
      "[525,     1] loss: 0.0000252530153\n",
      "[526,     1] loss: 0.0000356270903\n",
      "[527,     1] loss: 0.0000217785651\n",
      "[528,     1] loss: 0.0000281265820\n",
      "[529,     1] loss: 0.0000343340944\n",
      "[530,     1] loss: 0.0000243808958\n",
      "[531,     1] loss: 0.0000280671753\n",
      "[532,     1] loss: 0.0000223666299\n",
      "[533,     1] loss: 0.0000289501040\n",
      "[534,     1] loss: 0.0000292236597\n",
      "[535,     1] loss: 0.0000289152144\n",
      "[536,     1] loss: 0.0000248685305\n",
      "[537,     1] loss: 0.0000223900002\n",
      "[538,     1] loss: 0.0000325955392\n",
      "[539,     1] loss: 0.0000231843529\n",
      "[540,     1] loss: 0.0000305385038\n",
      "[541,     1] loss: 0.0000300176704\n",
      "[542,     1] loss: 0.0000267415133\n",
      "[543,     1] loss: 0.0000287670584\n",
      "[544,     1] loss: 0.0000301559223\n",
      "[545,     1] loss: 0.0000255264313\n",
      "[546,     1] loss: 0.0000255408522\n",
      "[547,     1] loss: 0.0000299480016\n",
      "[548,     1] loss: 0.0000221708702\n",
      "[549,     1] loss: 0.0000248136843\n",
      "[550,     1] loss: 0.0000251768652\n",
      "[551,     1] loss: 0.0000323011220\n",
      "[552,     1] loss: 0.0000275739440\n",
      "[553,     1] loss: 0.0000354938500\n",
      "[554,     1] loss: 0.0000207857171\n",
      "[555,     1] loss: 0.0000272337929\n",
      "[556,     1] loss: 0.0000286808587\n",
      "[557,     1] loss: 0.0000234372448\n",
      "[558,     1] loss: 0.0000287393457\n",
      "[559,     1] loss: 0.0000290679280\n",
      "[560,     1] loss: 0.0000330146489\n",
      "[561,     1] loss: 0.0000265644514\n",
      "[562,     1] loss: 0.0000266120158\n",
      "[563,     1] loss: 0.0000297496095\n",
      "[564,     1] loss: 0.0000200272480\n",
      "[565,     1] loss: 0.0000220748057\n",
      "[566,     1] loss: 0.0000312195421\n",
      "[567,     1] loss: 0.0000279083557\n",
      "[568,     1] loss: 0.0000275515456\n",
      "[569,     1] loss: 0.0000306788512\n",
      "[570,     1] loss: 0.0000225826225\n",
      "[571,     1] loss: 0.0000280658045\n",
      "[572,     1] loss: 0.0000305535650\n",
      "[573,     1] loss: 0.0000317180180\n",
      "[574,     1] loss: 0.0000334200799\n",
      "[575,     1] loss: 0.0000348841626\n",
      "[576,     1] loss: 0.0000259506691\n",
      "[577,     1] loss: 0.0000227057928\n",
      "[578,     1] loss: 0.0000318857899\n",
      "[579,     1] loss: 0.0000279552100\n",
      "[580,     1] loss: 0.0000253804028\n",
      "[581,     1] loss: 0.0000291325472\n",
      "[582,     1] loss: 0.0000286198629\n",
      "[583,     1] loss: 0.0000296738901\n",
      "[584,     1] loss: 0.0000251193007\n",
      "[585,     1] loss: 0.0000354939926\n",
      "[586,     1] loss: 0.0000249492994\n",
      "[587,     1] loss: 0.0000256788480\n",
      "[588,     1] loss: 0.0000344585802\n",
      "[589,     1] loss: 0.0000196728215\n",
      "[590,     1] loss: 0.0000249968492\n",
      "[591,     1] loss: 0.0000269313809\n",
      "[592,     1] loss: 0.0000297080463\n",
      "[593,     1] loss: 0.0000225889657\n",
      "[594,     1] loss: 0.0000293446967\n",
      "[595,     1] loss: 0.0000306930247\n",
      "[596,     1] loss: 0.0000304649933\n",
      "[597,     1] loss: 0.0000309420604\n",
      "[598,     1] loss: 0.0000208909812\n",
      "[599,     1] loss: 0.0000287437608\n",
      "[600,     1] loss: 0.0000372878334\n",
      "[601,     1] loss: 0.0000359174330\n",
      "[602,     1] loss: 0.0000296891463\n",
      "[603,     1] loss: 0.0000272366131\n",
      "[604,     1] loss: 0.0000206175697\n",
      "[605,     1] loss: 0.0000214560700\n",
      "[606,     1] loss: 0.0000346253888\n",
      "[607,     1] loss: 0.0000332735071\n",
      "[608,     1] loss: 0.0000328492635\n",
      "[609,     1] loss: 0.0000242887021\n",
      "[610,     1] loss: 0.0000236239313\n",
      "[611,     1] loss: 0.0000300012849\n",
      "[612,     1] loss: 0.0000248591794\n",
      "[613,     1] loss: 0.0000274006656\n",
      "[614,     1] loss: 0.0000350681221\n",
      "[615,     1] loss: 0.0000314362871\n",
      "[616,     1] loss: 0.0000266059127\n",
      "[617,     1] loss: 0.0000246072043\n",
      "[618,     1] loss: 0.0000285193266\n",
      "[619,     1] loss: 0.0000368014444\n",
      "[620,     1] loss: 0.0000301599357\n",
      "[621,     1] loss: 0.0000266184012\n",
      "[622,     1] loss: 0.0000261068140\n",
      "[623,     1] loss: 0.0000255605992\n",
      "[624,     1] loss: 0.0000257624459\n",
      "[625,     1] loss: 0.0000236905558\n",
      "[626,     1] loss: 0.0000250874698\n",
      "[627,     1] loss: 0.0000250692654\n",
      "[628,     1] loss: 0.0000271930592\n",
      "[629,     1] loss: 0.0000207163845\n",
      "[630,     1] loss: 0.0000355371245\n",
      "[631,     1] loss: 0.0000356797944\n",
      "[632,     1] loss: 0.0000249541918\n",
      "[633,     1] loss: 0.0000335719553\n",
      "[634,     1] loss: 0.0000293501216\n",
      "[635,     1] loss: 0.0000289386633\n",
      "[636,     1] loss: 0.0000229417434\n",
      "[637,     1] loss: 0.0000336680678\n",
      "[638,     1] loss: 0.0000289769319\n",
      "[639,     1] loss: 0.0000322678185\n",
      "[640,     1] loss: 0.0000254434737\n",
      "[641,     1] loss: 0.0000304656016\n",
      "[642,     1] loss: 0.0000271641591\n",
      "[643,     1] loss: 0.0000272525504\n",
      "[644,     1] loss: 0.0000312057207\n",
      "[645,     1] loss: 0.0000242718714\n",
      "[646,     1] loss: 0.0000317039550\n",
      "[647,     1] loss: 0.0000291343400\n",
      "[648,     1] loss: 0.0000254658895\n",
      "[649,     1] loss: 0.0000327146874\n",
      "[650,     1] loss: 0.0000279672997\n",
      "[651,     1] loss: 0.0000283696194\n",
      "[652,     1] loss: 0.0000224468604\n",
      "[653,     1] loss: 0.0000291682867\n",
      "[654,     1] loss: 0.0000202013791\n",
      "[655,     1] loss: 0.0000251140154\n",
      "[656,     1] loss: 0.0000271410478\n",
      "[657,     1] loss: 0.0000273765152\n",
      "[658,     1] loss: 0.0000279347674\n",
      "[659,     1] loss: 0.0000348109228\n",
      "[660,     1] loss: 0.0000319933752\n",
      "[661,     1] loss: 0.0000255753781\n",
      "[662,     1] loss: 0.0000290314492\n",
      "[663,     1] loss: 0.0000307721552\n",
      "[664,     1] loss: 0.0000274742779\n",
      "[665,     1] loss: 0.0000237981425\n",
      "[666,     1] loss: 0.0000332632073\n",
      "[667,     1] loss: 0.0000259452790\n",
      "[668,     1] loss: 0.0000291289121\n",
      "[669,     1] loss: 0.0000243113522\n",
      "[670,     1] loss: 0.0000289262680\n",
      "[671,     1] loss: 0.0000294158381\n",
      "[672,     1] loss: 0.0000241897156\n",
      "[673,     1] loss: 0.0000255164341\n",
      "[674,     1] loss: 0.0000267023745\n",
      "[675,     1] loss: 0.0000292096462\n",
      "[676,     1] loss: 0.0000353129086\n",
      "[677,     1] loss: 0.0000341907638\n",
      "[678,     1] loss: 0.0000239967660\n",
      "[679,     1] loss: 0.0000296640210\n",
      "[680,     1] loss: 0.0000264713570\n",
      "[681,     1] loss: 0.0000304782414\n",
      "[682,     1] loss: 0.0000223858486\n",
      "[683,     1] loss: 0.0000295986625\n",
      "[684,     1] loss: 0.0000316858612\n",
      "[685,     1] loss: 0.0000294827914\n",
      "[686,     1] loss: 0.0000282108056\n",
      "[687,     1] loss: 0.0000340701081\n",
      "[688,     1] loss: 0.0000264760311\n",
      "[689,     1] loss: 0.0000279351400\n",
      "[690,     1] loss: 0.0000297638646\n",
      "[691,     1] loss: 0.0000281458400\n",
      "[692,     1] loss: 0.0000276428036\n",
      "[693,     1] loss: 0.0000240839290\n",
      "[694,     1] loss: 0.0000254983985\n",
      "[695,     1] loss: 0.0000316686492\n",
      "[696,     1] loss: 0.0000218613044\n",
      "[697,     1] loss: 0.0000280629465\n",
      "[698,     1] loss: 0.0000274679856\n",
      "[699,     1] loss: 0.0000281363260\n",
      "[700,     1] loss: 0.0000302201282\n",
      "[701,     1] loss: 0.0000320147636\n",
      "[702,     1] loss: 0.0000314206671\n",
      "[703,     1] loss: 0.0000296174752\n",
      "[704,     1] loss: 0.0000276583567\n",
      "[705,     1] loss: 0.0000303696143\n",
      "[706,     1] loss: 0.0000270744262\n",
      "[707,     1] loss: 0.0000207451056\n",
      "[708,     1] loss: 0.0000274796155\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[709,     1] loss: 0.0000274592254\n",
      "[710,     1] loss: 0.0000272260542\n",
      "[711,     1] loss: 0.0000220183443\n",
      "[712,     1] loss: 0.0000300237909\n",
      "[713,     1] loss: 0.0000265519688\n",
      "[714,     1] loss: 0.0000275500090\n",
      "[715,     1] loss: 0.0000289499614\n",
      "[716,     1] loss: 0.0000292588811\n",
      "[717,     1] loss: 0.0000240509224\n",
      "[718,     1] loss: 0.0000278568361\n",
      "[719,     1] loss: 0.0000269683718\n",
      "[720,     1] loss: 0.0000262331741\n",
      "[721,     1] loss: 0.0000330575043\n",
      "[722,     1] loss: 0.0000213909705\n",
      "[723,     1] loss: 0.0000316935970\n",
      "[724,     1] loss: 0.0000272086618\n",
      "[725,     1] loss: 0.0000332420808\n",
      "[726,     1] loss: 0.0000347942667\n",
      "[727,     1] loss: 0.0000342945888\n",
      "[728,     1] loss: 0.0000348298403\n",
      "[729,     1] loss: 0.0000219726557\n",
      "[730,     1] loss: 0.0000337157137\n",
      "[731,     1] loss: 0.0000270696852\n",
      "[732,     1] loss: 0.0000233315863\n",
      "[733,     1] loss: 0.0000308061542\n",
      "[734,     1] loss: 0.0000350494636\n",
      "[735,     1] loss: 0.0000324411551\n",
      "[736,     1] loss: 0.0000269356329\n",
      "[737,     1] loss: 0.0000273346843\n",
      "[738,     1] loss: 0.0000264943490\n",
      "[739,     1] loss: 0.0000278207037\n",
      "[740,     1] loss: 0.0000241054368\n",
      "[741,     1] loss: 0.0000231522834\n",
      "[742,     1] loss: 0.0000265176495\n",
      "[743,     1] loss: 0.0000241281916\n",
      "[744,     1] loss: 0.0000329520582\n",
      "[745,     1] loss: 0.0000228029501\n",
      "[746,     1] loss: 0.0000274653779\n",
      "[747,     1] loss: 0.0000255758699\n",
      "[748,     1] loss: 0.0000303558627\n",
      "[749,     1] loss: 0.0000292869488\n",
      "[750,     1] loss: 0.0000236738095\n",
      "[751,     1] loss: 0.0000239354558\n",
      "[752,     1] loss: 0.0000289005140\n",
      "[753,     1] loss: 0.0000261121400\n",
      "[754,     1] loss: 0.0000269516022\n",
      "[755,     1] loss: 0.0000254306477\n",
      "[756,     1] loss: 0.0000302244414\n",
      "[757,     1] loss: 0.0000326455367\n",
      "[758,     1] loss: 0.0000310497679\n",
      "[759,     1] loss: 0.0000263233756\n",
      "[760,     1] loss: 0.0000329156523\n",
      "[761,     1] loss: 0.0000302593689\n",
      "[762,     1] loss: 0.0000276803708\n",
      "[763,     1] loss: 0.0000240957204\n",
      "[764,     1] loss: 0.0000278615888\n",
      "[765,     1] loss: 0.0000231753307\n",
      "[766,     1] loss: 0.0000229808589\n",
      "[767,     1] loss: 0.0000267974945\n",
      "[768,     1] loss: 0.0000268023112\n",
      "[769,     1] loss: 0.0000254785875\n",
      "[770,     1] loss: 0.0000312121614\n",
      "[771,     1] loss: 0.0000335640187\n",
      "[772,     1] loss: 0.0000277276995\n",
      "[773,     1] loss: 0.0000272534700\n",
      "[774,     1] loss: 0.0000185074343\n",
      "[775,     1] loss: 0.0000316342455\n",
      "[776,     1] loss: 0.0000321157189\n",
      "[777,     1] loss: 0.0000297025166\n",
      "[778,     1] loss: 0.0000246522774\n",
      "[779,     1] loss: 0.0000293671590\n",
      "[780,     1] loss: 0.0000271731522\n",
      "[781,     1] loss: 0.0000250670390\n",
      "[782,     1] loss: 0.0000285363436\n",
      "[783,     1] loss: 0.0000261533220\n",
      "[784,     1] loss: 0.0000277858984\n",
      "[785,     1] loss: 0.0000345187378\n",
      "[786,     1] loss: 0.0000291108765\n",
      "[787,     1] loss: 0.0000312609540\n",
      "[788,     1] loss: 0.0000272378704\n",
      "[789,     1] loss: 0.0000298794097\n",
      "[790,     1] loss: 0.0000253480132\n",
      "[791,     1] loss: 0.0000256804604\n",
      "[792,     1] loss: 0.0000303014735\n",
      "[793,     1] loss: 0.0000242747759\n",
      "[794,     1] loss: 0.0000282944966\n",
      "[795,     1] loss: 0.0000276590989\n",
      "[796,     1] loss: 0.0000263951195\n",
      "[797,     1] loss: 0.0000264869246\n",
      "[798,     1] loss: 0.0000314182893\n",
      "[799,     1] loss: 0.0000259559602\n",
      "[800,     1] loss: 0.0000261550245\n",
      "[801,     1] loss: 0.0000222146497\n",
      "[802,     1] loss: 0.0000285499002\n",
      "[803,     1] loss: 0.0000328140770\n",
      "[804,     1] loss: 0.0000235814921\n",
      "[805,     1] loss: 0.0000271959143\n",
      "[806,     1] loss: 0.0000252411177\n",
      "[807,     1] loss: 0.0000360956124\n",
      "[808,     1] loss: 0.0000285681774\n",
      "[809,     1] loss: 0.0000234452717\n",
      "[810,     1] loss: 0.0000312880264\n",
      "[811,     1] loss: 0.0000307025504\n",
      "[812,     1] loss: 0.0000217314591\n",
      "[813,     1] loss: 0.0000316725054\n",
      "[814,     1] loss: 0.0000317965372\n",
      "[815,     1] loss: 0.0000313004217\n",
      "[816,     1] loss: 0.0000324270368\n",
      "[817,     1] loss: 0.0000252452010\n",
      "[818,     1] loss: 0.0000284524285\n",
      "[819,     1] loss: 0.0000294365425\n",
      "[820,     1] loss: 0.0000349512964\n",
      "[821,     1] loss: 0.0000387173350\n",
      "[822,     1] loss: 0.0000311171345\n",
      "[823,     1] loss: 0.0000312551419\n",
      "[824,     1] loss: 0.0000341930252\n",
      "[825,     1] loss: 0.0000205092583\n",
      "[826,     1] loss: 0.0000302554283\n",
      "[827,     1] loss: 0.0000261670793\n",
      "[828,     1] loss: 0.0000282576279\n",
      "[829,     1] loss: 0.0000238634660\n",
      "[830,     1] loss: 0.0000262174435\n",
      "[831,     1] loss: 0.0000246601878\n",
      "[832,     1] loss: 0.0000283197121\n",
      "[833,     1] loss: 0.0000234355975\n",
      "[834,     1] loss: 0.0000274982740\n",
      "[835,     1] loss: 0.0000274928432\n",
      "[836,     1] loss: 0.0000282883586\n",
      "[837,     1] loss: 0.0000262760033\n",
      "[838,     1] loss: 0.0000255558669\n",
      "[839,     1] loss: 0.0000280667882\n",
      "[840,     1] loss: 0.0000248002645\n",
      "[841,     1] loss: 0.0000332231080\n",
      "[842,     1] loss: 0.0000349068810\n",
      "[843,     1] loss: 0.0000311762240\n",
      "[844,     1] loss: 0.0000302482833\n",
      "[845,     1] loss: 0.0000266695220\n",
      "[846,     1] loss: 0.0000274130114\n",
      "[847,     1] loss: 0.0000326357695\n",
      "[848,     1] loss: 0.0000301200722\n",
      "[849,     1] loss: 0.0000267464027\n",
      "[850,     1] loss: 0.0000311530690\n",
      "[851,     1] loss: 0.0000234169071\n",
      "[852,     1] loss: 0.0000273686281\n",
      "[853,     1] loss: 0.0000234417399\n",
      "[854,     1] loss: 0.0000306923059\n",
      "[855,     1] loss: 0.0000186911202\n",
      "[856,     1] loss: 0.0000343288528\n",
      "[857,     1] loss: 0.0000297621678\n",
      "[858,     1] loss: 0.0000242477414\n",
      "[859,     1] loss: 0.0000298607018\n",
      "[860,     1] loss: 0.0000272110483\n",
      "[861,     1] loss: 0.0000310191390\n",
      "[862,     1] loss: 0.0000292133947\n",
      "[863,     1] loss: 0.0000330347830\n",
      "[864,     1] loss: 0.0000264341070\n",
      "[865,     1] loss: 0.0000201434086\n",
      "[866,     1] loss: 0.0000279865693\n",
      "[867,     1] loss: 0.0000222532137\n",
      "[868,     1] loss: 0.0000261446461\n",
      "[869,     1] loss: 0.0000326895271\n",
      "[870,     1] loss: 0.0000248811091\n",
      "[871,     1] loss: 0.0000249811274\n",
      "[872,     1] loss: 0.0000191903178\n",
      "[873,     1] loss: 0.0000204520140\n",
      "[874,     1] loss: 0.0000246622483\n",
      "[875,     1] loss: 0.0000241498477\n",
      "[876,     1] loss: 0.0000245235424\n",
      "[877,     1] loss: 0.0000314369274\n",
      "[878,     1] loss: 0.0000264943024\n",
      "[879,     1] loss: 0.0000242302529\n",
      "[880,     1] loss: 0.0000283286528\n",
      "[881,     1] loss: 0.0000308887946\n",
      "[882,     1] loss: 0.0000300136860\n",
      "[883,     1] loss: 0.0000259134220\n",
      "[884,     1] loss: 0.0000302876142\n",
      "[885,     1] loss: 0.0000261318870\n",
      "[886,     1] loss: 0.0000274194201\n",
      "[887,     1] loss: 0.0000235894913\n",
      "[888,     1] loss: 0.0000327897258\n",
      "[889,     1] loss: 0.0000266058923\n",
      "[890,     1] loss: 0.0000326533715\n",
      "[891,     1] loss: 0.0000300316489\n",
      "[892,     1] loss: 0.0000301446824\n",
      "[893,     1] loss: 0.0000272750010\n",
      "[894,     1] loss: 0.0000262824527\n",
      "[895,     1] loss: 0.0000330445677\n",
      "[896,     1] loss: 0.0000281142391\n",
      "[897,     1] loss: 0.0000338975457\n",
      "[898,     1] loss: 0.0000274435442\n",
      "[899,     1] loss: 0.0000253172824\n",
      "[900,     1] loss: 0.0000229185549\n",
      "[901,     1] loss: 0.0000264003698\n",
      "[902,     1] loss: 0.0000341743435\n",
      "[903,     1] loss: 0.0000282820140\n",
      "[904,     1] loss: 0.0000214862463\n",
      "[905,     1] loss: 0.0000326608366\n",
      "[906,     1] loss: 0.0000248203520\n",
      "[907,     1] loss: 0.0000301104621\n",
      "[908,     1] loss: 0.0000217922905\n",
      "[909,     1] loss: 0.0000225517608\n",
      "[910,     1] loss: 0.0000217472683\n",
      "[911,     1] loss: 0.0000283456146\n",
      "[912,     1] loss: 0.0000298256578\n",
      "[913,     1] loss: 0.0000323791610\n",
      "[914,     1] loss: 0.0000308902119\n",
      "[915,     1] loss: 0.0000266954827\n",
      "[916,     1] loss: 0.0000321378087\n",
      "[917,     1] loss: 0.0000242462440\n",
      "[918,     1] loss: 0.0000223415671\n",
      "[919,     1] loss: 0.0000246843934\n",
      "[920,     1] loss: 0.0000280649983\n",
      "[921,     1] loss: 0.0000290580763\n",
      "[922,     1] loss: 0.0000233499522\n",
      "[923,     1] loss: 0.0000265254203\n",
      "[924,     1] loss: 0.0000244768802\n",
      "[925,     1] loss: 0.0000342506100\n",
      "[926,     1] loss: 0.0000339630729\n",
      "[927,     1] loss: 0.0000328267139\n",
      "[928,     1] loss: 0.0000280543871\n",
      "[929,     1] loss: 0.0000312280288\n",
      "[930,     1] loss: 0.0000323468063\n",
      "[931,     1] loss: 0.0000291286764\n",
      "[932,     1] loss: 0.0000317274040\n",
      "[933,     1] loss: 0.0000270249060\n",
      "[934,     1] loss: 0.0000328261463\n",
      "[935,     1] loss: 0.0000233688232\n",
      "[936,     1] loss: 0.0000267574767\n",
      "[937,     1] loss: 0.0000268264470\n",
      "[938,     1] loss: 0.0000290680386\n",
      "[939,     1] loss: 0.0000246096548\n",
      "[940,     1] loss: 0.0000294554105\n",
      "[941,     1] loss: 0.0000296819664\n",
      "[942,     1] loss: 0.0000290114142\n",
      "[943,     1] loss: 0.0000251414283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[944,     1] loss: 0.0000267336087\n",
      "[945,     1] loss: 0.0000271531084\n",
      "[946,     1] loss: 0.0000259193941\n",
      "[947,     1] loss: 0.0000291476899\n",
      "[948,     1] loss: 0.0000338687445\n",
      "[949,     1] loss: 0.0000292476616\n",
      "[950,     1] loss: 0.0000256937783\n",
      "[951,     1] loss: 0.0000243394345\n",
      "[952,     1] loss: 0.0000264723029\n",
      "[953,     1] loss: 0.0000294222438\n",
      "[954,     1] loss: 0.0000222328963\n",
      "[955,     1] loss: 0.0000302026659\n",
      "[956,     1] loss: 0.0000291634089\n",
      "[957,     1] loss: 0.0000253332226\n",
      "[958,     1] loss: 0.0000348186033\n",
      "[959,     1] loss: 0.0000241378482\n",
      "[960,     1] loss: 0.0000305237831\n",
      "[961,     1] loss: 0.0000255683204\n",
      "[962,     1] loss: 0.0000260929432\n",
      "[963,     1] loss: 0.0000241220274\n",
      "[964,     1] loss: 0.0000325415196\n",
      "[965,     1] loss: 0.0000315053883\n",
      "[966,     1] loss: 0.0000253659964\n",
      "[967,     1] loss: 0.0000231055688\n",
      "[968,     1] loss: 0.0000296259881\n",
      "[969,     1] loss: 0.0000221666429\n",
      "[970,     1] loss: 0.0000279839936\n",
      "[971,     1] loss: 0.0000329145521\n",
      "[972,     1] loss: 0.0000270397315\n",
      "[973,     1] loss: 0.0000221038877\n",
      "[974,     1] loss: 0.0000285289047\n",
      "[975,     1] loss: 0.0000198117108\n",
      "[976,     1] loss: 0.0000310395291\n",
      "[977,     1] loss: 0.0000311079464\n",
      "[978,     1] loss: 0.0000291316741\n",
      "[979,     1] loss: 0.0000305401423\n",
      "[980,     1] loss: 0.0000240915615\n",
      "[981,     1] loss: 0.0000279594737\n",
      "[982,     1] loss: 0.0000257174426\n",
      "[983,     1] loss: 0.0000193418193\n",
      "[984,     1] loss: 0.0000235392101\n",
      "[985,     1] loss: 0.0000231959260\n",
      "[986,     1] loss: 0.0000277304760\n",
      "[987,     1] loss: 0.0000201003699\n",
      "[988,     1] loss: 0.0000223721363\n",
      "[989,     1] loss: 0.0000277111307\n",
      "[990,     1] loss: 0.0000284187321\n",
      "[991,     1] loss: 0.0000267632538\n",
      "[992,     1] loss: 0.0000319520099\n",
      "[993,     1] loss: 0.0000247494812\n",
      "[994,     1] loss: 0.0000335707155\n",
      "[995,     1] loss: 0.0000243277595\n",
      "[996,     1] loss: 0.0000236240492\n",
      "[997,     1] loss: 0.0000290497177\n",
      "[998,     1] loss: 0.0000262930727\n",
      "[999,     1] loss: 0.0000328158611\n",
      "[1000,     1] loss: 0.0000224726595\n",
      "[1001,     1] loss: 0.0000333477656\n",
      "[1002,     1] loss: 0.0000237958840\n",
      "[1003,     1] loss: 0.0000306011498\n",
      "[1004,     1] loss: 0.0000285597984\n",
      "[1005,     1] loss: 0.0000250803336\n",
      "[1006,     1] loss: 0.0000255518768\n",
      "[1007,     1] loss: 0.0000259787048\n",
      "[1008,     1] loss: 0.0000300594082\n",
      "[1009,     1] loss: 0.0000333816948\n",
      "[1010,     1] loss: 0.0000302895816\n",
      "[1011,     1] loss: 0.0000229237281\n",
      "[1012,     1] loss: 0.0000345060951\n",
      "[1013,     1] loss: 0.0000253349543\n",
      "[1014,     1] loss: 0.0000257214968\n",
      "[1015,     1] loss: 0.0000285440707\n",
      "[1016,     1] loss: 0.0000235674219\n",
      "[1017,     1] loss: 0.0000252371392\n",
      "[1018,     1] loss: 0.0000242690396\n",
      "[1019,     1] loss: 0.0000288024778\n",
      "[1020,     1] loss: 0.0000274302874\n",
      "[1021,     1] loss: 0.0000333832111\n",
      "[1022,     1] loss: 0.0000315661222\n",
      "[1023,     1] loss: 0.0000262063288\n",
      "[1024,     1] loss: 0.0000300970918\n",
      "[1025,     1] loss: 0.0000230630321\n",
      "[1026,     1] loss: 0.0000271212368\n",
      "[1027,     1] loss: 0.0000233641564\n",
      "[1028,     1] loss: 0.0000256604457\n",
      "[1029,     1] loss: 0.0000259291526\n",
      "[1030,     1] loss: 0.0000301057997\n",
      "[1031,     1] loss: 0.0000305073394\n",
      "[1032,     1] loss: 0.0000261546171\n",
      "[1033,     1] loss: 0.0000239143090\n",
      "[1034,     1] loss: 0.0000215969820\n",
      "[1035,     1] loss: 0.0000237242610\n",
      "[1036,     1] loss: 0.0000323999324\n",
      "[1037,     1] loss: 0.0000231610320\n",
      "[1038,     1] loss: 0.0000260781962\n",
      "[1039,     1] loss: 0.0000292138604\n",
      "[1040,     1] loss: 0.0000326398469\n",
      "[1041,     1] loss: 0.0000215724707\n",
      "[1042,     1] loss: 0.0000296127168\n",
      "[1043,     1] loss: 0.0000199709189\n",
      "[1044,     1] loss: 0.0000332081574\n",
      "[1045,     1] loss: 0.0000255700725\n",
      "[1046,     1] loss: 0.0000260101311\n",
      "[1047,     1] loss: 0.0000268326141\n",
      "[1048,     1] loss: 0.0000192826847\n",
      "[1049,     1] loss: 0.0000291564211\n",
      "[1050,     1] loss: 0.0000267362804\n",
      "[1051,     1] loss: 0.0000323510845\n",
      "[1052,     1] loss: 0.0000275009603\n",
      "[1053,     1] loss: 0.0000298019848\n",
      "[1054,     1] loss: 0.0000256146188\n",
      "[1055,     1] loss: 0.0000215377033\n",
      "[1056,     1] loss: 0.0000310853298\n",
      "[1057,     1] loss: 0.0000328748865\n",
      "[1058,     1] loss: 0.0000314346078\n",
      "[1059,     1] loss: 0.0000272982026\n",
      "[1060,     1] loss: 0.0000278029882\n",
      "[1061,     1] loss: 0.0000279875123\n",
      "[1062,     1] loss: 0.0000222909308\n",
      "[1063,     1] loss: 0.0000279701810\n",
      "[1064,     1] loss: 0.0000237249216\n",
      "[1065,     1] loss: 0.0000259582128\n",
      "[1066,     1] loss: 0.0000264437927\n",
      "[1067,     1] loss: 0.0000284672278\n",
      "[1068,     1] loss: 0.0000280684530\n",
      "[1069,     1] loss: 0.0000356645905\n",
      "[1070,     1] loss: 0.0000252773869\n",
      "[1071,     1] loss: 0.0000261080568\n",
      "[1072,     1] loss: 0.0000277159095\n",
      "[1073,     1] loss: 0.0000243677321\n",
      "[1074,     1] loss: 0.0000284057722\n",
      "[1075,     1] loss: 0.0000313198369\n",
      "[1076,     1] loss: 0.0000197204412\n",
      "[1077,     1] loss: 0.0000298294064\n",
      "[1078,     1] loss: 0.0000277512969\n",
      "[1079,     1] loss: 0.0000270026503\n",
      "[1080,     1] loss: 0.0000264657632\n",
      "[1081,     1] loss: 0.0000288382755\n",
      "[1082,     1] loss: 0.0000299011503\n",
      "[1083,     1] loss: 0.0000375069212\n",
      "[1084,     1] loss: 0.0000229185476\n",
      "[1085,     1] loss: 0.0000292378303\n",
      "[1086,     1] loss: 0.0000184596909\n",
      "[1087,     1] loss: 0.0000262562593\n",
      "[1088,     1] loss: 0.0000262042886\n",
      "[1089,     1] loss: 0.0000276386912\n",
      "[1090,     1] loss: 0.0000288319279\n",
      "[1091,     1] loss: 0.0000272813748\n",
      "[1092,     1] loss: 0.0000254069571\n",
      "[1093,     1] loss: 0.0000253384671\n",
      "[1094,     1] loss: 0.0000229281359\n",
      "[1095,     1] loss: 0.0000305589114\n",
      "[1096,     1] loss: 0.0000337264908\n",
      "[1097,     1] loss: 0.0000258745975\n",
      "[1098,     1] loss: 0.0000211531573\n",
      "[1099,     1] loss: 0.0000281294750\n",
      "[1100,     1] loss: 0.0000326339592\n",
      "[1101,     1] loss: 0.0000314238627\n",
      "[1102,     1] loss: 0.0000302155036\n",
      "[1103,     1] loss: 0.0000255517720\n",
      "[1104,     1] loss: 0.0000293188059\n",
      "[1105,     1] loss: 0.0000264882518\n",
      "[1106,     1] loss: 0.0000308598392\n",
      "[1107,     1] loss: 0.0000292457087\n",
      "[1108,     1] loss: 0.0000288652140\n",
      "[1109,     1] loss: 0.0000294928381\n",
      "[1110,     1] loss: 0.0000254194863\n",
      "[1111,     1] loss: 0.0000232347098\n",
      "[1112,     1] loss: 0.0000307071139\n",
      "[1113,     1] loss: 0.0000280752400\n",
      "[1114,     1] loss: 0.0000337811041\n",
      "[1115,     1] loss: 0.0000290447642\n",
      "[1116,     1] loss: 0.0000247771299\n",
      "[1117,     1] loss: 0.0000298414816\n",
      "[1118,     1] loss: 0.0000272718200\n",
      "[1119,     1] loss: 0.0000245761941\n",
      "[1120,     1] loss: 0.0000243159229\n",
      "[1121,     1] loss: 0.0000246695068\n",
      "[1122,     1] loss: 0.0000211678765\n",
      "[1123,     1] loss: 0.0000245193864\n",
      "[1124,     1] loss: 0.0000257366046\n",
      "[1125,     1] loss: 0.0000289687741\n",
      "[1126,     1] loss: 0.0000306419359\n",
      "[1127,     1] loss: 0.0000297109887\n",
      "[1128,     1] loss: 0.0000250134442\n",
      "[1129,     1] loss: 0.0000251857098\n",
      "[1130,     1] loss: 0.0000271803467\n",
      "[1131,     1] loss: 0.0000231807659\n",
      "[1132,     1] loss: 0.0000252863363\n",
      "[1133,     1] loss: 0.0000281654444\n",
      "[1134,     1] loss: 0.0000255675754\n",
      "[1135,     1] loss: 0.0000258690678\n",
      "[1136,     1] loss: 0.0000253316102\n",
      "[1137,     1] loss: 0.0000323612679\n",
      "[1138,     1] loss: 0.0000342739513\n",
      "[1139,     1] loss: 0.0000218092158\n",
      "[1140,     1] loss: 0.0000340170518\n",
      "[1141,     1] loss: 0.0000300362299\n",
      "[1142,     1] loss: 0.0000260362955\n",
      "[1143,     1] loss: 0.0000210789294\n",
      "[1144,     1] loss: 0.0000244913797\n",
      "[1145,     1] loss: 0.0000252724742\n",
      "[1146,     1] loss: 0.0000308011629\n",
      "[1147,     1] loss: 0.0000293286314\n",
      "[1148,     1] loss: 0.0000223595023\n",
      "[1149,     1] loss: 0.0000360673614\n",
      "[1150,     1] loss: 0.0000249052042\n",
      "[1151,     1] loss: 0.0000286617898\n",
      "[1152,     1] loss: 0.0000287927251\n",
      "[1153,     1] loss: 0.0000264604314\n",
      "[1154,     1] loss: 0.0000215607317\n",
      "[1155,     1] loss: 0.0000219231923\n",
      "[1156,     1] loss: 0.0000259039865\n",
      "[1157,     1] loss: 0.0000252760976\n",
      "[1158,     1] loss: 0.0000222032235\n",
      "[1159,     1] loss: 0.0000300622487\n",
      "[1160,     1] loss: 0.0000308774586\n",
      "[1161,     1] loss: 0.0000332242373\n",
      "[1162,     1] loss: 0.0000324981520\n",
      "[1163,     1] loss: 0.0000329451606\n",
      "[1164,     1] loss: 0.0000293164892\n",
      "[1165,     1] loss: 0.0000254791172\n",
      "[1166,     1] loss: 0.0000267538155\n",
      "[1167,     1] loss: 0.0000316805381\n",
      "[1168,     1] loss: 0.0000255633437\n",
      "[1169,     1] loss: 0.0000218199304\n",
      "[1170,     1] loss: 0.0000273509882\n",
      "[1171,     1] loss: 0.0000243863353\n",
      "[1172,     1] loss: 0.0000227576980\n",
      "[1173,     1] loss: 0.0000317232363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1174,     1] loss: 0.0000271455181\n",
      "[1175,     1] loss: 0.0000238833061\n",
      "[1176,     1] loss: 0.0000298216444\n",
      "[1177,     1] loss: 0.0000250170531\n",
      "[1178,     1] loss: 0.0000352103874\n",
      "[1179,     1] loss: 0.0000253488659\n",
      "[1180,     1] loss: 0.0000287113711\n",
      "[1181,     1] loss: 0.0000281003769\n",
      "[1182,     1] loss: 0.0000291072589\n",
      "[1183,     1] loss: 0.0000251846039\n",
      "[1184,     1] loss: 0.0000284139329\n",
      "[1185,     1] loss: 0.0000273175101\n",
      "[1186,     1] loss: 0.0000239898480\n",
      "[1187,     1] loss: 0.0000271397672\n",
      "[1188,     1] loss: 0.0000270108314\n",
      "[1189,     1] loss: 0.0000255490275\n",
      "[1190,     1] loss: 0.0000288901763\n",
      "[1191,     1] loss: 0.0000304870162\n",
      "[1192,     1] loss: 0.0000277010549\n",
      "[1193,     1] loss: 0.0000240033027\n",
      "[1194,     1] loss: 0.0000200511742\n",
      "[1195,     1] loss: 0.0000267166091\n",
      "[1196,     1] loss: 0.0000316737714\n",
      "[1197,     1] loss: 0.0000209109392\n",
      "[1198,     1] loss: 0.0000234962965\n",
      "[1199,     1] loss: 0.0000316311140\n",
      "[1200,     1] loss: 0.0000328236667\n",
      "[1201,     1] loss: 0.0000204399214\n",
      "[1202,     1] loss: 0.0000264466391\n",
      "[1203,     1] loss: 0.0000318710372\n",
      "[1204,     1] loss: 0.0000268958480\n",
      "[1205,     1] loss: 0.0000341888051\n",
      "[1206,     1] loss: 0.0000326223468\n",
      "[1207,     1] loss: 0.0000350842485\n",
      "[1208,     1] loss: 0.0000249133707\n",
      "[1209,     1] loss: 0.0000352540548\n",
      "[1210,     1] loss: 0.0000343528314\n",
      "[1211,     1] loss: 0.0000309503812\n",
      "[1212,     1] loss: 0.0000238775741\n",
      "[1213,     1] loss: 0.0000262416143\n",
      "[1214,     1] loss: 0.0000255215506\n",
      "[1215,     1] loss: 0.0000286746712\n",
      "[1216,     1] loss: 0.0000346319168\n",
      "[1217,     1] loss: 0.0000312200282\n",
      "[1218,     1] loss: 0.0000302945904\n",
      "[1219,     1] loss: 0.0000296099199\n",
      "[1220,     1] loss: 0.0000252968282\n",
      "[1221,     1] loss: 0.0000250355195\n",
      "[1222,     1] loss: 0.0000214847227\n",
      "[1223,     1] loss: 0.0000287727802\n",
      "[1224,     1] loss: 0.0000264277915\n",
      "[1225,     1] loss: 0.0000247063930\n",
      "[1226,     1] loss: 0.0000201927935\n",
      "[1227,     1] loss: 0.0000269511947\n",
      "[1228,     1] loss: 0.0000287415081\n",
      "[1229,     1] loss: 0.0000327477203\n",
      "[1230,     1] loss: 0.0000294467347\n",
      "[1231,     1] loss: 0.0000324854889\n",
      "[1232,     1] loss: 0.0000270699675\n",
      "[1233,     1] loss: 0.0000242788956\n",
      "[1234,     1] loss: 0.0000276743260\n",
      "[1235,     1] loss: 0.0000344997767\n",
      "[1236,     1] loss: 0.0000231574231\n",
      "[1237,     1] loss: 0.0000316783291\n",
      "[1238,     1] loss: 0.0000254360231\n",
      "[1239,     1] loss: 0.0000244587631\n",
      "[1240,     1] loss: 0.0000282422639\n",
      "[1241,     1] loss: 0.0000257460983\n",
      "[1242,     1] loss: 0.0000283975329\n",
      "[1243,     1] loss: 0.0000261572102\n",
      "[1244,     1] loss: 0.0000262588437\n",
      "[1245,     1] loss: 0.0000299633830\n",
      "[1246,     1] loss: 0.0000274004444\n",
      "[1247,     1] loss: 0.0000254560873\n",
      "[1248,     1] loss: 0.0000306094647\n",
      "[1249,     1] loss: 0.0000251024583\n",
      "[1250,     1] loss: 0.0000233481842\n",
      "[1251,     1] loss: 0.0000291748554\n",
      "[1252,     1] loss: 0.0000235441723\n",
      "[1253,     1] loss: 0.0000301807915\n",
      "[1254,     1] loss: 0.0000276118604\n",
      "[1255,     1] loss: 0.0000257231499\n",
      "[1256,     1] loss: 0.0000233484534\n",
      "[1257,     1] loss: 0.0000320342690\n",
      "[1258,     1] loss: 0.0000266066549\n",
      "[1259,     1] loss: 0.0000279178232\n",
      "[1260,     1] loss: 0.0000223191804\n",
      "[1261,     1] loss: 0.0000308911665\n",
      "[1262,     1] loss: 0.0000229023150\n",
      "[1263,     1] loss: 0.0000335756282\n",
      "[1264,     1] loss: 0.0000326759007\n",
      "[1265,     1] loss: 0.0000293737277\n",
      "[1266,     1] loss: 0.0000252163300\n",
      "[1267,     1] loss: 0.0000257991458\n",
      "[1268,     1] loss: 0.0000276650302\n",
      "[1269,     1] loss: 0.0000291846431\n",
      "[1270,     1] loss: 0.0000269878103\n",
      "[1271,     1] loss: 0.0000256277155\n",
      "[1272,     1] loss: 0.0000233250845\n",
      "[1273,     1] loss: 0.0000297707593\n",
      "[1274,     1] loss: 0.0000280018721\n",
      "[1275,     1] loss: 0.0000226255492\n",
      "[1276,     1] loss: 0.0000346144108\n",
      "[1277,     1] loss: 0.0000268256670\n",
      "[1278,     1] loss: 0.0000190923383\n",
      "[1279,     1] loss: 0.0000274038088\n",
      "[1280,     1] loss: 0.0000300971442\n",
      "[1281,     1] loss: 0.0000319441082\n",
      "[1282,     1] loss: 0.0000284222013\n",
      "[1283,     1] loss: 0.0000295452133\n",
      "[1284,     1] loss: 0.0000264270144\n",
      "[1285,     1] loss: 0.0000271186145\n",
      "[1286,     1] loss: 0.0000223176365\n",
      "[1287,     1] loss: 0.0000316738850\n",
      "[1288,     1] loss: 0.0000280665583\n",
      "[1289,     1] loss: 0.0000314815697\n",
      "[1290,     1] loss: 0.0000246530923\n",
      "[1291,     1] loss: 0.0000292004115\n",
      "[1292,     1] loss: 0.0000261560752\n",
      "[1293,     1] loss: 0.0000286332477\n",
      "[1294,     1] loss: 0.0000221425900\n",
      "[1295,     1] loss: 0.0000250126031\n",
      "[1296,     1] loss: 0.0000253460603\n",
      "[1297,     1] loss: 0.0000292145822\n",
      "[1298,     1] loss: 0.0000323371525\n",
      "[1299,     1] loss: 0.0000328634807\n",
      "[1300,     1] loss: 0.0000200921888\n",
      "[1301,     1] loss: 0.0000266306393\n",
      "[1302,     1] loss: 0.0000303362962\n",
      "[1303,     1] loss: 0.0000328693073\n",
      "[1304,     1] loss: 0.0000250780577\n",
      "[1305,     1] loss: 0.0000266169984\n",
      "[1306,     1] loss: 0.0000237124259\n",
      "[1307,     1] loss: 0.0000269836659\n",
      "[1308,     1] loss: 0.0000239675661\n",
      "[1309,     1] loss: 0.0000209587364\n",
      "[1310,     1] loss: 0.0000318028906\n",
      "[1311,     1] loss: 0.0000293615216\n",
      "[1312,     1] loss: 0.0000304311223\n",
      "[1313,     1] loss: 0.0000218603862\n",
      "[1314,     1] loss: 0.0000251650170\n",
      "[1315,     1] loss: 0.0000245125091\n",
      "[1316,     1] loss: 0.0000264526781\n",
      "[1317,     1] loss: 0.0000335042423\n",
      "[1318,     1] loss: 0.0000220555099\n",
      "[1319,     1] loss: 0.0000296516751\n",
      "[1320,     1] loss: 0.0000270770193\n",
      "[1321,     1] loss: 0.0000269695127\n",
      "[1322,     1] loss: 0.0000290141761\n",
      "[1323,     1] loss: 0.0000256969157\n",
      "[1324,     1] loss: 0.0000260923494\n",
      "[1325,     1] loss: 0.0000336564204\n",
      "[1326,     1] loss: 0.0000238686960\n",
      "[1327,     1] loss: 0.0000322817126\n",
      "[1328,     1] loss: 0.0000247563759\n",
      "[1329,     1] loss: 0.0000271105062\n",
      "[1330,     1] loss: 0.0000270369812\n",
      "[1331,     1] loss: 0.0000282066409\n",
      "[1332,     1] loss: 0.0000302886823\n",
      "[1333,     1] loss: 0.0000264453614\n",
      "[1334,     1] loss: 0.0000239369881\n",
      "[1335,     1] loss: 0.0000251659192\n",
      "[1336,     1] loss: 0.0000296905433\n",
      "[1337,     1] loss: 0.0000291393546\n",
      "[1338,     1] loss: 0.0000238631561\n",
      "[1339,     1] loss: 0.0000317282655\n",
      "[1340,     1] loss: 0.0000291407254\n",
      "[1341,     1] loss: 0.0000200901530\n",
      "[1342,     1] loss: 0.0000338025653\n",
      "[1343,     1] loss: 0.0000272852456\n",
      "[1344,     1] loss: 0.0000259393477\n",
      "[1345,     1] loss: 0.0000216240194\n",
      "[1346,     1] loss: 0.0000263548311\n",
      "[1347,     1] loss: 0.0000288712326\n",
      "[1348,     1] loss: 0.0000244036419\n",
      "[1349,     1] loss: 0.0000245414238\n",
      "[1350,     1] loss: 0.0000300607615\n",
      "[1351,     1] loss: 0.0000252556812\n",
      "[1352,     1] loss: 0.0000243782764\n",
      "[1353,     1] loss: 0.0000281873159\n",
      "[1354,     1] loss: 0.0000205155025\n",
      "[1355,     1] loss: 0.0000261650741\n",
      "[1356,     1] loss: 0.0000230177131\n",
      "[1357,     1] loss: 0.0000290317810\n",
      "[1358,     1] loss: 0.0000311561191\n",
      "[1359,     1] loss: 0.0000337647012\n",
      "[1360,     1] loss: 0.0000262702699\n",
      "[1361,     1] loss: 0.0000267256255\n",
      "[1362,     1] loss: 0.0000332092633\n",
      "[1363,     1] loss: 0.0000252479891\n",
      "[1364,     1] loss: 0.0000231093203\n",
      "[1365,     1] loss: 0.0000244604191\n",
      "[1366,     1] loss: 0.0000268092321\n",
      "[1367,     1] loss: 0.0000216214059\n",
      "[1368,     1] loss: 0.0000259463966\n",
      "[1369,     1] loss: 0.0000277248997\n",
      "[1370,     1] loss: 0.0000258448476\n",
      "[1371,     1] loss: 0.0000216200540\n",
      "[1372,     1] loss: 0.0000291581586\n",
      "[1373,     1] loss: 0.0000295404607\n",
      "[1374,     1] loss: 0.0000271501514\n",
      "[1375,     1] loss: 0.0000253174308\n",
      "[1376,     1] loss: 0.0000236516658\n",
      "[1377,     1] loss: 0.0000278295454\n",
      "[1378,     1] loss: 0.0000317190454\n",
      "[1379,     1] loss: 0.0000336527533\n",
      "[1380,     1] loss: 0.0000275704107\n",
      "[1381,     1] loss: 0.0000269152020\n",
      "[1382,     1] loss: 0.0000291350792\n",
      "[1383,     1] loss: 0.0000315504061\n",
      "[1384,     1] loss: 0.0000310979958\n",
      "[1385,     1] loss: 0.0000266828341\n",
      "[1386,     1] loss: 0.0000282813067\n",
      "[1387,     1] loss: 0.0000316336722\n",
      "[1388,     1] loss: 0.0000241194299\n",
      "[1389,     1] loss: 0.0000242728711\n",
      "[1390,     1] loss: 0.0000286676077\n",
      "[1391,     1] loss: 0.0000323974353\n",
      "[1392,     1] loss: 0.0000294881465\n",
      "[1393,     1] loss: 0.0000285097252\n",
      "[1394,     1] loss: 0.0000242004040\n",
      "[1395,     1] loss: 0.0000247842021\n",
      "[1396,     1] loss: 0.0000257574662\n",
      "[1397,     1] loss: 0.0000236861408\n",
      "[1398,     1] loss: 0.0000307596521\n",
      "[1399,     1] loss: 0.0000285790156\n",
      "[1400,     1] loss: 0.0000296361075\n",
      "[1401,     1] loss: 0.0000328425842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1402,     1] loss: 0.0000301590393\n",
      "[1403,     1] loss: 0.0000226790697\n",
      "[1404,     1] loss: 0.0000214493091\n",
      "[1405,     1] loss: 0.0000284382986\n",
      "[1406,     1] loss: 0.0000251492282\n",
      "[1407,     1] loss: 0.0000243153656\n",
      "[1408,     1] loss: 0.0000242770824\n",
      "[1409,     1] loss: 0.0000222914445\n",
      "[1410,     1] loss: 0.0000308769289\n",
      "[1411,     1] loss: 0.0000299470616\n",
      "[1412,     1] loss: 0.0000299406238\n",
      "[1413,     1] loss: 0.0000247017801\n",
      "[1414,     1] loss: 0.0000212378800\n",
      "[1415,     1] loss: 0.0000235652362\n",
      "[1416,     1] loss: 0.0000255400751\n",
      "[1417,     1] loss: 0.0000312868127\n",
      "[1418,     1] loss: 0.0000235677726\n",
      "[1419,     1] loss: 0.0000283540023\n",
      "[1420,     1] loss: 0.0000295135484\n",
      "[1421,     1] loss: 0.0000306520000\n",
      "[1422,     1] loss: 0.0000286854221\n",
      "[1423,     1] loss: 0.0000291638513\n",
      "[1424,     1] loss: 0.0000244917814\n",
      "[1425,     1] loss: 0.0000240547393\n",
      "[1426,     1] loss: 0.0000234102292\n",
      "[1427,     1] loss: 0.0000283605186\n",
      "[1428,     1] loss: 0.0000244451076\n",
      "[1429,     1] loss: 0.0000262298214\n",
      "[1430,     1] loss: 0.0000216142507\n",
      "[1431,     1] loss: 0.0000244216120\n",
      "[1432,     1] loss: 0.0000224693984\n",
      "[1433,     1] loss: 0.0000252093130\n",
      "[1434,     1] loss: 0.0000246313721\n",
      "[1435,     1] loss: 0.0000245388568\n",
      "[1436,     1] loss: 0.0000281568180\n",
      "[1437,     1] loss: 0.0000348536181\n",
      "[1438,     1] loss: 0.0000293511985\n",
      "[1439,     1] loss: 0.0000218070796\n",
      "[1440,     1] loss: 0.0000250636484\n",
      "[1441,     1] loss: 0.0000303694804\n",
      "[1442,     1] loss: 0.0000263457245\n",
      "[1443,     1] loss: 0.0000329468690\n",
      "[1444,     1] loss: 0.0000268097239\n",
      "[1445,     1] loss: 0.0000230715872\n",
      "[1446,     1] loss: 0.0000218599831\n",
      "[1447,     1] loss: 0.0000249336648\n",
      "[1448,     1] loss: 0.0000281087036\n",
      "[1449,     1] loss: 0.0000286457216\n",
      "[1450,     1] loss: 0.0000333734992\n",
      "[1451,     1] loss: 0.0000331406889\n",
      "[1452,     1] loss: 0.0000335430348\n",
      "[1453,     1] loss: 0.0000283195172\n",
      "[1454,     1] loss: 0.0000292420329\n",
      "[1455,     1] loss: 0.0000255905092\n",
      "[1456,     1] loss: 0.0000257731532\n",
      "[1457,     1] loss: 0.0000248723110\n",
      "[1458,     1] loss: 0.0000256862870\n",
      "[1459,     1] loss: 0.0000299320411\n",
      "[1460,     1] loss: 0.0000251995079\n",
      "[1461,     1] loss: 0.0000283674424\n",
      "[1462,     1] loss: 0.0000244780444\n",
      "[1463,     1] loss: 0.0000247741526\n",
      "[1464,     1] loss: 0.0000222006187\n",
      "[1465,     1] loss: 0.0000266619085\n",
      "[1466,     1] loss: 0.0000232947015\n",
      "[1467,     1] loss: 0.0000255297666\n",
      "[1468,     1] loss: 0.0000333082950\n",
      "[1469,     1] loss: 0.0000288127834\n",
      "[1470,     1] loss: 0.0000308208575\n",
      "[1471,     1] loss: 0.0000254913175\n",
      "[1472,     1] loss: 0.0000199775925\n",
      "[1473,     1] loss: 0.0000286868308\n",
      "[1474,     1] loss: 0.0000274604070\n",
      "[1475,     1] loss: 0.0000320150779\n",
      "[1476,     1] loss: 0.0000333491684\n",
      "[1477,     1] loss: 0.0000266455347\n",
      "[1478,     1] loss: 0.0000281295419\n",
      "[1479,     1] loss: 0.0000276543578\n",
      "[1480,     1] loss: 0.0000284911133\n",
      "[1481,     1] loss: 0.0000321093481\n",
      "[1482,     1] loss: 0.0000345529406\n",
      "[1483,     1] loss: 0.0000293538760\n",
      "[1484,     1] loss: 0.0000281152636\n",
      "[1485,     1] loss: 0.0000287466508\n",
      "[1486,     1] loss: 0.0000253178267\n",
      "[1487,     1] loss: 0.0000217317734\n",
      "[1488,     1] loss: 0.0000289463089\n",
      "[1489,     1] loss: 0.0000257054577\n",
      "[1490,     1] loss: 0.0000284849550\n",
      "[1491,     1] loss: 0.0000299509062\n",
      "[1492,     1] loss: 0.0000258956861\n",
      "[1493,     1] loss: 0.0000299807987\n",
      "[1494,     1] loss: 0.0000296991435\n",
      "[1495,     1] loss: 0.0000262435817\n",
      "[1496,     1] loss: 0.0000297812192\n",
      "[1497,     1] loss: 0.0000330349081\n",
      "[1498,     1] loss: 0.0000262348214\n",
      "[1499,     1] loss: 0.0000264533766\n",
      "[1500,     1] loss: 0.0000301536231\n",
      "[1501,     1] loss: 0.0000254158775\n",
      "[1502,     1] loss: 0.0000286066672\n",
      "[1503,     1] loss: 0.0000323604123\n",
      "[1504,     1] loss: 0.0000218123576\n",
      "[1505,     1] loss: 0.0000275683677\n",
      "[1506,     1] loss: 0.0000279903237\n",
      "[1507,     1] loss: 0.0000276504667\n",
      "[1508,     1] loss: 0.0000302119442\n",
      "[1509,     1] loss: 0.0000274950959\n",
      "[1510,     1] loss: 0.0000299549953\n",
      "[1511,     1] loss: 0.0000281763350\n",
      "[1512,     1] loss: 0.0000305971160\n",
      "[1513,     1] loss: 0.0000279449014\n",
      "[1514,     1] loss: 0.0000352866133\n",
      "[1515,     1] loss: 0.0000243219649\n",
      "[1516,     1] loss: 0.0000271295430\n",
      "[1517,     1] loss: 0.0000265140610\n",
      "[1518,     1] loss: 0.0000210538143\n",
      "[1519,     1] loss: 0.0000241225003\n",
      "[1520,     1] loss: 0.0000275021768\n",
      "[1521,     1] loss: 0.0000341613981\n",
      "[1522,     1] loss: 0.0000260251254\n",
      "[1523,     1] loss: 0.0000257992098\n",
      "[1524,     1] loss: 0.0000318892824\n",
      "[1525,     1] loss: 0.0000303561421\n",
      "[1526,     1] loss: 0.0000285799004\n",
      "[1527,     1] loss: 0.0000283951755\n",
      "[1528,     1] loss: 0.0000277358020\n",
      "[1529,     1] loss: 0.0000263845664\n",
      "[1530,     1] loss: 0.0000273049867\n",
      "[1531,     1] loss: 0.0000224031770\n",
      "[1532,     1] loss: 0.0000281972170\n",
      "[1533,     1] loss: 0.0000276433420\n",
      "[1534,     1] loss: 0.0000249224802\n",
      "[1535,     1] loss: 0.0000347390829\n",
      "[1536,     1] loss: 0.0000307849434\n",
      "[1537,     1] loss: 0.0000291656674\n",
      "[1538,     1] loss: 0.0000228418619\n",
      "[1539,     1] loss: 0.0000250718760\n",
      "[1540,     1] loss: 0.0000259362248\n",
      "[1541,     1] loss: 0.0000257943117\n",
      "[1542,     1] loss: 0.0000293880032\n",
      "[1543,     1] loss: 0.0000298643688\n",
      "[1544,     1] loss: 0.0000266039890\n",
      "[1545,     1] loss: 0.0000348967034\n",
      "[1546,     1] loss: 0.0000219505237\n",
      "[1547,     1] loss: 0.0000282962515\n",
      "[1548,     1] loss: 0.0000318580365\n",
      "[1549,     1] loss: 0.0000242643058\n",
      "[1550,     1] loss: 0.0000318288076\n",
      "[1551,     1] loss: 0.0000277489977\n",
      "[1552,     1] loss: 0.0000246513606\n",
      "[1553,     1] loss: 0.0000279130443\n",
      "[1554,     1] loss: 0.0000305173598\n",
      "[1555,     1] loss: 0.0000279833068\n",
      "[1556,     1] loss: 0.0000229710771\n",
      "[1557,     1] loss: 0.0000298320345\n",
      "[1558,     1] loss: 0.0000269993237\n",
      "[1559,     1] loss: 0.0000297372520\n",
      "[1560,     1] loss: 0.0000278802443\n",
      "[1561,     1] loss: 0.0000267986645\n",
      "[1562,     1] loss: 0.0000221688446\n",
      "[1563,     1] loss: 0.0000268657896\n",
      "[1564,     1] loss: 0.0000273792131\n",
      "[1565,     1] loss: 0.0000229617362\n",
      "[1566,     1] loss: 0.0000299539388\n",
      "[1567,     1] loss: 0.0000199058588\n",
      "[1568,     1] loss: 0.0000329165283\n",
      "[1569,     1] loss: 0.0000332596770\n",
      "[1570,     1] loss: 0.0000209525693\n",
      "[1571,     1] loss: 0.0000342929969\n",
      "[1572,     1] loss: 0.0000202106865\n",
      "[1573,     1] loss: 0.0000322876382\n",
      "[1574,     1] loss: 0.0000249937089\n",
      "[1575,     1] loss: 0.0000237249216\n",
      "[1576,     1] loss: 0.0000236305961\n",
      "[1577,     1] loss: 0.0000257673848\n",
      "[1578,     1] loss: 0.0000304564746\n",
      "[1579,     1] loss: 0.0000362373132\n",
      "[1580,     1] loss: 0.0000224353033\n",
      "[1581,     1] loss: 0.0000276507431\n",
      "[1582,     1] loss: 0.0000263778755\n",
      "[1583,     1] loss: 0.0000321391999\n",
      "[1584,     1] loss: 0.0000254163955\n",
      "[1585,     1] loss: 0.0000250234065\n",
      "[1586,     1] loss: 0.0000236762629\n",
      "[1587,     1] loss: 0.0000272422563\n",
      "[1588,     1] loss: 0.0000267333613\n",
      "[1589,     1] loss: 0.0000282850582\n",
      "[1590,     1] loss: 0.0000307638140\n",
      "[1591,     1] loss: 0.0000259456749\n",
      "[1592,     1] loss: 0.0000288283976\n",
      "[1593,     1] loss: 0.0000228814490\n",
      "[1594,     1] loss: 0.0000308134622\n",
      "[1595,     1] loss: 0.0000241372443\n",
      "[1596,     1] loss: 0.0000229194280\n",
      "[1597,     1] loss: 0.0000340985745\n",
      "[1598,     1] loss: 0.0000286083086\n",
      "[1599,     1] loss: 0.0000340988132\n",
      "[1600,     1] loss: 0.0000259093271\n",
      "[1601,     1] loss: 0.0000328433787\n",
      "[1602,     1] loss: 0.0000308764662\n",
      "[1603,     1] loss: 0.0000261626352\n",
      "[1604,     1] loss: 0.0000296495447\n",
      "[1605,     1] loss: 0.0000230322315\n",
      "[1606,     1] loss: 0.0000268538395\n",
      "[1607,     1] loss: 0.0000256823434\n",
      "[1608,     1] loss: 0.0000267027935\n",
      "[1609,     1] loss: 0.0000259934372\n",
      "[1610,     1] loss: 0.0000243851609\n",
      "[1611,     1] loss: 0.0000306425267\n",
      "[1612,     1] loss: 0.0000288015348\n",
      "[1613,     1] loss: 0.0000338076818\n",
      "[1614,     1] loss: 0.0000297743740\n",
      "[1615,     1] loss: 0.0000265994429\n",
      "[1616,     1] loss: 0.0000288734271\n",
      "[1617,     1] loss: 0.0000307280658\n",
      "[1618,     1] loss: 0.0000242003705\n",
      "[1619,     1] loss: 0.0000359815749\n",
      "[1620,     1] loss: 0.0000311470096\n",
      "[1621,     1] loss: 0.0000243339644\n",
      "[1622,     1] loss: 0.0000247780321\n",
      "[1623,     1] loss: 0.0000261515204\n",
      "[1624,     1] loss: 0.0000237872489\n",
      "[1625,     1] loss: 0.0000288682058\n",
      "[1626,     1] loss: 0.0000256128929\n",
      "[1627,     1] loss: 0.0000244609691\n",
      "[1628,     1] loss: 0.0000332930300\n",
      "[1629,     1] loss: 0.0000299655338\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1630,     1] loss: 0.0000284396112\n",
      "[1631,     1] loss: 0.0000322553591\n",
      "[1632,     1] loss: 0.0000264447241\n",
      "[1633,     1] loss: 0.0000263773982\n",
      "[1634,     1] loss: 0.0000283108675\n",
      "[1635,     1] loss: 0.0000211137361\n",
      "[1636,     1] loss: 0.0000183248689\n",
      "[1637,     1] loss: 0.0000247321004\n",
      "[1638,     1] loss: 0.0000293292687\n",
      "[1639,     1] loss: 0.0000310981122\n",
      "[1640,     1] loss: 0.0000282089546\n",
      "[1641,     1] loss: 0.0000258400687\n",
      "[1642,     1] loss: 0.0000208674741\n",
      "[1643,     1] loss: 0.0000293127086\n",
      "[1644,     1] loss: 0.0000319694605\n",
      "[1645,     1] loss: 0.0000260042521\n",
      "[1646,     1] loss: 0.0000244520896\n",
      "[1647,     1] loss: 0.0000284198031\n",
      "[1648,     1] loss: 0.0000237861590\n",
      "[1649,     1] loss: 0.0000288417708\n",
      "[1650,     1] loss: 0.0000275178667\n",
      "[1651,     1] loss: 0.0000332392723\n",
      "[1652,     1] loss: 0.0000273009035\n",
      "[1653,     1] loss: 0.0000313564466\n",
      "[1654,     1] loss: 0.0000303320267\n",
      "[1655,     1] loss: 0.0000244103780\n",
      "[1656,     1] loss: 0.0000199672329\n",
      "[1657,     1] loss: 0.0000266306801\n",
      "[1658,     1] loss: 0.0000337871286\n",
      "[1659,     1] loss: 0.0000264991890\n",
      "[1660,     1] loss: 0.0000340489874\n",
      "[1661,     1] loss: 0.0000292653567\n",
      "[1662,     1] loss: 0.0000287949515\n",
      "[1663,     1] loss: 0.0000269434648\n",
      "[1664,     1] loss: 0.0000281092071\n",
      "[1665,     1] loss: 0.0000232610168\n",
      "[1666,     1] loss: 0.0000287197559\n",
      "[1667,     1] loss: 0.0000285491318\n",
      "[1668,     1] loss: 0.0000303212408\n",
      "[1669,     1] loss: 0.0000259933673\n",
      "[1670,     1] loss: 0.0000285730552\n",
      "[1671,     1] loss: 0.0000253804610\n",
      "[1672,     1] loss: 0.0000275396829\n",
      "[1673,     1] loss: 0.0000263124588\n",
      "[1674,     1] loss: 0.0000320755382\n",
      "[1675,     1] loss: 0.0000291826349\n",
      "[1676,     1] loss: 0.0000294117955\n",
      "[1677,     1] loss: 0.0000285219139\n",
      "[1678,     1] loss: 0.0000240045192\n",
      "[1679,     1] loss: 0.0000255184073\n",
      "[1680,     1] loss: 0.0000260681816\n",
      "[1681,     1] loss: 0.0000242435926\n",
      "[1682,     1] loss: 0.0000275448692\n",
      "[1683,     1] loss: 0.0000226627031\n",
      "[1684,     1] loss: 0.0000292433571\n",
      "[1685,     1] loss: 0.0000273141370\n",
      "[1686,     1] loss: 0.0000242296504\n",
      "[1687,     1] loss: 0.0000331637624\n",
      "[1688,     1] loss: 0.0000265664858\n",
      "[1689,     1] loss: 0.0000261148787\n",
      "[1690,     1] loss: 0.0000292804296\n",
      "[1691,     1] loss: 0.0000278613676\n",
      "[1692,     1] loss: 0.0000314665638\n",
      "[1693,     1] loss: 0.0000288137700\n",
      "[1694,     1] loss: 0.0000246974669\n",
      "[1695,     1] loss: 0.0000237929387\n",
      "[1696,     1] loss: 0.0000349572045\n",
      "[1697,     1] loss: 0.0000232854843\n",
      "[1698,     1] loss: 0.0000252719386\n",
      "[1699,     1] loss: 0.0000225850192\n",
      "[1700,     1] loss: 0.0000288583309\n",
      "[1701,     1] loss: 0.0000258363900\n",
      "[1702,     1] loss: 0.0000315363810\n",
      "[1703,     1] loss: 0.0000260939705\n",
      "[1704,     1] loss: 0.0000265778945\n",
      "[1705,     1] loss: 0.0000305766589\n",
      "[1706,     1] loss: 0.0000242842638\n",
      "[1707,     1] loss: 0.0000251142890\n",
      "[1708,     1] loss: 0.0000338757236\n",
      "[1709,     1] loss: 0.0000223346331\n",
      "[1710,     1] loss: 0.0000232801685\n",
      "[1711,     1] loss: 0.0000274881604\n",
      "[1712,     1] loss: 0.0000292747485\n",
      "[1713,     1] loss: 0.0000289310381\n",
      "[1714,     1] loss: 0.0000256488711\n",
      "[1715,     1] loss: 0.0000240270936\n",
      "[1716,     1] loss: 0.0000238840381\n",
      "[1717,     1] loss: 0.0000334726559\n",
      "[1718,     1] loss: 0.0000276387756\n",
      "[1719,     1] loss: 0.0000225215597\n",
      "[1720,     1] loss: 0.0000265269133\n",
      "[1721,     1] loss: 0.0000223932831\n",
      "[1722,     1] loss: 0.0000241476606\n",
      "[1723,     1] loss: 0.0000340359547\n",
      "[1724,     1] loss: 0.0000289663905\n",
      "[1725,     1] loss: 0.0000226795557\n",
      "[1726,     1] loss: 0.0000247396645\n",
      "[1727,     1] loss: 0.0000225200725\n",
      "[1728,     1] loss: 0.0000214374129\n",
      "[1729,     1] loss: 0.0000279097934\n",
      "[1730,     1] loss: 0.0000239818852\n",
      "[1731,     1] loss: 0.0000261510664\n",
      "[1732,     1] loss: 0.0000263804133\n",
      "[1733,     1] loss: 0.0000281342887\n",
      "[1734,     1] loss: 0.0000324213353\n",
      "[1735,     1] loss: 0.0000288993382\n",
      "[1736,     1] loss: 0.0000254804996\n",
      "[1737,     1] loss: 0.0000303507055\n",
      "[1738,     1] loss: 0.0000306978502\n",
      "[1739,     1] loss: 0.0000260877219\n",
      "[1740,     1] loss: 0.0000347430381\n",
      "[1741,     1] loss: 0.0000267358992\n",
      "[1742,     1] loss: 0.0000283389061\n",
      "[1743,     1] loss: 0.0000263319467\n",
      "[1744,     1] loss: 0.0000295907492\n",
      "[1745,     1] loss: 0.0000275877479\n",
      "[1746,     1] loss: 0.0000260289176\n",
      "[1747,     1] loss: 0.0000303689041\n",
      "[1748,     1] loss: 0.0000247664662\n",
      "[1749,     1] loss: 0.0000302532339\n",
      "[1750,     1] loss: 0.0000220624861\n",
      "[1751,     1] loss: 0.0000291268370\n",
      "[1752,     1] loss: 0.0000221459981\n",
      "[1753,     1] loss: 0.0000270554039\n",
      "[1754,     1] loss: 0.0000198234004\n",
      "[1755,     1] loss: 0.0000284382113\n",
      "[1756,     1] loss: 0.0000265939074\n",
      "[1757,     1] loss: 0.0000254524668\n",
      "[1758,     1] loss: 0.0000302401721\n",
      "[1759,     1] loss: 0.0000217197317\n",
      "[1760,     1] loss: 0.0000303924840\n",
      "[1761,     1] loss: 0.0000280529959\n",
      "[1762,     1] loss: 0.0000315553189\n",
      "[1763,     1] loss: 0.0000317906844\n",
      "[1764,     1] loss: 0.0000259895343\n",
      "[1765,     1] loss: 0.0000277493935\n",
      "[1766,     1] loss: 0.0000230955673\n",
      "[1767,     1] loss: 0.0000271484227\n",
      "[1768,     1] loss: 0.0000238424356\n",
      "[1769,     1] loss: 0.0000253261009\n",
      "[1770,     1] loss: 0.0000298818428\n",
      "[1771,     1] loss: 0.0000254522689\n",
      "[1772,     1] loss: 0.0000246946060\n",
      "[1773,     1] loss: 0.0000350857445\n",
      "[1774,     1] loss: 0.0000324109657\n",
      "[1775,     1] loss: 0.0000291497039\n",
      "[1776,     1] loss: 0.0000198085603\n",
      "[1777,     1] loss: 0.0000315329584\n",
      "[1778,     1] loss: 0.0000284373760\n",
      "[1779,     1] loss: 0.0000310230535\n",
      "[1780,     1] loss: 0.0000331394083\n",
      "[1781,     1] loss: 0.0000300811342\n",
      "[1782,     1] loss: 0.0000333035190\n",
      "[1783,     1] loss: 0.0000275228405\n",
      "[1784,     1] loss: 0.0000253640261\n",
      "[1785,     1] loss: 0.0000252133934\n",
      "[1786,     1] loss: 0.0000305397436\n",
      "[1787,     1] loss: 0.0000272260193\n",
      "[1788,     1] loss: 0.0000256852451\n",
      "[1789,     1] loss: 0.0000273538230\n",
      "[1790,     1] loss: 0.0000372130307\n",
      "[1791,     1] loss: 0.0000281468005\n",
      "[1792,     1] loss: 0.0000338621408\n",
      "[1793,     1] loss: 0.0000272604433\n",
      "[1794,     1] loss: 0.0000293929712\n",
      "[1795,     1] loss: 0.0000351753930\n",
      "[1796,     1] loss: 0.0000282106281\n",
      "[1797,     1] loss: 0.0000287447678\n",
      "[1798,     1] loss: 0.0000341660576\n",
      "[1799,     1] loss: 0.0000260452187\n",
      "[1800,     1] loss: 0.0000306609611\n",
      "[1801,     1] loss: 0.0000338032405\n",
      "[1802,     1] loss: 0.0000241395930\n",
      "[1803,     1] loss: 0.0000261224777\n",
      "[1804,     1] loss: 0.0000319065119\n",
      "[1805,     1] loss: 0.0000281864050\n",
      "[1806,     1] loss: 0.0000341189443\n",
      "[1807,     1] loss: 0.0000223472845\n",
      "[1808,     1] loss: 0.0000364060077\n",
      "[1809,     1] loss: 0.0000276246283\n",
      "[1810,     1] loss: 0.0000203107862\n",
      "[1811,     1] loss: 0.0000331260730\n",
      "[1812,     1] loss: 0.0000309594208\n",
      "[1813,     1] loss: 0.0000270004792\n",
      "[1814,     1] loss: 0.0000255355262\n",
      "[1815,     1] loss: 0.0000359847239\n",
      "[1816,     1] loss: 0.0000340388564\n",
      "[1817,     1] loss: 0.0000316270482\n",
      "[1818,     1] loss: 0.0000235358573\n",
      "[1819,     1] loss: 0.0000249515433\n",
      "[1820,     1] loss: 0.0000262783229\n",
      "[1821,     1] loss: 0.0000261809328\n",
      "[1822,     1] loss: 0.0000315108220\n",
      "[1823,     1] loss: 0.0000247673306\n",
      "[1824,     1] loss: 0.0000247729797\n",
      "[1825,     1] loss: 0.0000285015150\n",
      "[1826,     1] loss: 0.0000302325352\n",
      "[1827,     1] loss: 0.0000273790269\n",
      "[1828,     1] loss: 0.0000243028480\n",
      "[1829,     1] loss: 0.0000179538052\n",
      "[1830,     1] loss: 0.0000276028411\n",
      "[1831,     1] loss: 0.0000288540003\n",
      "[1832,     1] loss: 0.0000344074302\n",
      "[1833,     1] loss: 0.0000218094079\n",
      "[1834,     1] loss: 0.0000334901822\n",
      "[1835,     1] loss: 0.0000280431821\n",
      "[1836,     1] loss: 0.0000306363392\n",
      "[1837,     1] loss: 0.0000273390906\n",
      "[1838,     1] loss: 0.0000274957216\n",
      "[1839,     1] loss: 0.0000263857219\n",
      "[1840,     1] loss: 0.0000274525490\n",
      "[1841,     1] loss: 0.0000282788329\n",
      "[1842,     1] loss: 0.0000248774566\n",
      "[1843,     1] loss: 0.0000278665510\n",
      "[1844,     1] loss: 0.0000232286038\n",
      "[1845,     1] loss: 0.0000299766223\n",
      "[1846,     1] loss: 0.0000244633091\n",
      "[1847,     1] loss: 0.0000309009629\n",
      "[1848,     1] loss: 0.0000217762296\n",
      "[1849,     1] loss: 0.0000338424521\n",
      "[1850,     1] loss: 0.0000304703368\n",
      "[1851,     1] loss: 0.0000254351849\n",
      "[1852,     1] loss: 0.0000181135008\n",
      "[1853,     1] loss: 0.0000282979250\n",
      "[1854,     1] loss: 0.0000292805984\n",
      "[1855,     1] loss: 0.0000273704034\n",
      "[1856,     1] loss: 0.0000317948638\n",
      "[1857,     1] loss: 0.0000272839563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1858,     1] loss: 0.0000270695280\n",
      "[1859,     1] loss: 0.0000329653994\n",
      "[1860,     1] loss: 0.0000276405510\n",
      "[1861,     1] loss: 0.0000261299399\n",
      "[1862,     1] loss: 0.0000300020416\n",
      "[1863,     1] loss: 0.0000243683375\n",
      "[1864,     1] loss: 0.0000274455640\n",
      "[1865,     1] loss: 0.0000267603784\n",
      "[1866,     1] loss: 0.0000335256918\n",
      "[1867,     1] loss: 0.0000296915736\n",
      "[1868,     1] loss: 0.0000234708452\n",
      "[1869,     1] loss: 0.0000237988468\n",
      "[1870,     1] loss: 0.0000287597009\n",
      "[1871,     1] loss: 0.0000308956834\n",
      "[1872,     1] loss: 0.0000291899487\n",
      "[1873,     1] loss: 0.0000268301461\n",
      "[1874,     1] loss: 0.0000245731062\n",
      "[1875,     1] loss: 0.0000295925915\n",
      "[1876,     1] loss: 0.0000228449411\n",
      "[1877,     1] loss: 0.0000296447310\n",
      "[1878,     1] loss: 0.0000222300718\n",
      "[1879,     1] loss: 0.0000217540364\n",
      "[1880,     1] loss: 0.0000226616263\n",
      "[1881,     1] loss: 0.0000321697036\n",
      "[1882,     1] loss: 0.0000228366640\n",
      "[1883,     1] loss: 0.0000275737839\n",
      "[1884,     1] loss: 0.0000277024024\n",
      "[1885,     1] loss: 0.0000274532678\n",
      "[1886,     1] loss: 0.0000293909397\n",
      "[1887,     1] loss: 0.0000299124455\n",
      "[1888,     1] loss: 0.0000259623834\n",
      "[1889,     1] loss: 0.0000271281431\n",
      "[1890,     1] loss: 0.0000213502906\n",
      "[1891,     1] loss: 0.0000274191785\n",
      "[1892,     1] loss: 0.0000201962495\n",
      "[1893,     1] loss: 0.0000305873837\n",
      "[1894,     1] loss: 0.0000283890025\n",
      "[1895,     1] loss: 0.0000281314889\n",
      "[1896,     1] loss: 0.0000327433081\n",
      "[1897,     1] loss: 0.0000311631738\n",
      "[1898,     1] loss: 0.0000334227749\n",
      "[1899,     1] loss: 0.0000231091704\n",
      "[1900,     1] loss: 0.0000248856755\n",
      "[1901,     1] loss: 0.0000234239415\n",
      "[1902,     1] loss: 0.0000224025644\n",
      "[1903,     1] loss: 0.0000269143522\n",
      "[1904,     1] loss: 0.0000271091616\n",
      "[1905,     1] loss: 0.0000280845561\n",
      "[1906,     1] loss: 0.0000242615497\n",
      "[1907,     1] loss: 0.0000325769826\n",
      "[1908,     1] loss: 0.0000280383189\n",
      "[1909,     1] loss: 0.0000243778806\n",
      "[1910,     1] loss: 0.0000294080179\n",
      "[1911,     1] loss: 0.0000231318671\n",
      "[1912,     1] loss: 0.0000258655811\n",
      "[1913,     1] loss: 0.0000262275920\n",
      "[1914,     1] loss: 0.0000247284304\n",
      "[1915,     1] loss: 0.0000259231048\n",
      "[1916,     1] loss: 0.0000321640633\n",
      "[1917,     1] loss: 0.0000309865805\n",
      "[1918,     1] loss: 0.0000248148222\n",
      "[1919,     1] loss: 0.0000265424547\n",
      "[1920,     1] loss: 0.0000279136992\n",
      "[1921,     1] loss: 0.0000312445482\n",
      "[1922,     1] loss: 0.0000296772108\n",
      "[1923,     1] loss: 0.0000302197936\n",
      "[1924,     1] loss: 0.0000243071830\n",
      "[1925,     1] loss: 0.0000182199845\n",
      "[1926,     1] loss: 0.0000356610981\n",
      "[1927,     1] loss: 0.0000244965893\n",
      "[1928,     1] loss: 0.0000273540412\n",
      "[1929,     1] loss: 0.0000298166968\n",
      "[1930,     1] loss: 0.0000243821938\n",
      "[1931,     1] loss: 0.0000272522127\n",
      "[1932,     1] loss: 0.0000312058517\n",
      "[1933,     1] loss: 0.0000262347952\n",
      "[1934,     1] loss: 0.0000224376519\n",
      "[1935,     1] loss: 0.0000278343941\n",
      "[1936,     1] loss: 0.0000314265548\n",
      "[1937,     1] loss: 0.0000274950726\n",
      "[1938,     1] loss: 0.0000305052585\n",
      "[1939,     1] loss: 0.0000298747298\n",
      "[1940,     1] loss: 0.0000295643404\n",
      "[1941,     1] loss: 0.0000286484661\n",
      "[1942,     1] loss: 0.0000312763324\n",
      "[1943,     1] loss: 0.0000252528640\n",
      "[1944,     1] loss: 0.0000314826524\n",
      "[1945,     1] loss: 0.0000311748561\n",
      "[1946,     1] loss: 0.0000305113994\n",
      "[1947,     1] loss: 0.0000292576558\n",
      "[1948,     1] loss: 0.0000316304126\n",
      "[1949,     1] loss: 0.0000310858188\n",
      "[1950,     1] loss: 0.0000259201130\n",
      "[1951,     1] loss: 0.0000335885910\n",
      "[1952,     1] loss: 0.0000267911761\n",
      "[1953,     1] loss: 0.0000288644340\n",
      "[1954,     1] loss: 0.0000292722514\n",
      "[1955,     1] loss: 0.0000291705044\n",
      "[1956,     1] loss: 0.0000295659731\n",
      "[1957,     1] loss: 0.0000277512212\n",
      "[1958,     1] loss: 0.0000278611114\n",
      "[1959,     1] loss: 0.0000245125353\n",
      "[1960,     1] loss: 0.0000261554524\n",
      "[1961,     1] loss: 0.0000311433687\n",
      "[1962,     1] loss: 0.0000293373334\n",
      "[1963,     1] loss: 0.0000221530863\n",
      "[1964,     1] loss: 0.0000234035702\n",
      "[1965,     1] loss: 0.0000263425114\n",
      "[1966,     1] loss: 0.0000235164887\n",
      "[1967,     1] loss: 0.0000283501751\n",
      "[1968,     1] loss: 0.0000286052935\n",
      "[1969,     1] loss: 0.0000313816359\n",
      "[1970,     1] loss: 0.0000228090255\n",
      "[1971,     1] loss: 0.0000280754437\n",
      "[1972,     1] loss: 0.0000323766086\n",
      "[1973,     1] loss: 0.0000256180443\n",
      "[1974,     1] loss: 0.0000266947434\n",
      "[1975,     1] loss: 0.0000227145487\n",
      "[1976,     1] loss: 0.0000247605669\n",
      "[1977,     1] loss: 0.0000293415214\n",
      "[1978,     1] loss: 0.0000264770089\n",
      "[1979,     1] loss: 0.0000355203229\n",
      "[1980,     1] loss: 0.0000262365502\n",
      "[1981,     1] loss: 0.0000290564436\n",
      "[1982,     1] loss: 0.0000221619353\n",
      "[1983,     1] loss: 0.0000232914623\n",
      "[1984,     1] loss: 0.0000307557260\n",
      "[1985,     1] loss: 0.0000281706598\n",
      "[1986,     1] loss: 0.0000304993649\n",
      "[1987,     1] loss: 0.0000260247267\n",
      "[1988,     1] loss: 0.0000219782072\n",
      "[1989,     1] loss: 0.0000336543249\n",
      "[1990,     1] loss: 0.0000256604748\n",
      "[1991,     1] loss: 0.0000299611624\n",
      "[1992,     1] loss: 0.0000234976702\n",
      "[1993,     1] loss: 0.0000257005915\n",
      "[1994,     1] loss: 0.0000301432243\n",
      "[1995,     1] loss: 0.0000198712791\n",
      "[1996,     1] loss: 0.0000244894880\n",
      "[1997,     1] loss: 0.0000256247498\n",
      "[1998,     1] loss: 0.0000273833051\n",
      "[1999,     1] loss: 0.0000267032417\n",
      "[2000,     1] loss: 0.0000218753834\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model_xyz.parameters(), lr=0.0005) #0.0001)\n",
    "#torch.optim.LBFGS(model_xyz.parameters(), lr=0.001, max_iter=20, max_eval=None, tolerance_grad=1e-07, tolerance_change=1e-09, line_search_fn=None)\n",
    "#optimizer = optim.SGD(model_xyz.parameters(), lr=0.001, momentum=0.1)\n",
    "nepochs = 2000\n",
    "\n",
    "train_loss = np.zeros(nepochs)\n",
    "test_loss = np.zeros(nepochs)\n",
    "\n",
    "train_acc = np.zeros(nepochs)\n",
    "test_acc = np.zeros(nepochs)\n",
    "\n",
    "#===========================================================================\n",
    "for epoch in range(nepochs):          # loop over the dataset multiple times\n",
    "#===========================================================================\n",
    "    \n",
    "    running_loss = 0.0                #  Initialise losses at the start of each epoch \n",
    "    epoch_train_loss = 0.0             \n",
    "    epoch_test_loss = 0.0\n",
    "    \n",
    "    \n",
    "    counter = 0                               # ranges from 0 to the number of elements in each batch \n",
    "                                              # eg if we have 900 train. ex. and 25 batches, there will\n",
    "                                              # be 36 elements in each batch.\n",
    "    #---------------------------------------\n",
    "    for i, data in enumerate(dataloader_xyz, 0):  # scan the whole dataset in each epoch, batch by batch (i ranges over batches)\n",
    "    #---------------------------------------\n",
    "        inputs, labels = data                 # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()                 # Calling .backward() mutiple times accumulates the gradient (by addition) \n",
    "                                              # for each parameter. This is why you should call optimizer.zero_grad() \n",
    "                                              # after each .step() call. \n",
    "\n",
    "        # forward + backward + optimize\n",
    "        \n",
    "        \n",
    "        outputs = torch.zeros(np.shape(inputs)[0])\n",
    "        for j in range(np.shape(inputs)[0]):\n",
    "            outputs[j] = model_xyz(inputs[j][0],inputs[j][1])  # The net is designed to take a (3 x num_features)\n",
    "            # tensor as an input, so when we are doing batch gd, we do a loop over all elements of the batch to create an\n",
    "            # output vector with as many elements as the batch size. If our net was designed to take one row as input we \n",
    "            # wouldn't have needed the for loop, we could have had a vectorised implementation, i.e. outputs = model_xyz(inputs)\n",
    "            \n",
    "            \n",
    "            \n",
    "        loss = criterion(outputs, labels) # a single value, same as loss.item(), which is the mean loss for each mini-batch\n",
    "        loss.backward()                   # performs one back-propagation step \n",
    "        optimizer.step()                  # update the network parameters (perform an update step)\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()        # loss.item() contains loss of entire mini-batch, divided by the batch size, i.e. mean\n",
    "                                           # we accumulate this loss over as many mini-batches as we like until we set it to zero after printing it\n",
    "        epoch_train_loss += loss.item()    # cumulative loss for each epoch (sum of mean loss for all mini-batches)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        net_test_set = torch.zeros(np.shape(test_set_xyz)[0]) # outputs(predictions) of network if we input the test set\n",
    "        with torch.no_grad():                             # The wrapper with torch.no_grad() temporarily sets all of \n",
    "                                                          # the requires_grad flags to false, i.e. makes all the \n",
    "                                                          # operations in the block have no gradients\n",
    "            for k in range(np.shape(test_set_xyz)[0]):\n",
    "                net_test_set[k] = model_xyz(test_set_xyz[k][0],test_set_xyz[k][1])\n",
    "            epoch_test_loss += criterion(net_test_set, test_labels).item() # sum test mean batch losses throughout epoch  \n",
    "#        print(i % 10)\n",
    "        if i % 10 == 0:    # print average loss every 10 mini-batches\n",
    "            print('[%d, %5d] loss: %.13f' %\n",
    "                  (epoch + 1, i + 1, running_loss/10))\n",
    "            running_loss = 0.0\n",
    "        counter += 1\n",
    "        #------------------------------------       \n",
    "    # Now we have added up the loss (both for training and test set) over all mini batches     \n",
    "    train_loss[epoch] = epoch_train_loss/counter   # divide by number or training examples in one batch \n",
    "                                                   # to obtain average training loss for each epoch\n",
    "    test_loss[epoch] = epoch_test_loss/counter\n",
    "    epoch_train_loss = 0.0\n",
    "    epoch_test_loss = 0.0\n",
    "\n",
    "#=================================================================================\n",
    "    \n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(1,nepochs+1)\n",
    "# plt.plot(x[:80],train_loss[:80],'blue',label = 'Training loss')\n",
    "# plt.plot(x[:80],test_loss[:80],'red',label = 'Test loss')\n",
    "\n",
    "plt.plot(x[:epoch],train_loss[:epoch],'blue',label = 'Training loss')\n",
    "plt.plot(x[:epoch],test_loss[:epoch],'red',label = 'Test loss')\n",
    "\n",
    "\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "plt.xlabel('Epoch',fontsize=14)\n",
    "plt.ylabel('Loss function',fontsize=14)\n",
    "plt.title('Average loss function per epoch for $H_2$ training and test set',fontsize=14)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "#plt.ylim([0,0.001])\n",
    "\n",
    "plt.savefig('loss_graph_H2_xyz',bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-657.1368671  -644.6609995  -634.55122817 -653.98674694 -653.78487151\n",
      " -657.2706512  -631.19449186 -634.50644535 -651.99609832 -657.02729452\n",
      " -621.5695997  -634.52244526 -656.92892469 -653.88343886 -636.24154688\n",
      " -657.54442746 -654.71300747 -654.25408937 -654.03361793 -646.56908067\n",
      " -633.63543782 -656.26604299 -653.41458784 -651.10436427 -635.49592842\n",
      " -655.30855974 -656.11829072 -631.59917393 -654.30127077 -634.1361899\n",
      " -653.50861201 -655.95815051 -652.09731822 -636.10632364 -655.49962747\n",
      " -655.46878461 -653.24366815 -656.42536486 -656.09055189 -654.66557211\n",
      " -638.98187743 -652.10798482 -652.2753772  -637.18288913 -655.78672288\n",
      " -655.43971951 -654.6966125  -654.22132765 -645.68578396 -654.23625526]\n",
      "(450,)\n",
      "-644.7451\n"
     ]
    }
   ],
   "source": [
    "prediction = np.zeros(50)\n",
    "for i in range(50):\n",
    "    x1,x2 = test_set_xyz[i]\n",
    "    prediction[i] = model_xyz(x1, x2)#[0]\n",
    "\n",
    "print(prediction*var_lab+mean_lab)\n",
    "\n",
    "\n",
    "print(np.shape(train_labels))\n",
    "train_labels = np.array(train_labels)\n",
    "print(np.mean(train_labels*var_lab+mean_lab,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-0.0226, dtype=torch.float64)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "prediction = torch.tensor(prediction)\n",
    "\n",
    "x = np.linspace(min(test_labels*var_lab+mean_lab), max(test_labels*var_lab+mean_lab))\n",
    "print(min(torch.cat((test_labels,prediction),0)))\n",
    "y = x\n",
    "plt.plot(test_labels*var_lab+mean_lab,prediction*var_lab+mean_lab, 'o', color='blue', label = 'Test set')\n",
    "plt.plot(x,y, color='red',label = 'y=x')\n",
    "plt.grid()\n",
    "#plt.xlim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction),0))])\n",
    "#plt.ylim([min(torch.cat((test_labels,prediction),0)), max(torch.cat((test_labels,prediction)))])\n",
    "#plt.ylim([-13822,-13800])\n",
    "#plt.ticklabel_format(axis=\"y\", style=\"plain\")\n",
    "plt.ticklabel_format(useOffset=False, style='plain')\n",
    "#plt.tick_params(axis='both',labelsize=14)\n",
    "plt.xlabel('Actual $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.ylabel('Predicted $H_2$ energy value (kcal/mol)',fontsize=14)\n",
    "plt.title('Actual vs Predicted energy value for $H_2$ molecules',fontsize=15)\n",
    "plt.legend()\n",
    "plt.savefig('predicted_energies_H2_xyz',bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
